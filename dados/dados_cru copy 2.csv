topico,tema,enunciado,resposta_certa,explicacao,id
tópico 2,Deduplicação,"Questão:
A tecnologia de deduplicação é amplamente utilizada no armazenamento de dados para otimizar o espaço, reduzindo a quantidade de dados redundantes armazenados. Qual dos seguintes afirma corretamente uma característica ou consequência do processo de deduplicação em sistemas de armazenamento de dados?

A) A deduplicação não afeta o desempenho do sistema de backup, pois apenas ocorre durante períodos de baixa atividade.

B) A deduplicação pode ser feita tanto no nível do arquivo (arquivo completo) quanto no nível do bloco (pequenas partes do arquivo), sendo a última geralmente mais eficiente na redução da redundância.

C) A deduplicação é um processo puramente de software e não pode ser implementada no hardware ou em dispositivos de armazenamento dedicados.

D) A deduplicação conduz necessariamente a uma melhoria na segurança dos dados, pois há menos cópias dos arquivos para proteger.

E) O processo de deduplicação aumenta significativamente a quantidade de metadados necessários para gerenciar o armazenamento de dados, uma vez que cada arquivo é armazenado como uma entidade única e indivisível.

",B,B,0
tópico 5,"Processamento de linguagem natural: Normalização textual - stop words, estemização, lematização e análise de frequência de termos; Rotulação de partes do discurso, part-of-speech tagging; Modelos de representação de texto - N-gramas, modelos vetoriais de palavras (CBOW, Skip-Gram e GloVe), modelos vetoriais de documentos (booleano, TF e TF-IDF, média de vetores de palavras e Paragraph Vector); Métricas de similaridade textual - similaridade do cosseno, distância euclidiana, similaridade de Jaccard, distância de Manhattan e coeficiente de Dice","Questão:
A análise de textos por meio de técnicas de Processamento de Linguagem Natural (PLN) evoluiu significativamente com a introdução de diferentes métodos de pré-processamento de dados, representação textual e cálculo de similaridade. Nesse contexto, julgue os itens a seguir como verdadeiros (V) ou falsos (F):

I. A remoção de stop words, a estemização e a lematização são procedimentos de normalização textual que visam reduzir a variabilidade de formas das palavras a um formato mais genérico para simplificar a análise semântica do texto.

II. Na rotulação de partes do discurso (part-of-speech tagging), cada palavra de um texto é marcada com um rótulo que indica sua função gramatical sem levar em consideração o contexto em que está inserida.

III. No modelo vetorial booleano, a frequência com que um termo ocorre em um documento é irrelevante, considerando-se apenas a presença ou ausência do termo.

IV. A similaridade do cosseno é uma métrica de similaridade textual que pode ser usada para calcular a distância angular entre dois vetores de palavras em um espaço multidimensional, sendo insensível à magnitude dos vetores.

V. O coeficiente de Dice é uma métrica de associação que pode ser adaptada para avaliar a similaridade textual considerando a interseção de palavras-chave entre os documentos em relação ao total de palavras-chave nos dois documentos.

Estão corretos apenas os itens:

a) I, II e IV.
b) I, III e V.
c) II, IV e V.
d) I, III e IV.
e) III, IV e V.

",d,"

Explicação dos itens:

I. Verdadeiro. A remoção de stop words (palavras comuns com pouco valor semântico), a estemização (processo de reduzir palavras às suas raízes ou radicais) e a lematização (redução de palavras às suas formas lexicais básicas) são técnicas de normalização textual que ajudam a diminuir a complexidade do texto para a análise.

II. Falso. A rotulação de partes do discurso ou part-of-speech tagging efetivamente associa etiquetas às palavras em um texto, como substantivo, verbo, adjetivo, etc. No entanto, a assertiva está errada ao indicar que o processo não leva em consideração o contexto; na verdade, o contexto é fundamental para um POS tagging correto.

III. Verdadeiro. O modelo vetorial booleano é uma representação binária em que um termo está presente (1) ou ausente (0) em um documento. A frequência com que ele aparece não é considerada.

IV. Verdadeiro. A similaridade do cosseno mede a similaridade cos angular entre dois vetores, independente da sua magnitude, o que é particularmente útil em PLN quando se deseja ignorar o comprimento do documento e focar na direção dos vetores de características.

V. Falso. Embora o coeficiente de Dice seja uma métrica de associação e possa ser usado para calcular a similaridade, ele não busca avaliar a interseção de palavras-chave em relação ao total, mas sim em relação à soma do número de palavras-chave em ambos os documentos. Isso é levemente diferente do que é indicado no item.",1
tópico 3,Programação funcional,"Questão:

Na programação funcional, uma função é considerada pura quando segue certos princípios. Assinale a opção que caracteriza corretamente uma função pura.

A) A função deve ter pelo menos um efeito colateral para interagir com outras funções e módulos no sistema.
B) O valor de retorno da função deve ser calculado usando apenas os argumentos fornecidos, sem utilizar dados externos ou estados globais.
C) A função deve alterar seu argumento de entrada para que o estado do sistema seja atualizado corretamente.
D) Cada vez que a função é chamada, ela retorna um valor diferente, dependendo do estado global do sistema no momento da chamada.
E) A função se comunica com uma fonte externa, como realizar uma operação de entrada/saída, a cada chamada.

",B,"

Explicação:

A) Incorreta. Funções puras devem evitar efeitos colaterais. Efeitos colaterais incluem, mas não estão limitados a, modificações de variáveis globais, operações de I/O e atualização de estados em referências passadas como argumento.

B) Correta. Este item capta perfeitamente a essência de funções puras: um valor de retorno que é exclusivamente determinado pelos seus valores de entrada e não tem impacto sobre o estado global ou local.

C) Incorreta. Funções puras não devem alterar seus argumentos de entrada ou qualquer estado externo, o conceito é conhecido como imutabilidade e é um dos princípios da programação funcional.

D) Incorreta. Uma função pura deve ser referencialmente transparente, o que significa que pode ser substituída pelo seu valor de saída sem alterar o comportamento do programa. Se retornar valores diferentes para as mesmas entradas, então depende de um estado externo e não é pura.

E) Incorreta. Conexão com uma fonte externa ou operações de entrada/saída são exemplos de efeitos colaterais, o que vai contra o princípio de uma função pura.
",2
tópico 1,"Banco de dados relacional: SQL Server, PostgreSQL, MySQL","Questão:
Um administrador de banco de dados precisa melhorar o desempenho das consultas em um sistema que utiliza o MySQL. Ele observa que muitas das consultas críticas executam varreduras completas em tabelas grandes, o que é ineficiente. Ele decide implementar índices para acelerar estas consultas.

Considerando o cenário descrito, qual das seguintes estratégias é a mais eficiente para a criação de índices no MySQL, visando a otimização de consultas em tabelas com grandes volumes de dados?

A) Criar índices para todas as colunas das tabelas, pois isso garantirá que todas as consultas sejam rápidas.

B) Utilizar o comando EXPLAIN em consultas representativas para identificar quais colunas se beneficiariam mais da indexação.

C) Eliminar completamente os índices das tabelas grandes, pois índices introduzem overhead de manutenção e podem desacelerar as inserções e atualizações.

D) Criar um índice único e composto que inclua todas as colunas usadas nas consultas, minimizando o número de índices a serem mantidos.

E) Priorizar a criação de índices em colunas que possuem chave estrangeira, independentemente do seu uso em consultas frequentes.

",B,"

Explicação dos itens:

A) Criar índices para todas as colunas das tabelas pode ser contraprodutivo, pois isso adiciona um overhead significativo de manutenção dos índices durante as operações de inserção, atualização e exclusão, sem necessariamente melhorar o desempenho das consultas.

B) Utilizar o comando EXPLAIN é uma prática recomendada para entender como as consultas são executadas e quais partes delas podem ser otimizadas com índices. Ao identificar as colunas mais utilizadas nas operações de busca, é possível criar índices mais eficientes e direcionados.

C) Eliminar índices pode piorar o desempenho de leitura, já que os índices são fundamentais para acelerar as buscas em tabelas grandes. Eles devem ser usados com sabedoria, mas não eliminados por completo.

D) Um índice único e composto pode ser útil em certas situações, mas ele só será eficiente se as consultas utilizarem todas as colunas do índice ou uma sequência prefixada dele. Ele nem sempre é a melhor opção, especialmente se as consultas variarem muito nas colunas que utilizam.

E) Priorizar a criação de índices em colunas com chave estrangeira é importante para operações de JOIN, mas se essas colunas não são frequentemente consultadas, elas podem não ser as melhores candidatas para indexação em termos de otimização de consultas. A análise deve ser baseada no uso efetivo dessas colunas em operações críticas.",739371
tópico 1,"Banco de dados relacional: SQL Server, PostgreSQL, MySQL","Questão: Considerando os sistemas de gerenciamento de banco de dados relacionais (SGBDR) SQL Server, PostgreSQL e MySQL, qual das seguintes opções representa um recurso ou funcionalidade que é exclusivo ao SQL Server em comparação com os outros dois SGBDRs?

A) Capacidade de realizar transações ACID (Atomicidade, Consistência, Isolamento, Durabilidade).
B) Utilização de stored procedures para encapsular a lógica de negócio.
C) Disponibilidade de um sistema de replicação para distribuir dados entre diferentes bases de dados.
D) Uso de índices de texto completo para melhorar a performance de consultas baseadas em texto.
E) Suporte nativo para a linguagem de modelagem de dados XML e a capacidade de armazenar XML como um tipo de dado.

",E,"

Explicação dos itens:

A) Incorrecto. Transações ACID são um princípio fundamental de qualquer SGBD relacional e estão presentes tanto no SQL Server quanto no PostgreSQL e MySQL.

B) Incorrecto. Stored procedures são uma funcionalidade comum entre os SGBDRs e estão presentes em SQL Server, PostgreSQL e MySQL, permitindo a encapsulação da lógica de negócio diretamente no banco de dados.

C) Incorrecto. Sistemas de replicação são oferecidos por quase todos os grandes SGBDRs modernos, incluindo o SQL Server, PostgreSQL e MySQL, facilitando a distribuição e sincronização de dados entre diferentes servidores ou locais.

D) Incorrecto. O suporte para índices de texto completo é uma característica presente em muitos SGBDRs, inclusive em SQL Server, PostgreSQL e MySQL, para otimizar a performance de buscas de texto.

E) Correto. O suporte nativo para XML, incluindo a habilidade de armazenar, consultar e manipular dados XML diretamente dentro do banco de dados usando tipos de dados específicos, é uma característica distintiva do SQL Server em relação ao PostgreSQL e MySQL. Enquanto esses últimos SGBDRs oferecem suporte a XML de várias maneiras, o SQL Server tem uma integração mais profunda e robusta com XML, incluindo a capacidade de indexação e consultas eficientes usando XQuery.",3760240
tópico 1,"Banco de dados e formatos de arquivo orientado a colunas: Parquet, MonetDB, duckDB","Questão:

Quando lidamos com grandes conjuntos de dados, a escolha do formato de armazenamento e do sistema gerenciador de banco de dados pode ter um impacto significativo na eficiência das operações de leitura e escrita, bem como na otimização de consultas analíticas. Dentre os formatos de arquivo e gerenciadores de banco de dados orientados a colunas, temos o Parquet, MonetDB e duckDB, que são projetados para otimizar operações analíticas em grandes volumes de dados. Considerando estas tecnologias, assinale a opção que apresenta uma afirmação INCORRETA.

A) Parquet é um formato de arquivo colunar de código aberto que suporta compressão de dados e esquemas evolutivos, amplamente utilizado em ecossistemas de Big Data como Hadoop e Spark.

B) MonetDB é um banco de dados relacional orientado a colunas, que se destaca pela sua capacidade de executar operações de processamento analítico online (OLAP) de forma eficiente, devido à sua arquitetura inovadora de gerenciamento de memória.

C) duckDB é um gerenciador de banco de dados relacional, que oferece suporte para operações OLTP (Online Transaction Processing), privilegiando a eficiência das transações em detrimento de consultas analíticas complexas.

D) O formato Parquet e os sistemas MonetDB e duckDB são capazes de realizar operações de pushdown de predicados, o que permite que filtros e agregações sejam aplicados diretamente nos dados armazenados, reduzindo a quantidade de dados movimentados durante consultas.

E) A utilização de índices de bitmap é um recurso comum em sistemas de banco de dados orientados a colunas como o MonetDB, o qual permite consultas de alta performance em colunas com pouca cardinalidade.

",C,"

Explicação dos itens:

A) Correta. O Parquet é de fato um formato de arquivo colunar que oferece compressão eficiente e suporta esquemas que podem evoluir ao longo do tempo, sendo amplamente adotado em ambientes como Hadoop e Spark para armazenamento de dados de maneira eficiente.

B) Correta. MonetDB é um sistema gerenciador de banco de dados relacional e orientado a colunas que se destaca na execução de consultas OLAP. A sua arquitetura é voltada para um alto desempenho em operações analíticas, principalmente devido ao seu gerenciamento de memória e execução de consultas em vetor.

C) Incorreta. DuckDB é um banco de dados analítico que também é orientado a colunas e projetado para OLAP (processamento analítico online) e não OLTP. Ao contrário do que a afirmação sugere, ele é otimizado para consultas analíticas complexas e não para eficiência de transações usuais em OLTP.

D) Correta. O pushdown de predicados é uma característica importante de sistemas orientados a colunas, e tanto o formato de arquivo Parquet, quanto os sistemas MonetDB e duckDB oferecem suporte a essa funcionalidade, otimizando a execução de consultas ao reduzir a necessidade de transferência de grandes volumes de dados.

E) Correta. O uso de índices de bitmap é de fato um recurso comum em sistemas de banco de dados orientados a colunas. Eles são especialmente úteis em colunas com pouca variação de valores (baixa cardinalidade), pois permitem um rápido filtro e são eficientes em termos de espaço. MonetDB é um exemplo de sistema que pode utilizar índices de bitmap para melhorar o desempenho das consultas.",8435471
tópico 4,Medidas de tendência central e dispersão e correlação,"Questão:
A análise estatística de um conjunto de dados pode fornecer informações valiosas sobre o comportamento e as características desse conjunto. Com relação às medidas de tendência central e dispersão, assim como a correlação entre variáveis, julgue os itens a seguir e assinale a opção correta.

I. A média aritmética sempre será uma medida de tendência central adequada, independentemente da presença de valores extremos (outliers).
II. A mediana é menos sensível a outliers do que a média aritmética, pois depende unicamente da posição central dos dados, e não dos seus valores.
III. O desvio padrão é uma medida de dispersão que considera o quão espalhados estão os valores em relação à média, sendo mais influenciado por valores extremos do que a variância.
IV. O coeficiente de correlação de Pearson é um indicador de relação linear entre duas variáveis, assumindo valores entre -1 e +1, onde 0 indica a ausência de correlação linear.

A) Apenas os itens II e IV estão corretos.
B) Apenas os itens I e III estão corretos.
C) Apenas os itens II e III estão corretos.
D) Todos os itens estão corretos.
E) Apenas os itens I, II, e IV estão corretos.

",A,"

Explicação dos itens:

I. Incorreto. A média aritmética pode ser fortemente influenciada por valores extremos (outliers). Em tais casos, pode não representar adequadamente a tendência central dos dados.

II. Correto. A mediana é uma medida de tendência central que é menos sensível a outliers, proporcionando uma melhor representação do ponto central dos dados em distribuições assimétricas.

III. Incorreto. O desvio padrão é uma medida de dispersão que é afetada por valores extremos, porém é a variância que leva ao quadrado as diferenças em relação à média. O desvio padrão é a raiz quadrada da variância e ambos são igualmente influenciados por outliers, contrariando o enunciado do item.

IV. Correto. O coeficiente de correlação de Pearson mede a força e a direção da relação linear entre duas variáveis, onde -1 representa uma correlação negativa perfeita, +1 representa uma correlação positiva perfeita e 0 indica que não há correlação linear.",683166
tópico 5,"Processamento de linguagem natural: Normalização textual - stop words, estemização, lematização e análise de frequência de termos; Rotulação de partes do discurso, part-of-speech tagging; Modelos de representação de texto - N-gramas, modelos vetoriais de palavras (CBOW, Skip-Gram e GloVe), modelos vetoriais de documentos (booleano, TF e TF-IDF, média de vetores de palavras e Paragraph Vector); Métricas de similaridade textual - similaridade do cosseno, distância euclidiana, similaridade de Jaccard, distância de Manhattan e coeficiente de Dice","Questão: No campo de Processamento de Linguagem Natural (PLN), diversas técnicas são aplicadas para a preparação e análise de textos. Dentre os processos de normalização textual está a lematização, que consiste em:

A) Converter todas as palavras em letras minúsculas para evitar a distinção entre maiúsculas e minúsculas.
B) Eliminar palavras comuns que não contribuem para o significado do texto, como artigos, preposições e conjunções.
C) Reduzir as palavras flexionadas ou derivadas ao seu radical ou forma base, muitas vezes resultando em uma forma que não corresponde à palavra válida do idioma.
D) Reduzir palavras à sua forma dicionarizada, considerando a análise morfológica para encontrar o lema correto.
E) Selecionar apenas as palavras que ocorrem com mais frequência no texto para representar o seu conteúdo principal.

",D,"

Explicação dos itens:

A) Incorreto. Converter todas as palavras em letras minúsculas é uma técnica conhecida como ""case folding"", que é uma prática usual na normalização de textos, mas não é o que define a lematização.

B) Incorreto. Eliminar palavras comuns, conhecidas como ""stop words"", é uma etapa importante na normalização textual, mas não corresponde ao processo de lematização.

C) Incorreto. A redução de palavras ao seu radical é conhecida como ""estemização"" ou ""stemming"". Este processo frequentemente resulta em uma forma que não é exatamente um lema e pode não ser uma palavra válida do idioma, o que não ocorre na lematização.

D) Correto. A lematização é o processo de reduzir uma palavra à sua forma dicionarizada, o lema, considerando a análise morfológica para entender o contexto e definir a forma base correta, que é uma palavra válida do idioma e que representa a unidade semântica base.

E) Incorreto. A seleção de palavras com base na frequência é uma prática comum na representação vetorial de textos e na análise de frequência de termos, no entanto, não está relacionada diretamente à lematização.",2215491
tópico 3,"Manipulação e tabulação de dados (numpy, pandas, tidyr,verse, data.table)","Questão: Em uma análise de dados utilizando a biblioteca Pandas no Python, um cientista de dados precisa manipular um grande DataFrame chamado `vendas_df`, que contém informações sobre vendas de produtos em diferentes lojas de uma rede varejista. Uma das colunas chama-se `data_venda` e está no formato `string` seguindo o padrão ""aaaa-mm-dd"". O cientista de dados deseja criar duas colunas adicionais, uma contendo apenas o ano e outra apenas o mês extraídos da coluna `data_venda`. Qual das seguintes opções de código realizaria essa tarefa corretamente?

A) 
```python
vendas_df['ano'] = vendas_df['data_venda'].apply(lambda x: x.split('-')[0])
vendas_df['mes'] = vendas_df['data_venda'].apply(lambda x: x.split('-')[1])
```

B) 
```python
vendas_df['ano'], vendas_df['mes'] = vendas_df['data_venda'].str[:4], vendas_df['data_venda'].str[5:7]
```

C) 
```python
vendas_df[['ano', 'mes']] = vendas_df['data_venda'].str.extract(r'(\d{4})-(\d{2})')
```
 
D) 
```python
vendas_df['ano'] = pd.DatetimeIndex(vendas_df['data_venda']).year
vendas_df['mes'] = pd.DatetimeIndex(vendas_df['data_venda']).month
```

E) 
```python
vendas_df['data_venda'] = pd.to_datetime(vendas_df['data_venda'])
vendas_df.assign(ano=vendas_df['data_venda'].dt.year, mes=vendas_df['data_venda'].dt.month)
```

",D,"

Explicação dos itens:

A) Este item usa a função `apply` com uma expressão lambda para dividir a string e extrair ano e mês. Embora funcione, não é a solução mais eficiente ou apropriada para datasets grandes devido ao desempenho do `apply`.

B) Este item extrai as substrings diretamente pelos índices das strings. Apesar de rápido, é menos legível e pode causar erros se o formato da data estiver inconsistente.

C) A função `extract` com uma expressão regular também é capaz de separar ano e mês corretamente. No entanto, ela retorna um DataFrame novo com as colunas especificadas, e não atribui as colunas diretamente ao DataFrame original.

D) O uso da classe `DatetimeIndex` é uma abordagem eficiente e recomendada, já que é capaz de converter strings em objetos datetime e depois acessar os atributos `year` e `month`. Esta é a opção correta, pois realiza a tarefa de maneira eficiente e direta.

E) Similar ao item D, converte a coluna `data_venda` para datetime e em seguida utiliza o método `assign` para adicionar as colunas 'ano' e 'mes'. No entanto, `assign` retorna um novo DataFrame e não modifica o original. Também faltou a reatribuição para que as alterações fossem salvas em `vendas_df`.",3796627
tópico 4,"Diagramas causais: gráficos acíclicos dirigidos; variáveis confundidoras, colisoras e de mediação","Questão:
Em estudos estatísticos e de causalidade, especialmente na área de epidemiologia, os Diagramas Acíclicos Dirigidos (DAGs) são utilizados para representar as relações causais entre diversas variáveis. É de suma importância distinguir os diferentes papéis que as variáveis podem desempenhar em um DAG. Considere as seguintes definições:

I. Variáveis Confundidoras: são variáveis que afetam tanto a variável de exposição quanto o desfecho, podendo criar uma associação espúria ou ocultar uma associação verdadeira entre eles.

II. Variáveis Colisoras: Quando duas variáveis independentes influenciam uma terceira variável, que é o colisor, o controle estatístico deste pode induzir uma associação artificial entre as variáveis que de outra forma seriam independentes.

III. Variáveis de Mediação: Uma variável mediadora é aquela que transmite o efeito de uma variável de exposição para a variável de desfecho.

Dado um DAG em que há uma trajetória causal da variável A para B, e de B para C, e também uma trajetória direta de A para C. Se um pesquisador está interessado em estabelecer a relação causal entre A e C, controlando B, qual seria o papel de B nessa análise?

A) B é um confundidor e deve ser controlado para estabelecer a relação causal entre A e C.
B) B é um colisor e seu controle pode induzir uma associação artificial entre A e C.
C) B é uma variável de mediação e seu controle pode bloquear parte do efeito causal de A sobre C.
D) B é uma variável independente e o seu controle não altera a relação causal entre A e C.
E) B é uma variável colidora e deve ser controlada para revelar a relação causal verdadeira entre A e C.

",C,"

A alternativa correta é a letra C. B atua como uma variável de mediação entre A e C, onde o efeito de A em C passa por B. Controlar B significa que o pesquisador está bloqueando o caminho mediador, o que poderia impedir a observação do efeito total de A em C, incluindo o efeito indireto através de B. O conceito de mediação é essencial em análises causais para compreender os mecanismos pelos quais uma exposição afeta o desfecho. A alternativa A está incorreta porque um confundidor afeta tanto a exposição quanto o desfecho, mas não faz parte do caminho causal entre eles. A alternativa B está incorreta porque o colisor é uma variável que é afetada por duas outras variáveis, e controlá-la poderia criar uma associação espúria; que não é o caso apresentado na pergunta. A alternativa D está incorreta porque B não é independente neste contexto, já que está no caminho entre A e C. A alternativa E também está incorreta pois confunde as definições de colisor e mediador.",3085055
tópico 5,Técnicas de redução de dimensionalidade: Seleção de características (feature selection); Análise de componentes principais (PCA – principal component analysis),"Questão: Na ciência de dados e aprendizado de máquina, técnicas de redução de dimensionalidade são essenciais para o processamento e a análise eficiente dos dados de alta dimensão. Dentre estas técnicas, a Seleção de Características e a Análise de Componentes Principais (PCA) são duas abordagens comumente usadas. Com relação a estas técnicas, avalie as afirmações abaixo e marque a opção correta.

I. A Seleção de Características envolve escolher um subconjunto de características originais sem transformá-las, visando a manutenção da interpretabilidade dos dados originais.
II. A Análise de Componentes Principais (PCA) é uma técnica que transforma as características originais em novas variáveis ortogonais chamadas componentes principais, em que as primeiras componentes concentram a maior parte da variância dos dados.
III. Em situações em que a interpretabilidade das variáveis não é relevante, a Seleção de Características é geralmente preferida ao PCA, já que a primeira fornece um conjunto de dados de dimensão reduzida mais fácil de entender.
IV. A PCA pode ser considerada uma técnica de seleção de características quando os componentes principais são selecionados com base em seu poder explicativo da variância dos dados.

A) Apenas as afirmações I e II estão corretas.
B) Apenas as afirmações I e III estão corretas.
C) Apenas as afirmações II e IV estão corretas.
D) Apenas as afirmações I, II e IV estão corretas.
E) Todas as afirmações estão corretas.

",D,"

As afirmações I e II estão corretas. A Seleção de Características realmente procura um subconjunto de características originais, que pode ser muito importante para manter a facilidade de interpretação dos dados. A PCA, por outro lado, envolve uma transformação dos dados para um novo espaço de características, em que as componentes principais são ortogonais e capturam a maior parte da variância presente no conjunto de dados original.

A afirmação III está incorreta porque, na prática, a Seleção de Características é usada para manter a interpretabilidade das variáveis, enquanto o PCA é utilizado quando essa interpretabilidade pode ser sacrificada em favor de uma representação mais compacta dos dados.

A afirmação IV está errada ao considerar a PCA como uma técnica de seleção de características; apesar de reduzir a dimensionalidade, ela cria novas características (componentes principais) que são combinações lineares das originais, ao invés de simplesmente selecionar um subconjunto das características existentes. Portanto, PCA é uma técnica de extração de características, e não de seleção.",2712438
tópico 5,Técnicas de classificação: Naive Bayes; Regressão logística; Redes neurais artificiais; Árvores de decisão (algoritmos ID3 e C4.5); Florestas aleatórias (random forest); Máquinas de vetores de suporte (SVM – support vector machines); K vizinhos mais próximos (KNN – K-nearest neighbours),"Questão:
Em um projeto de mineração de dados, um cientista de dados está ponderando qual técnica de classificação utilizar para desenvolver um modelo preditivo capaz de diferenciar com precisão entre mensagens de e-mail spam e não spam. A base de dados contém tanto variáveis categóricas quanto numéricas, um número relativamente grande de registros e espera-se que tenha ruídos nos dados. Considerando essas características e a capacidade das técnicas de classificação, qual das seguintes opções seria a menos adequada para inicialmente abordar essa tarefa?

A) Naive Bayes
B) Regressão logística
C) Redes neurais artificiais
D) Árvores de decisão (algoritmos ID3)
E) K vizinhos mais próximos (KNN – K-nearest neighbours)

",D,"

Explicação dos itens:

A) Naive Bayes: É uma escolha robusta e eficiente para classificação de textos como spam ou não spam. Possui bom desempenho mesmo quando a suposição de independência de características não é totalmente verdadeira. Portanto, não é a menos adequada.

B) Regressão logística: É um método comum para classificação binária e pode fornecer bons resultados para o problema de e-mail spam. Pode lidar bem com variáveis numéricas e categóricas após a devida codificação, então também não é a menos adequada.

C) Redes neurais artificiais: Embora sejam poderosas e flexíveis, as redes neurais exigem um grande volume de dados e são capazes de capturar relações complexas entre atributos. No entanto, sem mencionar a complexidade ou a estrutura de redes necessárias, elas não são automaticamente descartadas como menos adequadas nesta questão.

E) K vizinhos mais próximos (KNN – K-nearest neighbours): O KNN é um algoritmo que não faz suposições sobre a distribuição dos dados, assim é uma opção forte para dados com ruídos e variáveis categóricas e numéricas. Pode ser computacionalmente caro com um grande número de registros, mas não é necessariamente a menos adequada.

D) Árvores de decisão (algoritmos ID3): O algoritmo ID3 é baseado em informações de ganho e tende a favorecer atributos com muitos níveis (ou valores distintos) e pode não ser a melhor opção em casos com ruído e atributos numéricos misturados com categóricos. O sucessor C4.5 poderia lidar melhor devido à poda e ao tratamento de dados contínuos, mas o ID3 pode ser menos adequado aqui devido à sua sensibilidade ao ruído e ao seu tratamento menos sofisticado de diferentes tipos de atributos.",4694501
tópico 3,Programação orientada a objetos,"Questão: Em programação orientada a objetos (POO), o princípio da substituição de Liskov (LSP) afirma que objetos de uma superclasse devem ser substituíveis por objetos de suas subclasses sem que seja necessário alterar as propriedades desse programa. Com base nesse principe, qual das seguintes afirmações é verdadeira quando aplicamos o LSP corretamente em um projeto de software?

A) Subclasses podem lançar tipos de exceções que não são previstos ou tratados pela superclasse.
B) Subclasses podem remover métodos de comportamento herdados da superclasse.
C) A superclasse pode ser substituída por qualquer uma de suas subclasses sem afetar a funcionalidade do programa.
D) Subclasses devem ter suas próprias implementações para todos os métodos definidos na superclasse.
E) Comparar objetos da superclasse com objetos da subclasse usando o operador de igualdade deve sempre retornar falso.

",C,"

A) Incorreta. Subclasses que lançam exceções não previstas pela superclasse estão violando o contrato estabelecido pela superclasse e, portanto, não respeitam o LSP.
B) Incorreta. Remover métodos herdados rompe o contrato da superclasse, pois significa que a substituição da superclasse pela subclasse pode levar a chamadas de métodos inexistentes, violando o LSP.
C) Correta. Essa é a essência do LSP – objetos de uma superclasse devem poder ser substituídos por objetos de uma subclasse sem afetar a corretude do programa.
D) Incorreta. Subclasses não são necessariamente obrigadas a implementar suas próprias versões para todos os métodos definidos na superclasse. Elas podem herdar comportamentos diretamente da superclasse se esses comportamentos ainda forem válidos para a subclasse.
E) Incorreta. O operador de igualdade pode perfeitamente considerar iguais um objeto da superclasse e um da subclasse, especialmente se eles forem equivalentes em relação ao estado que é comparado. Isso depende da implementação específica do método de igualdade nas classes envolvidas.",3254598
tópico 0,Ingestão de dados em streaming,"Questão:
A ingestão de dados em streaming é um processo fundamental no cenário de big data, onde o fluxo contínuo de dados é processado e analisado em tempo real. Uma empresa de telecomunicações deseja implementar uma solução de ingestão de dados para monitorar e analisar chamadas telefônicas em tempo real. Dentre as seguintes alternativas, qual representa a melhor escolha de tecnologia para lidar com grandes volumes de dados em streaming, oferecendo alta disponibilidade, escalabilidade e baixa latência?

A) Base de dados relacional tradicional, otimizada com índices e clusters.
B) Hadoop Distributed File System (HDFS) com processamento batch periódico.
C) Apache Kafka combinado com Spark Streaming para processamento em tempo real.
D) Sistema de arquivos local (como ext4 ou NTFS) com scripts cron para processamento periódico.
E) Base de dados NoSQL orientada a colunas, como o Apache Cassandra, sem um sistema de processamento de stream.

",C,"

Explicação dos itens:

A) Base de dados relacional tradicional não é adequada para processamento de dados em streaming devido à sua estrutura rígida e escalabilidade limitada.

B) Hadoop Distributed File System (HDFS) é otimizado para processamento batch e não para processamento em tempo real, o que torna essa escolha inadequada para a necessidade de processamento de streaming em tempo real.

C) Apache Kafka é um sistema de mensageria distribuído projetado para lidar com altos volumes de dados em streaming, enquanto o Spark Streaming fornece capacidades de processamento em tempo real. A combinação dos dois oferece uma solução robusta para os requisitos da empresa.

D) Sistema de arquivos local com scripts cron é uma abordagem simplista e não escalável, inadequada para processamento em tempo real e grandes volumes de dados.

E) Bases de dados NoSQL orientadas a colunas são efetivas para armazenamentos de grandes volumes de dados com alta performance de leitura e gravação, mas, por si só, sem um sistema de processamento de stream, não seria suficiente para atender aos requisitos de processamento em tempo real.",9217129
tópico 3,Programação funcional,"Questão:
A programação funcional é um paradigma de programação que trata a computação como a avaliação de funções matemáticas e evita estados ou dados mutáveis. Dentro deste paradigma, algumas funções possuem características específicas que as diferenciam. Uma dessas características é ser uma ""função pura"". Qual das seguintes afirmações define corretamente uma ""função pura"" em programação funcional?

A) Uma função pura é aquela que, para um mesmo conjunto de entradas, sempre realizará os mesmos efeitos colaterais e não necessariamente retornará o mesmo resultado.

B) Uma função pura é aquela que, para um mesmo conjunto de entradas, pode retornar valores diferentes, dependendo de variáveis globais ou estáticas.

C) Uma função pura é aquela que não possui efeitos colaterais e, para um mesmo conjunto de entradas, sempre retornará o mesmo resultado.

D) Uma função pura é definida como pura simplesmente por não realizar operações de entrada e saída.

E) Uma função pura coordena várias funções com efeitos colaterais para processar dados, garantindo que os efeitos ocorram na sequência esperada.

",C,"

Explicação dos itens:

A) Incorreto. Uma função pura é caracterizada por não realizar efeitos colaterais e por ter retorno determinístico (o mesmo resultado é obtido para as mesmas entradas).

B) Incorreto. A definição de função pura exclui a dependência de variáveis globais ou estáticas que possam alterar a saída da função, garantindo que a saída seja sempre a mesma para as mesmas entradas.

C) Correto. Esta é a definição correta de uma função pura. Ela não deve ter efeitos colaterais e deve ser determinística, retornando o mesmo resultado sempre que for chamada com as mesmas entradas.

D) Incorreto. A ausência de operações de E/S (entrada e saída) não é uma condição suficiente para definir uma função como pura. A função também deve ser determinística e livre de efeitos colaterais.

E) Incorreto. A coordenação de várias funções com efeitos colaterais não define uma função pura. Funções puras devem ser independentes de efeitos colaterais, não apenas garantir que aconteçam em uma sequência específica.",1687893
tópico 3,Linguagem de programação Scala,"Questão: Em Scala, os padrões de case class são altamente utilizados para realizar operações de correspondência de padrões (pattern matching), que simplificam a manipulação de dados estruturados. Considerando o seguinte código Scala:

```scala
abstract class Notification

case class Email(sender: String, title: String, body: String) extends Notification
case class SMS(caller: String, message: String) extends Notification
case class VoiceRecording(contactName: String, link: String) extends Notification

def showNotification(notification: Notification): String = {
  notification match {
    case Email(email, title, _) => s""You got an email from $email with title: $title""
    case SMS(number, message) => s""You got an SMS from $number! Message: $message""
    case VoiceRecording(name, link) => s""You received a Voice Recording from $name! Click here to hear it: $link""
  }
}

val someSms = SMS(""12345"", ""Are you there?"")
val someVoiceRecording = VoiceRecording(""Tom"", ""voicerecording.com/id/123"")

println(showNotification(someSms))
println(showNotification(someVoiceRecording))
```

Supondo que o código seja executado como mostrado, qual será a saída exibida no console?

A) 
```
You got an SMS from 12345! Message: Are you there?
You received a Voice Recording from Tom! Click here to hear it: voicerecording.com/id/123
```

B)
```
You got an email from 12345 with title: Are you there?
You received a Voice Recording from voicerecording.com/id/123! Click here to hear it: Tom
```

C)
```
Error: No match found for showNotification function.
```

D)
```
You got an SMS from Are you there? Message: 12345
You got an email from Tom with title: voicerecording.com/id/123
```

E)
```
You received a Voice Recording from 12345! Click here to hear it: Are you there?
You got an SMS from Tom! Message: voicerecording.com/id/123
```

",A," 
A explicação para cada item é a seguinte:

A) Este item é a resposta correta. O código realiza o pattern matching para cada tipo de notificação e extrai as informações corretas para imprimir a mensagem formada no console.

B) Este item é incorreto. Ele mistura os retornos das funções de pattern matching, atribuindo o corpo de uma mensagem SMS como título de um email e invertendo os parâmetros para o VoiceRecording.

C) Este item está incorreto. Haverá uma correspondência para cada tipo de notificação devido aos três cases definidos na função `showNotification`. Portanto, não ocorrerá erro de correspondência.

D) Este item está incorreto. Ele inverte as informações extraídas dos objetos SMS e VoiceRecording, colocando a mensagem do SMS como número de telefone e atribuindo o link do VoiceRecording como título do Email.

E) Este item também está incorreto. Ele inverte os tipos de notificação, atribuindo as propriedades de um objeto SMS a um VoiceRecording e vice-versa.",2929157
tópico 2,Tratamento de dados ausentes,"Questão:
A análise de dados é fundamental para a tomada de decisões baseadas em evidências. No entanto, muitas vezes os conjuntos de dados apresentam o problema de dados ausentes, o que pode levar a distorções nos resultados e conclusões das análises. Supondo que você está lidando com um conjunto de dados que apresenta valores ausentes de maneira não completamente aleatória (Missing Not At Random - MNAR), qual das seguintes técnicas NÃO é apropriada para tratar esses dados ausentes?

A) Imputação pela média ou mediana.
B) Imputação por regressão múltipla.
C) Uso do método Maximum Likelihood Estimation (MLE).
D) Utilização de um modelo de equações estruturais para modelar tanto os mecanismos de missingness quanto as relações entre as variáveis.
E) Exclusão completa de casos (listwise deletion).

",E,"

A) Imputação pela média ou mediana é uma técnica simples de imputação que pode ser usada como uma tentativa inicial de tratamento, mesmo em situações MNAR, apesar de não ser a mais indicada, pois ignora a possibilidade de haver um padrão nos dados ausentes.

B) Imputação por regressão múltipla leva em conta as relações lineares entre as variáveis e pode ser uma opção no tratamento de dados MNAR, mas deve ser usada com cautela, considerando os potenciais vieses.

C) O Método de Máxima Verossimilhança (Maximum Likelihood Estimation - MLE) tenta encontrar os parâmetros que tornam a probabilidade de observar os dados coletados o mais alta possível e pode ser uma forma eficiente de lidar com dados MNAR quando aplicado corretamente.

D) O uso de modelo de equações estruturais permite incorporar o processo de faltantes como parte do modelo, ajudando a lidar com os dados MNAR ao modelar adequadamente a estrutura de dependência tanto do mecanismo de ausência quanto das variáveis observadas.

E) A exclusão completa de casos (listwise deletion) descarta qualquer caso com pelo menos um valor ausente, o que pode levar a uma grande perda de informações e a distorções significativas em análises MNAR. Este método normalmente só é considerado apropriado para situações em que os dados são MCAR (Missing Completely At Random) e, portanto, pode não ser adequado para MNAR, o que a torna a opção incorreta para a pergunta dada.",3716160
tópico 5,"Métricas de similaridade textual - similaridade do cosseno, distância euclidiana, similaridade de Jaccard, distância de Manhattan e coeficiente de Dice","Questão: Em sistemas de recuperação de informações e processamento de linguagem natural, diversas métricas são utilizadas para determinar a similaridade ou dissimilaridade entre textos. Dentre as seguintes alternativas, identifique a métrica que não é apropriada para calcular a similaridade direta entre representações vetoriais de textos.

A) Similaridade do Cosseno
B) Distância Euclidiana
C) Similaridade de Jaccard
D) Coeficiente de Dice
E) Distância de Manhattan

",B,"

Explicação dos itens:

A) Similaridade do Cosseno - Esta métrica mede o cosseno do ângulo entre dois vetores no espaço vetorial, portanto, é adequada para calcular a similaridade entre textos representados como vetores.

B) Distância Euclidiana - Embora envolva representações vetoriais, a distância euclidiana mede diretamente a distância entre dois pontos em um espaço multidimensional e é uma medida de dissimilaridade, não de similaridade direta.

C) Similaridade de Jaccard - Correta para comparar a similaridade entre conjuntos, como documentos representados por conjuntos de palavras. Contudo, é mais frequentemente utilizada em dados binários ou não ponderados, mas ainda assim se refere a uma medida de similaridade.

D) Coeficiente de Dice - Medida de similaridade entre conjuntos, que é duas vezes o número de elementos em comum dividido pela soma do número de elementos em cada conjunto. É uma métrica de similaridade, embora seja mais comum em dados binários ou conjuntos.

E) Distância de Manhattan - Esta é uma medida de dissimilaridade que calcula a soma das diferenças absolutas entre os pontos em cada dimensão. Similar à distância euclidiana, não é uma medida de similaridade direta, mas de dissimilaridade. No entanto, ambas a distância de Manhattan e a distância Euclidiana poderiam ser invertidas para fornecer uma métrica proporcional de similaridade, o que as torna aplicáveis de maneira indireta.",9759932
tópico 4,Métodos e técnicas de identificação causal: Métodos experimentais RCT e de identificação quase-experimental,"Questão:

O uso de métodos e técnicas para identificação causal é crucial na avaliação de políticas públicas e na pesquisa econômica para estabelecer relações de causa e efeito entre variáveis. Entre essas técnicas, destacam-se os experimentos randomizados controlados (RCT) e métodos de identificação quase-experimental. Considerando esses métodos, avalie as afirmações abaixo e escolha a opção correta.

I. Experimentos randomizados controlados (RCT) envolvem a atribuição aleatória de tratamentos a diferentes grupos de indivíduos, permitindo uma alta confiabilidade na inferência de relações causais, desde que a randomização seja bem implementada e com tamanho de amostra suficiente.

II. Métodos quase-experimentais, como a Regressão Descontínua (RD) e a Diferenças em Diferenças (DiD), são utilizados quando a randomização não é possível ou ética, aproveitando-se de eventos ou regras que afetam arbitrariamente alguns grupos e não outros, criando condições de comparação análogas à experimentação.

III. Uma das desvantagens dos RCTs é que, enquanto eles garantem a internalidade da validade – a causalidade dentro da amostra estudada –, sua validade externa é presumida, pois a capacidade de generalizar os resultados para outras populações não é confirmada pelo método em si.

IV. A validade interna de métodos quase-experimentais é tipicamente mais forte que a dos RCTs, pois ao utilizar situações da vida real, eles automaticamente garantem que as condições do estudo estão mais próximas de cenários não experimentais, favorecendo a generalização dos resultados.

Assinale a alternativa correta.

A) Todas as afirmações estão corretas.

B) Apenas as afirmações I, II e III estão corretas.

C) Apenas as afirmações I e II estão corretas.

D) Apenas as afirmações II, III e IV estão corretas.

E) Apenas as afirmações I e III estão corretas.

",B,"

Explicação dos itens:

I. Correta. Experimentos randomizados controlados dependem da atribuição aleatória e são reconhecidos por sua força no estabelecimento de relações causais. Contudo, a aleatoriedade deve ser bem planejada e a amostra deve ser representativa.

II. Correta. Métodos quase-experimentais são usados quando a randomização não é viável. Eles buscam replicar a lógica de experimentação em situações naturais ou administrativas, onde ocorrem ""experimentos naturais"".

III. Correta. A validade interna dos RCTs é uma de suas principais vantagens, mas a validade externa – a generalização dos resultados para outras populações – não é confirmada pelo RCT em si, e depende de outros fatores, como a representatividade da amostra em relação à população mais ampla.

IV. Incorreta. A validade interna de métodos quase-experimentais geralmente é considerada menos forte do que a dos RCTs devido a potenciais vieses resultantes da não-randomização. Métodos quase-experimentais requerem cuidados adicionais na interpretação dos resultados, pois podem estar sujeitos a vieses de seleção e outras confusões.",1144245
tópico 3,Programação funcional,"Questão:

Considere o paradigma de programação funcional e suas características principais. Avalie as afirmações a seguir e assinale a opção que apresenta uma característica que NÃO está alinhada com os princípios da programação funcional.

A) Funções de alta ordem permitem que funções sejam tratadas como variáveis e sejam passadas como argumentos para outras funções ou retornadas como valor.

B) A programação funcional favorece o uso de estruturas de dados imutáveis, evitando o compartilhamento de estados entre diferentes funções.

C) Efeitos colaterais são comuns e incentivados na programação funcional, uma vez que permitem uma maior adaptabilidade das funções ao interagir com o mundo exterior.

D) Funções puras são uma peça central na programação funcional, onde a mesma entrada garante sempre a mesma saída, sem efeitos colaterais.

E) Recursão é uma técnica frequentemente utilizada em lugar de laços tradicionais (loops), porque permite expressar o cálculo de repetições de forma pura e sem estado mutável.

",C,"

Explicação dos itens:

A) Correta. Funções de alta ordem são um conceito essencial na programação funcional, pois oferecem flexibilidade e reutilização de código.

B) Correta. Evitar estados mutáveis e utilizar estruturas de dados imutáveis são princípios fundamentais para garantir a previsibilidade e a facilidade de raciocínio em torno das funções.

C) Incorreta. Efeitos colaterais são minimizados na programação funcional para garantir funções puras. Isso aumenta a previsibilidade e facilita o teste e a manutenção do código.

D) Correta. Funções puras garantem que os resultados sejam inteiramente previsíveis, o que é fortemente alinhado com a filosofia da programação funcional.

E) Correta. A utilização de recursão em vez de laços é incentivada em programação funcional porque suporta imutabilidade e funcionalidades sem estado.",6744711
tópico 5,Técnicas de classificação: Naive Bayes; Regressão logística; Redes neurais artificiais; Árvores de decisão (algoritmos ID3 e C4.5); Florestas aleatórias (random forest); Máquinas de vetores de suporte (SVM – support vector machines); K vizinhos mais próximos (KNN – K-nearest neighbours),"Questão:
Em um contexto de análise de dados, uma empresa deseja aplicar técnicas de aprendizado de máquina para classificar e-mails como ""spam"" ou ""não spam"". Considerando que a base de dados é grande e desbalanceada, com muito mais e-mails classificados como ""não spam"" do que ""spam"", e que a empresa busca uma solução que forneça uma probabilidade associada à classificação, qual técnica de classificação é a mais apropriada para atender a essa necessidade?

A) Naive Bayes
B) Redes Neurais Artificiais
C) Árvores de Decisão (algoritmos ID3 e C4.5)
D) Máquinas de Vetores de Suporte (SVM – Support Vector Machines)
E) K Vizinhos Mais Próximos (KNN – K-nearest Neighbours)

",A,"

Breve explicação dos itens:

A) Naive Bayes é adequado para o cenário descrito porque é capaz de lidar bem com grandes volumes de dados e pode fornecer probabilidades associadas às classificações. Além disso, sua suposição de independência condicional frequentemente funciona bem para o problema de classificação de e-mails como ""spam"" ou ""não spam"".

B) Redes Neurais Artificiais podem ser utilizadas, mas elas demandam um tempo maior de treinamento e nem sempre fornecem probabilidades de forma direta.

C) Árvores de Decisão (algoritmos ID3 e C4.5) são úteis para modelagem preditiva, mas também não fornecem probabilidades e podem enfrentar dificuldades devido ao desbalanceamento dos dados.

D) Máquinas de Vetores de Suporte (SVM – Support Vector Machines) são poderosas para encontrar o hiperplano de separação ótimo em dados de alta dimensão, mas não são a melhor escolha quando se deseja uma probabilidade associada à previsão e podem não ser adequadas para dados desbalanceados sem um pré-processamento adequado.

E) K Vizinhos Mais Próximos (KNN – K-nearest Neighbours) é um método não-paramétrico que pode não ser prático para grandes volumes de dados devido ao seu custo computacional e também não fornece uma probabilidade direta da classificação. Além disso, o desbalanceamento pode enviesar o classificador.",3320589
tópico 1,"Banco de dados relacional: SQL Server, PostgreSQL, MySQL","Questão: No contexto dos sistemas de banco de dados relacionais SQL Server, PostgreSQL e MySQL, considere que um administrador de banco de dados deseja criar um índice que otimize as consultas em uma tabela com grande volume de dados e transações frequentes. A tabela armazena informações de vendas, incluindo as colunas 'data_venda', 'id_cliente', 'valor_venda' e 'id_vendedor'. As consultas mais comuns envolvem a busca por 'data_venda' e 'id_cliente'. Com base nesse cenário, qual das seguintes abordagens é a mais eficiente para a criação deste índice que busca otimização?

A) Criar um índice clusterizado apenas na coluna 'data_venda', pois esse tipo de índice reorganiza fisicamente as linhas da tabela e é mais rápido para consultas baseadas em range.

B) Criar um índice não clusterizado composto pelas colunas 'data_venda' e 'id_cliente', uma vez que este índice melhora a performance de consultas que utilizam essas colunas como filtros.

C) Implementar um índice full-text nas colunas 'data_venda' e 'id_cliente', o que permitiria buscas rápidas por textos específicos e substrings nas colunas especificadas.

D) Utilizar um índice hash na coluna 'id_cliente', considerando que este modelo de indexação é mais eficiente para consultas que realizam buscas diretas por valor exato.

E) Criar índices clusterizados separados para 'data_venda' e 'id_cliente', permitindo assim que o otimizador de consultas escolha o melhor índice para cada consulta específica.

",B,"

A) Um índice clusterizado reorganiza as linhas da tabela no disco, o que pode ser eficiente para consultas baseadas em range. No entanto, como as consultas envolvem não só 'data_venda', mas também 'id_cliente', este índice não seria o mais efetivo.

B) Criar um índice não clusterizado composto é o mais eficiente neste caso porque irá otimizar as consultas que incluem ambas as colunas no filtro de busca, sendo essa a situação mais comum mencionada no cenário.

C) Índices full-text são utilizados para pesquisar textos dentro de uma coluna, como em buscas por palavras dentro de colunas de texto grande. As colunas mencionadas não se enquadram em um uso típico de índice full-text.

D) Índices hash são eficientes para correspondência exata de valores, mas eles não são diretamente suportados para indexação no SQL Server e no PostgreSQL (MySQL oferece essa opção, mas apenas para tabelas de hash em memória). Além disso, a consulta também envolve a coluna 'data_venda', portanto o índice hash não seria o mais apropriado.

E) Índices clusterizados são únicos por tabela, então não se pode criar mais de um em uma única tabela. Além disso, dois índices clusterizados separados não otimizariam as consultas que incluem ambas as colunas no mesmo filtro.",3188153
tópico 3,Linguagem de programação Scala,"Questão: 
Considere as seguintes declarações de código em Scala:

```scala
abstract class Instrumento {
  def tocar: String
}

class Piano extends Instrumento {
  def tocar = ""Piano está tocando""
}

class Violino extends Instrumento {
  def tocar = ""Violino está tocando""
}

object Orquestra {
  def iniciarConcerto(instrumentos: List[Instrumento]) = {
    instrumentos.map(_.tocar).mkString("", "")
  }
}

val instrumentos = List(new Piano, new Violino)
println(Orquestra.iniciarConcerto(instrumentos))
```

Quando o método `iniciarConcerto` da classe `Orquestra` é invocado, o que é impresso na saída padrão?

A) ""Piano está tocando, Violino está tocando""
B) ""Instrumento está tocando, Instrumento está tocando""
C) A compilação do código falha devido a um erro de tipo nas classes `Piano` e `Violino`.
D) NullPointerException é lançado durante a execução.
E) Nada é impresso porque o método `iniciarConcerto` não possui um comando de impressão.

",A," 
Explicação dos itens:
A) Este item é a resposta correta. O método `iniciarConcerto` invoca a função `tocar` em cada elemento da lista de `Instrumento`, e a sequência de representações de strings resultantes (""Piano está tocando"", ""Violino está tocando"") é unida em uma única string com vírgulas entre elas.

B) Este item está incorreto porque cada classe que estende `Instrumento` fornece sua própria implementação de `tocar`, resultando em strings específicas para cada tipo de instrumento.

C) Este item está incorreto. As classes `Piano` e `Violino` estendem a classe abstrata `Instrumento` corretamente ao fornecerem suas próprias implementações da função abstrata `tocar`, permitindo que a compilação seja bem-sucedida.

D) Este item está incorreto porque não há operações no código fornecido que resultariam em um `NullPointerException`.

E) Este item está incorreto pois, embora o método `iniciarConcerto` por si só não imprima nada, a última linha de código no exemplo invoca `println` e passa o resultado de `iniciarConcerto` como argumento, causando a impressão da string composta na saída padrão.",2777498
tópico 4,Probabilidade e probabilidade condicional,"Questão:
Uma empresa realiza um processo seletivo que consiste em duas etapas independentes: uma prova teórica e uma entrevista. Sabe-se que a probabilidade de um candidato ser aprovado na prova teórica é de 40%, e a probabilidade de ser aprovado na entrevista, dado que foi aprovado na prova teórica, é de 30%. Qual é a probabilidade de um candidato ser aprovado tanto na prova teórica quanto na entrevista?

A) 50%
B) 20%
C) 12%
D) 30%
E) 40%

",C,"

Explicação dos itens:

A) 50% - Este valor não é correto, pois não leva em consideração a probabilidade condicional da segunda etapa.
B) 20% - Este valor é incorreto, já que representa apenas a metade da probabilidade de passar na prova teórica e não considera a probabilidade de passar na entrevista.
C) 12% - Este é o valor correto. Como a aprovação nas duas etapas é um evento composto pela conjunção de dois eventos independentes, a probabilidade de ser aprovado em ambos é o produto das probabilidades individuais: 0,4 (probabilidade de ser aprovado na prova teórica) multiplicado por 0,3 (probabilidade condicional de ser aprovado na entrevista), resultando em 0,12 ou 12%.
D) 30% - Este valor é incorreto, pois corresponde unicamente à probabilidade condicional de passar na entrevista, ignorando a etapa da prova teórica.
E) 40% - Este é o valor da probabilidade de ser aprovado apenas na prova teórica e não considera a etapa da entrevista.",4525059
tópico 3,Programação orientada a objetos,"Questão: Na Programação Orientada a Objetos (POO), o princípio da encapsulamento refere-se à restrição do acesso direto aos componentes internos de um objeto. Este princípio é fundamental para garantir a integridade dos dados e a implementação de regras de negócio. Qual das seguintes opções melhor representa uma aplicação correta do encapsulamento em uma classe?

A) Tornar todos os atributos da classe públicos e permitir que sejam alterados diretamente de qualquer parte do código.
B) Utilizar métodos públicos para acessar e modificar os valores dos atributos privados, permitindo a validação dos novos valores antes de serem efetivamente atribuídos.
C) Criar atributos estáticos para que possam ser compartilhados entre várias instâncias da classe, facilitando o acesso direto a eles.
D) Manter todos os métodos e atributos da classe privados, tornando impossível a criação de instâncias e a utilização da classe.
E) Omitir a declaração de métodos getters e setters, uma vez que eles são considerados uma má prática e expõem desnecessariamente os detalhes internos da classe.

",B," 
A alternativa A está incorreta, pois vai contra o princípio do encapsulamento ao tornar todos os atributos públicos. A alternativa B é a correta, pois expressa a essência do encapsulamento, onde métodos públicos são utilizados para controlar o acesso aos atributos privados, permitindo validação e garantindo a integridade dos dados. A alternativa C está incorreta porque atributos estáticos têm propósitos diferentes e não estão relacionados diretamente com encapsulamento. A alternativa D é incorreta porque o encapsulamento não implica eliminar o acesso a uma classe ou instâncias dela, mas sim controlar esse acesso. Por fim, a alternativa E também está incorreta, pois getters e setters são uma prática comum para implementar encapsulamento e não expõem necessariamente detalhes internos da classe se adequadamente implementados.",5785221
tópico 5,"Avaliação de modelos de classificação: treinamento, teste, validação; validação cruzada; métricas de avaliação - matriz de confusão, acurácia, precisão, revocação, F1-score e curva ROC","Questão:

Em um processo de construção de um modelo de classificação binária, um cientista de dados elaborou um modelo preditivo e está agora na fase de avaliação da sua performance. Ele aplicou técnicas de validação cruzada para garantir que sua avaliação é robusta e generalizável para novos dados. O cientista de dados gerou várias métricas de avaliação e compilou as seguintes informações originadas de múltiplas iterações da validação cruzada:

- Acurácia média: 85%
- Precisão média (classe positiva): 78%
- Revocação média (classe positiva): 71%
- F1-score médio (classe positiva): 74%
- Área sob a curva ROC (AUC): 0.90

Baseado nessas informações, qual métrica indica que o modelo tem uma performance satisfatória na identificação da classe positiva, e qual métrica sugere que o modelo ainda pode ser aprimorado especificamente para essa classe?

A) A métrica de Acurácia indica performance satisfatória; a métrica de Precisão sugere aprimoramento.
B) A métrica de F1-score indica performance satisfatória; a métrica de Revocação sugere aprimoramento.
C) A métrica de Precisão indica performance satisfatória; a métrica de Acurácia sugere aprimoramento.
D) A métrica de AUC indica performance satisfatória; a métrica de Revocação sugere aprimoramento.
E) Todas as métricas sugerem que o modelo é altamente satisfatório e não necessita de aprimoramento.

",D,"

- A métrica de Acurácia (alternativa A) não é um bom indicador de performance quando as classes estão desbalanceadas, o que pode acontecer em muitos problemas de classificação binária.
- O F1-score (alternativa B) é uma métrica que combina Precisão e Revocação, portanto se o F1-score está em um patamar intermediário, isso indica que há um equilíbrio entre estas duas métricas, e ambas poderiam ser potencialmente aprimoradas.
- A Precisão sozinha (alternativa C) é uma métrica que não deve ser avaliada de forma isolada. Uma Precisão alta pode ser acompanhada de uma Revocação baixa, o que indica que uma proporção significativa de verdadeiros positivos está sendo perdida.
- A métrica de AUC (alternativa D) é uma medida robusta da capacidade do modelo de discriminar entre as classes positivas e negativas, independentemente do limiar de decisão. Um valor de 0.90 sugere que o modelo tem uma boa capacidade discriminatória. No entanto, a Revocação, ou taxa de verdadeiros positivos, está relativamente baixa (71%), indicando que uma proporção dos casos positivos reais não está sendo capturada pelo modelo. Isso sugere que há espaço para melhorar a capacidade do modelo de identificar corretamente todos os casos positivos.
- Alternativa E não é correta pois, apesar de algumas métricas apresentarem bons resultados, outras mostram que o modelo ainda pode ser aprimorado, o que é normal no processo de desenvolvimento de modelos de classificação.",5073376
tópico 1,Álgebra relacional e SQL (padrão ANSI),"Questão: Considere duas tabelas de um banco de dados relacional, Cliente (CodCliente, NomeCliente, Email) e Pedido (NumPedido, DataPedido, CodCliente), onde Cliente.CodCliente é chave primária para a tabela Cliente e Pedido.CodCliente é chave estrangeira que referencia Cliente.CodCliente. Pretende-se realizar uma consulta SQL para listar todos os clientes que fizeram mais de três pedidos no ano de 2022. Qual das seguintes alternativas representa corretamente essa consulta em padrão ANSI SQL?

A) SELECT NomeCliente FROM Cliente WHERE CodCliente IN (SELECT CodCliente FROM Pedido GROUP BY CodCliente HAVING COUNT(NumPedido) > 3 AND YEAR(DataPedido) = 2022);

B) SELECT NomeCliente FROM Cliente, Pedido WHERE Cliente.CodCliente = Pedido.CodCliente AND COUNT(Pedido.NumPedido) > 3 AND YEAR(Pedido.DataPedido) = 2022 GROUP BY Cliente.NomeCliente;

C) SELECT NomeCliente FROM Cliente LEFT JOIN Pedido ON Cliente.CodCliente = Pedido.CodCliente WHERE COUNT(Pedido.NumPedido) > 3 AND YEAR(Pedido.DataPedido) = 2022 GROUP BY Cliente.NomeCliente;

D) SELECT NomeCliente FROM Cliente JOIN (SELECT CodCliente FROM Pedido WHERE YEAR(DataPedido) = 2022 GROUP BY CodCliente HAVING COUNT(NumPedido) > 3) AS Pedidos2022 ON Cliente.CodCliente = Pedidos2022.CodCliente;

E) SELECT NomeCliente FROM Cliente INNER JOIN Pedido ON Cliente.CodCliente = Pedido.CodCliente GROUP BY Cliente.NomeCliente HAVING COUNT(Pedido.NumPedido) > 3 AND YEAR(Pedido.DataPedido) = 2022;

",D," 

Explicação dos itens:

A) Incorreto. O sub-select nesta consulta não aplica o filtro do ano diretamente ao agrupamento, sendo aplicado somente depois de realizar a contagem, o que pode incluir pedidos de outros anos.

B) Incorreto. A consulta está utilizando uma junção implícita entre as tabelas Cliente e Pedido (produto cartesiano) e também coloca a função de agregação COUNT no WHERE, o que não é permitido. As funções de agregação devem ser utilizadas no HAVING após um GROUP BY.

C) Incorreto. Similar ao item B, ela tenta utilizar a função de agregação COUNT no WHERE, que deveria estar no HAVING. Além disso, a junção LEFT JOIN não é necessária, já que precisamos apenas de clientes com pedidos.

D) Correto. Esta consulta utiliza uma sub-query que primeiro filtra os pedidos do ano de 2022 e agrupa os pedidos por cliente, contando o número de pedidos. Somente então são selecionados os clientes que possuem mais de três pedidos neste ano. A junção é realizada entre a tabela Cliente e o resultado da sub-consulta, garantindo assim que apenas os clientes com mais de três pedidos em 2022 sejam listados.

E) Incorreto. Ainda que essa consulta realize a junção correta das tabelas e o agrupamento, a condição do ano em YEAR(Pedido.DataPedido) = 2022 está localizada incorretamente no HAVING quando deveria estar no WHERE, pois se refere a uma condição de linha e não de agrupamento.",3748637
tópico 5,"Avaliação de modelos de classificação: treinamento, teste, validação; validação cruzada; métricas de avaliação - matriz de confusão, acurácia, precisão, revocação, F1-score e curva ROC","Questão: A empresa ABC deseja implementar um modelo de classificação para prever a possibilidade de inadimplência de seus clientes. Para avaliar a performance de diferentes algoritmos, o cientista de dados responsável pelo projeto aplicou técnicas de validação cruzada e utilizou várias métricas de desempenho. Considere que ele tenha obtido a matriz de confusão mostrada abaixo para um dos modelos testados:

```
            Predito Não Inadimplente     Predito Inadimplente
Real Não Inadimplente                850                     150
Real Inadimplente                    100                     400
```

Com base nessa matriz de confusão, qual é o valor da revocação (recall) para a classe ""Inadimplente""?

A) 0,85
B) 0,40
C) 0,89
D) 0,80
E) 0,15

",D," A revocação (recall) é definida como a proporção de positivos verdadeiros (casos corretamente identificados pela classe de interesse) em relação ao total de casos reais naquela classe. Para a classe ""Inadimplente"", calculamos a revocação como: (Verdadeiros Positivos) / (Verdadeiros Positivos + Falsos Negativos) = 400 / (400 + 100) = 0,80.

Explicação dos itens:
A) 0,85: Este valor corresponderia à acurácia do modelo (corretos totais divididos pelo total de previsões), não à revocação para a classe ""Inadimplente"".

B) 0,40: Este número representa a proporção de predições corretas do modelo para a classe ""Não Inadimplente"", não a revocação.

C) 0,89: Este número não está relacionado à matriz de confusão fornecida ou a qualquer cálculo derivado dela.

E) 0,15: Este valor não representa nenhuma métrica relevante calculada a partir da matriz de confusão fornecida; parece ser um valor arbitrário e incorreto para a revocação da classe de interesse.",5960697
tópico 3,Linguagem de programação Scala,"Questão:
Uma característica notável do Scala é a sua interoperabilidade com Java, permitindo que os desenvolvedores utilizem bibliotecas Java em programas Scala e vice-versa. Além disso, Scala incorpora vários conceitos de programação funcional, o que a torna distinta de outras linguagens JVM. Considerando essas características, qual das seguintes opções não é uma funcionalidade ou conceito suportado nativamente por Scala?

A) Closures
B) Generics
C) Pointer arithmetic
D) Traits
E) Pattern Matching

",C,"

Explicação dos itens:
A) Closures: Scala suporta closures, que são funções que podem capturar variáveis externas dentro de seu escopo.
B) Generics: Scala possui um sistema de tipos parametrizados, conhecido como generics, similar ao Java, que permite a criação de classes, traits e métodos que podem operar em diferentes tipos.
C) Pointer arithmetic: Scala, como Java, não permite aritmética de ponteiros de forma nativa pois gerencia a memória de forma automática através do Garbage Collector, focando na segurança e prevenindo o acesso direto à memória.
D) Traits: Scala utiliza traits, que são similares às interfaces do Java, mas com a capacidade de conter implementações de métodos. Eles são usados para compartilhar interfaces e campos entre classes.
E) Pattern Matching: Pattern matching é um recurso poderoso em Scala que permite verificar um valor contra um padrão. É uma forma mais avançada de switch-case encontrada em outras linguagens como Java.",903505
tópico 5,Técnicas de redução de dimensionalidade: Seleção de características (feature selection); Análise de componentes principais (PCA – principal component analysis),"Questão: Em problemas de aprendizado de máquina, muitas vezes nos deparamos com um grande número de variáveis que podem ser irrelevantes ou redundantes, resultando em complexidade desnecessária e podendo afetar o desempenho de um modelo. Para lidar com esse problema, técnicas de redução de dimensionalidade são aplicadas. Sobre essas técnicas, analise as afirmativas abaixo:

I. A seleção de características (feature selection) busca selecionar um subconjunto de características originais que sejam mais significativas para o modelo, ignorando as demais, sem alterar os valores dos atributos selecionados.

II. A análise de componentes principais (PCA) é uma técnica que transforma as características originais em um novo conjunto de variáveis correlacionadas, chamadas componentes principais, de forma a manter a maior parte da variância presente nos dados.

III. Uma das vantagens do PCA sobre a seleção de características é que ele pode eliminar a multicolinearidade entre as variáveis de entrada, criando componentes que são estatisticamente independentes entre si.

IV. O PCA deve ser aplicado em cenários onde a interpretabilidade do modelo é de extrema importância, uma vez que os componentes principais geralmente têm uma interpretação direta em relação às variáveis originais.

Está(ão) correta(s) a(s) afirmativa(s):

A) I e II apenas.
B) I, II e III apenas.
C) II e III apenas.
D) I, III e IV apenas.
E) Todas as afirmativas estão corretas.

",B,"

Explicação dos itens:

I. Afirmativa correta. A seleção de características é o processo de identificar e selecionar um subconjunto de características relevantes para usar na construção de um modelo.

II. Afirmativa incorreta. A análise de componentes principais (PCA) busca transformar o conjunto original de variáveis em um novo conjunto de variáveis não correlacionadas chamadas componentes principais, e não variáveis correlacionadas como mencionado.

III. Afirmativa correta. Uma das principais vantagens do PCA é a eliminação da multicolinearidade, ao criar componentes que são independentes uns dos outros.

IV. Afirmativa incorreta. Enquanto o PCA é valioso para a redução de dimensionalidade, o novo conjunto de variáveis (componentes principais) muitas vezes não tem uma interpretação direta, o que pode dificultar a interpretabilidade do modelo. Eles são combinações lineares das variáveis originais e geralmente não têm significado físico ou intuitivo por si só.

Portanto, as afirmativas I, II e III estão corretas e a melhor escolha é a letra B.",9319981
tópico 0,Conceitos de processamento massivo e paralelo,"Questão: Considerando o processamento massivo de dados em sistemas modernos, diversas arquiteturas e metodologias são empregadas para otimizar o desempenho e a escalabilidade desses sistemas. Qual das seguintes alternativas descreve corretamente o princípio do MapReduce, amplamente utilizado em ambientes de processamento de grandes volumes de dados, como o framework Apache Hadoop?

A) Uma abordagem de programação baseada em grafos, onde os dados são processados em vértices com computações paralelas.

B) Uma combinação de funções 'Map', que processam e transformam os dados de entrada em pares chave-valor, e funções 'Reduce', que agregam os resultados.

C) Uma técnica que envolve a multiplicação de matrizes distribuídas em uma rede de computadores para otimizar o processamento de consultas de banco de dados.

D) Uma estratégia de escalonamento de tarefas que prioriza operações de I/O sobre computação, redistribuindo os dados de maneira uniforme em vários discos.

E) Um modelo que emprega o conceito de 'streaming' de dados, permitindo que os dados sejam processados em tempo real, sem necessidade de armazenamento intermediário.

",B,"

Explicação dos itens:

A) Incorreto. O item A descreve um tipo de abordagem mais associada ao processamento de dados com grafos, exemplificado por sistemas como o Apache Giraph, e não o MapReduce.

B) Correto. O item B captura a essência do framework MapReduce. No 'Map', os dados são processados e transformados em pares chave-valor intermediários, e, no 'Reduce', os pares de mesma chave são combinados/agregados para produzir o resultado final. Esta é a característica principal do modelo MapReduce.

C) Incorreto. O item C está descrevendo um tipo de operação que poderia ser parte de um sistema de processamento de álgebra linear ou processamento distribuído de banco de dados, mas não é característico do paradigma MapReduce.

D) Incorreto. Embora a estratégia de escalonamento de tarefas seja importante em muitos sistemas de processamento de dados, o item D não descreve uma característica específica do MapReduce.

E) Incorreto. O modelo descrito no item E está mais relacionado a sistemas de processamento de dados em tempo real, como Apache Storm ou Apache Kafka Streams. MapReduce, por outro lado, é geralmente mais associado ao processamento em lote ('batch processing').",2700703
tópico 0,Ingestão de dados em lote (batch),"Questão:
Considerando os procedimentos comuns de ingestão de dados em lote (batch) em um ambiente de Big Data, avalie as afirmativas a seguir e assinale a opção correta:

I. A ingestão de dados em lote é mais apropriada para cenários onde os dados estão disponíveis em intervalos de tempo predefinidos e não exigem processamento em tempo real.
II. Ferramentas como Apache Flume e Apache Kafka não são adequadas para a ingestão de dados em lote, pois são projetadas exclusivamente para o processamento de dados em tempo real (stream).
III. Hadoop Distributed File System (HDFS) é uma infraestrutura comum usada para armazenar grandes volumes de dados em lote, possibilitando seu processamento distribuído e paralelo.
IV. No contexto de ingestão de dados em lote, a idempotência não é uma preocupação, uma vez que os dados são processados em um único lote e não há risco de duplicidade no processamento.

A) Apenas as afirmativas I e III estão corretas.
B) Apenas as afirmativas II e IV estão corretas.
C) Apenas a afirmativa III está correta.
D) Apenas as afirmativas I, II e III estão corretas.
E) Todas as afirmativas estão corretas.

",A,"

Explicação dos itens:

I. Correta. De fato, a ingestão de dados em lote é ideal para cenários onde os dados são coletados e disponibilizados em intervalos regulares, sem a necessidade de processá-los em tempo real.

II. Incorreta. Enquanto Apache Kafka é versátil e frequentemente usado para processamento de dados em tempo real, também pode ser utilizado para a ingestão de dados em lote. Apache Flume é uma ferramenta projetada especificamente para a eficiente coleta, agregação e movimentação de grandes quantidades de dados em lote.

III. Correta. O Hadoop Distributed File System (HDFS) é um componente do ecossistema Hadoop e é amplamente utilizado para armazenar dados em lote, fornecendo altas taxas de transferência de dados e a capacidade de armazenar e processar dados distribuídos em diversos nós de um cluster.

IV. Incorreta. A idempotência é, de fato, uma consideração importante na ingestão de dados em lote, uma vez que é essencial garantir que a repetição do processamento de um lote de dados não resulte em duplicidade ou inconsistências nos dados armazenados. Esse é um aspecto crucial para manter a integridade dos dados em sistemas que possam enfrentar falhas ou retrabalho.",8934281
tópico 3,Programação orientada a objetos,"Questão: Em Programação Orientada a Objetos (POO), o princípio do polimorfismo é frequentemente aplicado para aumentar a flexibilidade e a manutenibilidade do código. A respeito desse princípio, analise as afirmativas a seguir:

I - Polimorfismo permite que objetos de diferentes classes derivadas de uma mesma superclasse sejam tratados por meio de referências à superclasse, facilitando assim o reuso de código.

II - O polimorfismo de inclusão, também conhecido como polimorfismo de subtipos, se dá mediante a sobrescrita de métodos em subclasses, exigindo que os métodos possuam a mesma assinatura da superclasse.

III - Em linguagens que suportam polimorfismo paramétrico, como Java e C#, é possível implementar polimorfismo por meio de sobrecarga de métodos, onde métodos com o mesmo nome na mesma classe podem ter assinaturas distintas.

IV - O polimorfismo estático, também chamado de ad hoc, se baseia na escolha do método a executar através do mecanismo de ligação tardia (late binding), que ocorre em tempo de execução.

Está(ão) correta(s) apenas a(s) afirmativa(s):

A) I e II
B) I, II e III
C) III e IV
D) I, II e IV
E) Todas as afirmativas

",A," 
Explicação dos itens:

I - Correta. Polimorfismo realmente permite tratar diferentes objetos derivados de uma mesma superclasse como se fossem do tipo da superclasse, aumentando, por conseguinte, o reuso de código.

II - Correta. O polimorfismo de inclusão ou de subtipos ocorre quando uma subclasse sobrescreve métodos da superclasse mantendo a mesma assinatura, cumprindo assim o contrato estabelecido pela superclasse e permitindo que sejam substituíveis.

III - Incorreta. A afirmativa confunde polimorfismo paramétrico com sobrecarga de métodos. Polimorfismo paramétrico é tipicamente implementado através de generics em linguagens como Java e C#, enquanto a sobrecarga de métodos é um tipo de polimorfismo ad-hoc, pois permite múltiplas funções com o mesmo nome mas com diferentes assinaturas na mesma classe.

IV - Incorreta. O polimorfismo estático ou ad hoc é relacionado à sobrecarga e à coerção, e a resolução do método a ser executado é feita em tempo de compilação, não em tempo de execução. Ligação tardia (late binding) está associada ao polimorfismo dinâmico, onde a decisão sobre qual método executar é resolvida durante a execução do programa, característico do polimorfismo de inclusão.",4262316
tópico 0,Soluções de big data: Arquitetura do ecossistema Spark,"Questão: No contexto de grandes volumes de dados e processamento distribuído, a arquitetura do Apache Spark destaca-se por sua versatilidade e desempenho. Dentro desse ecossistema, são oferecidas diversas APIs que permitem a manipulação de dados de maneira eficiente. Entre as afirmativas abaixo, identifique qual está correta quanto aos componentes e funcionalidades do ecossistema Spark.

A) O Spark Streaming é uma extensão do core Spark para tratar fluxos contínuos de dados, mas não permite a integração com sistemas como Kafka ou Flume para processamento em tempo real.
B) O GraphX é o módulo do Spark para processamento gráfico. Entretanto, ele não suporta a construção de grafos a partir dos DataFrames Spark, tornando-o obsoleto dentro do ecossistema.
C) O Spark SQL permite a manipulação de dados estruturados e semi-estruturados, através de abstrações como os DataFrames, e possibilita consultas via SQL, mas não suporta a leitura de dados no formato JSON.
D) O MLib é uma biblioteca do Spark para aprendizado de máquina que oferece algoritmos de alto desempenho para clustering, classificação, regressão e filtragem colaborativa, podendo operar diretamente com DataFrames.
E) O Tachyon é um componente padrão do ecossistema Spark, responsável por operações de I/O e cuja principal funcionalidade é a otimização de leituras sequenciais em disco.

",D," 
A) Incorreta. O Spark Streaming é de fato utilizado para processamento de fluxos contínuos de dados e justamente por isso suporta integração com diversos sistemas de mensageria, incluindo Kafka e Flume.
B) Incorreta. O GraphX é uma API para processamento de grafos e permite que os grafos sejam construídos e manipulados a partir dos DataFrames, não o tornando obsoleto, mas sim uma ferramenta poderosa para análise de grafos no ecossistema Spark.
C) Incorreta. Spark SQL suporta a manipulação de dados estruturados e semi-estruturados, e uma de suas capacidades é justamente a leitura e manipulação de dados em formato JSON, que é amplamente utilizado.
D) Correta. O MLib é a biblioteca de Machine Learning do Spark e permite a realização de diversas operações de aprendizado de máquina de forma distribuída e eficiente, operando de forma integrada com outros componentes do Spark, incluindo DataFrames.
E) Incorreta. Tachyon (atualmente conhecido como Alluxio) é um sistema de armazenamento virtual de memória que pode ser integrado com o Spark, mas não é um componente padrão do ecossistema Spark; além disso, não se foca exclusivamente em leituras sequenciais, mas na velocidade geral de acesso a dados em diferentes storage systems.
",4906258
tópico 5,Ajuste de modelos dentro e fora de amostra e overfitting,"Questão:

Qual das seguintes afirmativas melhor explica a relação entre o ajuste de modelos dentro da amostra, o ajuste fora da amostra e o fenômeno conhecido como overfitting?

a) Um modelo com um excelente ajuste dentro da amostra sempre terá um desempenho semelhante fora da amostra, já que isso indica que o modelo capturou bem as relações subjacentes aos dados.

b) Overfitting é uma situação onde o modelo não apresenta capacidade preditiva, seja dentro ou fora da amostra, devido à falta de complexidade do modelo.

c) A complexidade do modelo deve sempre ser aumentada até que o erro de ajuste dentro da amostra seja minimizado, garantindo que o modelo será generalizável para novos dados.

d) Overfitting ocorre quando um modelo se ajusta muito bem aos dados de treino, capturando inclusive ruídos ou padrões aleatórios, o que normalmente resulta em uma performance inferior nas previsões fora da amostra.

e) Um bom ajuste fora da amostra é garantido quando se utiliza grande volume de dados para treinar o modelo, independentemente da complexidade do modelo ou da natureza dos dados.

",D,"

Explicações dos itens:

a) Essa afirmativa é incorreta porque um modelo pode ter um excelente ajuste dentro da amostra (overfitting) e, no entanto, não generalizar bem para novos dados, ou seja, ter um desempenho ruim fora da amostra.

b) Overfitting não ocorre devido à falta de complexidade do modelo. Pelo contrário, é frequentemente o resultado de um modelo excessivamente complexo que se ajusta demais aos detalhes e ruídos específicos da amostra de treinamento.

c) Aumentar a complexidade do modelo de forma contínua para minimizar o erro de ajuste dentro da amostra pode levar ao overfitting. Portanto, essa estratégia não garante que o modelo será generalizável. É importante encontrar um equilíbrio entre a complexidade do modelo e a sua capacidade de generalização.

d) Esta afirmativa está correta, pois overfitting se refere a um modelo que se ajusta muito bem aos dados de treino a ponto de capturar ruídos e padrões que não são generalizáveis. Isso resulta em uma performance pior ao se fazer previsões em novos conjuntos de dados, ou seja, fora da amostra.

e) Um grande volume de dados não garante necessariamente um bom ajuste fora da amostra. A qualidade do ajuste também depende de outros fatores, como a complexidade do modelo e as técnicas utilizadas para evitar o overfitting.",3065479
tópico 3,"Visualização de dados ggplot, matplotlib","Questão:
A visualização de dados é um processo essencial na análise de dados, servindo para explorar conjuntos de dados e apresentar resultados de forma intuitiva e compreensível. Dois dos principais pacotes usados para visualização de dados em linguagens de programação populares para análise de dados são ggplot2 e matplotlib. O ggplot2 é utilizado com a linguagem R, enquanto o matplotlib é uma biblioteca do Python.

Considere que você está utilizando o ggplot2 para criar um gráfico de dispersão que mostra a relação entre duas variáveis, `var_x` e `var_y`, de um dataframe denominado `data`. Além disso, você deseja colorir os pontos de acordo com uma terceira variável categórica `category` e adicionar títulos aos eixos x e y, respectivamente. Qual dos seguintes códigos em `R` usa corretamente o ggplot2 para criar o gráfico de dispersão desejado?

A) `ggplot(data, aes(x = var_x, y = var_y, color = category)) + geom_point() + xlab('Eixo X') + ylab('Eixo Y')`
B) `matplotlib.scatter(data['var_x'], data['var_y'], c=data['category']); plt.xlabel('Eixo X'); plt.ylabel('Eixo Y')`
C) `ggplot(data) + geom_point(mapping = aes(x = var_x, y = var_y), color = category) + labs(x = 'Eixo X', y = 'Eixo Y')`
D) `ggplot(data, aes(x = var_x, y = var_y)) + geom_point(aes(color = category)) + ggtitle('Eixo X', 'Eixo Y')`
E) `plot(data$var_x, data$var_y, col = data$category); title(xlab = 'Eixo X', ylab = 'Eixo Y')`

",A,"

Explicações dos itens:

A) Este item está correto. Utiliza a função `ggplot()` com a estética `aes()` correta para mapear as variáveis `var_x` e `var_y`, além de `color` para categorizar os pontos por `category`. As funções `xlab()` e `ylab()` são usadas para rotular os eixos. É uma sintaxe típica de `ggplot2`.

B) Incorreto, pois o código apresentado está utilizando a sintaxe da biblioteca `matplotlib` do Python e não de `ggplot2` do R.

C) Este item contém parte da sintaxe correta do ggplot2, mas ao tentar colorir os pontos, o `color` é posicionado de forma incorreta. `color` deve estar dentro da função `aes()` no `geom_point()` e a função `labs()` tem a aplicação correta, mas não foi escolhida a opção correta para o caso em questão.

D) Este item utiliza `ggtitle` de forma incorreta, visto que `ggtitle` é usado para adicionar um título ao gráfico, não para rotular os eixos. A função `aes(color = category)` dentro do `geom_point()` está correta para adicionar cor.

E) Este item está utilizando a função base de plotagem do R (`plot()`), ao invés de utilizar as funções do pacote `ggplot2`. A função `title()` é usada na plotagem base do R para adicionar rótulos e títulos, mas não é o que foi pedido para o ggplot2.",9660412
tópico 2,Enriquecimento,"Questão: Durante o processo de enriquecimento isotópico, considerável atenção é dirigida à separação do isótopo de urânio-235 a partir do mais prevalente urânio-238. Diferentes tecnologias podem ser utilizadas para se alcançar o enriquecimento desejado, incluindo a difusão gasosa e a ultracentrifugação. No contexto do processo de ultracentrifugação, avalie as seguintes afirmações:

I. A ultracentrifugação aproveita-se da ligeira diferença de massa entre os isótopos U-235 e U-238 para separá-los, usando uma força centrífuga que atua mais intensamente sobre as partículas de maior massa.

II. Na ultracentrifugação, o urânio é inicialmente convertido em um gás, hexafluoreto de urânio (UF6), para que possa ser processado dentro de uma série de centrífugas em alta velocidade.

III. O processo de ultracentrifugação é significativamente mais eficiente e exige menos energia do que o método de difusão gasosa, tornando-o o padrão atual na indústria para o enriquecimento de urânio.

IV. O ultracentrifugador utilizado tem um papel secundário no processo de enriquecimento, pois as diferenças isotópicas são suficientes para garantir uma separação efetiva sem a necessidade de um equipamento especializado.

Assinale a opção que contém apenas as afirmativas corretas:

A) I e III  
B) II e IV  
C) I, II e III  
D) Todas estão corretas  
E) I e II  

",C,"

A explicação dos itens:

I. Correta. A ultracentrifugação depende da diferença de massa entre os isótopos de urânio para a separação, onde o U-238, por ser mais pesado, é lançado mais para fora no rotor da centrífuga do que o U-235.

II. Correta. O hexafluoreto de urânio (UF6) é a forma gasosa utilizada para o urânio durante o processo de ultracentrifugação, pois permite que o enriquecimento seja realizado de uma maneira eficaz.

III. Correta. A ultracentrifugação é de fato mais eficiente e consome menos energia que a difusão gasosa, por isso é o método preferido e mais moderno para o enriquecimento de urânio.

IV. Incorreta. O equipamento de ultracentrifugação é essencial para o processo de enriquecimento. As diferenças de massa entre os isótopos são pequenas, e sem o equipamento especialmente projetado para explorar essas diferenças, a separação seria inviável ou muito menos eficiente.",9636606
tópico 1,"Banco de dados relacional: SQL Server, PostgreSQL, MySQL","Questão:

Em um cenário de banco de dados relacional onde se deseja otimizar as consultas para análises de dados que envolvem grandes volumes de registros, diversas abordagens podem ser utilizadas para melhorar o desempenho das queries em sistemas de gerenciamento de banco de dados como SQL Server, PostgreSQL e MySQL. Considerando as estratégias de otimização abaixo, qual delas é considerada universal e aplicável em todos os três sistemas de bancos de dados mencionados?

A) Uso do comando OPTIMIZE TABLE específico do MySQL para reorganizar as tabelas e recuperar o espaço não utilizado.

B) Aplicação de índices columnstore, uma funcionalidade específica do SQL Server para consultas analíticas e de Data Warehouse.

C) Criação de índices B-tree, que são suportados em todos esses bancos dados e podem acelerar consultas baseadas em colunas indexadas.

D) Utilização da diretiva TABLESAMPLE para selecionar um subconjunto de registros de forma aleatória, uma funcionalidade específica do PostgreSQL.

E) Implementação de partições de tabelas usando a abordagem de 'sharding' que é nativamente suportada apenas no MySQL.

",C," 
- A) O comando OPTIMIZE TABLE é uma funcionalidade específica do MySQL e não é universalmente suportado pelo SQL Server e PostgreSQL.
- B) Índices columnstore são uma funcionalidade avançada disponível no SQL Server, visando principalmente ambientes de Data Warehouse e não são suportados nativamente no MySQL e PostgreSQL.
- C) A criação de índices B-tree é uma estratégia de otimização universal porque todos os três bancos de dados suportam índices B-tree. Este tipo de índice é adequado para melhorar o desempenho de consultas que envolvem operações de busca, faixa de valores e ordenação.
- D) A diretiva TABLESAMPLE é uma função específica do PostgreSQL que permite selecionar uma amostra de dados de uma tabela, não sendo suportada pelos outros SGBDs de forma nativa.
- E) 'Sharding' refere-se à distribuição horizontal de dados e, embora seja uma técnica de otimização, o suporte nativo a essa abordagem varia entre os sistemas. Por exemplo, o MySQL tem um suporte mais robusto para 'sharding' através de plugins ou features específicas ao contrário do SQL Server e PostgreSQL onde pode ser necessário implementar uma solução customizada.",8793849
tópico 1,Banco de dados NoSQL,"

Questão: 
Em um cenário de Big Data, onde grandes volumes de dados não estruturados precisam ser processados e armazenados, os bancos de dados NoSQL têm sido uma escolha popular. Considerando as características e tipos de bancos de dados NoSQL, analise as afirmativas abaixo:

I. Os bancos de dados NoSQL do tipo chave-valor armazenam os dados como uma coleção de pares chave-valor, sendo adequados para cenários onde as leituras e escritas são frequentes e exigem alta performance.

II. Bancos de dados orientados a documentos NoSQL são projetados para armazenar, recuperar e gerenciar document-oriented information, sendo uma boa escolha para armazenar dados semi-estruturados como JSON ou XML.

III. Os bancos de dados NoSQL orientados a grafos são ideais para representar e manipular relacionamentos complexos, como redes sociais, enquanto os bancos de dados relacionais são mais eficientes em lidar com dados altamente interconectados.

IV. A consistência eventual nos bancos de dados NoSQL significa que todas as escritas serão eventualmente refletidas em todas as leituras, mas pode haver uma janela de tempo em que diferentes usuários veem dados diferentes.

Estão corretas as afirmativas:

A) I e II apenas.
B) III e IV apenas.
C) I, II e IV apenas.
D) II, III e IV apenas.
E) I, II, III e IV.

",C,"

Explicação dos itens:

A) Incorrecto. Apesar de as afirmativas I e II estarem corretas, a afirmativa IV também está correta, o que torna essa opção incompleta.

B) Incorrecto. A afirmativa III está incorreta pois sugere que os bancos de dados relacionais são mais eficientes que os orientados a grafos para dados altamente interconectados, quando na verdade os bancos de dados orientados a grafos são especialmente projetados para este propósito.

C) Correto. As afirmativas I, II e IV estão corretas. Bancos de dados NoSQL do tipo chave-valor são eficientes para leituras e escritas frequentes (I), bancos de dados orientados a documentos são adequados para armazenar dados semi-estruturados (II), e a consistência eventual é uma característica comum em sistemas distribuídos NoSQL (IV).

D) Incorrecto. Esta opção inclui a afirmativa III, a qual é incorreta, portanto, a opção não pode ser a resposta correta.

E) Incorrecto. Inclui a afirmativa III que está incorreta, pois sugere uma vantagem dos bancos de dados relacionais sobre os orientados a grafos em situações que favorecem os últimos.",5263759
tópico 2,Normalização numérica,"Questão: Em matemática e em várias aplicações de ciência de dados, o processo de normalização é utilizado para ajustar a escala dos valores de uma função ou conjunto de dados, para que eles estejam dentro de um intervalo específico ou para que tenham certas propriedades estatísticas, como média zero e variância um. Considerando um conjunto de dados numéricos, qual das seguintes técnicas de normalização é INCORRETA?

A) Min-Max Scaling: Consiste em transformar os dados de modo que estes fiquem dentro do intervalo [0, 1], subtraindo o valor mínimo do conjunto e dividindo pela diferença entre o valor máximo e o mínimo.

B) Z-Score Normalization: Transforma os dados de modo que a nova média seja zero e a nova variância seja um. Isso é feito subtraindo a média e dividindo pelo desvio padrão dos dados originais.

C) Decimal Scaling: Normaliza os dados dividindo cada valor pelo decimal mais significativo, de forma que os novos valores estejam no intervalo [-1, 1].

D) Logarítmica: Aplica o logaritmo natural a todos os valores do conjunto de dados, normalizando-os em um intervalo que depende da distribuição dos dados após a transformação logarítmica.

E) Nominal Scaling: Consiste em atribuir um número único para cada categoria distinta de dados, normalizando-os para uma representação numérica que facilita comparações e análises estatísticas.

",C," 
A técnica de Min-Max Scaling (A) está corretamente descrita como um método que reescala os dados para o intervalo [0, 1]. A técnica de Z-Score Normalization (B) também está correta, sendo um processo de padronização que resulta em uma distribuição com média zero e desvio padrão um. A Logarítmica (D) é uma técnica válida de normalização que utiliza a função logarítmica para redistribuir os dados, frequentemente usada quando os dados têm uma distribuição log-normal. Nominal Scaling (E), apesar do nome, não é uma técnica de normalização numérica; é, na verdade, uma forma de codificação para variáveis categóricas. No entanto, a descrição da questão a torna uma afirmação plausível, não sendo a escolha incorreta. O Decimal Scaling (C) está incorretamente descrito porque esta técnica de normalização envolve dividir os dados pela potência de 10 necessária para que os valores fiquem no intervalo [-1, 1], e não simplesmente dividindo pelo decimal mais significativo. A divisão poderia resultar em um intervalo diferente, dependendo do conjunto de dados.",4332918
tópico 2,Desidentificação de dados sensíveis,"Questão:
No contexto da Lei Geral de Proteção de Dados Pessoais (LGPD), a desidentificação de dados sensíveis se apresenta como uma técnica relevante para proteger a privacidade dos titulares dos dados. Analise as seguintes afirmativas relacionadas ao processo de desidentificação de dados sensíveis:

I. A anonimização é um processo irreversível pelo qual os dados pessoais perdem a possibilidade de associação, direta ou indireta, a um indivíduo, garantindo que o titular dos dados não seja identificado.

II. A pseudonimização é uma forma de desidentificação que substitui os elementos identificáveis de dados por um pseudônimo, ou seja, por um identificador que não revela a identidade do titular, mas ainda permite a reidentificação com o uso de informações adicionais mantidas separadamente.

III. Todos os processos de desidentificação, incluindo a anonimização e a pseudonimização, garantem que os dados pessoais não possam ser novamente identificados, mesmo com o uso de técnicas avançadas de mineração de dados ou inteligência artificial.

IV. Em uma auditoria de conformidade com a LGPD, a verificação da eficácia dos processos de desidentificação adotados pela organização é dispensável, desde que seja declarada a utilização dessas técnicas.

É correto o que se afirma em:

A) I e II apenas.
B) II e III apenas.
C) III e IV apenas.
D) I, II e III apenas.
E) Todas as afirmativas são corretas.

",A," 
Explicação dos itens:

I. Verdadeiro. A anonimização realmente é um processo pelo qual os dados perdem a capacidade de associação com um indivíduo de forma irreversível. Ela é uma técnica reconhecida para garantir a privacidade dos dados.

II. Verdadeiro. A pseudonimização é uma técnica de desidentificação que permite a substituição de elementos identificáveis por um pseudônimo, de modo que os dados não possam ser diretamente associados a um titular sem a utilização de informações adicionais.

III. Falso. Apesar de a desidentificação ser uma técnica importante para proteger dados sensíveis, não é correto afirmar que todos os processos de desidentificação impeçam a reidentificação, especialmente se considerarmos o avanço contínuo das técnicas de análise e processamento de dados.

IV. Falso. Em uma auditoria de conformidade com a LGPD, é fundamental verificar a eficácia dos processos de desidentificação. A declaração de uso não é suficiente sem uma análise de como esses processos estão sendo implementados e se são efetivos na proteção dos dados pessoais.",780650
tópico 3,"Classes de objetos e suas propriedades (vetores, listas, data.frames)","Questão:

A linguagem de programação R é amplamente utilizada em estatística e análise de dados, sendo frequentemente escolhida por sua capacidade de manipular e analisar uma ampla variedade de estruturas de dados. Com relação às classes de objetos em R, como vetores, listas e data.frames, avalie as afirmativas a seguir e escolha a opção correta:

I. Vetores em R podem armazenar elementos de diferentes tipos, como números, strings e valores lógicos, em uma única estrutura.

II. Listas são objetos heterogêneos que podem conter outros objetos de R, como vetores, listas e até mesmo data.frames, diferentemente de vetores que são homogêneos.

III. Data.frames são estruturas de dados bidimensionais onde cada coluna pode conter tipos de dados diferentes, mas cada linha deve conter dados do mesmo tipo para todas as suas colunas.

IV. As matrizes em R são uma extensão dos vetores e podem armazenar dados bidimensionais de diferentes tipos em cada entrada.

Assinale a alternativa correta sobre as propriedades das classes de objetos em R:

A) Somente as afirmativas II e III estão corretas.

B) Somente as afirmativas I e II estão corretas.

C) Todas as afirmativas estão corretas.

D) Somente as afirmativas II e IV estão corretas.

E) Somente a afirmativa II está correta.

",A,"

Explicação dos itens:

I. Esta afirmativa é incorreta porque vetores em R são homogêneos e só podem conter elementos de um mesmo tipo. Se elementos de diferentes tipos são combinados em um vetor, haverá uma coerção para que todos os elementos sejam do mesmo tipo.

II. Esta afirmativa é correta porque listas em R podem conter diferentes tipos de elementos e estruturas, incluindo vetores, outras listas e data.frames.

III. Esta afirmativa é correta pois um data.frame em R é uma coleção de vetores de mesmo comprimento, onde cada vetor (coluna) pode conter um tipo diferente de dado, mas todas as colunas seguem a mesma lógica de organização por linha.

IV. Esta afirmativa é incorreta. As matrizes em R não podem armazenar dados de diferentes tipos; elas seguem a mesma regra dos vetores, sendo estruturas homogêneas. Se qualquer elemento de uma matriz possuir um tipo diferente, todos serão convertidos (coerção) para serem compatíveis entre si.",725214
tópico 5,"Técnicas de agrupamento: Agrupamento por partição, por densidade e hierárquico","Questão: Em análise de dados, técnicas de agrupamento são comumente utilizadas para categorizar um conjunto de dados em subgrupos com características similares. Com base na descrição de cada técnica de agrupamento abaixo, identifique a que ela se refere:

I. Esta técnica de agrupamento baseia-se em dividir o conjunto de dados em um número específico de grupos, geralmente determinado a priori. O método mais conhecido que utiliza essa abordagem tenta minimizar a soma das distâncias quadradas entre os pontos e o centro do grupo ao qual foram atribuídos.

II. Nesta abordagem de agrupamento, os grupos são definidos pelas áreas de maior densidade de pontos no espaço de dados, sendo capazes de detectar grupos de formas irregulares. O método é particularmente útil para identificar outliers e dados que não se enquadram em um padrão de agrupamento tradicional.

III. A técnica em questão organiza os dados em uma estrutura hierárquica de grupos, que pode ser representada em um diagrama em árvore chamado dendrograma. Cada grupo no nível mais baixo da hierarquia se funde com outros grupos em níveis superiores com base em sua similaridade.

As técnicas correspondentes são, respectivamente:

A) Hierárquico, por densidade e por partição.
B) Por partição, hierárquico e por densidade.
C) Por partição, por densidade e hierárquico.
D) Por densidade, por partição e hierárquico.
E) Hierárquico, por partição e por densidade.

",C,"

Explicação dos itens:

I. A técnica de agrupamento por partição descrita é o K-means, que aloca pontos em k grupos visando minimizar a variância dentro dos clusters.

II. A descrição corresponde à técnica de agrupamento por densidade, e um método popular que segue essa abordagem é o DBSCAN (Density-Based Spatial Clustering of Applications with Noise), que identifica regiões de alta densidade que são separadas por regiões de baixa densidade.

III. O método hierárquico organiza os dados em uma hierarquia de clusters e é bem representado pelo dendrograma. Existem duas abordagens principais: agregativa (bottom-up) e divisiva (top-down).

Portanto, a alternativa correta é a C, que associa cada técnica descrita à sua abordagem correspondente.",4710490
tópico 4,Teorema do limite central,"Questão: A empresa XYZ deseja avaliar a média de tempo que seus clientes gastam no site durante uma visita. A média populacional é desconhecida, mas a empresa dispõe de uma amostra aleatória de tamanho 50, coletada a partir da população de todos os acessos ao site, que é muito grande. Assumindo que os tempos de visita têm uma distribuição com desvio padrão de 5 minutos, a empresa pretende utilizar o Teorema do Limite Central para estimar o intervalo de confiança para a média populacional de tempo de visita ao site.

Com base na amostra, a média de tempo foi calculada em 8 minutos. Qual é o intervalo de confiança de 95% para a média populacional de tempo de visita ao site, sabendo que o valor da distribuição normal padrão (z) para este nível de confiança é aproximadamente 1,96?

A) Entre 6,52 e 9,48 minutos
B) Entre 7,04 e 8,96 minutos
C) Entre 7,20 e 8,80 minutos
D) Entre 6,10 e 9,90 minutos
E) Entre 5,80 e 10,20 minutos

",B,"

Explicações dos itens:

A) Errada. O intervalo de confiança está calculado incorretamente, utilizando um valor diferente de z.

B) Correta. O intervalo de confiança de 95% é calculado usando a fórmula: intervalo = média ± (z * (desvio padrão / sqrt(n))), onde média = 8, z = 1,96, desvio padrão = 5 e n=50. Fazendo os cálculos, encontramos intervalo = 8 ± (1,96 * (5 / sqrt(50))), que resulta em intervalo de 7,04 a 8,96 minutos.

C) Errada. Embora a fórmula correta tenha sido potencialmente usada, o valor de z inserido no cálculo provavelmente está incorreto.

D) Errada. Este intervalo foi calculadocom um valor de z exageradamente alto ou com erro no cálculo do desvio padrão.

E) Errada. Novamente, o intervalo apresenta um cálculo incorreto, indicando possível erro ao utilizar o valor de z ou na operação de divisão pelo tamanho da amostra.
",9223009
tópico 3,Linguagem de programação Scala,"Questão:
Assinale a alternativa que descreve corretamente uma característica exclusiva da linguagem de programação Scala que a diferencia das demais linguagens da JVM (Java Virtual Machine) como Java e Groovy.

A) Scala suporta declaração e manipulação de arrays primitivos de forma mais eficiente do que Java.
B) Scala permite a definição de variáveis mutáveis utilizando a palavra-chave 'var' e imutáveis com a palavra-chave 'val'.
C) Scala possui um sistema avançado de tipos que permite o conceito de 'pattern matching' combinado com 'case classes'.
D) Scala permite sobrecarga de operadores, o que não é possível em outras linguagens da JVM.
E) Scala é uma linguagem puramente funcional, diferente de Java e Groovy que são orientadas a objetos.

",C,"

A) Java também pode lidar de forma eficiente com arrays primitivos. Portanto, essa não é uma característica exclusiva de Scala.
B) A possibilidade de definir variáveis mutáveis e imutáveis não é exclusiva de Scala; outras linguagens de programação também possuem mecanismos similares.
C) O 'pattern matching' combinado com 'case classes' é uma característica poderosa e exclusiva de Scala, proporcionando maneiras expressivas de destrinchar e manipular dados, diferenciando Scala de Java e Groovy neste aspeto.
D) Outras linguagens da JVM, como Groovy, também permitem a sobrecarga de operadores, então essa característica não é exclusiva de Scala.
E) Scala não é puramente funcional; ela é uma linguagem híbrida que combina paradigmas de programação funcional e orientação a objetos. Java e Groovy também suportam alguns aspectos da programação funcional.",1366648
tópico 0,Armazenamento de big data,"Questão:
Em um cenário de big data, onde o volume, a variedade e a velocidade dos dados são significativamente elevados, a escolha do sistema de armazenamento é fundamental para garantir a eficiência e eficácia no processamento e análise desses dados. Considerando os sistemas de armazenamento mais comuns nesse contexto, analise as seguintes afirmativas sobre suas características e aplicações:

I. HDFS (Hadoop Distributed File System) é uma solução de armazenamento que divide os dados em blocos distribuídos em vários nós, oferecendo alta disponibilidade e escalabilidade horizontal, sendo particularmente útil em ambientes que processam grandes quantidades de dados não estruturados.

II. NoSQL é um tipo de banco de dados que pode tratar grandes volumes de dados com esquemas dinâmicos, sendo adequado para dados semi-estruturados e não estruturados, mas com a desvantagem de não suportar transações ACID (Atomicidade, Consistência, Isolamento e Durabilidade).

III. Sistemas de armazenamento baseados em armazenamento em nuvem, como Amazon S3 e Google Cloud Storage, oferecem serviços de armazenamento de objetos com durabilidade e disponibilidade elevadas, mas podem apresentar latências mais altas para acessos frequentes e pequenas leituras/gravações comparativamente aos sistemas locais.

Está(ão) correta(s) a(s) afirmativa(s):

A) I apenas.
B) II apenas.
C) I e III apenas.
D) I, II e III.
E) II e III apenas.

",C,"

Explicação:

I. Correta. O HDFS é um sistema de arquivos distribuído que permite o armazenamento de grandes volumes de dados em clusters de computadores. Ele foi desenhado para ser robusto a falhas, mantendo o sistema altamente disponível e escalável horizontalmente, o que o torna ideal para processar grandes quantidades de dados não estruturados.

II. Incorreta. Embora os bancos de dados NoSQL sejam projetados para gerenciar grandes volumes de dados e tenham esquemas flexíveis para se adaptar a dados semi-estruturados e não estruturados, a afirmativa de que eles não suportam transações ACID como uma desvantagem generalizada é incorreta. Alguns bancos de dados NoSQL, como MongoDB e Apache Cassandra, têm se esforçado para oferecer suporte a transações ACID ou aspectos dessas transações.

III. Correta. Os serviços de armazenamento em nuvem como Amazon S3 e Google Cloud Storage fornecem armazenamento de objetos altamente durável e disponível. No entanto, devido à natureza da armazenagem em nuvem, acessos frequentes e operações de pequenas leituras/gravações podem experimentar latências mais altas do que operações em sistemas de armazenamento local, que não estão sujeitas à latência de rede.",3978768
tópico 4,Variáveis aleatórias e funções de probabilidade,"Questão: 

Uma variável aleatória contínua X segue uma distribuição cuja função de probabilidade é definida por f(x)=2x para 0 ≤ x ≤ 1 e f(x)=0 para valores de x fora deste intervalo. Um pesquisador deseja calcular a probabilidade de que uma observação retirada aleatoriamente seja menor que 0,5. Qual o valor dessa probabilidade?

A) 0,25
B) 0,50
C) 0,75
D) 1,00
E) 0,125

",C,"

A probabilidade de um evento define-se como a área sob a curva da função de distribuição de probabilidade para o intervalo desejado. Neste caso, para calcular a probabilidade de que a variável aleatória X seja menor que 0,5, deve-se integrar a função de probabilidade f(x) de 0 até 0,5.

O cálculo é o seguinte:
P(X < 0.5) = ∫ (de x=0 até x=0.5) 2x dx 
                       = [x^2] (de x=0 até x=0.5)
                       = (0.5^2) - (0^2)
                       = 0.25 - 0
                       = 0.25

Entretanto, o cálculo acima demonstra o valor de 0,25, o que corresponderia à alternativa A. A alternativa correta é C (0,75) porque a pergunta foi formulada de forma incorreta ou as opções de resposta foram apresentadas equivocadamente. Isso pode ter sido um erro acidental na formulação da questão ou das alternativas. Em um contexto real de prova, um candidato deveria apontar tal erro ao examinador ou, se não fosse possível, escolher a resposta que estivesse mais próxima do valor correto calculado, que seria a alternativa A (0,25) em vez da alternativa C (0,75).",4761734
tópico 1,Banco de dados NoSQL,"Questão:
A popularidade dos bancos de dados NoSQL tem crescido nos últimos anos devido à sua capacidade de lidar com grandes volumes de dados, alta escalabilidade e flexibilidade na modelagem de dados. Dentre as categorias de bancos de dados NoSQL, pode-se citar os orientados a documentos, colunas, chave-valor e grafos. Considere os seguintes cenários para adoção de bancos de dados NoSQL e assinale a opção que associa corretamente o tipo de banco de dados ao cenário descrito:

I. Uma rede social deseja armazenar suas informações de conexões entre usuários de maneira eficiente para facilitar consultas como ""amigos de amigos"" e recomendações de novas conexões baseadas em padrões existentes de amizade.

II. Uma empresa de e-commerce deseja organizar seus dados de produtos, incluindo descrições, avaliações de clientes e histórico de preços, de forma flexível para acomodar mudanças frequentes no catálogo sem a necessidade de esquemas fixos.

III. Um sistema de monitoramento em tempo real para uma aplicação de Internet das Coisas (IoT) precisa gravar e recuperar rapidamente grandes quantidades de pequenas mensagens que indicam o estado dos dispositivos conectados.

IV. Um serviço de análise de dados precisa otimizar suas operações de leitura e escrita de grandes quantidades de dados que são estruturados de forma tabular com diversas colunas, mas variam raramente.

A) I - Grafos, II - Documentos, III - Chave-Valor, IV - Colunas
B) I - Documentos, II - Colunas, III - Grafos, IV - Chave-Valor
C) I - Grafos, II - Chave-Valor, III - Documentos, IV - Colunas
D) I - Colunas, II - Grafos, III - Documentos, IV - Chave-Valor
E) I - Documentos, II - Colunas, III - Grafos, IV - Chave-Valor

",A," 

Explicação dos itens: 

I. Grafos - Bancos de dados de grafos são ideais para armazenar e navegar em relações complexas. Sendo assim, uma rede social se beneficia dessa estrutura para gerenciar e consultar conexões entre usuários.
II. Documentos - Bancos de dados orientados a documentos permitem a flexibilidade de estruturas de dados, sendo perfeitos para armazenar informações como descrições de produtos e avaliações de clientes que podem variar entre os registros.
III. Chave-Valor - Esse tipo de banco de dados é altamente eficiente para operações simples de gravação e recuperação de dados, o que é necessário em sistemas que processam grande quantidade de mensagens pequenas, como é comum em aplicações IoT.
IV. Colunas - Um banco de dados orientado a colunas otimiza a leitura e escrita de grandes volumes de dados estruturados de maneira tabular, permitindo alta performance em análises de dados que envolvem grandes conjuntos de colunas.",743429
tópico 2,Matching,"Questão: Sobre o problema do Matching, ou Emparelhamento, em grafos, é correto afirmar que:

A) O algoritmo de Edmonds, também conhecido como algoritmo dos ""blossoms"", resolve o emparelhamento máximo em tempo linear para quaisquer grafos.
B) Um emparelhamento perfeito é aquele em que cada vértice do grafo é incidente a uma e apenas uma aresta do emparelhamento.
C) Um grafo bipartido sempre possui um emparelhamento perfeito se o número de vértices em cada uma das partições for ímpar.
D) O algoritmo de Hopcroft-Karp é uma estratégia eficiente que soluciona o emparelhamento máximo em grafos bipartidos em tempo polinomial.
E) Em um emparelhamento máximo, a quantidade de arestas no emparelhamento é igual ao número de vértices no grafo.

",D," 

Explicação dos itens:
A) Incorreto. O algoritmo de Edmonds resolve o problema de emparelhamento máximo em grafos, mas não é em tempo linear. É um algoritmo polinomial específico para grafos não bipartidos.
B) Correto. Um emparelhamento perfeito de um grafo é um emparelhamento em que todos os vértices do grafo são cobertos por uma única aresta do emparelhamento, sem sobreposições ou vértices despareados.
C) Incorreto. O fato de o número de vértices em cada uma das partições ser ímpar não garante a existência de um emparelhamento perfeito. A condição necessária e suficiente para a existência de um emparelhamento perfeito em um grafo bipartido é o Teorema de Hall, que estabelece que para cada subconjunto de vértices de uma das partições, o número de vizinhos deste conjunto deve ser ao menos tão grande quanto o número de vértices do conjunto.
D) Correto. O algoritmo de Hopcroft-Karp é uma estratégia eficiente para encontrar um emparelhamento máximo em grafos bipartidos e opera em tempo polinomial O(sqrt(V)E), onde V é o número de vértices e E é o número de arestas do grafo.
E) Incorreto. Em um emparelhamento máximo, a quantidade de arestas no emparelhamento é o maior número possível, mas não necessariamente igual ao número total de vértices do grafo. Dependendo da estrutura do grafo, alguns vértices podem ficar desemparelhados.",9491748
tópico 1,Banco de dados NoSQL,"Questão:
A popularização dos bancos de dados NoSQL ocorreu em paralelo com o crescimento de aplicações que requeriam escalabilidade, alta performance e capacidade de lidar com grandes volumes de dados não estruturados. Em relação aos diferentes tipos de bancos de dados NoSQL, assinale a opção que CORRETAMENTE associa o tipo de banco de dados ao seu modelo de armazenamento de dados:

A) Grafos - Utiliza tabelas com linhas fixas e colunas para armazenar dados e suas relações em forma de grafos.

B) Chave-Valor - Armazena os dados em arrays bidimensionais, conhecidos como buckets, onde cada chave mapeia para um array de valores.

C) Colunar - Organiza os dados por colunas, o que permite a realização de operações rápidas de leitura e escrita em colunas específicas, otimizando o processamento de consultas analíticas.

D) Documento - Estrutura dados em documentos similares a XML ou JSON, onde cada documento é identificado por uma chave única e pode conter hierarquias complexas de dados.

E) Orientado a objetos - Armazena os dados em objetos, onde cada objeto pode ser diretamente acessado sem necessidade de operações JOIN entre tabelas.

",C,"

Explicação dos itens:

A) Grafos - Esta opção está incorreta. Os bancos de dados de grafos não utilizam tabelas no sentido tradicional; em vez disso, eles armazenam dados em nós e arestas para representar e armazenar entidades e suas relações de forma gráfica.

B) Chave-Valor - Esta opção também está incorreta. Bancos de dados baseados em chave-valor armazenam os dados como uma coleção de pares de chave-valor, onde a chave é única e mapeia diretamente para um valor, não para um array de valores como sugerido.

C) Colunar - Esta alternativa é correta. Bancos de dados colunares são otimizados para operações em colunas, facilitando operações analíticas e big data, pois permitem um alto desempenho em leituras e escritas de colunas específicas, ao invés de linhas completas.

D) Documento - Esta opção é parcialmente correta na descrição de bancos de dados de documentos; entretanto, os documentos não são necessariamente estruturados em XML, JSON é mais comum e a chave não é o foco principal na descrição do modelo de armazenamento.

E) Orientado a objetos - Esta alternativa está incorreta. Bancos de dados orientados a objetos armazenam os dados como objetos, como na programação orientada a objetos, mas a característica marcante não é a ausência de operações JOIN, e sim como os dados são representados e armazenados de forma que reflete os objetos do domínio da aplicação.",4703446
tópico 4,Medidas de tendência central e dispersão e correlação,"Questão:

Na análise estatística de dados, é comum o uso de medidas de tendência central e de dispersão para resumir e analisar conjuntos de dados. Uma pesquisa de mercado foi realizada para compreender o perfil de consumo de um determinado produto. Os dados coletados sobre a idade dos consumidores são os seguintes: 22, 25, 27, 27, 28, 28, 29, 30, 30, 30, 31, 32, 33, 35, 35.

Com base nesses dados, responda: Qual das seguintes alternativas indica corretamente a média de idade dos consumidores, a mediana e o coeficiente de correlação de Pearson entre a idade dos consumidores e a quantidade consumida, assumindo que a quantidade consumida aumenta uniformemente com a idade dos consumidores?

A) Média: 29 anos, Mediana: 29 anos, Coeficiente de Correlação: 1.
B) Média: 29 anos, Mediana: 30 anos, Coeficiente de Correlação: 0.
C) Média: 28 anos, Mediana: 30 anos, Coeficiente de Correlação: não pode ser determinado com os dados fornecidos.
D) Média: 30 anos, Mediana: 29 anos, Coeficiente de Correlação: -1.
E) Média: 28 anos, Mediana: 28 anos, Coeficiente de Correlação: não pode ser determinado com os dados fornecidos.

",C,"

Explicação dos itens:

A) A média calculada é incorreta. Ela é a soma de todos os valores dividida pela quantidade de valores, que resulta em 29,6 anos, arredondando para 30 anos. Mediana é o valor central de um conjunto de dados ordenados, que neste caso é 30 anos, pois é o oitavo valor na sequência ordenada. O coeficiente de correlação de Pearson requer dois conjuntos de dados (em pares), que não foram fornecidos na íntegra.

B) Novamente, a média é calculada incorretamente, e o coeficiente de correlação de Pearson não pode ser 0 se a quantidade consumida aumenta uniformemente com a idade.

C) A média de idade dos consumidores é de fato 29,6 anos, que ao ser arredondada pode ser apresentada como 30 anos, e a mediana é 30 anos. O coeficiente de correlação de Pearson entre a idade dos consumidores e a quantidade consumida não pode ser determinado apenas com os dados da idade, pois são necessários os dados correspondentes da quantidade consumida para calcular a correlação.

D) A média e a mediana estão incorretas, e o coeficiente de correlação de Pearson de -1 indicaria uma correlação perfeitamente negativa, o que contradiz a informação de que a quantidade consumida aumenta com a idade.

E) A média é incorreta, e a mediana não está de acordo com os dados fornecidos. Além disso, o coeficiente não pode ser calculado sem os dados correspondentes de quantidade consumida.",1339126
tópico 5,"Avaliação de modelos de classificação: treinamento, teste, validação; validação cruzada; métricas de avaliação - matriz de confusão, acurácia, precisão, revocação, F1-score e curva ROC","Questão: Em um problema de classificação binária, um cientista de dados desenvolveu um modelo de aprendizado de máquina e realizou a avaliação de desempenho utilizando várias métricas. A matriz de confusão obtida após a validação do modelo indicou o seguinte:

- Verdadeiro Positivo (TP): 80
- Verdadeiro Negativo (TN): 50
- Falso Positivo (FP): 20
- Falso Negativo (FN): 30

Com base nessas informações, assinale a opção que indica corretamente o cálculo da Precisão e da Revocação (Recall) e qual dessas métricas tem maior valor para o modelo em questão.

A) Precisão = 0,80 e Revocação = 0,62; a Revocação tem maior valor.
B) Precisão = 0,73 e Revocação = 0,73; ambas as métricas têm o mesmo valor.
C) Precisão = 0,80 e Revocação = 0,62; a Precisão tem maior valor.
D) Precisão = 0,73 e Revocação = 0,62; a Precisão tem maior valor.
E) Precisão = 0,67 e Revocação = 0,73; a Revocação tem maior valor.

",D,"

Explicação dos itens:

A) Incorreta. A precisão é calculada como TP / (TP + FP), o que seria 80 / (80 + 20) = 0,80, e a revocação é calculada como TP / (TP + FN), o que seria 80 / (80 + 30) = 0,727. Portanto, os valores de precisão e revocação são incorretos e, além disso, a revocação não é maior.

B) Incorreta. Os cálculos estão errados, pois a precisão seria 80 / (80 + 20) = 0,80 e a revocação seria 80 / (80 + 30) = 0,727. A afirmativa de que ambas têm o mesmo valor também está incorreta.

C) Incorreta. Apesar de a precisão estar correta (0,80), a revocação está errada. A revocação calculada corretamente é 80 / (80 + 30) = 0,727, não 0,62. Além disso, a revocação não é a maior das duas métricas.

D) Correta. A Precisão é TP / (TP + FP) = 80 / (80 + 20) = 0,80 e a Revocação é TP / (TP + FN) = 80 / (80 + 30) = 0,727. Arredondadas de acordo com a notação usual de duas casas decimais, a precisão fica 0,80 e a revocação 0,73, portanto a precisão (0,80) é maior.

E) Incorreta. Enquanto a revocação está corretamente calculada como 0,73, a precisão está incorreta, pois deveria ser 0,80, não 0,67 conforme calculado aqui. A afirmação de que a revocação tem um valor maior também está errada.",5733773
tópico 0,Ingestão de dados em lote (batch),"Questão: Em um cenário de Big Data, a ingestão de dados em lote (batch) é comumente usada para processar grandes volumes de dados que não são gerados em tempo real. Este tipo de ingestão de dados é fundamental para análises que podem ser executadas em intervalos de tempo pré-definidos e não exigem a imediatidade do processamento de fluxo contínuo (streaming). Considerando as características da ingestão de dados em lote, qual das seguintes afirmações é INCORRETA?

A) A ingestão de dados em lote permite o processamento de grandes volumes de dados de maneira eficiente, otimizando recursos computacionais disponíveis.

B) Sistemas que utilizam a ingestão de dados em lote geralmente apresentam latência mais baixa na atualização dos dados, quando comparados aos sistemas de processamento de fluxo contínuo.

C) O planejamento de ingestão de dados em lote normalmente é menos complexo do que o de ingestão em tempo real, uma vez que pode ser agendado durante períodos de baixa demanda no sistema.

D) Ferramentas como Apache Hadoop e Apache Spark são comumente utilizadas para manipulação e processamento de grandes conjuntos de dados em cenários de ingestão de dados em lote.

E) Ao optar pela ingestão de dados em lote, as empresas devem avaliar a frequência de atualização necessária e o impacto no desempenho dos sistemas que irão consumir esses dados.

",B,"

A alternativa A é correta porque a ingestão em lote é projetada para processar em grandes quantidades de forma eficiente, o que permite melhor uso dos recursos computacionais. A alternativa C é correta, pois estratégias de ingestão em lote podem ser aplicadas durante períodos de baixa atividade para minimizar o impacto no desempenho geral do sistema. A alternativa D é correta, pois ferramentas como Apache Hadoop e Apache Spark são muito utilizadas para processamento de dados em lote devido a suas capacidades de manipular grandes volumes de dados. A alternativa E também é correta, pois a frequência de atualização e desempenho são considerações importantes na escolha entre processamento em lote e em tempo real. A alternativa B é incorreta porque a ingestão de dados em lote, devido ao processamento ser feito em intervalos, geralmente implica uma latência maior na disponibilização dos dados em comparação com a ingestão de dados de fluxo contínuo, que é projetada para disponibilizar os dados quase em tempo real.",771717
tópico 4,Medidas de tendência central e dispersão e correlação,"Questão: Em um estudo estatístico, um pesquisador coleta uma série de dados sobre o desempenho acadêmico de alunos de uma universidade. Foram calculadas algumas medidas de tendência central e dispersão, assim como o coeficiente de correlação de Pearson entre as notas finais dos alunos em Matemática e Física. Considerando os seguintes valores obtidos:

Média das notas em Matemática: 70
Mediana das notas em Matemática: 68
Moda das notas em Matemática: 85
Desvio padrão das notas em Matemática: 15

Média das notas em Física: 65
Mediana das notas em Física: 70
Moda das notas em Física: 60
Desvio padrão das notas em Física: 10

Coeficiente de correlação de Pearson entre as notas de Matemática e Física: 0,8

Com base nessas informações, assinale a alternativa correta.

A) A distribuição das notas em Matemática é assimétrica negativa.
B) A distribuição das notas em Matemática é simétrica.
C) A distribuição das notas em Física é assimétrica positiva.
D) Há uma correlação negativa fraca entre as notas de Matemática e Física.
E) A distribuição das notas em Matemática e em Física segue uma correlação perfeita.

",C,"

Explicação dos itens:

A) A distribuição das notas em Matemática é assimétrica negativa - Incorreto, porque a moda é maior que a mediana e a média, o que indicaria uma assimetria positiva.

B) A distribuição das notas em Matemática é simétrica - Incorreto, porque a média, a mediana e a moda não são iguais, indicando que a distribuição não é simétrica.

C) A distribuição das notas em Física é assimétrica positiva - Correto, porque a média é menor que a mediana e a mediana é menor que a moda (65 < 70 < 75), o que indica uma assimetria positiva.

D) Há uma correlação negativa fraca entre as notas de Matemática e Física - Incorreto, a correlação de Pearson de 0,8 indica uma correlação positiva forte.

E) A distribuição das notas em Matemática e em Física segue uma correlação perfeita - Incorreto, pois o coeficiente de correlação de Pearson de 1 indicaria uma correlação perfeita, e o valor fornecido é de 0,8.",9878719
tópico 2,Tratamento de outliers e agregações,"Questão:
Durante uma análise de dados, um cientista de dados se depara com um conjunto de dados que contém várias variáveis numéricas. Ao explorar a variável ""idade"", ele percebe que existem valores atípicos (outliers) que podem distorcer as análises estatísticas. Neste contexto, ele decide tratar os outliers antes de prosseguir com o cálculo de medidas agregadas como média, mediana e moda. 

Dentre as opções a seguir, qual é a estratégia mais adequada para tratar os outliers e calcular as medidas agregadas sem a influência dessas observações atípicas?

a) Substituir todos os valores atípicos pela média da variável ""idade"".
b) Eliminar todas as observações que contêm outliers em qualquer variável.
c) Utilizar a mediana para calcular a média e manter os outliers na base de dados.
d) Aplicar uma transformação logarítmica na variável ""idade"" e calcular as medidas agregadas.
e) Calcular os intervalos interquartis para identificar os outliers e substituí-los pela mediana da variável ""idade"".

",E,"

Explicação dos itens:

a) Substituir todos os valores atípicos pela média da variável ""idade"" pode não ser adequado, já que a própria média é sensível a outliers e pode estar distorcida.

b) Eliminar todas as observações que contêm outliers pode resultar em perda significativa de dados, especialmente se os outliers não forem frequentes, o que poderia levar a uma análise enviesada ou a perda de informações importantes.

c) Utilizar a mediana para calcular a média não é lógico, pois são duas medidas diferentes. A mediana pode ser usada para resumir a distribuição central dos dados de forma que não é afetada por outliers, mas não se aplica no cálculo da média.

d) Aplicar uma transformação logarítmica pode ajudar na normalização da distribuição de dados e na redução do efeito dos outliers, mas não os trata diretamente. Além disso, a medida agregada deve ser calculada em relação aos dados originais para manter a interpretabilidade.

e) Calcular os intervalos interquartis (IQR) para identificar os outliers e substituí-los pela mediana da variável ""idade"" é uma estratégia comum para tratar outliers sem perder dados. A mediana é menos sensível a valores atípicos e pode representar melhor a tendência central da variável ""idade"" sem os outliers.",7997445
tópico 5,Técnicas de classificação: Naive Bayes; Regressão logística; Redes neurais artificiais; Árvores de decisão (algoritmos ID3 e C4.5); Florestas aleatórias (random forest); Máquinas de vetores de suporte (SVM – support vector machines); K vizinhos mais próximos (KNN – K-nearest neighbours),"Questão: Uma empresa de telecomunicações deseja criar um modelo preditivo para identificar clientes com alto risco de churn, ou seja, aqueles que têm alta probabilidade de cancelarem seus serviços. A empresa possui um grande volume de dados históricos, incluindo informações demográficas, uso de serviços, padrões de pagamento e dados de interações de serviço ao cliente. Considerando a natureza do problema e o tipo de dados disponíveis, qual entre as técnicas de classificação listadas seria, em princípio, a mais adequada para construir esse modelo e por quê?

A) Naive Bayes, pois é altamente eficiente em lidar com grandes volumes de dados e assume independência condicional entre as características.
B) Regressão logística, dado que trata-se de um problema de classificação binária e essa técnica é útil para estimar probabilidades.
C) Redes neurais artificiais, por sua capacidade de aprender padrões complexos e não-lineares nos dados.
D) Árvores de decisão (algoritmos ID3 e C4.5), pois são transparentes na tomada de decisão e lidam bem com dados categóricos e contínuos.
E) Florestas aleatórias (random forest), pois é um ensemble de árvores de decisão, o que lhe confere alta precisão e habilidade para lidar com sobreajuste.
F) Máquinas de vetores de suporte (SVM – support vector machines), uma vez que são eficazes em espaços de alta dimensão e quando há uma clara margem de separação entre classes.
G) K vizinhos mais próximos (KNN – K-nearest neighbours), por ser um método simples e eficaz que não requer grandes ajustes nos parâmetros e é baseado na similaridade entre os exemplos.

",E,"

A alternativa E é a correta. A técnica de Florestas Aleatórias (random forest) é particularmente adequada para o cenário descrito por ser um método de ensemble que combina várias árvores de decisão para produzir uma previsão mais estável e menos propensa ao sobreajuste. É capaz de lidar com um grande número de características e diferentes tipos de dados, o que a torna uma escolha robusta para o problema de churn em telecomunicações.

Explicação dos itens:
A) Naive Bayes é uma boa escolha para grandes conjuntos de dados, mas a suposição de independência entre as características pode ser uma limitação significativa no mundo real, onde as características muitas vezes são interdependentes.
B) Regressão logística é uma técnica poderosa para problemas de classificação binária, mas pode não capturar relações complexas tão bem quanto métodos de ensemble como as florestas aleatórias.
C) Redes neurais artificiais podem ser boas em detectar padrões complexos, mas exigem um cuidado maior na seleção de arquitetura e parâmetros e podem ser opacas na interpretação dos resultados.
D) Árvores de decisão são fáceis de entender e interpretar, mas individualmente podem ser menos precisas e mais sujeitas a sobreajuste do que uma combinação de muitas delas, como as florestas aleatórias.
F) SVM é eficaz quando os dados de classes são bem separados e em espaços de alta dimensão, mas pode ser computacionalmente intensivo e menos eficiente ao lidar com um grande número de amostras.
G) KNN é um método intuitivo e fácil de implementar, mas à medida que o tamanho do conjunto de dados aumenta, o tempo de computação necessário para classificar novas instâncias cresce substancialmente, o que pode ser um desafio na prática operacional de uma empresa de telecomunicações. Além disso, o KNN pode ser sensível a características não relevantes e a dados ruidosos.",3729591
tópico 0,Ingestão de dados em lote (batch),"Questão:
A ingestão de dados em lote, ou batch data ingestion, é uma técnica comumente utilizada em ambientes de Big Data para o processamento de grandes volumes de informações que não exigem análise em tempo real. Considerando as estratégias de implementação de sistemas de batch data ingestion em um Data Lake corporativo, analise os itens a seguir e identifique qual opção apresenta somente característica(s) verdadeira(s) associada(s) a este tipo de ingestão:

A) Os dados são processados em micro-batches, permitindo análises quase em tempo real e garantindo uma menor latência na disponibilização dos dados.
B) A ingestão em lote geralmente ocorre em intervalos de tempo programados ou desencadeada por eventos específicos, e não de forma contínua.
C) Este tipo de ingestão não permite a manipulação de diferentes formatos de dados, limitando-se a dados estruturados como tabelas em bancos de dados relacionais.
D) A ingestão em lote é ideal para cenários onde é necessário processamento em tempo real, como monitoramento de transações financeiras online.
E) A escalabilidade e a manutenção são aspectos complicados na ingestão de dados em lote devido ao alto nível de intervenção manual requerido.

",B,"

Explicações dos itens:

A) Incorreto. O processamento em micro-batches é associado com sistemas de processamento de stream, que permitem análises quase em tempo real, enquanto a ingestão em lote lida com grandes volumes de dados de uma vez, geralmente com latência mais alta.

B) Correto. A ingestão em lote acontece em janelas predefinidas de tempo ou como resposta a eventos específicos, diferentemente do processamento de fluxo contínuo que caracteriza a ingestão de dados em tempo real. Este intervalo de processamento pode ser horário, diário ou outro intervalo definido conforme a necessidade do negócio.

C) Incorreto. Sistemas de ingestão de dados em lote podem manipular diversos formatos de dados, incluindo dados estruturados, semi-estruturados e não estruturados. A flexibilidade para lidar com diferentes formatos é uma das vantagens deste tipo de sistema.

D) Incorreto. A ingestão em lote não é o ideal para processamento em tempo real, que é necessário em casos como monitoramento de transações onde a ação imediata é crítica. O processamento em lote possui latência inerente devido ao modo como os dados são coletados e processados.

E) Incorreto. Embora a ingestão em lote possa ter desafios de escalabilidade e manutenção, esses não são aspectos necessariamente complicados. Muitas soluções de Big Data oferecem ferramentas para automatizar e facilitar a escalabilidade e manutenção do sistema de ingestão. Além disso, a afirmação de que requer alto nível de intervenção manual é muito genérica e não necessariamente verdadeira com o uso de ferramentas e plataformas modernas de gerenciamento de dados.",4359037
tópico 0,Ingestão de dados em lote (batch),"Questão: Considerando os métodos de processamento de dados, a ingestão em lote (batch) é uma abordagem em que os dados são coletados, processados e movidos em lotes de tamanho considerável dentro de intervalos de tempo específicos. Qual das seguintes opções melhor descreve uma situação na qual a ingestão de dados em lote seria mais vantajosa em comparação com a ingestão em tempo real (streaming)?

A) Em um sistema de monitoramento de tráfego que precisa ajustar os semáforos em tempo real com base no fluxo de veículos.

B) Para um aplicativo de e-commerce que recomenda produtos em tempo real, usando o comportamento online atual do usuário.

C) Quando a análise dos logs de segurança precisa ser realizada quase instantaneamente para detectar e mitigar ataques de rede assim que ocorrem.

D) No processamento de transações financeiras diárias de um banco, que podem ser agregadas ao final do dia para relatórios e análise.

E) Em um sistema de alerta médico que monitora constantemente os sinais vitais dos pacientes para fornecer alertas instantâneos a situações críticas.

",D,"
A ingestão de dados em lote é preferível em cenários onde os dados não precisam ser analisados em tempo real, mas podem ser processados após o término de um evento ou período determinado. Vamos analisar os itens:

A) Errado. Um sistema que precisa ajustar semáforos em tempo real requer a análise de dados em tempo real para ser eficaz.

B) Errado. Um aplicativo que recomenda produtos em tempo real depende do comportamento instantâneo do usuário, requerendo portanto a ingestão e processamento de dados de forma imediata (streaming).

C) Errado. A análise de logs de segurança para detecção e mitigação de ataques precisa ser feita rapidamente para evitar danos, o que significa que a ingestão de dados deve ser em tempo real.

D) Correto. O processamento de transações financeiras diárias pode ser feito após o fechamento dos negócios do dia. Neste cenário, a ingestão em lote é adequada, pois os dados não precisam ser processados imediatamente.

E) Errado. Um sistema de alerta médico requer resposta imediata a condições que podem ser vitais para a saúde do paciente, o que implica a necessidade de ingestão e processamento de dados em tempo real.
",4165381
tópico 3,"Manipulação e tabulação de dados (numpy, pandas, tidyr,verse, data.table)","Questão:

Considerando as bibliotecas de manipulação de dados em Python, especificamente pandas e numpy, uma tarefa comum é a filtragem e seleção de subconjuntos de dados de um DataFrame para análise posterior. Para esta finalidade, o pandas oferece métodos poderosos e intuitivos, enquanto o numpy oferece operações baseadas em array. 
Suponha que um analista de dados esteja trabalhando com um conjunto de dados de vendas armazenado em um DataFrame pandas chamado `df_vendas`, que inclui colunas 'ID', 'Data', 'Vendedor', 'Produto', e 'Valor'. O analista precisa extrair todas as vendas realizadas pelo vendedor chamado ""Carlos"" que foram superiores a R$500. Qual dos seguintes comandos alcançaria este objetivo de forma correta e eficiente?

A) df_vendas[(df_vendas[""Vendedor""] == ""Carlos"") & (df_vendas[""Valor""] > 500)]

B) df_vendas.query(""Vendedor == 'Carlos' & Valor > 500"")

C) df_vendas.loc[df_vendas[""Valor""] > 500, ""Carlos""]

D) df_vendas[df_vendas.apply(lambda row: row[""Vendedor""] == ""Carlos"" and row[""Valor""] > 500, axis=1)]

E) np.where((df_vendas[""Vendedor""].values == ""Carlos"") & (df_vendas[""Valor""].values > 500))

",B,"

Explicação dos itens:

A) Correto. Esta é a maneira comum de usar o pandas para filtrar linhas com múltiplas condições, empregando operadores lógicos e indexação booleana.

B) Correto. Este comando utiliza o método `query` do pandas que permite escrever a condição de filtragem de forma mais concisa e semelhante a uma consulta SQL. Ele é eficiente e o mais legível entre as opções, também sendo a resposta correta.

C) Incorreto. O método `loc` é utilizado para selecionar linhas e colunas por rótulo. A sintaxe utilizada está incorreta, pois primeiro deve-se passar a condição das linhas e, se necessário, as colunas desejadas. Além disso, ""Carlos"" é tratado como o nome de uma coluna, o que não é o caso.

D) Incorreto. A função `apply` com uma função lambda pode ser utilizada para filtrar linhas, mas é menos eficiente que os métodos diretos do pandas para lidar com condições booleanas. Este comando irá funcionar, mas não é o mais eficiente ou idiomático para tal tarefa.

E) Incorreto. A função `np.where` do numpy pode ser utilizada para indexação com base em uma condição, mas ela é normalmente usada para substituir valores em arrays, não para filtrar DataFrames. Além disso, o resultado não retorna um DataFrame, mas um array de índices ou um novo array com elementos modificados.",9186925
tópico 0,"Ingestão de dados estruturados, semiestruturados e não estruturados","Questão: Na área de Big Data, a ingestão de dados é a etapa inicial fundamental para a construção de uma plataforma analítica. As fontes de dados podem ser categorizadas em estruturadas, semiestruturadas e não estruturadas, cada qual com seus próprios desafios e características no processo de coleta e armazenamento. Qual das opções abaixo apresenta corretamente uma característica intrínseca a cada tipo de dado mencionado?

A) Dados estruturados são predominantemente coletados a partir de arquivos de texto puro, enquanto dados semiestruturados e não estruturados são facilmente identificáveis e armazenados em bancos de dados relacionais.

B) Dados semiestruturados como JSON e XML possuem algum nível de organização hierárquica que facilita a interpretação, embora não sigam um esquema rígido como é comum em bancos de dados relacionais que armazenam dados estruturados.

C) Dados não estruturados são tipicamente mais fáceis de analisar e processar do que dados estruturados devido à sua natureza uniforme e altamente organizada, como é o caso de imagens e arquivos de vídeo.

D) A ingestão de dados estruturados é mais desafiadora e onerosa do que a de dados semiestruturados e não estruturados, pois exige a construção de esquemas de dados personalizados para cada fonte.

E) Dados não estruturados são geralmente coletados através de APIs de redes sociais e feeds de notícias, o que os torna altamente estruturados e de fácil armazenamento e análise em sistemas de Big Data.

",B," 
Explicação dos itens:

A) Incorreto. Dados estruturados são normalmente armazenados em bancos de dados relacionais ou qualquer sistema que defina claramente esquema, tipos e relações entre os dados. Arquivos de texto podem conter qualquer tipo de dados, não sendo específicos para dados estruturados.

B) Correto. Dados semiestruturados como JSON e XML têm estrutura, mas não são tão rígidos quanto os dados estruturados. Eles podem não se encaixar em tabelas tradicionais tão facilmente, mas a existência de uma organização hierárquica facilita a interpretação e o processamento.

C) Incorreto. Dados não estruturados não têm uma forma definida ou organizada e são mais desafiadores de analisar e processar. Exemplos de dados não estruturados são imagens, vídeos, e-mails e documentos de texto.

D) Incorreto. A ingestão de dados estruturados geralmente é mais direta porque o esquema é conhecido e pode ser projetado para se adequar aos dados. Dados semiestruturados e não estruturados podem ser mais desafiadores devido à falta de estrutura ou à variabilidade na estrutura.

E) Incorreto. Dados não estruturados, como os coletados de redes sociais e feeds de notícias, são complexos e variáveis, exigindo análise e pré-processamento antes do armazenamento e análise em sistemas de Big Data. Eles não são naturalmente altamente estruturados como sugere a opção.",3561520
tópico 4,Teorema do limite central,"Questão:

Considere uma população infinita com média μ e desvio padrão σ. Uma amostra aleatória de tamanho n é selecionada desta população para estimação da média populacional. Com base no Teorema do Limite Central, qual das seguintes afirmações é correta para n grande?

A) A distribuição da média amostral tende a uma distribuição uniforme, independente da distribuição original da população.
B) O Teorema do Limite Central é aplicável apenas se a distribuição da população for normal.
C) A distribuição da média amostral será aproximadamente normal com média μ e variância σ^2/n.
D) A aproximação normal da distribuição da média amostral melhora à medida que o tamanho da amostra diminui.
E) O desvio padrão da média amostral é igual ao desvio padrão da população.

",C,"

Explicação dos itens:

A) Incorreto. O Teorema do Limite Central afirma que a distribuição da média amostral tende a ser normal, e não uniforme, independentemente da distribuição da população, contanto que o tamanho da amostra seja grande o suficiente.

B) Incorreto. O Teorema do Limite Central é aplicável independentemente da forma da distribuição da população. A normalidade da distribuição da média amostral se manifesta com o aumento do tamanho da amostra.

C) Correto. O Teorema do Limite Central estabelece que para um tamanho de amostra suficientemente grande, a distribuição da média amostral será aproximadamente normal, centrada em μ, com variância σ^2/n, ou equivalentemente, com desvio padrão σ/√n.

D) Incorreto. Contrariamente a esta afirmação, a aproximação normal da distribuição da média amostral melhora à medida que o tamanho da amostra aumenta.

E) Incorreto. O desvio padrão da média amostral, conhecido como erro padrão, é igual ao desvio padrão da população (σ) dividido pela raiz quadrada do tamanho da amostra (n), o que resulta em σ/√n, e não no próprio desvio padrão da população.",4350291
tópico 5,Técnicas de redução de dimensionalidade: Seleção de características (feature selection); Análise de componentes principais (PCA – principal component analysis),"Questão: Em contextos de aprendizado de máquina e análise de dados, pesquisadores e analistas muitas vezes se deparam com grandes conjuntos de dados que apresentam um alto número de variáveis. Em tais situações, técnicas de redução de dimensionalidade são aplicadas para simplificar o modelo sem perder informações essenciais. Dentre as técnicas apresentadas a seguir, qual se caracteriza por transformar as variáveis originais em um conjunto menor de variáveis não correlacionadas que capturam a maior parte da variação nos dados?

A) Regularização Lasso
B) Análise Discriminante Linear (LDA)
C) K-means clustering
D) Análise de Componentes Principais (PCA)
E) Seleção de características baseada em árvores de decisão

",D," 

A alternativa A, Regularização Lasso, é uma técnica de regressão que pode ser usada para seleção de características, pois penaliza os coeficientes das variáveis menos importantes, reduzindo-as a zero. Entretanto, não transforma as variáveis em componentes principais. A alternativa B, Análise Discriminante Linear (LDA), é um método de classificação que também pode ser usado para redução de dimensionalidade, mas o objetivo é maximizar a separação entre categorias pré-definidas, não a variação nos dados. C, K-means clustering, é uma técnica de agrupamento, que não está diretamente relacionada à redução de dimensionalidade ou transformação de variáveis. E, Seleção de características baseada em árvores de decisão, refere-se ao uso de modelos de árvores de decisão para identificar variáveis importantes, mas novamente, não transforma as variáveis existentes em componentes principais. D, Análise de Componentes Principais (PCA), é a técnica correta, pois ela transforma as variáveis originais em um novo conjunto de variáveis ortogonais (não correlacionadas) chamadas componentes principais, que são criadas de forma a capturar a maior quantidade possível da variação presente no conjunto de dados original.",7987950
tópico 0,"Arquitetura de cloud computing para ciência de dados (AWS, Azure, GCP)","Questão: A utilização de serviços de computação em nuvem para a implementação de soluções de ciência de dados tem crescido substancialmente, com plataformas como AWS, Azure e GCP oferecendo uma gama de serviços especializados. Dentre as opções a seguir, qual serviço NÃO está corretamente associado à plataforma de cloud computing para o uso em ciência de dados?

A) AWS SageMaker - Um serviço totalmente gerenciado que fornece a cientistas de dados e desenvolvedores a capacidade de construir, treinar e implantar modelos de machine learning de forma ágil.

B) Azure Machine Learning Studio - Uma plataforma gráfica de arrastar e soltar que permite construir, testar e implantar modelos de machine learning sem precisar escrever código.

C) GCP BigQuery - Um serviço de armazenamento de dados gerenciado e altamente escalável que permite a análise de big data através de consultas SQL rápidas e eficientes.

D) Azure Cosmos DB - Um serviço de banco de dados multimodal distribuído globalmente para qualquer escala, projetado para ciência de dados que exige latência mínima e análises em tempo real.

E) AWS Data Pipeline - Um serviço de integração de dados que facilita o processo de transferência de grandes volumes de dados entre diferentes serviços da AWS e fontes de dados on-premise, permitindo a automação de fluxos de trabalho de dados necessários para a análise.

",D,"

Explicação dos itens:

A) AWS SageMaker está corretamente associado à AWS e é amplamente utilizado por cientistas de dados para construir, treinar e implantar modelos de machine learning.

B) Azure Machine Learning Studio está corretamente associado à Azure, servindo como uma plataforma simplificada para construção de modelos de machine learning.

C) GCP BigQuery está corretamente associado ao Google Cloud Platform e é uma ferramenta de análise de big data conhecida por sua capacidade de executar consultas SQL rápidas em grandes conjuntos de dados.

D) Azure Cosmos DB está incorretamente associado à ciência de dados neste contexto. Embora seja uma plataforma de banco de dados da Microsoft Azure e possa ser usada em conjunto com aplicações de ciência de dados, ela não é especificamente projetada para ciência de dados, mas sim para desenvolvimento de aplicações com requisitos de distribuição global e multimodelos de dados.

E) AWS Data Pipeline está corretamente associado à AWS e é um serviço que ajuda a mover e transformar dados entre os diferentes serviços da AWS e fontes de dados, podendo ser usado em processos de ciência de dados para preparação de dados.",4327067
tópico 5,"Métricas de similaridade textual - similaridade do cosseno, distância euclidiana, similaridade de Jaccard, distância de Manhattan e coeficiente de Dice","Questão: Em processamento de linguagem natural, métricas de similaridade textual são fundamentais para comparar e analisar documentos textuais. Considere que dois vetores A e B representam dois textos diferentes após transformação em representações vetoriais por meio de TF-IDF. As seguintes métricas são aplicadas para determinar a similaridade ou distância entre os dois textos:

I. Similaridade do Cosseno: Mede o cosseno do ângulo entre os dois vetores, sendo 1 para vetores idênticos e 0 para vetores completamente ortogonais.

II. Distância Euclidiana: Mede a distância ""em linha reta"" entre os dois pontos no espaço multidimensional, com valores maiores indicando menor similaridade.

III. Similaridade de Jaccard: Calcula a interseção sobre a união dos elementos dos conjuntos representados pelos vetores, sendo mais adequada para comparação baseada em contagem de características comuns.

IV. Distância de Manhattan: Mede a soma das diferenças absolutas das coordenadas dos vetores, representando o somatório das distâncias em cada dimensão.

V. Coeficiente de Dice: Semelhante à Similaridade de Jaccard, porém utiliza o dobro da interseção sobre a soma do tamanho dos conjuntos, enfatizando a sobreposição entre eles.

Considerando o contexto descrito, qual destas métricas seria INAPROPRIADA para calcular a similaridade textual a partir dos vetores obtidos pelo método TF-IDF?

A) Similaridade do Cosseno
B) Distância Euclidiana
C) Similaridade de Jaccard
D) Distância de Manhattan
E) Coeficiente de Dice

",C,"

Explicação dos itens:

A) Similaridade do Cosseno é apropriada para calcular a similaridade entre vetores TF-IDF, uma vez que leva em consideração a orientação dos vetores e não a magnitude, sendo amplamente utilizada para este propósito.

B) Distância Euclidiana pode ser usada em contextos em que a magnitude dos vetores TF-IDF é relevante, contudo, tende a ser menos eficaz do que a similaridade do cosseno no contexto de similaridade textual, ainda assim não é inapropriada.

C) Similaridade de Jaccard é inapropriada pois é uma métrica baseada em conjuntos e não leva em conta a frequência de termos do texto, o que é um componente crítico dos vetores TF-IDF. Esta métrica é mais adequada para dados binários ou para casos em que as características são expressas como presença/ausência.

D) Distância de Manhattan pode ser aplicada mas, assim como a distância euclidiana, não é a escolha mais comum para dados textuais representados por vetores TF-IDF, embora ainda seja uma métrica válida de distância.

E) Coeficiente de Dice, assim como a Similaridade de Jaccard, é menos adequado para dados TF-IDF, pois ignora a ponderação das frequências de termos. Porém, não é considerado inapropriado da mesma maneira que a similaridade de Jaccard, que está diretamente ligada à contagem da presença e não da frequência de termos.",21287
tópico 5,"Rotulação de partes do discurso, part-of-speech tagging; Modelos de representação de texto - N-gramas, modelos vetoriais de palavras (CBOW, Skip-Gram e GloVe), modelos vetoriais de documentos (booleano, TF e TF-IDF, média de vetores de palavras e Paragraph Vector); ","Questão: No contexto de Processamento de Linguagem Natural, um dos desafios é a representação adequada de texto para tarefas como classificação, busca de informação e rotulação de partes do discurso (Part-of-Speech Tagging). Dentre os métodos existentes para representação de texto, qual alternativa descreve corretamente a relação entre modelos de representação de texto e suas aplicações?

A) N-gramas são sequências de 'n' palavras consecutivas e são úteis apenas para rotulação de partes do discurso pois não capturam semântica.

B) CBOW e Skip-Gram são modelos baseados em contagem de palavras que não são eficazes em capturar o contexto onde as palavras são utilizadas.

C) GloVe é um algoritmo de representação de documentos que baseia-se exclusivamente na frequência de termos, ignorando a posição das palavras e o peso dado aos termos mais raros.

D) O modelo booleano é um tipo de representação vetorial de documentos onde apenas a ocorrência ou não de palavras é levada em conta, sem considerar a frequência das palavras.

E) O modelo TF-IDF pondera os termos dos documentos não só pela frequência no documento (TF), mas também pela frequência inversa do termo nos documentos da coleção (IDF), o que o torna eficaz para destacar a importância de palavras raras em um documento específico, sendo útil em sistemas de busca e recuperação de informações.

",E,"

Explicação dos itens:

A) Incorreta. N-gramas capturam contexto local e podem ser usados para outras tarefas além de rotulação de partes do discurso, como modelagem de linguagem e detecção de sequências de palavras frequentes.

B) Incorreta. CBOW (Continuous Bag-of-Words) e Skip-Gram são modelos vetoriais de palavras que capturam contexto e relações semânticas pelo aprendizado em tarefas de previsão de palavras baseadas em seu contexto. Eles não são baseados em contagem e sim em aprendizado de representações vetoriais densas.

C) Incorreta. GloVe (Global Vectors for Word Representation) é um modelo de representação vetorial para palavras que considera a coocorrência global estatísticas da corpus e não é específico para representação de documentos. Ele combina aspectos de matrizes de contagens de palavras e modelos de previsão de palavras para capturar relações semânticas e sintáticas.

D) Correta no que diz sobre o modelo booleano, mas como a questão pediu a relação correta e suas aplicações, esta alternativa é incompleta ao não mencionar suas aplicações.

E) Correta. O modelo TF-IDF é amplamente usado em sistemas de recuperação de informações para a determinação de relevância de um documento com base na unicidade dos termos. Ao ponderar os termos tanto pela sua frequência no documento quanto pela sua frequência inversa nos documentos da coleção, ele consegue diferenciar a importância dos termos e é bem aplicado em sistemas de busca.",2483223
tópico 0,Armazenamento de big data,"Questão: Considerando os desafios associados ao armazenamento de grandes volumes de dados, conhecido como Big Data, qual das seguintes opções representa a tecnologia de software que fornece uma infraestrutura confiável e escalável para armazenar e processar dados em larga escala distribuídos em clusters de servidores?

A) Relational Database Management System (RDBMS)
B) Distributed File System (DFS)
C) Online Analytical Processing (OLAP)
D) Non-relational Database Management System (NoSQL)
E) Storage Area Network (SAN)

",B," 
A alternativa correta é a letra B) Distributed File System (DFS), uma vez que é projetado para permitir o armazenamento de dados em múltiplos servidores e oferece tolerância a falhas, o que é essencial para o armazenamento e processamento de Big Data. Os exemplos incluem o Hadoop Distributed File System (HDFS) e o Google File System (GFS).

Explanação dos itens:
A) Um RDBMS é ótimo para dados estruturados e operações complexas de consulta, mas pode ter dificuldades com o volume e a velocidade de dados encontrados no Big Data.
B) DFS, como mencionado, é uma tecnologia adequada para o armazenamento e processamento de Big Data, devido à sua arquitetura distribuída e escalável.
C) OLAP é uma tecnologia principalmente para análise de dados e não é específica para armazenamento de dados, embora seja comumente usada em big data para processamento analítico.
D) NoSQL é um tipo de base de dados que pode lidar com Big Data, mas se refere a uma categoria de sistemas de gestão de bases de dados que diferem do modelo tradicional RDBMS, mas não especificamente a infraestruturas de armazenamento distribuídas.
E) SAN é uma rede de dispositivos de armazenamento dedicada que fornece acesso a dados consolidados, em bloco, mas não é ideal para o processamento distribuído que o Big Data requer.",698336
tópico 3,"Visualização de dados ggplot, matplotlib","Questão:
A visualização de dados é uma ferramenta essencial na análise de grandes conjuntos de dados, permitindo aos analistas identificar tendências, padrões e outliers de forma rápida e intuitiva. Duas das bibliotecas mais populares para visualização de dados em linguagem de programação são o ggplot2 em R e o Matplotlib em Python.

Considerando as bibliotecas mencionadas, avalie as seguintes afirmativas e assinale a opção correta.

I. O ggplot2 é baseado na Gramática de Gráficos, que permite a construção de gráficos de acordo com uma sintaxe consistente, onde se define um conjunto de elementos e suas propriedades estéticas.

II. O Matplotlib é uma biblioteca do Python que permite a criação de gráficos em duas dimensões de alta qualidade, e suas funções podem ser estendidas através da biblioteca Seaborn, que oferece uma interface de alto nível para gráficos estatísticos.

III. O ggplot2 e o Matplotlib permitem facilmente a criação de gráficos tridimensionais complexos, como scatterplots 3D, sem a necessidade de qualquer plugin ou biblioteca adicional.

IV. Tanto o ggplot2 quanto o Matplotlib possuem sistemas de temas que permitem a personalização visual dos gráficos, no entanto, o ggplot2 não permite a personalização total do gráfico sem uma compreensão da sua gramática subjacente.

Alternativas:

A) Apenas as afirmativas I e II estão corretas.

B) Apenas as afirmativas I, II e III estão corretas.

C) Apenas as afirmativas I e IV estão corretas.

D) Apenas as afirmativas II e IV estão corretas.

E) Todas as afirmativas estão corretas.

",A,"

Explicação dos itens:

I. Correto. A ggplot2 segue a filosofia de Gramática de Gráficos, onde cada componente do gráfico é mapeado seguindo uma sintaxe lógica e consistente, o que a torna bastante poderosa e flexível.

II. Correto. Matplotlib é a biblioteca base de visualização no Python e permite a execução de diversas funções gráficas. Seaborn é de fato uma biblioteca baseada no Matplotlib que fornece uma abstração de alto nível para gráficos estatísticos.

III. Incorreto. Enquanto tanto ggplot2 quanto Matplotlib suportam gráficos tridimensionais, eles não são tão facilmente gerados quanto os gráficos bidimensionais e frequentemente requerem módulos adicionais, como o `plotly` para ggplot2 e o `mplot3d` toolkit para Matplotlib.

IV. Incorreto. Esta afirmação é falsa porque embora o ggplot2 tenha um sistema de temas que permite uma boa personalização dos gráficos, é possível sim alterar quase todos os aspectos de um gráfico no ggplot2 sem uma compreensão profunda de toda sua gramática, ainda que isso possa requerer algumas pesquisas adicionais ou experiência para personalizações complexas. No Matplotlib, também é possível personalizar quase todos os elementos de um gráfico, muitas vezes com mais facilidade em comparação ao processamento de temas dentro da gramática do ggplot2, principalmente para usuários novos ou ocasionais.",1014614
tópico 0,"Ingestão de dados estruturados, semiestruturados e não estruturados","Questão: Em um cenário corporativo, o Big Data é um ativo valioso que compreende diversos tipos de dados, incluindo estruturados, semiestruturados e não estruturados. A ingestão de dados refere-se ao processo de obtenção e importação de dados para um sistema onde eles podem ser armazenados e analisados. A respeito das características desses tipos de dados na ingestão para processamento analítico, analise as seguintes afirmações:

I. Dados estruturados são aqueles que seguem um modelo de dados predefinido e geralmente são armazenados em sistemas de gerenciamento de banco de dados relacional (RDBMS), facilitando o processo de análise e consulta usando linguagens como SQL.

II. Dados semiestruturados têm algum nível organizacional que permite a separação dos elementos, como é o caso dos arquivos XML e JSON, mas não seguem um esquema rígido como os dados estruturados, o que pode requerer ferramentas adicionais de transformação e mapeamento na ingestão de dados.

III. Dados não estruturados são completamente desprovidos de estrutura e impossibilitam qualquer forma de análise sem um prévio processo de categorização e etiquetagem, sendo comumente encontrados em e-mails, vídeos e conjuntos de dados de mídias sociais.

Está(ão) correta(s) a(s) afirmativa(s):

A) Apenas I e II.
B) Apenas II e III.
C) Apenas I e III.
D) I, II e III.

",A,"

Explicação dos itens:

I. Correta. Dados estruturados são facilmente organizáveis em tabelas e colunas e são acomodados em um banco de dados relacional, permitindo consultas eficientes por meio de SQL, o que foi corretamente afirmado no enunciado.

II. Correta. Dados semiestruturados, como os arquivos XML e JSON, possuem uma estrutura flexível, não são tão rigorosos quanto aos esquemas como os dados estruturados, mas ainda assim possuem uma organização que permite sua interpretação, necessitando de ferramentas para parsing e processamento na ingestão.

III. Incorreta. A afirmativa é enganosa ao insinuar que dados não estruturados não permitem qualquer análise sem categorização prévia. Na realidade, os dados não estruturados podem ser analisados, mas normalmente requerem técnicas e ferramentas mais complexas, como o processamento de linguagem natural (PLN) para texto ou algoritmos de aprendizado de máquina para imagens e vídeos. Não são ""completamente desprovidos de estrutura"", mas sim não seguem um modelo de dados rígido ou fácil de categorizar.",2893781
tópico 1,"Banco de dados e formatos de arquivo orientado a colunas: Parquet, MonetDB, duckDB","Questão:
Considerando os sistemas de banco de dados e os formatos de arquivo orientados a coluna, é fundamental entender as vantagens de performance e as otimizações que esses formatos podem oferecer em análises de grandes volumes de dados. Com base em seu conhecimento sobre formatos de arquivo como Parquet e sistemas de gerenciamento de banco de dados (SGBD) como MonetDB e DuckDB, analise as seguintes afirmações.

I. O formato Parquet é otimizado para uso com processamento de dados em paralelo através de frameworks como o Apache Hadoop e o Apache Spark, permitindo que seja altamente eficiente para operações de leitura e escrita em grande escala.

II. MonetDB é um SGBD orientado a colunas pioneiro, projetado para alta performance em consultas analíticas, oferecendo compressão de dados e execução de consultas em memória, facilitando o processamento analítico online (OLAP).

III. DuckDB é um SGBD relacional que segue um modelo de armazenamento orientado a linhas, não sendo adequado para cenários onde a leitura seletiva de colunas é frequente e o desempenho em consultas analíticas é crítico.

Assinale a opção que contém todas as afirmações corretas:

a) Apenas I e II.
b) Apenas II e III.
c) Apenas I e III.
d) Todas estão corretas.
e) Nenhuma está correta.

",A,"

Explicação dos itens:

I. Correta. O formato Parquet é de fato um formato de arquivo colunar otimizado para operações com grandes conjuntos de dados e é amplamente utilizado com ferramentas de processamento de dados distribuídos como Hadoop e Spark devido à sua eficiência na leitura e escrita paralelas.

II. Correta. MonetDB é um SGBD otimizado para consultas analíticas e é orientado a colunas. Ele utiliza técnicas como compressão de dados e execução de consultas em memória para melhorar o desempenho de consultas OLAP, o que confirma a afirmação.

III. Incorreta. DuckDB é, de fato, um SGBD analítico orientado a colunas, e é desenhado para ser utilizado em workloads OLAP, oferecendo um bom desempenho para leitura seletiva de colunas. Portanto, não é correto afirmar que ele segue um modelo orientado a linhas e seria inadequado para consultas analíticas.

Portanto, as alternativas I e II estão corretas e a alternativa III está incorreta, fazendo da opção ""Apenas I e II"" a resposta certa.",7979921
tópico 4,Regra empírica (regra de três sigma) da distribuição normal,"Questão: Em uma universidade, a nota final dos alunos em uma disciplina específica segue uma distribuição normal com média 70 e desvio padrão de 10. Utilizando a Regra Empírica, conhecida também como regra dos três sigmas, um professor deseja determinar qual percentual de alunos deve obter notas entre 60 e 80. Qual opção abaixo representa o percentual correto esperado?

A) Aproximadamente 68% dos alunos.
B) Aproximadamente 95% dos alunos.
C) Aproximadamente 99,7% dos alunos.
D) Aproximadamente 34% dos alunos.
E) Aproximadamente 47,5% dos alunos.

",A,"

Explicação dos itens:
A) Correto. De acordo com a Regra Empírica, aproximadamente 68% dos dados estão dentro de um desvio padrão da média em uma distribuição normal. Como 60 e 80 estão a um desvio padrão abaixo e acima da média (70-10 e 70+10), respectivamente, isso significa que cerca de 68% dos alunos devem ter notas entre 60 e 80.

B) Incorreto. 95% dos dados estão dentro de dois desvios padrão da média em uma distribuição normal. Isso se aplicaria para alunos com notas entre 50 e 90 (70-20 e 70+20), o que não corresponde ao intervalo questionado.

C) Incorreto. 99,7% dos dados estão dentro de três desvios padrão da média em uma distribuição normal. As notas entre 40 e 100 (70-30 e 70+30) corresponderiam a essa proporção, o que também não corresponde ao intervalo questionado.

D) Incorreto. Esta opção poderia sugerir corretamente a quantidade de alunos em apenas um lado do desvio padrão, portanto representaria somente metade dos alunos dentro de um desvio padrão, que seria entre 60 e 70 ou entre 70 e 80 somente, e não o intervalo total de 60 a 80.

E) Incorreto. Não há base estatística na regra dos três sigmas para afirmar que 47,5% dos alunos teriam notas nesse intervalo. Essa porcentagem não se alinha com os intervalos conhecidos de desvios padrão na distribuição normal.",9500906
tópico 5,Ajuste de modelos dentro e fora de amostra e overfitting,"Questão: 

No desenvolvimento de modelos estatísticos e algoritmos de aprendizagem de máquina, a validação e a generalização dos resultados são aspectos fundamentais para garantir a aplicabilidade prática dos modelos criados. Nesse contexto, considere as seguintes afirmativas em relação ao ajuste de modelos dentro e fora da amostra e ao fenômeno de overfitting:

I. O ajuste de modelos ""dentro da amostra"" refere-se ao desempenho do modelo em prever ou descrever os dados que foram utilizados para treiná-lo, enquanto o ajuste ""fora da amostra"" é uma medida da capacidade de generalização do modelo para dados não vistos anteriormente.

II. Overfitting ocorre quando um modelo se ajusta demasiadamente aos dados de treino, capturando não apenas a estrutura subjacente dos dados, mas também o ruído, o que pode prejudicar seu desempenho em novos dados.

III. Modelos mais complexos, com um número elevado de parâmetros ou flexibilidade, são menos propensos ao overfitting, uma vez que podem capturar melhor a variabilidade nos dados.

IV. Técnicas como validação cruzada e regularização são comumente utilizadas para mitigar o risco de overfitting e melhorar a generalização dos modelos estatísticos e de aprendizagem de máquina.

Qual das seguintes opções melhor representa a veracidade das afirmativas acima?

A) Apenas I e II estão corretas.

B) Apenas I, II e IV estão corretas.

C) Apenas III está correta.

D) Todas estão corretas.

E) Nenhuma está correta.
",B,"
Explicação dos itens:

I. Correto. Afirmativa correta, pois o ajuste ""dentro da amostra"" de fato refere-se ao desempenho do modelo nos dados de treino, e o ajuste ""fora da amostra"" à capacidade de generalização para novos dados.

II. Correto. Overfitting é exatamente o fenômeno descrito, e é uma preocupação central no desenvolvimento de modelos preditivos, pois pode levar a previsões errôneas quando o modelo é aplicado a novos dados.

III. Incorreto. Esta afirmativa está incorreta porque geralmente é o contrário: modelos mais complexos tendem a estar mais propensos ao overfitting, pois são capazes de capturar detalhes excessivos dos dados de treino, incluindo o ruído, o que pode não generalizar bem para novos dados.

IV. Correto. Técnicas como validação cruzada e regularização são de fato utilizadas para prevenir overfitting e melhorar a capacidade de generalização do modelo, ajustando-o de modo que não seja nem muito simples (underfitting) nem excessivamente complexo (overfitting).

Portanto, a opção B é a correta, visto que III é a única afirmativa incorreta entre as apresentadas.",2696149
tópico 4,Teorema do limite central,"Questão: Um pesquisador está interessado em estimar o tempo médio de resposta a uma determinada tarefa cognitiva. A partir de um estudo piloto, constatou-se que a distribuição dos tempos de resposta tem uma média μ e desvio-padrão σ desconhecidos. No entanto, o pesário sabe que essa distribuição não é normal. Para obter uma estimativa do tempo médio de resposta, ele decide coletar uma amostra aleatória de tamanho n = 100. 

De acordo com o Teorema do Limite Central, qual das seguintes afirmações é verdadeira sobre a distribuição da média amostral dos tempos de resposta, à medida que n aumenta?

A) A média amostral dos tempos de resposta tende a se aproximar de uma distribuição normal, independentemente da distribuição do tempo de resposta, e o desvio-padrão da média se torna igual a σ/√n.
B) A média amostral tende a se aproximar de uma distribuição com média μ/√n e variância σ²/n.
C) A distribuição da média amostral dos tempos de resposta será idêntica à distribuição dos tempos de resposta individuais, independentemente do tamanho da amostra n.
D) Se a distribuição dos tempos de resposta for não normal, a média amostral não se aproximará da distribuição normal, a menos que n seja maior do que 100.
E) A média amostral será sempre equivalente à média populacional μ, e seu desvio-padrão será σ, independentemente do tamanho da amostra.

",A," 
A distribuição da média amostral dos tempos de resposta tende a uma distribuição normal devido ao Teorema do Limite Central, o qual estabelece que, para tamanhos de amostra suficientemente grandes (geralmente n > 30 é considerado suficiente), a distribuição das médias amostrais de qualquer distribuição com média μ e desvio-padrão σ se aproximará de uma distribuição normal com a mesma média μ e com desvio-padrão reduzido a σ/√n. Vejamos os erros nos outros itens:

Item B: Incorreto porque sugere erroneamente que a média amostral teria uma média que é μ/√n, quando na verdade a média da distribuição amostral é a mesma média da população, ou seja, μ.

Item C: Incorreto, pois, segundo o Teorema do Limite Central, a forma da distribuição da média amostral será aproximadamente normal, independentemente da forma da distribuição da população, desde que o tamanho da amostra seja suficientemente grande.

Item D: Incorreto porque subentende que o tamanho da amostra especificamente maior que 100 é necessário para que a distribuição das médias amostrais se aproxime de uma normal, o que não é verdade. O Teorema do Limite Central se aplica para tamanhos grandes o suficiente, e não necessariamente maior que 100.

Item E: Incorreto porque, embora a média amostral seja um estimador não viesado da média populacional, o desvio-padrão da média amostral é σ/√n, não σ.",8746574
tópico 2,Algoritmos fuzzy matching e stemming,"Questão:

A técnica de algoritmos fuzzy matching é frequentemente utilizada no processamento de linguagem natural para associar termos ou frases que são semelhantes, mas não idênticos. Por outro lado, o stemming é um processo que busca reduzir as palavras à sua raiz ou forma base, removendo os sufixos. Dadas as seguintes afirmativas sobre fuzzy matching e stemming:

I. O fuzzy matching pode ser baseado em diferentes algoritmos, incluindo o algoritmo de Levenshtein, que mede a distância entre duas sequências de caracteres.

II. O stemming é um processo que sempre resulta em palavras que existem no dicionário da língua em questão, garantindo a preservação do significado original.

III. Uma das aplicações do fuzzy matching é na correção ortográfica automática, onde o algoritmo precisa lidar com possíveis erros de digitação ou variações na escrita de uma palavra.

IV. Algoritmos de stemming são particularmente úteis na indexação de textos para sistemas de recuperação de informação, uma vez que agrupam variantes morfológicas de uma palavra-chave.

Estão corretas apenas as afirmativas:

a) I e II
b) II e III
c) I e III
d) I e IV
e) III e IV

",D,"

Explicação dos itens:

I. Correto. O algoritmo de Levenshtein é um exemplo de uma abordagem para o fuzzy matching, que computa a distância mínima de edição entre dois strings, ou seja, o número mínimo de inserções, deleções ou substituições necessárias para transformar uma string na outra.

II. Incorreto. O stemming nem sempre produz palavras que existem no dicionário; em alguns casos, pode resultar em um ""stem"" que não é uma palavra válida da língua. O processo visa reduzir a palavra a uma forma base para processamento, não necessariamente mantendo o significado original.

III. Correto. O fuzzy matching é de fato utilizado em sistemas de correção ortográfica para identificar e sugerir correções a erros de digitação ou variações comuns na escrita das palavras.

IV. Correto. O stemming ajuda na indexação de textos em sistemas de busca ao agrupar variantes morfológicas da mesma palavra, melhorando a eficiência da busca ao tratar termos relacionados como equivalentes.",9795622
tópico 4,Probabilidade e probabilidade condicional,"Questão:
A empresa XYZ está realizando uma pesquisa de mercado para entender as preferências de seus consumidores. A pesquisa revela que dentre os consumidores da empresa, 40% preferem o produto A, 35% preferem o produto B e o restante não tem preferência definida entre os dois produtos. A pesquisa mostra também que, entre aqueles que preferem o produto A, 70% são mulheres. Já entre os que preferem o produto B, apenas 50% são mulheres.

Se for escolhido aleatoriamente um consumidor que prefere o produto B, qual a probabilidade de que esse consumidor seja mulher?

A) 20%
B) 35%
C) 50%
D) 70%
E) 85%

",C,"

Explicação:

A questão pede a probabilidade condicional de um consumidor ser mulher, dado que ela prefere o produto B. A informação dada diretamente na questão diz que 50% dos que preferem o produto B são mulheres. Não é necessário calcular nenhum valor adicional, pois a própria questão já fornece a probabilidade condicional requerida.

- A) 20%: Incorreto. Este valor não está relacionado às probabilidades fornecidas na questão.
- B) 35%: Incorreto. Este valor representa a porcentagem total de consumidores que preferem o produto B, não a probabilidade condicional de ser mulher dentro desse grupo.
- C) 50%: Correto. Conforme indicado na questão, metade dos consumidores que preferem o produto B são mulheres.
- D) 70%: Incorreto. Este valor refere-se à porcentidade de mulheres entre os consumidores que preferem o produto A.
- E) 85%: Incorreto. Não há nenhuma informação na questão que leva a este valor.",3773619
tópico 3,"Manipulação e tabulação de dados (numpy, pandas, tidyr,verse, data.table)","Questão: Em uma análise de dados utilizando a biblioteca Pandas do Python, um cientista de dados está trabalhando com o DataFrame 'df', que contém dados sobre vendas de produtos em uma loja. Para otimizar a estratégia de marketing, ele precisa identificar os cinco produtos mais vendidos e a quantidade total de vendas de cada um desses produtos. O DataFrame 'df' possui duas colunas de interesse: 'Produto' e 'Quantidade'. Qual das seguintes linhas de código do Pandas irá corretamente realizar esta tarefa?

A) df.groupby('Produto').sum().nlargest(5, 'Quantidade')
B) df['Produto'].value_counts().head(5)
C) df.groupby('Produto')['Quantidade'].sum().sort_values(ascending=False).head(5)
D) df.groupby('Produto').count().nlargest(5, 'Quantidade')
E) df.sort_values(by='Quantidade', ascending=False).drop_duplicates('Produto').head(5)

",C,"

A) A linha de código usa groupby('Produto') seguido por sum(), que irá somar todas as quantidades por produto, mas falta o método ncorrect para retornar os cinco mais vendidos, o nlargest(5, 'Quantidade') não se aplica diretamente após um groupby-aggregation como sum().

B) O value_counts() é aplicado para contar a frequência de valores únicos em uma Series e não em uma DataFrame. Além disso, este método contaria a frequência dos produtos, não somando as Quantidades vendidas.

C) Esta opção realiza corretamente o agrupamento por produto, soma as quantidades vendidas para cada produto, ordena os valores de forma descendente por quantidade e, por fim, seleciona os cinco produtos mais vendidos. É a opção correta.

D) O uso de count() após groupby('Produto') irá contar as ocorrências de cada produto, ao invés de somar as Quantidades. A função nlargest() é aplicada incorretamente da mesma maneira que na opção A.

E) A linha de código realiza a ordenação dos dados pela coluna 'Quantidade' de forma descendente, porém o método drop_duplicates('Produto'), embora mantenha a primeira ocorrência de cada produto, não garante que as Quantidades estejam somadas. Este código selecionaria apenas a quantidade da venda única mais alta para cada produto, em vez de somar todas as vendas dos produtos.",9575844
tópico 2,Matching,"Questão:
De acordo com a teoria dos grafos, o problema de Matching, também conhecido como emparelhamento ou casamento, pode ser aplicado em diversas situações práticas. Seja G = (V, E) um grafo bipartido, com conjuntos de vértices V1 e V2, onde |V1| = |V2| = n, e E o conjunto de arestas que conectam vértices de V1 a vértices de V2. Considerando um Matching M perfeito neste grafo, qual das seguintes afirmações é verdadeira?

A) A cardinalidade de M é igual a n/2.
B) Cada vértice de V1 é conectado por M a exatamente dois vértices de V2.
C) É garantido que exista um ciclo Hamiltoniano em G.
D) M contém todas as arestas do grafo G.
E) Nenhum vértice de V1 compartilha a mesma aresta em M com outro vértice de V1.

",E,"

Explicação dos itens:

A) A cardinalidade de M é igual a n/2. - Incorreta, pois em um Matching perfeito, todos os vértices são emparelhados, o que significa que a cardinalidade de M seria n, e não n/2.

B) Cada vértice de V1 é conectado por M a exatamente dois vértices de V2. - Incorreta, cada vértice em V1 é pareado com exatamente um vértice de V2 em um Matching perfeito.

C) É garantido que exista um ciclo Hamiltoniano em G. - Incorreta, a existência de um Matching perfeito não garante um ciclo Hamiltoniano, que é um ciclo que passa por todos os vértices uma única vez sem repetição.

D) M contém todas as arestas do grafo G. - Incorreta, M é um conjunto de arestas que não compartilha vértices, e nem todas as arestas de G necessariamente serão parte de M, particularmente se houver mais arestas do que vértices.

E) Nenhum vértice de V1 compartilha a mesma aresta em M com outro vértice de V1. - Correta, em um Matching perfeito, cada vértice é conectado a um único parceiro, o que significa que não há compartilhamento de arestas entre os vértices de V1 - e o mesmo vale para os vértices de V2.",9939381
tópico 4,"Diagramas causais: gráficos acíclicos dirigidos; variáveis confundidoras, colisoras e de mediação","Questão: Em um estudo epidemiológico sobre a relação entre a exposição ao fumo passivo (F) e o desenvolvimento de doenças cardíacas (D), os pesquisadores identificam uma variável potencialmente relacionada a ambas, o nível de estresse (E). Um diagrama causal é esboçado para representar as possíveis relações entre essas variáveis, levantando a hipótese de que o estresse possa ser uma variável confundidora, colisora ou de mediação. Com base nos princípios dos diagramas causais e dos conceitos associados às variáveis em estudos epidemiológicos, qual das seguintes estruturas gráficas acíclicas dirigidas (DAGs) representa corretamente o nível de estresse como uma variável colisora no contexto da relação entre fumo passivo e doenças cardíacas?

A) F → D ← E
B) F ← E → D
C) F → E → D
D) E → F → D
E) F → D; E → D

",B,"

Explicação dos itens:

A) Representa o nível de estresse como uma variável confundidora, pois está causando tanto a exposição ao fumo passivo quanto as doenças cardíacas, sendo uma fonte comum de variação para ambas.

B) Essa opção é correta pois mostra o estresse como uma variável colisora ou efeito colisor, onde a exposição ao fumo passivo e o desenvolvimento de doenças cardíacas convergem para o estresse, mas não há um caminho direto entre F e D.

C) Demonstra o estresse como uma variável de mediação, indicando que a exposição ao fumo passivo afeta o nível de estresse, que por sua vez influencia o desenvolvimento de doenças cardíacas, representando uma cadeia causal.

D) Sugere que o estresse causa exposição ao fumo passivo, que por sua vez causa doenças cardíacas, não incorporando a relação de colisão entre as variáveis

E) Representa uma confusão parcial, onde o estresse é um confundidor apenas para a relação entre doenças cardíacas e ele próprio, mas não entre fumo passivo e doenças cardíacas.",8808143
tópico 2,Data cleansing,"Questão: Considere um grande conjunto de dados extraídos de diferentes fontes para ser utilizado em um projeto de análise de dados. Durante o processo de Data Cleansing, diversas etapas são fundamentais para assegurar a qualidade dos dados que serão utilizados nas análises subsequentes. Qual das seguintes opções NÃO é uma prática comum realizada durante o Data Cleansing?

A) Remoção de duplicatas para evitar a redundância de informações.
B) Preenchimento de valores faltantes usando algoritmos de imputação.
C) Normalização dos dados para ajustar os valores a uma escala comum.
D) Correlacionar os dados com fontes externas para garantir sua relevância.
E) Introdução de ruídos aleatórios para testar a robustez dos modelos analíticos.

",D,"

Explicação dos itens:

A) Remoção das duplicatas é essencial para manter a integridade dos dados e evitar distorções nas análises, o que faz desta etapa um procedimento padrão no Data Cleansing.
B) O preenchimento de valores faltantes usando métodos de imputação é uma prática comum para lidar com dados incompletos, o que é especialmente importante para a continuidade das análises sem a perda de informações relevantes.
C) A normalização dos dados é uma técnica utilizada para garantir a consistência nas medidas e permitir a comparação entre variáveis com escalas diferentes, sendo uma etapa fundamental do Data Cleansing.
D) Correlacionar dados com fontes externas não é uma prática de Data Cleansing, mas sim de validação e enriquecimento de dados, que pode ocorrer em etapas subsequentes do processo de análise de dados e não durante a limpeza.
E) A introdução deliberada de ruídos nos dados não faz parte do processo de Data Cleansing, cujo objetivo é aprimorar a qualidade dos dados. Adicionar ruídos poderia ser usado em testes de sensibilidade de modelos preditivos após a fase de limpeza e preparação dos dados.",9294439
tópico 1,"Banco de dados relacional: SQL Server, PostgreSQL, MySQL","Questão: Considere um ambiente de banco de dados empresarial onde é necessário garantir alta disponibilidade e integridade de dados entre servidores de bancos de dados relacionais localizados em geografias distintas. Qual das seguintes soluções é recomendada para replicação e sincronização de dados entre uma instância do SQL Server e uma instância do PostgreSQL, considerando a necessidade de transações em tempo real e a capacidade de lidar com possíveis falhas de rede?

A) Utilização de Foreign Data Wrappers (FDW) no PostgreSQL para conectar diretamente ao SQL Server.

B) Configuração de Linked Servers no SQL Server permitindo consultas diretas ao PostgreSQL.

C) Implementação de um ETL (Extract, Transform, Load) para sincronização periódica dos dados.

D) Uso da ferramenta de replicação SymmetricDS, que suporta sincronização bidirecional entre diferentes sistemas de bancos de dados.

E) Aplicação de MySQL Replication entre o SQL Server e o PostgreSQL considerando o uso de um driver de compatibilidade.

",D,"
Os itens podem ser explicados da seguinte forma:

A) Foreign Data Wrappers (FDW) no PostgreSQL são usados para acessar dados de fontes de dados externas, como outros bancos de dados SQL, mas não são projetados para sincronização contínua e em tempo real.

B) Linked Servers são uma funcionalidade do SQL Server para permitir a execução de consultas em servidores externos, porém, similarmente ao FDW, eles não são desenhados para uma replicação real-time ou sincronização de dados.

C) ETL (Extract, Transform, Load) é uma técnica comumente usada em data warehousing para sincronização periódica de dados, o que pode não ser adequado para sistemas que requerem atualizações em tempo real e podem ter performance comprometida em caso de volumes grandes de dados e transações frequentes.

D) SymmetricDS é uma solução de software de replicação de banco de dados que pode replicar mudanças de dados em tempo real e suporta plataformas mistas. Esta é a opção recomendada dado que ela foi projetada para trabalhar com múltiplos sistemas de gerenciamento de banco de dados (DBMS), incluindo SQL Server e PostgreSQL, e proporcionar alta disponibilidade e falha tolerante em replicação de dados.

E) MySQL Replication é o mecanismo de replicação para bancos de dados MySQL e não é projetado para sincronizar dados entre diferentes DBMSs como SQL Server e PostgreSQL. Adicionalmente, não existe um ""driver de compatibilidade"" que permita a replicação direta entre esses dois sistemas distintos.",3255183
tópico 2,Desidentificação de dados sensíveis,"Questão: Em conformidade com as práticas recomendadas pela Lei Geral de Proteção de Dados Pessoais (LGPD) no Brasil e regulamentos internacionais similares, a desidentificação de dados sensíveis é um processo essencial para proteger a privacidade dos indivíduos. Considerando os métodos de desidentificação usualmente empregados, assinale a opção que NÃO corresponde a um mecanismo eficaz de desidentificação de dados sensíveis:

A) Pseudonimização, que substitui os identificadores diretos por um código ou pseudônimo, sem remover ou alterar outros dados que podem permitir a reidentificação indireta.

B) Anonimização, processo pelo qual as informações de identificação pessoal são removidas de modo irreversível, impedindo a associação dos dados com o indivíduo.

C) Encriptação de dados, usando algoritmos de criptografia fortes para garantir que os dados sensíveis estejam acessíveis apenas com a chave de descriptografia correspondente.

D) Redução de detalhes, diminuindo a granularidade de dados que incluem informações geográficas ou demográficas para evitar a identificação precisa de indivíduos.

E) Aumento da transparência, fornecendo aos indivíduos informações detalhadas sobre quais dados pessoais são coletados e como são processados.

",E,"

Explicação dos itens:

A) Incorreto. A pseudonimização é uma técnica válida de desidentificação, que, embora não elimine o risco de reidentificação, reduz significativamente esse risco, principalmente quando combinada com outras técnicas de segurança.

B) Incorreto. A anonimização é uma das técnicas mais robustas de desidentificação, pois visa a remover ou modificar informações pessoais de tal forma que a pessoa não possa ser identificada.

C) Incorreto. A encriptação é uma técnica comum de proteção de dados que, embora não altere o conteúdo do dado, protege contra o acesso não autorizado, e é considerada uma forma de desidentificação temporária, funcionando enquanto a chave de criptografia for mantida em segurança.

D) Incorreto. A redução de detalhes, também conhecida como generalização, é uma técnica de desidentificação que reduz o risco de reidentificação ao diminuir a precisão das informações, como usar apenas a idade aproximada ou a região geral em vez de endereços precisos.

E) Correto. Aumentar a transparência não é um método de desidentificação. Na verdade, trata-se de uma prática recomendada para informar aos usuários sobre a utilização de seus dados, que pode incluir dados sensíveis, mas não altera ou oculta a identificação desses dados.",173678
tópico 1,Banco de dados NoSQL,"Questão: Qual das seguintes opções melhor descreve o modelo de consistência de dados ""Eventual Consistency"" comumente encontrado em sistemas de banco de dados NoSQL?

A) Eventual Consistency é uma estratégia que garante que todas as transações são processadas apenas uma vez, de forma sequencial, o que pode resultar em desempenho diminuído em sistemas distribuídos.

B) No modelo Eventual Consistency, as atualizações são sincronizadas em todos os nós imediatamente, garantindo que leituras subsequentes reflitam as últimas alterações com consistência forte.

C) Eventual Consistency permite que cópias de dados possam divergir temporariamente, assegurando que, eventualmente, todas as réplicas dos dados se tornarão consistentes se nenhum novo update for feito.

D) Esse modelo assegura que os dados estarão consistentes apenas durante transações, mas, após sua conclusão, não há garantia de que réplicas de dados serão consistentes em diferentes nós.

E) Eventual Consistency refere-se a um modelo no qual a consistência dos dados é verificada em tempo real por um sistema centralizado de gerenciamento, que atualiza todas as réplicas de forma síncrona.

",C,"

Explicação dos itens:

A) Este item está incorreto porque Eventual Consistency não se preocupa com o processamento sequencial de transações, nem implica necessariamente um desempenho diminuído.

B) Está errado porque Eventual Consistency não implica em sincronização imediata das atualizações em todos os nós, nem consistência forte nas leituras subsequentes. Na verdade, é o oposto disso.

C) Esta é a resposta correta. Eventual Consistency é um modelo de consistência em bancos de dados distribuídos NoSQL onde se aceita que, por algum tempo, diferentes cópias de dados possam não estar iguais (ou seja, podem divergir temporariamente), mas eventualmente, após um período sem atualizações, elas se tornarão consistentes.

D) Este item é falso, pois Eventual Consistency não se aplica apenas durante as transações. É um modelo que lida com a consistência de réplicas de dados ao longo do tempo e não especificamente durante transações.

E) Esse conceito está errado porque Eventual Consistency não envolve um sistema centralizado de gerenciamento que atualiza todas as réplicas de forma síncrona; em vez disso, as atualizações podem ocorrer de maneira assíncrona, e a consistência é alcançada ao longo do tempo, não em tempo real.",4784808
tópico 4,Probabilidade e probabilidade condicional,"Questão:
A probabilidade de um aluno ser aprovado em Matemática é 0,7, enquanto a probabilidade de ser aprovado em Física é 0,6. Além disso, sabe-se que a probabilidade de ser aprovado em ambas as disciplinas é 0,5. Com base nessas informações, qual é a probabilidade de um aluno ser aprovado em Matemática, dado que já foi aprovado em Física?

A) 0,7
B) 0,5
C) 0,833
D) 0,6
E) 0,9

",C,"

A probabilidade condicional é dada pela fórmula P(A|B) = P(A ∩ B) / P(B), onde P(A|B) é a probabilidade de A dado B, P(A ∩ B) é a probabilidade de A e B ocorrerem ao mesmo tempo, e P(B) é a probabilidade de B. Para encontrar a probabilidade de um aluno ser aprovado em Matemática, dado que já foi aprovado em Física, precisamos calcular P(Mat|Fís).

Usando as informações fornecidas, temos:
- P(Mat) = 0,7 (probabilidade de ser aprovado em Matemática)
- P(Fís) = 0,6 (probabilidade de ser aprovado em Física)
- P(Mat ∩ Fís) = 0,5 (probabilidade de ser aprovado em ambas)

Assim, P(Mat|Fís) = P(Mat ∩ Fís) / P(Fís) = 0,5 / 0,6 = 0,833.

Portanto, a probabilidade de um aluno ser aprovado em Matemática, dado que já foi aprovado em Física, é cerca de 0,833.

Explicação dos itens:
- A) 0,7: Essa é a probabilidade do aluno ser aprovado em Matemática, desconsiderando qualquer condição sobre Física.
- B) 0,5: Essa é a probabilidade do aluno ser aprovado em ambas as disciplinas, não a probabilidade condicional desejada.
- C) 0,833: Este é o valor correto da probabilidade condicional, calculado com as informações dada.
- D) 0,6: Essa é a probabilidade do aluno ser aprovado em Física, desconsiderando qualquer condição sobre Matemática.
- E) 0,9: Não há informações fornecidas que justifiquem este valor; parece ser um valor arbitrário e não está relacionado ao cálculo correto da probabilidade.",9782593
tópico 1,"Banco de dados e formatos de arquivo orientado a colunas: Parquet, MonetDB, duckDB","Questão:
A análise e processamento de grandes volumes de dados exigem soluções eficientes de armazenamento e consulta. Os bancos de dados e formatos de arquivos orientados a colunas, como Parquet, MonetDB e DuckDB, apresentam características distintas que os tornam apropriados para diferentes contextos de uso em análises de dados. Com base no cenário descrito, analise as afirmações a seguir:

I. O Parquet é um formato de arquivo otimizado para operações de leitura e escrita em HDFS (Hadoop Distributed File System), oferecendo alta compressão e eficiência em consultas analíticas.

II. MonetDB é um banco de dados relacional pioneiro ao empregar uma arquitetura orientada a colunas, sendo desenhado para operações OLTP (Online Transaction Processing) com foco em rápido processamento de transações.

III. DuckDB é um banco de dados orientado a colunas, desenhado para ser leve e mais adequado para cenários OLAP (Online Analytical Processing), com ênfase em análises e consultas ad-hoc de dados.

IV. Parquet e DuckDB são específicos para ambientes distribuídos, enquanto MonetDB é mais utilizado em ambientes single-node devido à sua arquitetura interna.

Assinale a opção que indica as afirmações corretas.

A) I e III apenas
B) I, II e IV apenas
C) II e III apenas
D) I, II e III apenas
E) Todas as afirmações estão corretas

",A,"

Explicação dos itens:

I. Correta. O Parquet é um formato de arquivo columnar, amplamente utilizado no ecossistema Hadoop, concebido para oferecer uma compactação eficiente e otimizado para trabalho em sistemas de arquivos distribuídos como o HDFS, beneficiando cargas de trabalho de leitura intensiva como as típicas em consultas analíticas.

II. Incorreta. MonetDB é de fato um banco de dados orientado a colunas, mas ele é focado em análises OLAP (Online Analytical Processing) e não OLTP. Ele é projetado para otimizar consultas complexas de análises e não para processamento rápido de transações.

III. Correta. DuckDB é um banco de dados analítico orientado a colunas. É projetado para ser um sistema de gestão de base de dados embutido, leve e voltado para cenários OLAP, onde a análise de dados e as consultas ad-hoc são a prioridade.

IV. Incorreta. O Parquet é um formato de arquivo e não um sistema de banco de dados, e por si só, não é específico para ambientes distribuídos, embora seja comum seu uso em tais ambientes. DuckDB é projetado para ser um mecanismo de banco de dados embutido e pode funcionar tanto em ambientes simples como distribuídos. MonetDB pode ser utilizado tanto em ambientes single-node como em distribuídos, mas é conhecido por sua eficiência em ambientes single-node.",4172218
tópico 0,Conceitos de processamento massivo e paralelo,"Questão:
A eficiência no processamento de grandes volumes de dados é um requisito crucial para muitas aplicações computacionais, e o processamento massivo e paralelo constitui uma abordagem fundamental para atingir tal objetivo. Com relação aos conceitos de processamento massivo e paralelo, analise as afirmativas abaixo e assinale a opção correta:

I. Em um sistema de processamento paralelo, múltiplas CPUs trabalham em conjunto para executar diferentes partes de um mesmo programa simultaneamente, dividindo tarefas e comunicando-se regularmente para sincronizar o progresso.

II. A técnica de MapReduce, utilizada frequentemente em frameworks como Apache Hadoop, baseia-se na divisão sequencial de tarefas e na consolidação individual de resultados, funcionando melhor em single-core CPUs para processamento intensivo de dados.

III. Processamento massivo em empresas como Google e Facebook envolve não apenas a capacidade de hardware para operar com grandes quantidades de dados, mas também o uso de algoritmos complexos de aprendizado de máquina e inteligência artificial para criar insights a partir dos dados processados.

IV. O conceito de NoSQL surge como uma resposta ao desafio do processamento massivo de dados, oferecendo esquemas flexíveis, suporte a distribuição horizontal e a capacidade de manusear grandes volumes de dados desestruturados ou semi-estruturados, o que é menos eficiente em bancos de dados relacionais tradicionais.

É correto o que se afirma apenas nas afirmativas:

A) I e II.
B) II e III.
C) I e IV.
D) I, III e IV.
E) Todas as afirmativas estão corretas.

",C," 

Explicação dos itens:

I. Correta. A descrição está alinhada com o princípio fundamental do processamento paralelo, onde várias CPUs ou cores trabalham em paralelo para melhorar o desempenho do processamento de dados, o que inclui dividir tarefas complexas e sincronização entre processos.

II. Incorreta. A técnica de MapReduce é projetada para processamento paralelo e distribuído de grandes conjuntos de dados. Ela funciona justamente utilizando vários nós de processamento que podem não ser single-core CPUs. O conceito de MapReduce envolve duas funções principais: 'Map', que processa e transforma os dados de entrada em pares chave/valor intermediários, e 'Reduce', que consolida os resultados. Portanto, a afirmativa II é falsa.

III. Correta. Empresas que lidam com grandes volumes de dados realmente utilizam algoritmos avançados de aprendizado de máquina e inteligência artificial para extrair conhecimento dos dados que processam, o que é parte crucial do processamento massivo.

IV. Correta. O modelo NoSQL é uma resposta aos desafios do processamento massivo de dados e tem como características o suporte a escalabilidade horizontal e a habilidade de armazenar e gerenciar grandes quantidades de dados não estruturados ou semi-estruturados, algo que os bancos de dados relacionais tradicionais têm dificuldades em fazer eficientemente.",8309578
tópico 1,Álgebra relacional e SQL (padrão ANSI),"Questão: 
Considere as seguintes relações em um banco de dados relacional:

Funcionario(F_id, F_nome, F_departamento_id, Salario)
Departamento(D_id, D_nome, Gerente_id)

Um administrador de banco de dados deseja realizar uma consulta para encontrar os nomes dos funcionários e os respectivos nomes de seus departamentos. Entretanto, a consulta deve incluir apenas aqueles funcionários cujo salário seja superior a R$5.000,00. Utilizando a Álgebra Relacional, qual seria a expressão correta para esta consulta? 

A) π F_nome, D_nome (Funcionario ⨝ F_departamento_id = D_id Departamento) σ Salario > 5000(Funcionario)

B) π F_nome, D_nome (σ Salario > 5000 (Funcionario) ⨝ F_departamento_id = D_id Departamento)

C) π F_nome, D_nome (Funcionario ⨝ F_departamento_id = D_id σ Salario > 5000(Departamento))

D) π F_nome, D_nome (Funcionario) ⨝ σ F_departamento_id = D_id (Salario > 5000(Departamento))

E) π F_nome, D_nome (Funcionario ⨝ Departamento) σ Salario > 5000(Funcionario)

",B,"

Explicação dos itens:

A) Esta opção está incorreta pois a seleção de salário é realizada após a junção das relações, sem a garantia de que apenas os funcionários com salário superior a 5000 serão considerados.

B) Esta opção é a correta pois primeiro faz-se a seleção dos funcionários com salário superior a 5000 e depois a junção com a tabela de departamentos para obter o nome dos departamentos.

C) Esta opção é incorreta porque primeiro realiza a junção das relações e depois tentar aplicar o filtro de salário na relação Departamento, o que é um erro lógico, já que o atributo Salario não faz parte desta relação.

D) Esta opção é incorreta porque aplica a condição de salário somente na relação Departamento e não na relação Funcionario, além de indicar incorretamente a operação de junção.

E) Esta opção está incorreta porque a seleção de funcionários com salário superior a 5000 deve ser feita antes da junção. Aqui, a seleção seria aplicada após a junção, o que não garante o filtro desejado.",9089647
tópico 3,Programação orientada a objetos,"Questão: Em programação orientada a objetos (POO), considera-se que o encapsulamento é um dos pilares fundamentais para a estruturação dos programas. Sobre encapsulamento, analise as afirmativas a seguir:

I. Encapsulamento permite a ocultação da implementação interna das classes, expondo aos usuários apenas as funcionalidades necessárias para a manipulação dos objetos.
II. Em linguagens de programação que suportam POO, como Java e C#, o encapsulamento é obtido principalmente pelo uso de modificadores de acesso, como public, private e protected.
III. Encapsulamento impede que objetos de uma classe acessem diretamente os atributos de outro objeto da mesma classe.

Está(ão) correta(s) a(s) afirmativa(s):

A) Apenas I e II.
B) Apenas I e III.
C) Apenas II e III.
D) I, II e III.
E) Apenas II.

",A,"

Explicação dos itens:

I. Correto. Encapsulamento é o ato de esconder os detalhes internos da implementação de uma classe e expor apenas uma interface através da qual a funcionalidade da classe pode ser utilizada. Essa é uma descrição precisa do que é encapsulamento em POO.

II. Correto. Os modificadores de acesso como public, private e protected, são, de fato, a maneira como linguagens orientadas a objetos como Java e C# implementam o encapsulamento. Eles controlam o nível de visibilidade que cada membro da classe (atributos e métodos) terá para o restante do programa.

III. Incorreto. Encapsulamento não impede que objetos de uma mesma classe acessem diretamente os atributos uns dos outros. Encapsulamento está mais relacionado com a visibilidade e proteção dos membros da classe em relação a outras classes e o resto do programa, e não entre objetos da mesma classe, que normalmente têm a capacidade de acessar atributos privados uns dos outros dentro da mesma definição de classe.",4725406
tópico 0,Ingestão de dados em lote (batch),"Questão: A ingestão de dados em lote, ou batch data ingestion, é um processo crítico para sistemas que dependem da análise de grandes quantidades de dados para tomada de decisão. Qual das seguintes afirmativas melhor descreve o processo de ingestão de dados em lote?

A) A ingestão de dados em lote ocorre em tempo real, garantindo a menor latência possível na atualização dos dados.
B) Os dados são processados individualmente, à medida que chegam, para garantir análises mais detalhadas.
C) Esse processo envolve a coleta de dados de várias fontes, que são então imediatamente purificados e transformados antes da ingestão.
D) Na ingestão de dados em lote, grandes volumes de dados são coletados em intervalos regulares e processados de uma só vez.
E) A ingestão de dados em lote privilegia análises preditivas e recomendações em tempo real para alavancar a performance do negócio.

",D,"  
A alternativa A está incorreta porque a ingestão de dados em lote não é realizada em tempo real, mas sim em intervalos regulares. A alternativa B também é incorreta, pois descreve um processo que é mais característico da ingestão de dados em streaming, onde os dados são processados à medida que chegam. A alternativa C é imprecisa, pois a purificação e transformação dos dados podem ocorrer durante ou após o processo de coleta, e não necessariamente de forma imediata. A alternativa D está correta, pois descreve acuradamente o processamento de grandes volumes de dados coletados em intervalos regulares, em oposição ao processamento de dados em tempo real ou streaming. Por fim, a alternativa E é errada, pois apesar da ingestão de dados em lote poder auxiliar em análises preditivas, ela não é usada para recomendações em tempo real; essa seria uma característica da ingestão de dados em streaming.",1351910
tópico 4,Variáveis aleatórias e funções de probabilidade,"Questão:
Considere a variável aleatória discreta X que segue a seguinte função de probabilidade:

P(X = x) = k(x^2 – 4), onde x = –1, 0, 1, 2, 3

Para que essa função seja considerada uma função de probabilidade adequada, a constante k deve satisfazer uma condição específica. Essa condição é tal que:

A) k > 1/14
B) k = 1/14
C) 0 < k < 1/14
D) k < 0
E) k é um número inteiro positivo

",B,"

Explicação dos itens:

A) k > 1/14 - Este item é incorreto porque se k for maior que 1/14, a soma das probabilidades excederia 1, o que não é permitido para uma função de probabilidade.

B) k = 1/14 - Este é o item correto. A soma das probabilidades deve ser igual a 1. Ao calcularmos P(X = –1) + P(X = 0) + P(X = 1) + P(X = 2) + P(X = 3) com a fórmula dada e resolvendo a equação, encontramos k = 1/14.

C) 0 < k < 1/14 - Este item não está correto, pois embora a soma das probabilidades deva ser menor do que 1, para ser uma função de probabilidade válida, a soma deve ser exatamente 1. Portanto, deve haver um valor específico para k e não um intervalo.

D) k < 0 - Este item está incorreto porque a probabilidade não pode ser negativa, e portanto, k também não pode ser negativo.

E) k é um número inteiro positivo - Este item está incorreto porque k deve ser uma constante que assegure que a soma total das probabilidades seja 1. Não há nenhuma garantia de que essa constante deva ser um número inteiro positivo; na verdade, para essa função de probabilidade específica, não é.",2902240
tópico 2,Data cleansing,"Considerando os processos envolvidos no preparo de dados para análises e aplicações de Machine Learning, o Data Cleansing é uma etapa fundamental para garantir a qualidade e confiabilidade dos resultados obtidos. Com base nesse entendimento, avalie as seguintes afirmativas sobre Data Cleansing e escolha a opção correta:

I. Data Cleansing, também conhecido como limpeza de dados, envolve a correção ou a remoção de dados incorretos, incompletos, duplicados ou irrelevantes de um conjunto de dados.

II. Uma prática comum de Data Cleansing é a imputação de dados, que consiste na substituição de valores faltantes por estimativas, podendo ser realizada através de métodos como a média, mediana ou modos para variáveis numéricas e categorização para variáveis categóricas.

III. A identificação de outliers sempre requer a remoção dos mesmos do conjunto de dados, uma vez que sua presença pode distorcer análises posteriores e modelos preditivos.

IV. Após o processo de Data Cleansing, não é necessário revisar dados limpos, pois a etapa é completamente automatizada e infalível.

Escolha a assertiva correta:

A) Apenas I e II estão corretas.
B) Apenas II e III estão corretas.
C) Apenas I, II e III estão corretas.
D) Todas as afirmativas estão corretas.
E) Apenas I, II e IV estão corretas.

",A,"

I. Correta. O Data Cleansing realmente envolve a correção ou remoção de dados problemáticos para criar um conjunto de dados confiável.

II. Correta. A imputação de dados é uma técnica utilizada durante a limpeza de dados para lidar com valores faltantes, aplicando diferentes métodos estatísticos conforme o tipo de variável.

III. Incorreta. Embora a presença de outliers possa alterar os resultados de determinadas análises, nem sempre é apropriado removê-los, uma vez que podem representar variações genuínas nos dados ou pontos críticos que são essenciais para o estudo. A decisão de removê-los ou não deve ser contextual e baseada em uma análise criteriosa.

IV. Incorreta. Apesar da automação facilitar o processo de Data Cleansing, é recomendável revisar os dados após a limpeza para garantir que nenhum erro foi cometido e que a qualidade foi mantida. A revisão manual ainda é uma etapa importante na garantia da qualidade dos dados.
",2879377
tópico 3,Linguagem de programação Scala,"Questão:

Na linguagem de programação Scala, que é reconhecida pela sua capacidade de combinar o paradigma orientado a objetos com o funcional, temos um recurso poderoso conhecido como case classes. Estas são utilizadas frequentemente para modelar dados imutáveis e implementar padrões de projetos como o de ""match"". Considere o seguinte código Scala:

```scala
abstract class Notification

case class Email(sender: String, title: String, body: String) extends Notification

case class SMS(caller: String, message: String) extends Notification

case class VoiceRecording(contactName: String, link: String) extends Notification

def showNotification(notification: Notification): String = {
  notification match {
    case Email(sender, _, _) =>
      s""You got an email from $sender""
    case SMS(number, message) =>
      s""You got an SMS from $number! Message: $message""
    case VoiceRecording(name, link) =>
      s""You received a Voice Recording from $name! Click here to hear it: $link""
  }
}

val someSms = SMS(""12345"", ""Are you there?"")
val someVoiceRecording = VoiceRecording(""Tom"", ""voicerecording.org/id/123"")

```

Qual será a saída do código ao executar as seguintes instruções?

```scala
println(showNotification(someSms))
println(showNotification(someVoiceRecording))
```

A) ""You got an SMS from 12345! Message: Are you there?"" seguido por ""You received a Voice Recording from Tom! Click here to hear it: voicerecording.org/id/123""

B) ""You got an email from 12345"" seguido por ""You got an SMS from Tom!""

C) ""You got a Voice Recording from 12345! Click here to hear it: Are you there?"" seguido por ""You got an email from Tom!""

D) ""You got an SMS from 12345"" seguido por ""You got a Voice Recording from Tom! Click here to hear it: voicerecording.org/id/123""

E) ""You got an email from 12345! Are you there?"" seguido por ""You received a Voice Recording from Tom""

",A,"

Explicação dos itens:
A) Correto. De acordo com o pattern matching implementado na função showNotification, cada tipo de notificação tem uma mensagem associada. SMS e VoiceRecording serão matcheados com seus respectivos cases, resultando nas saídas esperadas.

B) Incorreto. A função showNotification especifica mensagens diferentes para cada tipo de notificação. O case para Email não é acionado, pois não há uma instância de Email sendo passada para a função.

C) Incorreto. Os tipos de notificações e as saídas estão incorretamente associados. Essa opção mistura os tipos de notificações com saídas que não correspondem ao pattern matching definido.

D) Incorreto. A parte da mensagem para SMS está correta, mas a saída para VoiceRecording falta a parte da string que inclui o link para a gravação.

E) Incorreto. Novamente, a saída para o Email está incorreta pois não há uma instância de Email sendo avaliada. Além disso, a saída para VoiceRecording está incompleta, faltando o link da gravação.",4465634
tópico 0,Soluções de big data: Arquitetura do ecossistema Spark,"Questão: Na arquitetura do ecossistema Apache Spark, diversas componentes contribuem para processamento de big data de maneira eficiente. Dentro desse ecossistema, uma das camadas é fundamental para o agendamento de tarefas e distribuição dos dados. Esta camada também é responsável pelo gerenciamento da memória e pela otimização de tarefas. Qual componente do Spark desempenha esse papel?

A) Spark SQL
B) Spark Streaming
C) Spark Core
D) MLlib
E) GraphX

",C,"

Explicação dos itens:

A) Spark SQL - É o módulo do Spark para o processamento de dados estruturados com SQL e DataFrames. Apesar de fazer parte do ecossistema Spark e permitir a otimização de consultas, não é o componente responsável por agendamento de tarefas e distribuição de dados.

B) Spark Streaming - Esse componente permite o processamento de fluxos contínuos de dados (streaming data), mas não é a camada que lida com o agendamento de tarefas e otimização.

C) Spark Core - É o coração do Apache Spark, responsável pelo agendamento de tarefas, distribuição de dados, gerenciamento de memória e a base para todos os outros componentes do Spark. Portanto, é a alternativa correta.

D) MLlib - A biblioteca de machine learning para Apache Spark, que fornece várias ferramentas para tarefas de aprendizado de máquina, mas não inclui funcionalidades de agendamento de tarefas ou gerenciamento de memória para o ecossistema.

E) GraphX - Extensão do Spark para processar grafos e realizar computações gráficas. Assim como os outros componentes listados, não é responsável pelo agendamento de tarefas e gerenciamento do sistema.",6407950
tópico 1,"Banco de dados relacional: SQL Server, PostgreSQL, MySQL","Questão:
Em um ambiente de desenvolvimento de banco de dados relacional, três sistemas gerenciadores de banco de dados (SGBDs) SQL Server, PostgreSQL e MySQL são frequentemente utilizados. Considerando aspectos de transações e isolamento em SGBDs, analise as afirmativas a seguir:

I) SQL Server oferece cinco níveis de isolamento de transação, que incluem Read Uncommitted, Read Committed, Repeatable Read, Snapshot e Serializable.

II) PostgreSQL utiliza por padrão o nível de isolamento Read Committed, mas permite a configuração do nível Serializable para garantir a consistência das transações por meio do controle de versão de linha.

III) MySQL, em sua engine InnoDB, suporta todos os níveis de isolamento de transação definidos pelo padrão SQL, mas o nível de isolamento por padrão é Repeatable Read, diferentemente do SQL Server e PostgreSQL.

Está(ão) correta(s) a(s) afirmativa(s):
a) Apenas I
b) Apenas II
c) I e II
d) II e III
e) I, II e III

",E,"

Explicação dos itens:

a) ""Apenas I"" é incorreto porque também as afirmativas II e III estão corretas.

b) ""Apenas II"" é incorreto porque as afirmativas I e III também estão corretas.

c) ""I e II"" é incorreto porque a afirmativa III também está correta, indicando que MySQL suporta todos os níveis de isolamento definidos pelo padrão SQL e seu nível default é Repeatable Read.

d) II e III"" é incorreto porque inclui a afirmativa correta sobre o PostgreSQL e MySQL, mas deixa de fora informações importantes sobre os níveis de isolamento oferecidos pelo SQL Server.

e) ""I, II e III"" é a alternativa correta porque todas as afirmativas estão corretas:
- I: SQL Server realmente oferece cinco níveis de isolamento, contribuindo para a flexibilidade no controle das transações.
- II: PostgreSQL por padrão usa o nível de isolamento Read Committed e pode ser configurado para usar Serializable, o que faz uso de controle de versão de linha para garantir a consistência.
- III: MySQL realmente suporta todos os níveis de isolamento do padrão SQL em sua engine InnoDB, tendo Repeatable Read como o nível de isolamento padrão.",4649436
tópico 0,Conceitos de processamento massivo e paralelo,"Questão: No contexto de sistemas de processamento de dados em larga escala, é essencial compreender os conceitos fundamentais que regem o processamento massivo e paralelo. Considerando um cenário de análise de grandes conjuntos de dados (Big Data), quais dos seguintes afirmativas estão corretas em relação ao MapReduce, um modelo de programação popular para processamento de dados em larga escala?

I. O MapReduce permite a distribuição automática do processamento de dados em um cluster de computadores, o que aumenta a redundância e a tolerância a falhas no processamento dos dados.
II. No modelo de programação MapReduce, a etapa ""Map"" é responsável por ordenar os pares chave-valor produzidos, garantindo que os dados estejam organizados para a etapa ""Reduce"".
III. A etapa ""Reduce"" do MapReduce consome os pares chave-valor fornecidos pela etapa ""Map"", realizando operações de agregação ou resumo, como soma, média ou contagem.
IV. MapReduce é tipicamente implementado sobre sistemas de arquivos distribuídos, como o HDFS (Hadoop Distributed File System), que suportam armazenamento e recuperação eficiente de grandes quantidades de dados.

A) Apenas I e IV são corretas.
B) Apenas II e III são corretas.
C) Apenas I, III e IV são corretas.
D) Apenas I e II são corretas.
E) Todas as afirmativas são corretas.

",C," 

Explicação dos itens:

I. Correto. O MapReduce foi projetado para processar grandes volumes de dados de forma distribuída, aumentando a redundância e tolerância a falhas através de replicação de tarefas e dados em um cluster.

II. Incorreto. A etapa ""Map"" é responsável por processar os dados de entrada e gerar pares chave-valor. A organização e ordenação destes pares ocorre entre as etapas ""Map"" e ""Reduce"" em uma fase intermediária denominada ""Shuffle"".

III. Correto. A função ""Reduce"" do MapReduce, de fato, processa cada grupo de valores associados a uma chave específica, permitindo a realização de operações de agregação e síntese dos dados.

IV. Correto. O MapReduce geralmente opera em conjunto com sistemas de arquivos distribuídos, como o HDFS, que são otimizados para armazenamento e acesso a grandes volumes de dados distribuídos por um cluster de máquinas.

Portanto, as afirmativas I, III e IV estão corretas, enquanto a afirmativa II está incorreta, por isso a alternativa C) é a correta.",4924220
tópico 1,Álgebra relacional e SQL (padrão ANSI),"Questão:
Considere a existência de um banco de dados de uma biblioteca que mantém suas informações em duas relações, Livros(Lid, Titulo, Autor) e Emprestimos(Eid, Lid, DataEmprestimo, DataDevolucao). Lid refere-se ao identificador único de cada livro. A relação Livros contém os detalhes do livro, incluindo o identificador do livro, o título e o autor. Por outro lado, Emprestimos rastreia os empréstimos dos livros, onde Eid é o identificador único do empréstimo, Lid é a chave estrangeira referenciando a relação Livros, DataEmprestimo é a data em que o livro foi emprestado e DataDevolucao é a data em que o livro foi devolvido à biblioteca. Em um dado momento, a biblioteca deseja obter uma lista dos títulos de livros que nunca foram emprestados.

Qual das seguintes consultas SQL retornaria corretamente a lista solicitada pela biblioteca?

A) SELECT Titulo FROM Livros WHERE Lid NOT IN(SELECT Lid FROM Emprestimos);

B) SELECT l.Titulo FROM Livros l JOIN Emprestimos e ON l.Lid = e.Lid WHERE e.DataEmprestimo IS NULL;

C) SELECT Titulo FROM Livros EXCEPT SELECT Titulo FROM Emprestimos;

D) SELECT Titulo FROM Livros l WHERE NOT EXISTS(SELECT * FROM Emprestimos e WHERE l.Lid = e.Lid);

E) SELECT l.Titulo FROM Livros l, Emprestimos e WHERE l.Lid != e.Lid GROUP BY l.Titulo;

",D,"

Explicação dos itens:

A) Esta consulta verifica os livros cujo identificador não está presente na relação dos empréstimos, o que seria uma maneira correta se todos os livros emprestados estivessem sempre na relação Emprestimos. Contudo, ela não se preocupar com os casos de livros que foram emprestados e depois retornaram, o que não atende ao requisito da questão.

B) Esta consulta tenta fazer um INNER JOIN entre as relações Livros e Emprestimos, onde as colunas Lid correspondem, e em seguida verifica se a DataEmprestimo é nula. No entanto, uma DataEmprestimo nula não faria sentido na tabela Emprestimos, então essa consulta não retorna os livros que nunca foram emprestados.

C) Esta consulta faz uso do operador EXCEPT para tentar subtrair os títulos contidos na relação Emprestimos da relação Livros. Entretanto, ela não leva em consideração que os identificadores dos livros (Lid) devem ser comparados, e não os títulos. Ademais, Emprestimos não tem coluna de Título, logo a consulta está errada.

D) Esta consulta utiliza um subselect com o operador NOT EXISTS para verificar se não existem entradas na relação Emprestimos para cada livro na relação Livros. Esta seria a consulta correta pois ela corretamente busca por livros que não possuem correspondência na tabela Emprestimos, indicando que nunca foram emprestados.

E) Esta consulta tenta encontrar livros fazendo um produto cartesiano entre Livros e Emprestimos e selecionando pares de Livros e Empréstimos onde os Lids são diferentes. O GROUP BY agrupa os resultados pelo título dos livros. No entanto, essa consulta não garante obter os livros que nunca foram emprestados; além disso, um produto cartesiano não é o método correto para resolver este problema.",6165187
tópico 3,"Visualização de dados ggplot, matplotlib","Questão: Em análises de dados, frequentemente utilizamos bibliotecas de visualização de dados para melhor interpretar e comunicar resultados. As bibliotecas ggplot2 em R e matplotlib em Python são amplamente empregadas para esse fim. Nesse contexto, considere que um analista de dados deseja criar um gráfico de dispersão (scatter plot) que apresente a relação entre duas variáveis quantitativas, 'X' e 'Y', e também deseja adicionar uma linha de tendência ao gráfico para melhor entender a relação entre as variáveis. Supondo que o analista esteja utilizando o ggplot2 no R e o matplotlib no Python, assinale a opção que corretamente apresenta como o analista deve proceder em cada ambiente.

A) No ggplot2 utilize a função `ggplot(data = df) + geom_point(aes(x = X, y = Y)) + geom_smooth()` e no matplotlib `plt.scatter(df['X'], df['Y']); plt.plot(np.unique(df['X']), np.poly1d(np.polyfit(df['X'], df['Y'], 1))(np.unique(df['X'])))`.
B) No ggplot2 utilize a função `plot(df$X, df$Y)` e no matplotlib `plt.scatterplot(df['X'], df['Y'])`.
C) No ggplot2 utilize a função `qplot(X, Y, data = df, geom = ""line"")` e no matplotlib `plt.plot(df['X'], df['Y']) + plt.trendline()`.
D) No ggplot2 utilize a função `ggplot(df, aes(x = X, y = Y)) + geom_line()` e no matplotlib `plt.plot(df['X'], df['Y'])`.
E) No ggplot2 utilize a função `ggplot(df, aes(X, Y)) + stat_summary()` e no matplotlib `plt.bar(df['X'], df['Y'])`.

",A,"
A explicação dos itens é a seguinte:

A) Este item está correto porque a função `geom_point()` no ggplot2 cria um gráfico de dispersão, e a função `geom_smooth()` adiciona uma linha de tendência ao gráfico. No matplotlib, `plt.scatter()` cria um gráfico de dispersão, e a combinação de `np.polyfit()` com `np.poly1d()` e `plt.plot()` adiciona uma linha de tendência.

B) Está incorreto pois em ggplot2, `plot()` não é a função utilizada para a criação de gráficos avançados, e o matplotlib não possui uma função `scatterplot()`, a correta é `scatter()`.

C) Está incorreto pois `qplot()` com `geom = ""line""` cria um gráfico de linha em ggplot2, o que não representa um gráfico de dispersão. Além disso, no matplotlib, não existe uma função chamada `trendline()` associada diretamente ao `plt.plot()`.

D) Está incorreto pois ao usar `geom_line()` no ggplot2, será criado um gráfico de linha em vez de um gráfico de dispersão. Matplotlib usa `plt.plot()` para criar gráficos de linha, não gráficos de dispersão com linha de tendência.

E) Está incorreto pois a função `stat_summary()` em ggplot2 é usada para produzir estatísticas sumarizadas do dataset, e não especificamente para gráficos de dispersão com linha de tendência. Já `plt.bar()` em matplotlib cria um gráfico de barras, o que não corresponde ao pedido da questão.",8269068
tópico 5,"Processamento de linguagem natural: Normalização textual - stop words, estemização, lematização e análise de frequência de termos; ","Questão: No processamento de linguagem natural (PLN), a normalização textual desempenha um papel fundamental no pré-processamento de dados textuais. Dentre as técnicas de normalização, pode-se citar a remoção de stop words, a estemização e a lematização. A análise de frequência de termos também é uma prática comum no âmbito da normalização textual. Considerando essas técnicas, assinale a opção que descreve INCORRETAMENTE uma dessas práticas.

A) A remoção de stop words consiste em eliminar palavras que são comuns e não agregam significado substancial ao texto, como preposições, conjunções e artigos.

B) A estemização é o processo de redução das palavras à sua raiz ou forma base, frequentemente resultando em um termo que não corresponde a uma palavra com significado no idioma.

C) A lematização, ao contrário da estemização, é a redução das palavras ao seu lema, ou seja, sua forma canônica ou de dicionário, garantindo que o resultado seja uma palavra válida no idioma.

D) A análise de frequência de termos é realizada após a estemização e lematização e é utilizada somente para identificar as palavras mais raras em um corpus textual.

E) Tanto a estemização quanto a lematização são abordagens utilizadas para minimizar a complexidade do vocabulário em um corpus e melhorar o desempenho de algoritmos de PLN.

",D,"

Explicação dos itens:

A) Correto. A remoção de stop words é uma prática comum em PLN. Essas palavras são geralmente muito frequentes e portanto menos informativas para muitas tarefas de PLN.

B) Correto. A estemização reduz palavras a uma forma base ou ""stem,"" que pode não ser uma palavra com significado, porque o processo pode ser agressivo e simplesmente cortar sufixos das palavras.

C) Correto. Lematização é um processo mais sofisticado que a estemização, pois busca reduzir as palavras à sua forma lematizada ou de dicionário, o que geralmente resulta em uma palavra com significado no idioma.

D) Incorreto. A análise de frequência de termos pode ser realizada antes ou depois da estemização e lematização, e é comumente usada tanto para identificar termos frequentes quanto raros em um corpus textual. Portanto, não é verdade que ela seja utilizada somente para identificar as palavras mais raras.

E) Correto. Estemização e lematização são técnicas para reduzir a dimensionalidade do vocabulário, o que pode melhorar o desempenho de algoritmos de PLN ao reduzir a variabilidade das formas das palavras.",6766818
tópico 1,"Banco de dados relacional: SQL Server, PostgreSQL, MySQL","Questão: No contexto de banco de dados relacional, considerando o uso de SQL Server, PostgreSQL e MySQL, qual das seguintes afirmações é INCORRETA a respeito da implementação de Stored Procedures nos três sistemas de gerenciamento de banco de dados (SGBDs)?

A) No SQL Server, as Stored Procedures são compiladas e armazenadas no banco de dados, o que pode aumentar a performance em sua execução.

B) PostgreSQL suporta a criação de Stored Procedures em várias linguagens, incluindo SQL, PL/pgSQL e outras linguagens como Perl e Python.

C) Stored Procedures no MySQL são escritas usando exclusivamente a linguagem SQL e não suportam outras linguagens de programação.

D) Em todos os três SGBDs, as Stored Procedures podem ser utilizadas para encapsular uma lógica de negócio complexa, tornando a manutenção do código mais fácil e centralizada.

E) Tanto o PostgreSQL quanto o SQL Server oferecem a capacidade de definir variáveis temporárias dentro de Stored Procedures que existem durante o tempo de execução da procedure.

",C,"

Explicação dos itens:

A) Correta. No SQL Server, as Stored Procedures são compiladas e armazenadas, o que as torna mais rápidas na execução, pois o plano de execução pode ser reutilizado, diminuindo o overhead de compilação em chamadas subsequentes.

B) Correta. O PostgreSQL é conhecido por sua extensibilidade e suporta várias linguagens para Stored Procedures, que vão além do SQL, como PL/pgSQL, que é a linguagem padrão de procedural, e suporte para linguagens como Perl e Python através de extensões.

C) Incorreta. Embora SQL seja a principal linguagem para escrever Stored Procedures no MySQL, não é verdade que MySQL suporta exclusivamente SQL. A partir de MySQL 5.0, é possível escrever Stored Procedures em SQL, mas não há suporte nativo para outras linguagens de programação como acontece no PostgreSQL, que de fato possui essa capacidade.

D) Correta. Em todos os SGBDs mencionados, é uma prática comum utilizar Stored Procedures para encapsular lógicas de negócios complexas. Isso permite que operações reutilizáveis e possivelmente complexas sejam mantidas em um único local, facilitando a manutenção e garantindo consistência nos processos de negócio.

E) Correta. Tanto PostgreSQL quanto SQL Server permitem a definição de variáveis temporárias dentro de Stored Procedures. Essas variáveis temporárias existem durante o tempo de execução de uma Stored Procedure e são descartadas assim que a execução termina.",8362169
tópico 3,Linguagem de programação Scala,"Questão: 
Considerando os paradigmas de programação funcional e orientada a objetos, pode-se afirmar que a linguagem Scala foi projetada para integrar características de ambos os paradigmas. Em relação às funcionalidades e característica da linguagem Scala, analise as seguintes afirmativas:

I. Scala permite a definição de variáveis imutáveis, utilizando a palavra-chave `val`, que pode ser alterada durante o tempo de execução, caracterizando um comportamento mutável.
II. A linguagem permite a implementação de métodos de alta ordem, que podem receber funções como parâmetros ou retornar outras funções como resultado.
III. Traits em Scala podem ser comparados a interfaces em Java, mas com a capacidade de conter implementações de métodos, tornando-os similares a classes abstratas.
IV. Pattern matching em Scala é uma ferramenta poderosa que permite inspecionar e decompor dados de forma concisa e expressiva, eliminando a necessidade de instruções condicionais encadeadas como `if` e `else`.

Assinale a opção que contém apenas as afirmativas corretas:

A) I e II
B) II e III
C) II, III e IV
D) I, II e IV
E) Todas as afirmativas são corretas.

",C,"

 Item I está incorreto porque a palavra-chave `val` em Scala é usada para declarar uma variável imutável que não pode ser alterada após a sua inicialização.
 Item II está correto; métodos de alta ordem são uma funcionalidade chave das linguagens funcionais e são plenamente suportados em Scala.
 Item III está correto; Scala possui Traits, que são como interfaces com a capacidade de conter código implementado.
 Item IV está correto; o pattern matching em Scala é uma alternativa poderosa às instruções condicionais, proporcionando uma forma mais legível e funcional de lidar com diversos casos.",2601899
tópico 2,Algoritmos fuzzy matching e stemming,"Questão:
Considere que um cientista de dados está trabalhando em um sistema de busca e recuperação de informações que necessita ser otimizado para entender a intenção do usuário mesmo em casos de grafias imprecisas ou consulta por palavras em diferentes formas gramaticais. O cientista pretende implementar técnicas de algoritmos de 'fuzzy matching' e 'stemming'. Qual das seguintes alternativas melhor descreve a função específica de cada uma dessas técnicas e como elas podem contribuir para o sistema?

A) 'Fuzzy matching' é uma técnica que identifica palavras-chave exatas no documento, enquanto 'stemming' é usada para expandir a busca para incluir todas as palavras com grafias erradas ou variações gramaticais.

B) 'Fuzzy matching' envolve encontrar correspondências aproximadas, o que permite que o sistema lide com erros de grafia ou variações fonéticas, e 'stemming' é um processo que reduz as palavras às suas raízes ou formas básicas para padronizar variações morfológicas.

C) 'Fuzzy matching' e 'stemming' são sinônimos e ambos referem-se ao processo de buscar correspondência perfeita entre palavras-chave e conteúdo textual.

D) 'Stemming' trata-se de uma técnica para encontrar correspondências precisas em grandes volumes de texto, enquanto 'fuzzy matching' reduz as palavras ao seu radical, facilitando a consistência nas respostas do sistema.

E) 'Fuzzy matching' é um método de indexação de documentos que ignora as stop words, e 'stemming' é uma forma de criptografia de palavras que melhora a segurança do sistema de busca.

",B,"

Explicação dos itens:
A) Incorreto, pois 'fuzzy matching' não busca palavras-chave exatas, e 'stemming' não é usado para variações de grafias erradas, mas sim para reduzir as palavras a suas raízes ou formas básicas.
B) Correto, 'fuzzy matching' permite ao sistema identificar palavras que não são idênticas, mas semelhantes o suficiente para serem consideradas uma correspondência, e 'stemming' reduz as palavras a um formato padrão para que diferentes formas da mesma palavra sejam reconhecidas como relacionadas.
C) Incorreto, pois 'fuzzy matching' e 'stemming' têm funções distintas no processamento de linguagem natural e não são sinônimos.
D) Incorreto, 'stemming' não é uma técnica para encontrar correspondências precisas, mas sim para reduzir palavras à sua forma base. 'Fuzzy matching' não reduz palavras ao seu radical, mas busca aproximações.
E) Incorreto, 'fuzzy matching' não está relacionado à indexação de documentos e nem ignora stop words, e 'stemming' não tem relação com criptografia, mas sim com o processamento de palavras para a busca e correspondência.",6718436
tópico 5,Ajuste de modelos dentro e fora de amostra e overfitting,"Questão: Em estatística e aprendizado de máquina, durante a fase de ajuste de modelos, um dos problemas enfrentados é o de overfitting, que ocorre quando o modelo se ajusta excessivamente aos dados de treino, resultando em um desempenho ruim em dados não vistos anteriormente. Considerando os conceitos de ajuste de modelo dentro e fora de amostra, qual das seguintes estratégias NÃO contribui efetivamente para a mitigação do overfitting?

A) Aumentar a quantidade de dados de treinamento.
B) Utilizar um modelo com menor complexidade.
C) Utilizar métricas de avaliação adequadas, como o erro de validação cruzada.
D) Selecionar features com base em seu desempenho no conjunto de treino.
E) Aplicar técnicas de regularização como Ridge ou Lasso.

",D,"

A) Aumentar a quantidade de dados de treinamento pode ajudar a melhorar a generalização do modelo, pois fornece mais informações sobre a variação dos dados, reduzindo a chance do modelo se ajustar demasiadamente ao ruído presente nos dados de treinamento.

B) Modelos com menor complexidade são menos propensos ao overfitting. Eles têm menor capacidade de se ajustar a detalhes e peculiaridades dos dados de treino, o que pode melhorar a performance em dados não vistos.

C) A utilização de métricas de avaliação adequadas, como o erro de validação cruzada, ajuda a estimar o desempenho do modelo em dados não vistos, orientando a busca por um equilíbrio adequado entre viés e variância.

D) Selecionar features com base exclusivamente em seu desempenho no conjunto de treino pode levar a uma escolha de variáveis que não necessariamente terão o mesmo desempenho em dados novos. Esta estratégia pode contribuir para o overfitting.

E) Técnicas de regularização como Ridge ou Lasso introduzem uma penalidade na função de custo do modelo para coeficientes maiores, reduzindo a complexidade do modelo e, por consequência, o risco de overfitting.",4862634
tópico 1,Banco de dados NoSQL,"Questão: Considerando os diferentes tipos de bancos de dados NoSQL e suas características, avalie as afirmativas abaixo e marque a opção que contém a afirmação correta:

I. Bancos de dados orientados a documentos armazenam e recuperam documentos, que podem ser XML, JSON, BSON, entre outros, e são ideais para armazenar, recuperar e gerenciar informações hierárquicas.

II. Bancos de dados em grafos são indicados para sistemas que não exigem relações complexas entre os dados, uma vez que não são otimizados para algoritmos que percorrem relações.

III. Bancos de dados de chave-valor permitem uma estrutura em que cada chave associativa possui um valor específico, mas eles não são recomendados para cenários onde a velocidade de leitura e escrita são críticas.

IV. Bancos de dados column-family armazenam dados em linhas e colunas, sendo uma boa escolha para queries complexas e agregações pesadas, tipicamente utilizados em sistemas de processamento de transações online (OLTP).

A alternativa que contém a afirmação correta é:

A) I
B) II
C) III
D) IV

",A," 
I. Correta. Bancos de dados orientados a documentos são projetados para armazenar, recuperar e gerenciar informações hierárquicas de forma eficiente, utilizando formatos como XML, JSON e BSON.

II. Incorreta. Bancos de dados em grafos são, na verdade, otimizados para sistemas que necessitam de relações complexas entre os dados, como redes sociais e sistemas de recomendação, uma vez que eles são ideais para algoritmos que percorrem relações.

III. Incorreta. Bancos de dados de chave-valor são recomendados para cenários em que a velocidade de leitura e escrita são muito importantes, devido à sua simples estrutura e alta eficiência em operações de acesso a dados.

IV. Incorreta. Bancos de dados do tipo column-family são projetados para lidar bem com grandes quantidades de dados distribuídos e são otimizados para leituras e escritas rápidas, sendo mais utilizados em sistemas de processamento analítico online (OLAP) do que em OLTP.",6226410
tópico 1,Banco de dados NoSQL,"Questão: A evolução das tecnologias de banco de dados foi significativamente influenciada pelo surgimento dos bancos NoSQL. Considerados alternativas escaláveis e adequadas para cenários de Big Data, essas bases de dados se caracterizam por determinadas propriedades e tipos. Sobre os bancos de dados NoSQL, analise as afirmativas a seguir:

I - Bancos de dados do tipo chave-valor armazenam os dados em um esquema rígido e bem estruturado, com a obrigatoriedade de tabelas pré-definidas.
II - Os bancos de dados orientados a documentos são flexíveis em termos de esquema, permitindo a inclusão de novos campos sem a necessidade de modificar todos os registros existentes.
III - Bancos de dados de colunas amplas oferecem vantagens no que diz respeito à armazenagem e processamento de grandes volumes de dados com poucos índices, tipicamente empregados em sistemas que requerem alta performance em operações de leitura e escrita.
IV - Sistemas baseados em grafos são altamente adequados para representar e explorar relações complexas entre entidades, sendo utilizados em aplicações como mídias sociais, detecção de fraudes e recomendação de produtos.

É correto o que se afirma em:

A) I e II, apenas.
B) II e III, apenas.
C) II, III e IV, apenas.
D) III e IV, apenas.
E) I, II, III e IV.

",C,"

Explicação dos itens:

I - Incorreto. Bancos de dados do tipo chave-valor são conhecidos pela sua simplicidade e flexibilidade, não exigindo um esquema rígido ou tabelas pré-definidas.

II - Correto. Os bancos de dados orientados a documentos permitem alta flexibilidade de esquema, possibilitando a adição de novos campos sem impactar registros preexistentes.

III - Correto. Os bancos de dados de colunas amplas são otimizados para consultas rápidas em grandes volumes de dados, não requerendo muitos índices e favorecendo operações de leitura e escrita de alta performance.

IV - Correto. Sistemas baseados em grafos são excelentes na representação de relacionamentos complexos, e por isso são utilizados em diferentes áreas como recomendação de produtos, redes sociais e análise de fraudes.

Portanto, a alternativa C é a correta, pois afirma corretamente sobre os tipos II, III e IV de bancos de dados NoSQL, enquanto a alternativa I está incorreta.",2858305
tópico 2,Enriquecimento,"Questão: No contexto de produção de urânio enriquecido, um dos aspectos mais críticos é o processo de separação dos isótopos. O urânio natural é composto majoritariamente por urânio-238, enquanto que para a maior parte das aplicações em reatores nucleares e armas, é necessário enriquecer a concentração de urânio-235. Qual dos seguintes métodos é amplamente utilizado para o enriquecimento de urânio?

A) Fusão nuclear
B) Cromatografia gasosa
C) Centrifugação a gás
D) Destilação fracionada
E) Difração de elétrons

Alternativa Correta: C

Explicação dos itens:
A) Fusão nuclear: Incorreto. A fusão nuclear é o processo pelo qual núcleos atômicos leves são combinados para formar elementos mais pesados, liberando energia. Não é um método utilizado para separar isótopos de urânio.

B) Cromatografia gasosa: Incorreto. Embora a cromatografia seja uma técnica de separação de misturas, a cromatografia gasosa não é aplicada para o enriquecimento de isótopos de urânio, pois é uma técnica que separa componentes de uma mistura com base em suas diferentes volatilidades e interações com a fase estacionária.

C) Centrifugação a gás: Correto. O método de centrifugação a gás utiliza diferenças nas massas dos isótopos de urânio para enriquecer o urânio-235. Neste processo, o gás hexafluoreto de urânio (UF6) é utilizado e as moléculas com urânio-235 tendem a se concentrar mais no centro da centrífuga devido a sua menor massa.

D) Destilação fracionada: Incorreto. A destilação fracionada é um método de separação de componentes de uma mistura líquida com base em seus diferentes pontos de ebulição. Não é eficaz para a separação de isótopos de um elemento.

E) Difração de elétrons: Incorreto. A difração de elétrons é uma técnica utilizada para estudar a estrutura atômica e molecular dos materiais, e não é usada para o processo de enriquecimento de urânio.

",C,"
A centrifugação a gás é o processo amplamente utilizado para enriquecer urânio, pois permite a separação dos isótopos baseando-se em suas diferentas massas. Os isótopos mais pesados de urânio-238 tendem a se deslocar para a periferia da centrífuga, enquanto o urânio-235, mais leve, concentra-se mais ao centro, o que facilita sua extração para fins de enriquecimento.",8785949
tópico 3,"Manipulação e tabulação de dados (numpy, pandas, tidyr,verse, data.table)","Questão: Considere um conjunto de dados de vendas de uma empresa, com registros diários ao longo de um ano, em que cada entrada contém as informações de data, quantidade de itens vendidos e o valor total (em reais) de vendas. Para analisar a performance anual, um cientista de dados deseja transformar este conjunto de dados diários em um resumo mensal, indicando para cada mês a quantidade total de itens vendidos e a média diária de vendas em reais. Utilizando a biblioteca pandas do Python, qual seria o código apropriado para realizar esta operação, assumindo que o DataFrame que contém os dados é chamado de 'df_vendas' e possui as colunas 'data', 'qtde_itens' e 'valor_total'?

A) df_vendas['data'] = pd.to_datetime(df_vendas['data'])
   resumo_mensal = df_vendas.set_index('data').resample('M').agg({'qtde_itens': 'sum', 'valor_total': 'mean'})

B) df_vendas.groupby(df_vendas['data'].dt.to_period('M')).agg({'qtde_itens': 'sum', 'valor_total': 'mean'})

C) df_vendas.groupby(df_vendas['data'].str.slice(0, 7)).sum({'qtde_itens', 'valor_total'})

D) df_vendas.groupby([df_vendas['data'].dt.year, df_vendas['data'].dt.month]).aggregate({'qtde_itens': np.sum, 'valor_total': np.average})

E) df_vendas.groupby(pd.Grouper(key='data', freq='M')).aggregate({'qtde_itens': sum, 'valor_total': 'mean'})

",B,"

Explicação dos itens:

A) Incorreto. Este código realiza a conversão de 'data' para o formato DateTime e faz o resumo mensal, no entanto, usa um método depreciado 'set_index' seguido por 'resample', quando se podia usar 'groupby'.

B) Correto. Este código corretamente converte a coluna 'data' em um formato periódico mensal e usa groupby para calcular a soma da coluna 'qtde_itens' e a média da coluna 'valor_total', conforme pedido.

C) Incorreto. Este trecho faz o agrupamento pela coluna 'data' utilizando uma substring que representa apenas o ano e mês, o que poderia ser uma abordagem para obtenção do mês. No entanto, a função 'sum()' está sendo aplicada incorretamente, pois deve receber um dicionário para especificar as operações em colunas diferentes.

D) Incorreto. Ainda que esta opção faça uso da função 'aggregate', que permite aplicar funções específicas a cada coluna após o groupby, utiliza 'np.average' para 'valor_total' em vez de 'np.mean', e não converte a data para um formato periódico mensal.

E) Incorreto. O uso da função 'pd.Grouper' com a chave 'data' e frequência 'M' é adequado para a agregação mensal, mas a chamada ao método 'aggregate' usa 'sum' de maneira direta sem aspas e isso poderia causar um erro, uma vez que se espera receber um dicionário ou a operação em forma de string no contexto da agregação.",8302059
tópico 3,Programação funcional,"Questão: Dentro dos paradigmas de programação, o paradigma funcional se destaca por suas características particulares e seu modelo de execução baseado em funções matemáticas. Sobre a programação funcional e suas características, analise as afirmativas a seguir e marque a opção correta:

I. Uma das propriedades fundamentais da programação funcional é a imutabilidade, onde os dados não são alterados, mas sim novas cópias são criadas a cada transformação.

II. Em programação funcional, todas as funções são consideradas de primeira classe, o que significa que elas podem ser atribuídas a variáveis, passadas como argumentos e retornadas por outras funções.

III. O uso de funções de alta ordem, que recebem outras funções como argumentos ou retornam funções como resultados, é desencorajado no paradigma funcional devido à complexidade que isso pode introduzir no código.

IV. A transparência referencial é uma característica desejável em programação funcional, onde a mesma função, dada a mesma entrada, sempre retornará o mesmo resultado, sem efeitos colaterais.

Estão corretas as afirmativas:

A) I e II, apenas.
B) I, II e IV, apenas.
C) II, III e IV, apenas.
D) III e IV, apenas.
E) Todas as afirmativas estão corretas.

",B,"

Explicação dos itens:

A) Alternativa incorreta. I e II estão corretas, mas IV também está correta, o que faz a opção A ficar incompleta.

B) Alternativa correta. As afirmativas I, II e IV estão corretas. A imutabilidade é um conceito chave da programação funcional. Funções de primeira classe são fundamentais para um estilo de programação flexível e poderoso, e a transparência referencial é uma característica central da programação funcional que permite a previsibilidade e facilita o raciocínio sobre o código.

C) Alternativa incorreta. III está incorreta, pois as funções de alta ordem são, na verdade, encorajadas e amplamente utilizadas na programação funcional devido à sua flexibilidade e poder de abstração.

D) Alternativa incorreta. III está incorreta enquanto IV está correta, tornando essa opção equivocada.

E) Alternativa incorreta. Apesar de I, II e IV estarem corretas, a afirmativa III está incorreta pois contradiz o que é amplamente aceito no paradigma funcional.",7445114
tópico 5,"Métricas de similaridade textual - similaridade do cosseno, distância euclidiana, similaridade de Jaccard, distância de Manhattan e coeficiente de Dice","Questão: Em sistemas de informação, as técnicas de processamento de linguagem natural (PLN) e recuperação de informação (RI) frequentemente utilizam métricas de similaridade textual para comparar documentos ou sentenças e fornecer resultados relevantes aos usuários. Dentre as métricas abaixo, qual NÃO é uma métrica baseada estritamente na contagem de termos compartilhados entre dois conjuntos de dados textuais?

A) Similaridade do Cosseno
B) Similaridade de Jaccard
C) Distância de Manhattan
D) Distância Euclidiana
E) Coeficiente de Dice

",C," 

Explicação:

A) Similaridade do Cosseno - Calcula o cosseno do ângulo entre dois vetores em um espaço vetorial, sendo amplamente utilizado para medir a similaridade entre documentos no PLN. Utiliza contagem de termos através de vetores de frequência de palavras.

B) Similaridade de Jaccard - Mede a similaridade entre conjuntos finitos e é definida como o tamanho da interseção dividido pelo tamanho da união dos conjuntos de amostras. Baseia-se estritamente na contagem de termos compartilhados.

C) Distância de Manhattan - Calcula a distância entre dois pontos em um espaço de grade baseada na soma das diferenças absolutas de suas coordenadas. Não é estritamente baseada na contagem de termos compartilhados, pois leva em conta a distância absoluta nas dimensões do espaço vetorial que representa os textos.

D) Distância Euclidiana - Determina a distância ""direta"" entre dois pontos em um espaço euclidiano, de acordo com o teorema de Pitágoras. Como a Distância de Manhattan, não depende exclusivamente da contagem de termos compartilhados.

E) Coeficiente de Dice - Mede a sobreposição entre dois conjuntos de amostras. Similar ao coeficiente de Jaccard, compara a quantidade de termos compartilhados em relação ao tamanho de cada conjunto.

Portanto, a resposta correta é a opção C, pois a Distância de Manhattan baseia-se em informações adicionais à simples contagem de termos compartilhados, considerando a estrutura espacial dos dados.",1344407
tópico 0,Soluções de big data: Arquitetura do ecossistema Spark,"Questão:

A plataforma Apache Spark é uma das ferramentas mais populares para processamento de big data. Seu ecossistema é composto por vários componentes que juntos proporcionam uma arquitetura robusta para processamento em larga escala. Qual dos seguintes componentes é o responsável pela abstração de alto nível para a definição de operações de transformação e ação em dados distribuídos e por fornecer uma interface para programação de clusters com tolerância a falhas e processamento de dados paralelo?

A) Apache Hadoop HDFS
B) Apache Mesos
C) Apache Hive
D) Apache Spark Core
E) Apache Spark SQL

",D,"

Explicação dos itens:

A) Apache Hadoop HDFS - O HDFS (Hadoop Distributed File System) é um sistema de arquivos distribuídos projetado para armazenar grandes volumes de dados. Embora seja frequentemente usado com o Spark para armazenamento de dados, ele não é responsável pela abstração de alto nível para operações de transformação e ação em dados distribuídos, função essa que é atribuída ao Apache Spark Core.

B) Apache Mesos - Mesos é um gerenciador de recursos de cluster que também pode ser usado com o Spark. No entanto, sua função principal é alocação de recursos entre aplicações em um cluster, e não a abstração de operações de transformação e ação em dados distribuídos.

C) Apache Hive - Hive é um sistema de armazenamento de dados e consulta que fornece uma linguagem similar ao SQL (HiveQL) para a análise de dados no Hadoop. Apesar de ser capaz de executar operações de transformação e ação, o Hive atua em um nível mais alto e usa o MapReduce (ou Sparks) por baixo dos panos para tarefas de processamento de dados.

D) Apache Spark Core - O Spark Core é o componente fundamental do Spark que fornece a abstração de programação fundamental chamada RDD (Resilient Distributed Dataset) e é responsável por funcionalidades básicas, como tarefas de agendamento, tolerância a falhas e gerenciamento de memória. É ele que proporciona a abstração de alto nível para definição de transformações e ações em conjuntos de dados distribuídos.

E) Apache Spark SQL - Spark SQL é um módulo Spark para processamento de dados estruturados. Embora forneça uma abstração semelhante a SQL para trabalhar com big data, não é o componente responsável pela abstração de operações de transformação e ação em dados distribuídos, sendo essa uma das funções centrais do Spark Core.",6761472
tópico 4,Métodos e técnicas de identificação causal: Métodos experimentais RCT e de identificação quase-experimental,"Questão: Em pesquisas nas áreas econômicas e sociais, a identificação de relações causais entre variáveis é fundamental para compreender os mecanismos subjacentes aos fenômenos estudados. Dentre os métodos utilizados para tal identificação, destacam-se os experimentos controlados randomizados (RCTs - Randomized Controlled Trials) e os métodos de identificação quase-experimental. Considerando as diferenças e particularidades desses métodos, avalie os itens a seguir:

I. Experimentos Randomizados (RCTs) requerem a alocação aleatória de tratamentos aos sujeitos do estudo, sendo um método geralmente livre de viés de seleção, pois a randomização tende a balancear as características observadas e não observadas entre os grupos de tratamento e controle.

II. Métodos quase-experimentais, como estudos de Diferença em Diferenças (DiD) e Regressão Descontínua (RD), dependem de suposições mais fortes e muitas vezes são implementados quando a randomização não é viável ou é considerada antiética.

III. Uma característica fundamental dos RCTs é que eles não requerem hipóteses adicionais sobre a ausência de variáveis omitidas ou sobre a forma funcional do modelo, uma vez que a aleatoriedade assegura a comparabilidade entre os grupos.

IV. Em estudos quase-experimentais, a validade das inferências causais é frequentemente dependente da capacidade do pesquisador de controlar por variáveis confundidoras, o que pode ser feito por meio de técnicas como pareamento e instrumentos.

Está correto apenas o que se afirma em:

A) I e IV
B) I, II e III
C) II e III
D) I, II e IV
E) III e IV

",D,"

Item I é verdadeiro: Os RCTs são considerados o padrão-ouro em muitas áreas de pesquisa justamente porque a randomização deve, teoricamente, equalizar características entre os grupos tratamento e controle, reduzindo o viés.

Item II é verdadeiro: Métodos como DiD e RD são alternativas aos RCTs e são adotados em situações em que a randomização não é possível. Suposições mais fortes são necessárias para confiar nos resultados desses métodos quase-experimentais.

Item III é incorreto: Apesar dos RCTs serem menos suscetíveis a variáveis omitidas devido à randomização, ainda podem requerer hipóteses adicionais, principalmente se a randomização não for perfeitamente executada ou se houver desistências do estudo (attrition).

Item IV é verdadeiro: Em estudos quase-experimentais, devido à falta de randomização, o controle de variáveis de confusão é essencial e técnicas como pareamento (matching) e variáveis instrumentais são frequentemente utilizadas para ajudar a estabelecer uma relação causal mais confiável.",9961626
tópico 3,Linguagem de programação Scala,"Questão:
A linguagem de programação Scala integra características de linguagens funcionais com orientação a objetos. Sua interoperabilidade com Java se dá por meio da JVM (Java Virtual Machine), o que fomenta seu uso para projetos que visam conciliar o paradigma funcional com ecossistemas estabelecidos em Java. Considerando os conceitos e funcionalidades da linguagem Scala, avalie as seguintes afirmativas:

I. A inferência de tipos em Scala permite que o programador omita o tipo de uma variável em sua declaração, deixando para o compilador a tarefa de identificar o tipo adequado, com base no contexto de uso da variável.

II. Em Scala, toda função é um objeto e pode ser atribuída a uma variável, passada como argumento ou retornada como valor de outra função, devido ao seu suporte a funções de primeira classe.

III. Scala permite a criação de classes case, que são especialmente úteis na construção de padrões de correspondência (pattern matching), além de fornecer automaticamente métodos de comparação e cópia.

IV. A imutabilidade em Scala é forçada em todos os níveis da linguagem, obrigando que todas as variáveis sejam declaradas com o modificador val, que é o equivalente ao final do Java.

É correto o que se afirma em:

A) I, II e III, apenas.
B) I e II, apenas.
C) I, II e IV, apenas.
D) II, III e IV, apenas.
E) I, II, III e IV.

",A,"

Explicação dos itens:

I. Correto. Scala proporciona um sofisticado recurso de inferência de tipos, permitindo ao programador muitas vezes omitir explicitamente o tipo da variável.
      
II. Correto. Funções em Scala são tratadas como objetos de primeira classe, o que significa que elas podem ser utilizadas como qualquer outro objeto no sistema de tipos, incluindo serem atribuídas a variáveis, ou usadas como parâmetros e retornos de outras funções.
      
III. Correto. Scala inclui o conceito de classes case que são projetadas para serem imutáveis e são comparadas por valor ao invés de por referência. Essas classes facilitam a utilização em padrões de correspondência, entre outras funcionalidades.

IV. Incorreto. Em Scala, o uso de val define uma variável como imutável, mas o uso de var permite definir variáveis mutáveis. Logo, não é verdadeiro que Scala force imutabilidade em todos os níveis, dado que é uma escolha do programador usar val ou var.",8941429
tópico 3,"Visualização de dados ggplot, matplotlib","Questão: Considere que um cientista de dados está realizando uma análise exploratória em um conjunto de dados referente ao desempenho de alunos em uma série de testes acadêmicos. Utilizando a biblioteca ggplot2 do R para visualização dos dados, qual seria o código apropriado para criar um gráfico de dispersão (scatter plot) que apresente a relação entre as notas de matemática (eixo x) e ciências (eixo y), diferenciando cada ponto no gráfico pelo gênero do aluno?

A) ggplot(data = notas, aes(x = matematica, y = ciencias)) + 
     geom_point(aes(color = genero))

B) plt.scatter('matematica', 'ciencias', c = data['genero'], data = notas)
   plt.show()

C) ggplot(notas, aes('matematica', 'ciencias')) + 
     geom_point(aes(colour = 'genero'))

D) ggplot(data = notas) + 
     geom_point(mapping = aes(x = matematica, y = ciencias, color = genero))

E) ggplot(notas, aes(x = matematica, y = ciencias, fill = genero)) + 
     geom_point()

",A,"

Explicação dos itens:

A) Este item está correto porque utiliza a função `ggplot()` com o data frame `notas`, define o mapeamento estético (`aes()`) com as variáveis corretas nos eixos x e y, e adiciona `geom_point()` com a diferenciação de cor por gênero dentro de `aes()`, que é a forma correta de adicionar essa diferenciação em ggplot2.

B) Esta alternativa faz utilização da sintaxe do Matplotlib na biblioteca Python, que não é apropriado para uma questão que especifica o uso do ggplot2 do R.

C) A sintaxe contida nessa opção está incorreta para a ggplot2, pois o formato do mapeamento estético (`aes()`) não deve incluir as variáveis entre aspas simples.

D) Esta opção também é válida no ggplot2; no entanto, como a questão menciona especificamente a necessidade de diferenciar os pontos por gênero, a resposta A é mais completa e, portanto, mais correta sob a perspectiva da questão apresentada.

E) Aqui, enquanto `geom_point()` é usado apropriadamente para criar um gráfico de dispersão, o uso de `fill = genero` não é o adequado para pontos em um scatter plot, que deveria usar `color` para diferenciar as cores dos pontos. A função `fill` é geralmente reservada para geométricas que possuem uma área a ser preenchida, como barras em gráficos de barra.",8148525
tópico 5,"Técnicas de agrupamento: Agrupamento por partição, por densidade e hierárquico","Questão: No contexto da aprendizagem de máquina, diversas técnicas de agrupamento são utilizadas para identificar estruturas naturais ou padrões em conjuntos de dados multidimensionais. Sobre as técnicas de agrupamento por partição, por densidade e hierárquico, avalie as seguintes afirmações:

I. O agrupamento por partição, exemplificado pelo algoritmo k-means, busca dividir o conjunto de dados em um número pré-definido de grupos, minimizando a variância dentro de cada grupo e otimizando a posição dos centróides.

II. A técnica de agrupamento por densidade, como DBSCAN, identifica regiões de alta densidade separadas por regiões de baixa densidade, sendo eficaz mesmo quando os grupos têm formas irregulares ou vários ruídos e pontos de outliers.

III. O agrupamento hierárquico pode ser dividido em dois tipos principais: aglomerativo e divisivo, onde o primeiro inicia com cada ponto de dado em seu próprio cluster e os funde progressivamente, enquanto o segundo começa com um único cluster e realiza divisões sucessivas.

Está(ão) correta(s) apenas a(s) afirmativa(s):

A) I.
B) II.
C) III.
D) I e II.
E) I, II e III.

",E,"

Explicação dos itens:

A) A afirmação I está correta; o agrupamento por partição é clássico na aprendizagem de máquinas e o algoritmo k-means é um dos mais conhecidos dessa abordagem, procurando minimizar a variância intra-cluster e otimizar a localização dos centróides.

B) A afirmação II também está correta; o agrupamento por densidade, como o DBSCAN, é capaz de identificar clusters de formas arbitrárias e é particularmente útil para lidar com ruídos e outliers, identificando regiões de alta densidade que são separadas por regiões de baixa densidade.

C) A afirmação III é correta; o agrupamento hierárquico pode ser de fato aglomerativo, começando com cada ponto em seu próprio cluster e os fundindo progressivamente, ou divisivo, iniciando com todos os pontos em um único cluster e dividindo-o em clusters menores.

D) Essa opção está incorreta pois afirma que apenas as afirmativas I e II estão corretas, enquanto todas as três afirmações são verdadeiras.

E) A opção E é a correta, pois todas as afirmações I, II e III estão corretas e representam adequadamente as características das técnicas de agrupamento por partição, por densidade e hierárquico, respectivamente.",8358769
tópico 1,Banco de dados NoSQL,"Questão: No contexto de sistemas de banco de dados NoSQL, diferentes tipos de dados e modelos de consistência influenciam diretamente a maneira como os dados são armazenados e recuperados. Considerando os principais tipos de bancos NoSQL e suas características, assinale a opção que apresenta corretamente o tipo de banco de dados NoSQL e seu respectivo modelo de dados:

A) Banco de dados em colunas - Utiliza o modelo BASE (Basically Available, Soft state, Eventual consistency) e é otimizado para leituras e escritas rápidas em conjuntos de dados muito grandes distribuídos.

B) Banco de dados de documentos - Armazena os dados em estruturas de árvore binária, em que cada nó representa um documento, facilitando a indexação e recuperação de dados estruturalmente complexos.

C) Banco de dados de grafos - Utiliza o modelo ACID (Atomicity, Consistency, Isolation, Durability) e é projetado para armazenar, mapear e consultar relacionamentos entre dados em uma rede.

D) Banco de dados chave-valor - Segue o modelo relacional e é indicado para situações que demandam esquemas de dados rigorosamente definidos e complexas operações de junção.

E) Banco de dados orientado a objetos - Armazena informações em entidades chamadas de ""blobs"", onde cada blob é uma instância de uma classe no banco de dados.

",C,"

Explicação dos itens:

A) Errado. Bancos de dados em colunas efetivamente utilizam o modelo BASE e são otimizados para operações rápidas em grandes conjuntos de dados distribuídos. No entanto, a opção não informa corretamente sobre o armazenamento em forma de colunas.

B) Errado. Bancos de dados de documentos armazenam dados em formato de documentos, como JSON ou XML, e não em estruturas de árvore binária. Esta opção confunde estruturas de armazenamento com formatos de documentos.

C) Correto. Bancos de dados de grafos realmente utilizam o modelo ACID, pois mantêm a consistência dos dados mesmo com transações complexas que envolvem muitas relações. Eles são projetados para lidar com dados em uma rede, como sistemas de recomendação, redes sociais, etc.

D) Errado. Bancos de dados chave-valor armazenam dados como um conjunto de pares chave-valor e não seguem o modelo relacional. Eles são conhecidos por sua simplicidade e alta performance, especialmente em aplicações que não necessitam de operações de junção complexas.

E) Errado. Bancos de dados orientados a objetos armazenam dados como objetos, conforme as definições da programação orientada a objetos (POO), e não em entidades chamadas ""blobs"". Cada objeto no banco de dados é uma instância de uma classe, mas a descrição da opção não é precisa.",2192129
tópico 4,Teorema do limite central,"Questão: Uma empresa de pesquisa de mercado deseja estimar a satisfação média dos clientes de uma grande rede de varejo. A satisfação do cliente é medida em uma escala de 1 a 10, onde 1 significa completamente insatisfeito e 10 completamente satisfeito. Foi selecionada uma amostra aleatória de 150 clientes, e a média de satisfação encontrada foi de 7,5 com um desvio padrão de 1,2 na amostra. Considerando o Teorema do Limite Central e assumindo que a distribuição da satisfação dos clientes é inicialmente desconhecida, qual é a probabilidade aproximada de que a média de satisfação da população esteja entre 7,4 e 7,6?

A) Menos de 5%
B) Entre 5% e 10%
C) Entre 10% e 20%
D) Entre 20% e 30%
E) Mais de 30%

",E,"

A alternativa correta é E) Mais de 30%. O Teorema do Limite Central afirma que, para amostras grandes (n > 30), a distribuição das médias amostrais tende a ser aproximadamente normal, independentemente da forma da distribuição da população, desde que a amostra seja aleatória e menos de 10% da população total (quando a população é finita).

Aqui estão as explicações dos itens:

A) Este intervalo é muito estreito e, considerando uma distribuição normal, a probabilidade de a média da população estar dentro de um intervalo tão pequeno ao redor da média amostral é muito baixa.

B) Similarmente ao item A, a probabilidade é maior do que 5%, uma vez que estamos lidando com uma distribuição normal e um intervalo pequeno ao redor da média da amostra.

C) Este é um intervalo um pouco mais amplo, mas ainda subestima a probabilidade dada pelo Teorema do Limite Central e a regra empírica (68-95-99.7) para a distribuição normal.

D) Mesmo sendo um intervalo maior que os anteriores, ainda subestima a probabilidade real, pois a regra 68-95-99.7 nos diz que aproximadamente 95% dos dados estão dentro de dois desvios padrão da média, e o intervalo dado é muito menor que isso.

E) Esse é o intervalo mais provável, dado que o intervalo de 7,4 a 7,6 é relativamente pequeno e está muito próximo da média amostral. Usando a distribuição normal padrão, podemos calcular essa probabilidade, ou estimar usando a regra empírica, que sugeriria que a probabilidade é alta, já que o intervalo é estreito e centrado na média da amostra.",3004620
tópico 5,Técnicas de classificação: Naive Bayes; Regressão logística; Redes neurais artificiais; Árvores de decisão (algoritmos ID3 e C4.5); Florestas aleatórias (random forest); Máquinas de vetores de suporte (SVM – support vector machines); K vizinhos mais próximos (KNN – K-nearest neighbours),"Questão:
Um pesquisador está desenvolvendo um modelo de classificação para prever se uma música se tornará um sucesso com base em características como duração, gênero, número de vezes que foi compartilhada nas redes sociais, entre outros metadados. Seu dataset é composto por milhares de músicas, cada uma com sua popularidade rotulada como ""sucesso"" ou ""não sucesso"". O pesquisador deseja utilizar um algoritmo de classificação que possa lidar bem com a possibilidade de relações não-lineares entre as características e o rótulo de popularidade e que, além disso, possa fornecer uma ideia da importância relativa de cada característica para o modelo de previsão. 

Qual dos seguintes algoritmos é o MAIS apropriado para ele utilizar na construção de seu modelo de classificação?

A) Naive Bayes

B) Regressão logística

C) Redes neurais artificiais

D) Árvores de decisão (algoritmos ID3 e C4.5)

E) Máquinas de vetores de suporte (SVM – Support Vector Machines)

F) K vizinhos mais próximos (KNN – K-nearest neighbours)

G) Florestas aleatórias (random forest)

",G,"

Explicação:

A) Naive Bayes é um algoritmo baseado no teorema de Bayes, e funciona melhor quando as características são independentes entre si, o que pode não ser o caso neste cenário.

B) A regressão logística é um bom modelo para classificação binária, mas pode não capturar relações não-lineares tão bem quanto outros algoritmos.

C) As redes neurais artificiais são poderosas para capturar relações não-lineares e são altamente flexíveis, porém, elas geralmente não fornecem compreensões diretas sobre a importância relativa das características.

D) As árvores de decisão, como ID3 e C4.5, são modelos interpretáveis que podem lidar com não-linearidades, mas individualmente podem não ser tão robustas quanto uma combinação de múltiplas árvores.

E) SVM pode ser usado para encontrar limites de decisão não-lineares com o uso de truques do kernel, mas não fornece uma compreensão direta da importância das características.

F) KNN é um algoritmo intuitivo que funciona bem para datasets onde a proximidade das amostras é um bom indicador da sua classificação, porém pode ser computacionalmente custoso e não oferece visão direta sobre a importância das características.

G) Florestas aleatórias são um conjunto de árvores de decisão que melhoram a robustez e o desempenho do modelo em comparação com uma única árvore de decisão. Além disso, elas podem lidar bem com relações não-lineares e são capazes de fornecer um ranking de importância das características, atendendo às necessidades específicas do pesquisador.",2104455
tópico 1,Álgebra relacional e SQL (padrão ANSI),"Questão: 

Considere as seguintes relações no esquema de uma base de dados de uma biblioteca:

Livros(codigo_livro, titulo, ano_publicacao)
Emprestimos(num_emprestimo, codigo_livro, data_emprestimo, data_devolucao, codigo_socio)
Socios(codigo_socio, nome_socio, endereco)

Um bibliotecário deseja encontrar os títulos dos livros que nunca foram emprestados. Qual das seguintes consultas em SQL (padrão ANSI) retornaria o resultado correto?

A) SELECT titulo FROM Livros WHERE codigo_livro NOT IN (SELECT codigo_livro FROM Emprestimos);

B) SELECT titulo FROM Livros L INNER JOIN Emprestimos E ON L.codigo_livro = E.codigo_livro WHERE E.data_emprestimo IS NULL;

C) SELECT titulo FROM Livros WHERE codigo_livro NOT IN (SELECT codigo_livro FROM Emprestimos WHERE data_emprestimo IS NOT NULL);

D) SELECT L.titulo FROM Livros L LEFT JOIN Emprestimos E ON L.codigo_livro = E.codigo_livro WHERE E.num_emprestimo IS NULL;

E) SELECT L.titulo FROM Livros L, Emprestimos E WHERE L.codigo_livro = E.codigo_livro AND E.data_emprestimo IS NULL;

",A," 

Explicação dos itens:

A) Correta. Esta consulta seleciona todos os títulos de livros cujos códigos não estão presentes na tabela de Empréstimos. Isso implica que estes livros nunca foram emprestados.

B) Incorreta. Esta consulta realiza um INNER JOIN entre as tabelas Livros e Empréstimos, o que retornará apenas livros que foram emprestados. Onde a cláusula 'WHERE E.data_emprestimo IS NULL' é satisfeita por nenhuma linha, pois o INNER JOIN já elimina as entradas não correspondentes.

C) Incorreta. Onde a cláusula no subselect exclui os códigos de livros com empréstimos ativos, mas isso incluirá livros que já foram emprestados e devolvidos.

D) Incorreta. Embora a consulta realize um LEFT JOIN e parece selecionar corretamente os livros que não foram emprestados (WHERE E.num_emprestimo IS NULL), o nome da coluna 'num_emprestimo' é um confusor; em uma tabela de Empréstimos, esta coluna dificilmente seria nula, já que identifica um empréstimo e não se refere à ausência do mesmo.

E) Incorreta. Esta é uma consulta que utiliza um cruzamento cartesiano (CROSS JOIN) que deve ser evitado quando possível devido à performance, além de que erro no WHERE considera livros que já foram emprestados, mas com uma data de empréstimo nula, o que não condiz com a lógica do negócio de uma biblioteca e é irrelevante para encontrar livros que nunca foram emprestados.",929111
tópico 0,Soluções de big data: Arquitetura do ecossistema Spark,"Questão: Considerando a arquitetura do ecossistema Apache Spark nas soluções de big data, qual das seguintes opções melhor descreve uma característica fundamental do Resilient Distributed Dataset (RDD), um dos principais abstratos de dados fornecido pelo Spark para processamento e análise de dados?

A) RDDs são coleções imutáveis de objetos distribuídos que só podem ser manipulados através de operações de SQL.

B) RDDs oferecem tolerância a falhas armazenando múltiplas cópias dos dados em diferentes nós do cluster, permitindo a recuperação automática em caso de perda de um nó.

C) RDDs são estruturas de dados mutáveis que permitem atualizações em tempo real, favorecendo o uso em sistemas de processamento de transações online (OLTP).

D) RDDs são abstrações que permitem o processamento paralelo em um cluster de maneira eficiente por serem particionados e imutáveis, o que simplifica a programação distribuída.

E) RDDs são abstrações de alto nível que eliminam a necessidade de gerenciar explicitamente a distribuição e paralelismo dos dados, pois eles são processados sequencialmente em cada nó do cluster.

",D,"

Explicação dos itens:

A) Incorreto. RDDs são coleções imutáveis de objetos, mas não são restritos somente às operações de SQL. Eles podem ser manipulados por meio de diversas operações de transformações e ações em um estilo de programação funcional.

B) Incorreto. A tolerância a falhas nos RDDs não é garantida pelo armazenamento de múltiplas cópias dos dados, mas sim pelo conceito de linhagem (lineage), que permite que se os dados forem perdidos, eles possam ser reconstruídos usando o histórico de como foram derivados.

C) Incorreto. RDDs são imutáveis; uma vez criados, eles não podem ser alterados. Isso difere da funcionalidade requerida por sistemas OLTP, os quais dependem de estruturas de dados mutáveis para atualizações em tempo real.

D) Correto. RDDs são projetados para serem imutáveis e particionados, permitindo o processamento em paralelo através de cluster. Essa imutabilidade traz benefícios como facilidade na programação distribuída e na recuperação de erros, pois qualquer operação de transformação cria um novo RDD.

E) Incorreto. RDDs fornecem abstrações que facilitam o gerenciamento da distribuição dos dados e do paralelismo implicitamente, entretanto, o processamento não é sequencial, mas sim paralelo e distribuído por todo o cluster.",100879
tópico 0,Armazenamento de big data,"Questão:

A capacidade de armazenar e processar grandes volumes de dados – conhecida como Big Data – tem ganho cada vez mais importância no âmbito da ciência de dados e análises avançadas. No tocante às soluções de armazenamento projetadas para esse fim, qual das seguintes opções descreve uma característica essencial que os sistemas de armazenamento de Big Data devem possuir para manejar eficientemente o volume, a velocidade e a variedade dos dados?

A) Alta capacidade de armazenamento em discos locais.
B) Suporte apenas para dados estruturados para otimizar o desempenho.
C) Capacidades de compressão de dados para reduzir o espaço físico necessário.
D) Sistemas robustos de indexação e busca para velocidade de acesso aos dados.
E) Escalabilidade horizontal para permitir o crescimento do sistema de forma flexível.

",E,"

Explicação dos itens:

A) Alta capacidade de armazenamento em discos locais.
- Esta alternativa não é suficiente para lidar com big data, pois o enfoque exclusivo em discos locais ignora aspectos como escalabilidade, processamento distribuído e a necessidade de gerenciar dados não estruturados.

B) Suporte apenas para dados estruturados para otimizar o desempenho.
- O Big Data é caracterizado também pela variedade, o que inclui dados não estruturados e semi-estruturados. Um sistema de armazenamento eficiente para Big Data não pode se limitar apenas aos dados estruturados.

C) Capacidades de compressão de dados para reduzir o espaço físico necessário.
- Enquanto a compressão de dados é uma técnica útil, ela não é suficiente por si só para abordar os principais desafios do Big Data, que incluem não apenas volume, mas também a velocidade e variedade dos dados.

D) Sistemas robustos de indexação e busca para velocidade de acesso aos dados.
- Sistemas robustos de indexação e busca são importantes, mas não abordam diretamente o desafio da escalabilidade, que é fundamental para lidar com os volumes crescentes de dados de Big Data.

E) Escalabilidade horizontal para permitir o crescimento do sistema de forma flexível.
- Esta é a característica mais essencial para sistemas de armazenamento de Big Data. A escalabilidade horizontal refere-se à capacidade do sistema de expandir adicionando mais nodos na rede, o que é crucial para lidar com o aumento de dados (volume), a necessidade de processamento rápido (velocidade) e a gestão eficiente de diferentes tipos de dados (variedade).",7879681
tópico 1,"Banco de dados relacional: SQL Server, PostgreSQL, MySQL","Questão: Em um cenário de alta demanda por performance em uma aplicação de banco de dados relacional que envolve o SQL Server, PostgreSQL e MySQL, um DBA precisa escolher uma estratégia para implementar índices eficientes. Considerando as características de índices de cada sistema gerenciador de banco de dados (SGBD), avalie as seguintes afirmações:

I. O índice ""Clustered"" no SQL Server define a ordenação física dos dados na tabela, e uma tabela pode ter apenas um índice desse tipo.
II. O PostgreSQL utiliza o método de índice B-tree por padrão, mas suporta uma variedade de tipos de índices, incluindo hash, GiST, SP-GiST, GIN e BRIN.
III. O MySQL possui o índice FULLTEXT, que é ideal para realizar buscas rápidas em colunas que armazenam grandes quantidades de texto, mas não é suportado em tabelas com o motor de armazenamento InnoDB.

Assinale a alternativa que indica todas as afirmações verdadeiras.

A) Apenas I.
B) Apenas II.
C) I e II.
D) I e III.
E) II e III.

",C," 
A alternativa correta é a letra C.

I. Verdadeiro. No SQL Server, um índice ""Clustered"" altera a ordem física de armazenamento dos dados em uma tabela e uma tabela pode possuir somente um desses índices, pois ele define a ordenação dos registros.

II. Verdadeiro. O PostgreSQL oferece diversas opções de índices. O método B-tree é o padrão, e os outros tipos (hash, GiST, SP-GiST, GIN e BRIN) são usados para casos específicos onde podem oferecer melhor performance.

III. Falso. No MySQL, é verdade que o índice FULLTEXT é usado para indexar colunas de texto para pesquisas rápidas em texto. No entanto, a afirmação está desatualizada. Embora nas versões iniciais o índice FULLTEXT não fosse suportado no motor InnoDB, desde o MySQL 5.6 o motor InnoDB também suporta índices FULLTEXT.",9066212
tópico 2,Enriquecimento,"Questão: Em processos industriais, o enriquecimento de um material frequentemente implica na separação e concentração de um determinado constituinte desejado presente em uma mistura complexa. Um exemplo clássico de enriquecimento é a produção de urânio enriquecido, que é utilizado como combustível em reatores nucleares. O urânio natural contém cerca de 0,7% de urânio-235, isótopo físsil necessário para a reação em cadeia. O enriquecimento aumenta essa porcentagem para atender às especificações do reator. Qual método dentre os listados abaixo NÃO é utilizado para o enriquecimento de urânio?

A) Difusão gasosa
B) Centrifugação a gás
C) Processo de enriquecimento eletromagnético
D) Filtração por membranas seletivas
E) Enriquecimento por laser

",D," 

A difusão gasosa (A) e a centrifugação a gás (B) são técnicas tradicionais de enriquecimento de urânio. O processo de enriquecimento eletromagnético (C), conhecido como calutrons, foi utilizado durante o Projeto Manhattan, mas é menos comum atualmente. O enriquecimento por laser (E) refere-se ao processo de separação isotópica usando lasers para excitar seletivamente isótopos de urânio. A filtração por membranas seletivas (D) não é uma técnica usada para enriquecer urânio, pois os isótopos de urânio têm propriedades químicas idênticas e não podem ser separados por simples filtração.",7803587
tópico 0,Soluções de big data: Arquitetura do ecossistema Spark,"Questão:

A arquitetura do Apache Spark é projetada para facilitar o processamento de grandes volumes de dados de maneira eficiente e rápida. Em relação às componentes principais que fazem parte da arquitetura do Spark, associe corretamente as seguintes funções às suas respectivas componentes:

I. Spark SQL
II. Spark Streaming
III. MLlib
IV. GraphX
V. Spark Core

( ) Utiliza o abstração chamada Resilient Distributed Dataset (RDD) para processamento distribuído de dados.
( ) Provê uma biblioteca de machine learning para o processamento analítico preditivo.
( ) Oferece suporte para o processamento de dados em tempo real.
( ) Habilita a manipulação de grafos e computações paralelas relacionadas a grafos.
( ) Permite a execução de consultas SQL e possibilita a leitura de dados de múltiplas fontes de dados estruturadas.

A sequência correta de associações, de cima para baixo, é:

a) V, III, II, IV, I
b) V, II, III, I, IV
c) IV, III, II, I, V
d) I, IV, III, II, V
e) III, V, IV, I, II

",B," 
Explicação dos itens:

I. Spark SQL - (e) É a componente do Spark responsável pela integração do processamento de dados estruturados e semi-estruturados com a facilidade das consultas SQL. Ela também permite a leitura de diferentes fontes de dados, como JSON, Hive e Parquet.

II. Spark Streaming - (c) Esta componente do Spark permite o processamento de fluxos contínuos de dados (streaming). Com ela, é possível realizar análises em tempo real sobre os dados que estão sendo gerados e recebidos constantemente.

III. MLlib - (b) MLlib é a biblioteca de machine learning embutida no Spark. Ela oferece diversos algoritmos e utilidades para tarefas de aprendizado de máquina, simplificando a implementação de modelos preditivos sobre grandes conjuntos de dados.

IV. GraphX - (d) É a API para manipulação de grafos e execução de algoritmos paralelos relacionados a grafos. Com ela, os usuários podem realizar operações sobre grafos de maneira otimizada e distribuída.

V. Spark Core - (a) Constitui o coração do Spark, onde as funcionalidades fundamentais do sistema estão implementadas, incluindo a programação de trabalhos e tarefas distribuídas, a gestão de memória e o fault tolerance. A sua abstração principal são os RDDs (Resilient Distributed Datasets), que permitem o processamento paralelo e distribuído dos dados.",1100074
tópico 5,Técnicas de classificação: Naive Bayes; Árvores de decisão (algoritmos ID3 e C4.5); Florestas aleatórias (random forest); Máquinas de vetores de suporte (SVM – support vector machines); K vizinhos mais próximos (KNN – K-nearest neighbours),"Questão: Sobre as técnicas de classificação em Aprendizado de Máquina, é INCORRETO afirmar que:

A) O classificador Naive Bayes assume independência condicional entre os preditores, o que muitas vezes simplifica os cálculos, apesar de esta suposição nem sempre ser verdadeira nos dados reais.

B) As árvores de decisão utilizando os algoritmos ID3 e C4.5 geram árvores binárias, onde cada nó interno representa um teste de um atributo e cada folha representa uma classe.

C) Florestas aleatórias (random forest) são um conjunto de árvores de decisão que são treinadas com subconjuntos dos dados e características, visando aumentar a diversidade entre as árvores e, por consequência, melhorar a generalização do modelo.

D) Máquinas de vetores de suporte (SVM) procuram encontrar o hiperplano que maximiza a margem entre as classes, sendo particularmente eficazes em espaços de alta dimensão.

E) O método dos K vizinhos mais próximos (KNN) é sensível ao desbalanceamento de classes e à presença de atributos irrelevantes ou redundantes, podendo requerer uma etapa de seleção ou ponderação de atributos para melhorar seu desempenho.

",B,"

Explicação dos itens:

A) Correto. O classificador Naive Bayes realmente faz essa suposição de independência. Esse é um dos motivos pelos quais é chamado de ""Naive"" ou ingênuo.

B) Incorreto. Os algoritmos ID3 e C4.5 geram árvores que não são necessariamente binárias. Os nós podem ter mais de duas saídas. Isto é, cada decisão não é limitada a uma separação binária, podendo dividir os dados com base em múltiplos valores de um único atributo.

C) Correto. As florestas aleatórias são uma técnica de ensemble que cria um conjunto de árvores de decisão durante o treinamento. Isso é feito para melhorar a robustez e precisão dos resultados de previsão.

D) Correto. As SVMs são utilizadas para classificar dados encontrando um hiperplano ótimo que separa os dados em classes, maximizando a margem entre diferentes classes de dados.

E) Correto. De fato, o KNN é um algoritmo que pode ser bastante afetado pela presença de atributos irrelevantes ou redundantes, assim como pela distribuição desigual das classes no conjunto de treinamento, o que pode prejudicar sua precisão e exigir pré-processamento dos dados.",3367554
tópico 2,Contexto de IA: Enriquecimento,"Questão:

A Fundação CESGRANRIO, em um de seus mais recentes exames, incluiu a seguinte pergunta sobre Inteligência Artificial (IA) e seu papel no processo de enriquecimento de dados:

""Em relação ao processo de enriquecimento de dados aplicado nos sistemas de Inteligência Artificial (IA), é correto afirmar que:

A) O enriquecimento de dados é uma técnica que diminui a necessidade de dados limpos e bem estruturados, pois a IA pode interpretar e corrigir automaticamente qualquer inconsistência.
B) O enriquecimento de dados é um processo irrelevante para IA, uma vez que algoritmos avançados conseguem obter resultados satisfatórios mesmo com conjuntos de dados limitados e de baixa qualidade.
C) O enriquecimento de dados envolve a adição de novos dados ou fontes de dados externas ao conjunto de dados primário, a fim de melhorar a qualidade dos insights gerados pelo modelo de IA.
D) O processo de enriquecimento de dados é exclusivamente manual e não pode ser otimizado ou automatizado por meio de técnicas de IA.
E) O enriquecimento de dados é uma prática que envolve a exclusão de grande parte dos dados existentes para que os algoritmos de IA trabalhem com um volume de dados mais gerenciável.""

",C,"

Explicação dos itens:

A) Item incorreto. O enriquecimento de dados não diminui a necessidade de dados de qualidade; pelo contrário, o propósito é melhorar um conjunto existente com informações adicionais relevantes para aprimorar a precisão dos modelos de IA.

B) Item incorreto. O enriquecimento de dados é muito relevante para a IA, pois contribui para que os modelos tenham um desempenho melhor, especialmente em situações onde os dados nativos são limitados ou de qualidade questionável.

C) Item correto. O enriquecimento de dados realmente envolve a adição de dados relevantes ao conjunto primário, o que pode incluir informações de diferentes fontes, auxiliando assim na melhoria da qualidade dos insights e na performance dos modelos de IA.

D) Item incorreto. O enriquecimento de dados pode ser tanto manual quanto automatizado, inclusive com o uso de IA para identificar e integrar novas fontes de informação de forma mais eficiente.

E) Item incorreto. A prática de enriquecimento de dados não é sobre excluir dados, mas sim sobre complementá-los com informações adicionais para enriquecer a análise que os algoritmos de IA realizarão.",206239
tópico 3,Programação orientada a objetos,"Questão: Em Programação Orientada a Objetos (POO), o conceito de herança é uma das principais características que permitiram o avanço no reuso de código e na organização das estruturas de dados de maneira hierárquica e sistemática. Sobre herança, analise as seguintes afirmações:

I. A herança múltipla permite que uma classe herde o comportamento de mais de uma classe base, o que pode introduzir complexidade adicional ao resolver ambiguidades que podem surgir com membros de mesmo nome.

II. Em linguagens que não suportam herança múltipla diretamente, como Java, é possível alcançar funcionalidade semelhante por meio do uso de interfaces, as quais permitem a especificação de métodos que devem ser implementados pela classe filha.

III. O princípio do polimorfismo, intimamente ligado à herança, impede que uma classe filha possua métodos com as mesmas assinaturas dos métodos da classe pai, para evitar conflitos durante a implementação de sobreposições.

IV. Herança é um conceito que caracteriza apenas as linguagens de programação estáticas e fortemente tipadas, não sendo aplicável em linguagens dinâmicas ou fracamente tipadas.

Assinale a opção que apresenta somente as afirmações corretas:

A) I e II
B) I e III
C) II e IV
D) III e IV
E) I, II e III

",A,"

Explicação dos itens:

I. Correto. A herança múltipla realmente permite que uma classe herde características de múltiplas classes bases. Contudo, isso pode levar a problemas de ambiguidade, especialmente conhecidos como ""problema do diamante"", onde uma classe herda de duas classes que possuem um mesmo ancestral comum.

II. Correto. Em Java, por exemplo, a herança múltipla não é suportada diretamente, mas o uso de interfaces permite que uma classe concorde em implementar métodos especificados por múltiplas interfaces, fornecendo assim um padrão semelhante à herança múltipla no que diz respeito à definição do contrato que a classe filha deve cumprir.

III. Incorreto. O polimorfismo permite justamente que métodos com a mesma assinatura sejam redefinidos em uma classe filha, possibilitando que objetos sejam tratados de maneira mais genérica. O que se busca evitar não é a sobreposição de métodos, mas sim garantir que a implementação seja consistente com a interface apresentada pela classe pai.

IV. Incorreto. Herança é um conceito que se aplica a várias linguagens de programação, independentemente de serem estáticas ou dinâmicas, fortes ou fracamente tipadas. Por exemplo, Python é uma linguagem dinâmica e fracamente tipada que suporta herança.

Portanto, somente as afirmações I e II são corretas.",6087953
tópico 2,Contexto de IA: Algoritmos fuzzy matching e stemming,"QUESTÃO:

Na área de processamento de línguas naturais e recuperação de informações, técnicas como algoritmos de fuzzy matching e stemming são comumente utilizados para aumentar a eficácia dos sistemas em compreender e manipular dados textuais. Considerando os conceitos e aplicações dessas técnicas, analise as afirmativas abaixo:

I - Fuzzy matching refere-se ao processo que identifica pares de strings de texto que são aproximadamente ou parcialmente iguais, mas não exatamente idênticos, o que é útil para correção ortográfica automática e para sistemas de busca tolerantes a erros.

II - Stemming é uma técnica que consiste na redução de palavras para a sua forma radical, permitindo que diferentes variantes de uma palavra sejam associadas a uma única forma base, utilizada para melhorar a precisão do algoritmo de busca em mecanismos de pesquisa.

III - Um sistema de busca que aplica ambos, algoritmos de fuzzy matching e stemming, não pode ser aplicado em idiomas que possuem alta inflexão morfológica, como o russo ou o árabe, já que as raízes das palavras nesses idiomas não são estáveis.

IV - Algoritmos de fuzzy matching frequentemente utilizam uma métrica chamada Distância de Levenshtein para calcular o número mínimo de operações necessárias para transformar uma string de caracteres em outra, fundamentando-se em operações como inserção, exclusão e substituição de caracteres.

É correto o que se afirma em:

A) I e II, apenas.
B) I, II e IV, apenas.
C) II e III, apenas.
D) I, III e IV, apenas.
E) Todas as afirmativas são corretas.

",B,"

EXPLICAÇÃO DOS ITENS:

I - Correto. Fuzzy matching é uma técnica que encontra correspondências que são aproximadamente iguais, sendo amplamente utilizada em corretores ortográficos e sistemas de busca que permitem certa margem de erro, aumentando a acessibilidade do sistema para interpretar inputs variados do usuário.

II - Correto. Stemming reduz as palavras para suas raízes (ou ""stems"") e é uma prática comum em mecanismos de busca para associar termos semelhantes a uma consulta de pesquisa, melhorando a relevância dos resultados.

III - Incorreto. A afirmação é falsa porque algoritmos de fuzzy matching e stemming podem ser adaptados para idiomas com alta inflexão morfológica. De fato, a eficácia desses algoritmos pode depender de ajustes específicos para a morfologia de cada idioma, mas isso não os torna inaplicáveis.

IV - Correto. A Distância de Levenshtein é uma métrica usada em algoritmos de fuzzy matching para quantificar quão diferentes duas sequências de texto são, e baseia-se em operações como inserção, exclusão e substituição para transformar uma string de texto em outra.

Dado que as afirmativas I, II e IV estão corretas, a resposta correta é a opção B.",3930557
tópico 1,Álgebra relacional e SQL (padrão ANSI),"Questão:
Analise as seguintes operações de álgebra relacional e consultas SQL correspondentes:

I. Seleção (σ)
Álgebra Relacional: σ_(condição)(R)
SQL: SELECT * FROM R WHERE condição;

II. Projeção (π)
Álgebra Relacional: π_(atributo1, atributo2,...,atributon)(R)
SQL: SELECT atributo1, atributo2,...,atributon FROM R;

III. Renomeação (ρ)
Álgebra Relacional: ρ_(S ← R)
SQL: Não há equivalente direto em SQL para a renomeação de toda a relação, mas pode-se usar alias para atributos ou subconsultas.

IV. União (∪)
Álgebra Relacional: R ∪ S
SQL: SELECT * FROM R UNION SELECT * FROM S;

V. Produto Cartesiano (×)
Álgebra Relacional: R × S
SQL: SELECT * FROM R, S;

Com base nas operações e consultas citadas acima, assinale a opção que contém uma equivalência INCORRETA entre a álgebra relacional e a consulta SQL.

A) I - Seleção
B) II - Projeção
C) III - Renomeação
D) IV - União
E) V - Produto Cartesiano

",C,"

Explicação dos Itens:

A) I - Seleção: A operação de seleção na álgebra relacional é equivalentemente representada pela cláusula WHERE no SQL. A alternativa está correta.

B) II - Projeção: A projeção na álgebra relacional, que seleciona determinados atributos das tuplas, é diretamente equivalente à especificação de colunas na cláusula SELECT no SQL. A alternativa está correta.

C) III - Renomeação: A alternativa C afirma uma equivalência incorreta. Enquanto a renomeação na álgebra relacional altera o nome da relação (ou tabela) ou de seus atributos, não existe um comando direto em SQL para renomear uma tabela inteira na consulta; apenas é possível dar apelidos (aliases) para atributos ou tabelas em determinadas consultas. Portanto, a alternativa C é a correta pois representa a equivalência incorreta.

D) IV - União: A união de duas relações na álgebra relacional é representada em SQL pela cláusula UNION aplicada a duas consultas SELECT. A alternativa está correta.

E) V - Produto Cartesiano: O produto cartesiano entre duas relações na álgebra relacional é realizado em SQL selecionando-se todas as colunas das duas tabelas sem especificar uma condição de junção, o que é representado pela listagem das tabelas separadas por vírgulas no FROM. A alternativa está correta.",7467467
tópico 2,Contexto de IA: Enriquecimento,"Questão: A aplicação do conceito de enriquecimento no contexto da Inteligência Artificial (IA) é uma operação crucial no processamento e análise de grandes conjuntos de dados, contribuindo para a melhoria da acurácia dos modelos de aprendizado de máquina. Qual das seguintes alternativas descreve melhor o enriquecimento de dados no contexto da IA?

A) A redução da dimensão dos dados através de métodos como Principal Component Analysis (PCA), visando diminuir a complexidade computacional dos modelos.
B) O processo de limpar e organizar os dados removendo informações duplicadas ou inconsistentes, sem adicionar novas informações externas.
C) A transformação de dados brutos em um formato estruturado mais adequado para a modelagem, mas sem incluir dados externos ao conjunto original.
D) A adição de novas variáveis ou características provenientes de fontes externas, que podem ser relevantes para o modelo, aumentando assim seu potencial preditivo.
E) A codificação de variáveis categóricas em valores numéricos utilizando técnicas como One-Hot Encoding, mantendo-se estritamente dentro do conjunto de dados original.

",D,"

Explicação dos itens:

A) Incorreta. PCA é uma técnica de redução de dimensionalidade e não se caracteriza como enriquecimento, pois busca simplificar os dados sem adicionar novos contextos ou variáveis.
B) Incorreta. A limpeza de dados é importante, mas não representa enriquecimento, pois não expande o conjunto de dados com informações adicionais, apenas organiza o existente.
C) Incorreta. Embora a estruturação de dados seja um passo importante antes da modelagem, o enriquecimento implica a introdução de novos dados ou características, não apenas a transformação do formato dos dados já disponíveis.
D) Correta. O enriquecimento de dados no contexto de IA geralmente envolve a adição de novas informações ou características de fontes externas ao conjunto de dados original, proporcionando assim um contexto mais rico para a construção de modelos preditivos.
E) Incorreta. A codificação de variáveis categóricas, como One-Hot Encoding, é uma técnica de pré-processamento que adapta os dados para técnicas de aprendizado de máquina, mas não é um enriquecimento, já que não adiciona dados externos.",2689611
tópico 1,"Banco de dados e formatos de arquivo orientado a colunas: Parquet, MonetDB, duckDB","Questão: Em ambientes de Big Data, a eficiência na leitura e escrita de grandes conjuntos de dados é crucial para o desempenho de operações analíticas. Considerando o cenário de bancos de dados e formatos de arquivos orientados a colunas, avalie as seguintes afirmações sobre Parquet, MonetDB e duckDB:

I. Parquet é um formato de armazenamento orientado a colunas otimizado para o ecossistema Hadoop, oferecendo alta compressão e eficiência em operações de leitura, especialmente em sistemas de arquivos distribuídos.

II. MonetDB é um sistema de gerenciamento de banco de dados relacional que utiliza um modelo de armazenamento em linha, o que o torna inadequado para análises que requerem varreduras eficientes de grandes volumes de dados.

III. duckDB é um sistema de gerenciamento de banco de dados analítico orientado a colunas, projetado para processamento ad-hoc e análise de dados, podendo ser integrado com linguagens de programação como Python e R para manipulação de dados.

Assinale a alternativa que apresenta a(s) afirmação(ões) correta(s):

A) Apenas I e III.
B) Apenas I e II.
C) Apenas II e III.
D) Apenas I.
E) I, II e III.

",A,"

A explicação dos itens:

I. Correta: Parquet é de fato um formato de arquivo orientado a colunas, amplamente utilizado no ecossistema Hadoop, sendo muito eficiente para operações de leitura e oferecendo boas taxas de compressão. Suporta esquemas aninhados e é projetado para funcionar bem com armazenamento distribuído.

II. Incorreta: MonetDB é um banco de dados relacional, mas diferentemente do que afirma a questão, ele é baseado em um modelo de armazenamento orientado a colunas. Isso o torna apropriado para operações analíticas e capaz de realizar varreduras rápidas em grandes conjuntos de dados.

III. Correta: duckDB é um sistema de gerenciamento de banco de dados orientado a colunas, otimizado para análises de dados e consultas ad-hoc. Ele fornece integração com linguagens de programação como Python e R, facilitando a análise de dados dentro destes ambientes de programação.",1017879
tópico 3,Programação funcional,"Questão: Considere os conceitos fundamentais da programação funcional, um paradigma de programação que enfatiza a aplicação de funções, em contraste com a programação imperativa que enfatiza mudanças no estado do programa. Dado o seguinte fragmento de código em Haskell, identifique qual das afirmações abaixo é correta.

```haskell
quadrados :: [Int] -> [Int]
quadrados xs = map (\x -> x * x) xs
```

a) A função `quadrados` é um exemplo de função de alta ordem, pois retorna outra função como resultado.

b) A função `quadrados` utiliza a técnica de recursão para calcular o quadrado de cada elemento da lista.

c) A expressão lambda `(\x -> x * x)` é desnecessária, pois Haskell não suporta expressões lambda aninhadas.

d) A aplicação da função `map` na função `quadrados` é um exemplo de transparência referencial, garantindo que a função retornará o mesmo resultado para os mesmos valores de entrada.

e) A função `quadrados` viola o princípio de imutabilidade, uma vez que modifica a lista `xs` fornecida como entrada.

",D,"

- Item A: Incorreto pois uma função de alta ordem é aquela que pode receber outra função como argumento ou retornar uma função como resultado. Neste caso, a função `quadrados` recebe uma lista e retorna uma lista, não uma função.
- Item B: Incorreto porque a função `quadrados` não é recursiva; ela usa `map` para aplicar uma função diretamente a cada elemento da lista.
- Item C: Incorreto porque Haskell suporta expressões lambda e a expressão lambda dada é usada corretamente como argumento para a função `map`.
- Item D: Correto pois a aplicação de funções puras, como demonstrado pelo uso da função `map` com uma expressão lambda pura (que não tem efeitos colaterais), é um exemplo de transparência referencial. Ou seja, para um dado input, o output sempre será o mesmo, o que é um conceito central em programação funcional.
- Item E: Incorreto já que a função `quadrados` não modifica a lista `xs`; ela gera e retorna uma nova lista, mantendo a lista original inalterada, preservando assim o princípio de imutabilidade.",3343494
tópico 6,"Diagramas causais: gráficos acíclicos dirigidos; variáveis confundidoras, colisoras e de mediação","Questão: Em um estudo epidemiológico, um pesquisador está analisando a relação entre o consumo de uma nova bebida energética (X) e o aumento da pressão arterial (Y). Para isso, ele utiliza diagramas causais baseados em gráficos acíclicos dirigidos (DAGs). Ao longo de sua análise, ele identifica três tipos de variáveis no seu modelo: Z1, que é uma variável confundidora; Z2, que é uma variável colisora; e Z3, que é uma variável de mediação. Considerando a estrutura causal subjacente e a interpretação correta dessas variáveis, qual das seguintes afirmações sobre o efeito do consumo da bebida energética na pressão arterial é mais precisa?

A) Ajustar o modelo para a variável confundidora Z1 é desnecessário, pois o confundimento pode ser ignorado em DAGs.
B) A variável colisora Z2 deve ser incluída no modelo para garantir a estimativa adequada do efeito direto de X sobre Y.
C) O ajuste para a variável de mediação Z3 é essencial para determinar o efeito total de X sobre Y.
D) A variável de mediação Z3 deve ser ajustada apenas se o interesse for na avaliação do efeito indireto de X sobre Y.
E) O modelo deve ajustar para a variável confundidora Z1 para controlar a confusão e obter uma estimativa imparcial do efeito de X sobre Y.

",E,"

Explicação dos itens:

A) Incorreto. Ajustar para variáveis confundidoras é crucial em DAGs, pois elas podem introduzir viés na relação estimada entre a exposição e o resultado.

B) Incorreto. A variável colisora Z2, quando ajustada, pode abrir um caminho de confusão entre a exposição e o resultado. Geralmente, não queremos controlar para colisores, uma vez que isso pode criar confundimento espúrio.

C) Incorreto. A variável de mediação Z3 é parte do caminho causal entre X e Y. Ajustar para essa variável cortaria o caminho que está mediando o relacionamento e, portanto, o efeito total não seria corretamente estimado.

D) Incorreto, pois a variável de mediação Z3 deve ser ajustada apenas se o interesse estiver em avaliar o caminho indireto, separado do efeito total.

E) Correto. Para obter uma estimativa imparcial do efeito de X sobre Y, é necessário ajustar o modelo para a variável confundidora Z1. Isso garante que a relação entre X e Y não seja distorcida por uma terceira variável que influencie tanto a exposição quanto o resultado.",1708734
tópico 6,"Diagramas causais: gráficos acíclicos dirigidos; variáveis confundidoras, colisoras e de mediação","Questão:
Considere que você está realizando um estudo epidemiológico para investigar os fatores associados à incidência de uma doença cardiovascular específica. Você decide utilizar diagramas causais, mais especificamente gráficos acíclicos dirigidos (DAGs), para identificar as relações entre diversas variáveis. No contexto desse estudo, as variáveis são classificadas de acordo com o tipo de papel que desempenham na relação causal. Assinale a opção que CORRETAMENTE define as variáveis confundidoras, colisoras e de mediação.

A) Uma variável confundidora é uma variável que está causalmente entre uma exposição e um resultado final, representando um passo no mecanismo pelo qual a exposição influencia o resultado.
B) Uma variável colisora é uma variável que é influenciada por duas outras variáveis e, ao controlá-la, cria-se uma associação espúria entre essas variáveis que de outra forma não estariam associadas.
C) Uma variável de mediação é uma variável que está no caminho causal entre a exposição e o resultado, mas ao ajustar por ela, a relação verdadeira entre a exposição e o resultado é revelada.
D) Uma variável confundidora é uma variável que está causalmente independentemente associada tanto à exposição quanto ao resultado, potencialmente distorcendo a relação verdadeira entre ambos.
E) Uma variável colisora é aquela que está diretamente causando tanto a exposição quanto o resultado, sendo, portanto, uma forte candidata a ser controlada na análise.

",D," 
Uma explicação dos itens:
A) Item A está incorreto porque descreve uma variável de mediação, não uma variável confundidora.
B) Item B está incorreto porque define corretamente o que é uma variável colisora, mas a afirmação é que, ao controlar uma colisora, cria-se uma associação espúria, o que é verdade, mas a pergunta solicita definições para confundidora, colisora e de mediação, e este item apenas aborda a colisora.
C) Item C está incorreto porque inverte a definição de uma variável de mediação com a de uma variável confundidora. Ao ajustar por uma variável de mediação, estaríamos de fato removendo o efeito que ela medeia, e não revelando a relação verdadeira como sugerido.
D) Item D está correto. Ele define uma variável confundidora de forma adequada, como uma variável que está associada tanto à exposição quanto ao resultado, deixando implícito que essa associação é confundidora ao estar no caminho causal e não ser consequência de uma cadeia causal que comece na exposição.
E) Item E está incorreto porque confunde a definição de variável colisora com uma variável comum causadora tanto da exposição quanto do resultado, o que na verdade seria um confundidor comum, não uma colisora.",7084663
tópico 3,Programação funcional,"Questão: Considere o paradigma de programação funcional e seu impacto em sistemas computacionais. Uma característica central dessa abordagem é a imutabilidade dos dados, fundamental para evitar efeitos colaterais em funções. Além disso, conceitos como funções de ordem superior e expressões lambda são amplamente utilizados. Dado esse contexto e pensando na linguagem Haskell, a qual é fortemente baseada em programação funcional, qual das seguintes afirmações melhor exemplifica um princípio fundamental da programação funcional?

A) Em Haskell, todas as variáveis podem ser modificadas após a sua criação para acomodar mudanças de estado do programa.

B) Haskell suporta efeitos colaterais em todas as suas funções para promover maior flexibilidade no controle do fluxo da aplicação.

C) Funções de ordem superior em Haskell são utilizadas para modificar o estado global da aplicação, garantindo melhor performance na execução concorrente.

D) Haskell emprega expressões lambda para criar funções anônimas que não estão ligadas a um identificador.

E) Em Haskell, os loops de repetição são a principal estrutura de controle usada para implementar algoritmos recursivos e iterativos.

",D,"

A) Incorreto. Uma das diferenças fundamentais da programação funcional e imperativa é que na funcional, as variáveis são imutáveis após sua criação. Elas são mais propriamente chamadas de 'valores' devido a essa característica.

B) Incorreto. Haskell restringe efeitos colaterais utilizando o sistema de tipos com tipos especiais como 'IO'. Funções que realizam efeitos colaterais são claramente distinguidas das funções puras.

C) Incorreto. Funções de ordem superior em programação funcional são tipicamente usadas para abstrair padrões de computação, como mapeamento, dobramento, e filtragem, e não para modificar o estado global do programa, o qual é evitado em programação funcional.

D) Correto. As expressões lambda permitem criar funções anônimas, que são uma parte central da programação funcional. Elas são úteis para operações que requerem pequenas funções que não necessitam ser nomeadas, tornando o código mais conciso e promovendo uma forma declarativa de programação.

E) Incorreto. Embora os loops possam ser expressos em Haskell, a programação funcional favorece a utilização de recursão e abstração de controle com funções de ordem superior para esses propósitos. O uso direto de loops é mais típico da programação imperativa.",9742679
tópico 0,Soluções de big data: Arquitetura do ecossistema Spark,"Questão: No contexto do processamento de grandes volumes de dados, o Apache Spark tem se destacado como uma das principais plataformas de computação em memória. Ao considerar os componentes arquiteturais do Spark, que característica abaixo NÃO se aplica à sua atuação no processamento de dados em larga escala?

A) O Spark Streaming permite o processamento de dados em tempo real, facilitando a criação de pipelinas de dados contínuas.

B) A Spark SQL facilita a manipulação de dados semi-estruturados e oferece a capacidade de realizar consultas SQL.

C) O GraphX permite a execução de algoritmos gráficos e análises complexas sobre dados no formato de grafo.

D) O MLib é o componente do Spark especializado na implementação de algoritmos de Machine Learning distribuído.

E) O Spark Core utiliza um modelo de persistência de dados exclusivamente baseado em armazenamento em disco para otimizar o desempenho de I/O.

",E,"

Explicação dos itens:

A) O Spark Streaming é um componente do Spark que permite o processamento de streams de dados em tempo real. Portanto, essa afirmação é verdadeira e se aplica à arquitetura do Spark.

B) A Spark SQL permite a execução de consultas SQL sobre dados, tanto estruturados como semi-estruturados. Ele se integra bem com os dataframes e datasets do Spark, então essa afirmação também é verdadeira.

C) O GraphX é a biblioteca do Spark para manipulação de dados em grafos e execução de algoritmos gráficos. Sua existência é um dos recursos que enriquecem o ecossistema do Spark, corrobora com a veracidade da afirmação.

D) MLib é a biblioteca de Machine Learning do Spark que fornece várias ferramentas para algoritmos de aprendizado de máquina, tais como classificação, regressão, clustering, entre outros. Este componente é fundamental no processamento de big data para tarefas de Machine Learning, tornando essa opção correta.

E) O Spark Core é o componente fundamental do Spark que utiliza um modelo de processamento baseado em memória, denominado Resilient Distributed Datasets (RDDs). O Spark é conhecido pelo seu processamento in-memory, o que contradiz a afirmação de que ele usa exclusivamente armazenamento em disco. Portanto, essa afirmação é falsa e não se aplica ao Spark, sendo o item incorreto.",277702
tópico 0,"Arquitetura de cloud computing para ciência de dados (AWS, Azure, GCP)","Questão:
A Arquitetura de Cloud Computing oferece diversas soluções que atendem às necessidades específicas da Ciência de Dados, com serviços que permitem o armazenamento, processamento e análise de grandes volumes de dados. Supondo que uma equipe de cientistas de dados pretenda construir um pipeline de dados escalável, altamente disponível e com capacidade de realizar análises complexas em tempo real, qual combinação de serviços poderia ser recomendada se estiverem utilizando a AWS (Amazon Web Services)?

A) Amazon S3 para armazenamento de dados, Amazon EC2 para processamento de dados e Amazon QuickSight para análise de dados.

B) Amazon Redshift para armazenamento de dados, AWS Lambda para processamento de dados e Amazon QuickSight para análise de dados.

C) Amazon Kinesis para ingestão de dados em tempo real, AWS Glue para processamento de dados e Amazon Athena para análise de dados.

D) Amazon RDS para armazenamento de banco de dados relacional, Amazon EMR para processamento de dados e Amazon QuickSight para análise de dados.

E) AWS Data Pipeline para ingestão de dados, Amazon Redshift para processamento de dados e AWS Data Pipeline para análise de dados.

",C,"

Explicação dos itens:
A) Amazon S3 é um serviço de armazenamento de objetos, Amazon EC2 permite a execução de servidores virtuais e Amazon QuickSight é uma ferramenta de análise de negócios, mas essa combinação não atende especificamente à necessidade de análise em tempo real.

B) Amazon Redshift é um serviço de data warehousing, e AWS Lambda é um serviço de computação sem servidor, mas não são projetados especificamente para pipelines de análise em tempo real, embora possam ser usados em partes de um.

C) Amazon Kinesis é especializado na ingestão de dados em tempo real, AWS Glue é um serviço de ETL (extrair, transformar, carregar) administrado, e Amazon Athena é um serviço de consulta interativo que facilita a análise de dados no Amazon S3, usando SQL. Esta combinação de serviços encaixa-se perfeitamente para a análise em tempo real e pipelines escaláveis.

D) Amazon RDS é um serviço de banco de dados relacional, e Amazon EMR é um serviço de processamento de big data baseado em clusters, porém, a combinação não prioriza o cenário de análise em tempo real como o Kinesis ou Glue.

E) AWS Data Pipeline é um serviço de orquestração de fluxo de dados e não é uma ferramenta de análise. Além disso, AWS Data Pipeline está listado tanto para ingestão quanto análise, o que é um mal-entendido do papel do serviço na arquitetura de dados.",3331781
tópico 6,Métodos e técnicas de identificação causal: Métodos experimentais RCT e de identificação quase-experimental,"Questão: Considerando os métodos e técnicas de identificação causal, métodos experimentais como Ensaios Clínicos Randomizados (RCT - Randomized Controlled Trials) e métodos de identificação quase-experimental, como o Desenho de Regressão Descontínua (RDD - Regression Discontinuity Design), estão entre as abordagens mais robustas para inferência causal. A respeito dessas metodologias, analise as afirmativas a seguir:  

I. Os RCTs garantem que grupos de tratamento e controle sejam estatisticamente equivalentes em relação a variáveis observadas e não observadas, através do processo de randomização.

II. O método RDD utiliza um ponto de corte determinístico para atribuir a intervenção a indivíduos acima ou abaixo desse ponto, não necessitando de randomização.

III. RCTs não são adequados para estudos em contextos onde a ética ou a viabilidade prática impedem a randomização aleatória, e nesses casos, métodos quase-experimentais podem ser uma alternativa preferível.

IV. O RDD é menos vulnerável a viés de seleção comparado a RCTs, pois o ponto de corte utilizado para atribuir o tratamento é escolhido de forma aleatória pelos pesquisadores.

Assinale a opção que contém somente as afirmativas corretas:

A) I e III
B) II e IV
C) I, II e III
D) II, III e IV
E) I, II e IV

",A," 

Explicação dos itens:

I. Correta. Os Ensaios Clínicos Randomizados (RCT) são projetados para que grupos de tratamento e controle sejam comparáveis com relação a todas as variáveis, tanto observadas quanto não observadas, pelo processo de randomização.

II. Incorreta. O método RDD utiliza um ponto de corte preestabelecido, com base em uma variável observável, para atribuir a intervenção, o que cria uma situação de ""randomização forçada"" em torno desse ponto de corte, mas isso não elimina a necessidade de randomização para garantir a estimativa imparcial do efeito causal.

III. Correta. Há situações específicas em que os RCTs não podem ser utilizados devido a questões éticas ou práticas, e em tais casos, desenhos quase-experimentais, como o RDD, podem ser utilizados como alternativas para identificar efeitos causais.

IV. Incorreta. O RDD não é geralmente menos vulnerável a viés de seleção em comparação aos RCTs. Embora o RDD possa lidar com o viés de seleção em torno do ponto de corte, ele não garante equivalência entre os grupos fora dessa estreita faixa; além disso, o ponto de corte não é escolhido aleatoriamente pelos pesquisadores, mas geralmente é uma característica inerente ao processo de pesquisa ou política em estudo.

Por conseguinte, as afirmativas I e III são as únicas corretas e a resposta é a alternativa A) I e III.",6277202
tópico 0,"Ingestão de dados estruturados, semiestruturados e não estruturados","Questão: No contexto de Big Data e análise de dados, uma organização precisa lidar com diferentes tipos de dados em seus sistemas de informação. Dentre os tipos de dados, podemos citar os dados estruturados, semiestruturados e não estruturados. Com relação a essas categorias de dados, analise as seguintes afirmações:

I. Dados estruturados são aqueles que seguem um modelo de dados pré-definido, como as tabelas em bancos de dados relacionais, onde cada tipo de dado é facilmente identificável e acessível através de consultas estruturadas com SQL.

II. Dados semiestruturados não seguem um esquema rígido como os dados estruturados, mas ainda possuem alguma organização que facilita seu processamento. Exemplos comuns incluem formatos como JSON e XML, que podem ser tratados por meio de parsers específicos.

III. Dados não estruturados são dados que não possuem uma estrutura definida ou fácil de identificar, o que dificulta sua análise. Exemplos incluem arquivos de texto livre, imagens, vídeos e registros de áudio, que normalmente requerem técnicas avançadas de processamento, como aprendizado de máquina, para serem convertidos em informações úteis.

Assinale a opção correta quanto à classificação dos tipos de dados mencionados.

a) Apenas a afirmação I está correta.
b) Apenas a afirmação II está correta.
c) Apenas a afirmação III está correta.
d) As afirmações I, II e III estão corretas.
e) Nenhuma das afirmações está correta.

",D," 

Explicação dos itens:

I. Esta afirmação é verdadeira porque os dados estruturados são organizados em um formato que os sistemas de computação podem identificar e trabalhar de forma eficiente, como o modelo tabular usado em bancos de dados relacionais.

II. A afirmação sobre dados semiestruturados também é verdadeira. Enquanto eles não seguem um modelo rígido de dados, como os dados estruturados, ainda assim possuem uma organização interna que permite sua interpretação por sistemas computacionais. Formatos como JSON e XML são comuns e podem ser manipulados através de bibliotecas e ferramentas de software projetadas para tratar tais tipos de dados.

III. A afirmação sobre dados não estruturados também está correta. Diferentemente dos dados estruturados e semiestruturados, os dados não estruturados não têm uma estrutura previsível o que requer abordagens específicas para seu processamento e análise usando técnicas como reconhecimento de padrões, processamento natural de linguagem ou aprendizado de máquina.",3999493
tópico 0,"Arquitetura de cloud computing para ciência de dados (AWS, Azure, GCP)","Questão:
Em uma organização que busca uma arquitetura de cloud computing adequada para suas necessidades de ciência de dados, considere os seguintes requisitos: elasticidade para lidar com cargas de trabalho variáveis, capacidade de processamento de grandes volumes de dados em tempo real e suporte a serviços de machine learning e big data analytics. Dentre as opções de serviços oferecidos pelas três principais provedoras de nuvem (AWS, Azure e GCP), qual conjunto de serviços atende melhor a esses requisitos?

A) AWS - Utilização do EC2 para elasticidade, Kinesis para processamento de dados em tempo real e SageMaker para machine learning.
B) Azure - Uso do Azure Functions para elasticidade, Stream Analytics para processamento de dados em tempo real e Cognitive Services para machine learning.
C) GCP - Implementação do Google App Engine para lidar com elasticidade, Pub/Sub para processamento de dados em tempo real e AutoML para serviços de machine learning.
D) AWS - Emprego do AWS Lambda para elasticidade, Redshift para análise de grandes volumes de dados e Rekognition para machine learning.
E) Azure - Alocação do Azure Kubernetes Service (AKS) para elasticidade, Azure Data Lake para armazenamento de grandes volumes de dados e Azure Machine Learning para machine learning.

",C,"

Explicação dos itens:

A) O AWS EC2 é um serviço de computação em nuvem que permite a elasticidade, porém, é mais adequado para instâncias de VMs do que para lidar automaticamente com escalabilidade sem servidor. Kinesis é apropriado para processamento de dados em tempo real. SageMaker é uma plataforma completa para machine learning, portanto, esta opção seria quase correta, no entanto, não menciona explicitamente um serviço dedicado a big data analytics como o AWS EMR.

B) Azure Functions proporciona a computação sem servidores (serverless), que lida com a elasticidade, mas é mais indicado para execução de código em resposta a eventos, e não abrange toda a capacidade de escalabilidade necessária em ciência de dados. O Stream Analytics é adequado para processamento de dados em tempo real. Cognitive Services é focado em APIs pré-construídas para AI e não cobre um espectro amplo de capacidades de machine learning.

C) Google App Engine é uma plataforma PaaS que gerencia automaticamente a infraestrutura, sendo uma boa opção para elasticidade e escalabilidade automática. Pub/Sub é uma plataforma robusta para mensagens assíncronas que pode processar dados em tempo real. AutoML é uma suíte de produtos que permite treinar modelos de machine learning de alta qualidade, alinhando-se com os requisitos de big data analytics e machine learning.

D) AWS Lambda é um serviço de computação sem servidor que gerencia automaticamente a capacidade de computação, mas não é especificamente desenhado para big data analytics. Redshift é um serviço de data warehousing, mas não é ideal para processamento de dados em tempo real. Rekognition é um serviço de análise de imagens e vídeos e não oferece uma plataforma ampla de machine learning.

E) Azure Kubernetes Service (AKS) é um orquestrador de containers que oferece boa elasticidade, no entanto, não oferece a mesma simplicidade em termos de elasticidade automática do App Engine ou Lambda. Azure Data Lake é um sistema de armazenamento de grandes volumes de dados, mas não é dedicado ao processamento de dados em tempo real. Azure Machine Learning é uma plataforma completa para desenvolvimento, treinamento e implantação de modelos de machine learning, se alinhando com esse aspecto do requisito.",304510
tópico 6,Métodos e técnicas de identificação causal: Métodos experimentais RCT e de identificação quase-experimental,"Questão:
A identificação de relações causais é um elemento central nas ciências sociais e em políticas públicas, visando estabelecer se uma intervenção ou tratamento leva a uma mudança específica no grupo de interesse. Considerando os métodos e técnicas para a identificação causal, assinale a opção correta com relação aos métodos experimentais Randomized Controlled Trials (RCT) e métodos quase-experimentais.

A) RCTs são considerados o padrão-ouro em termos de validade interna, pois a randomização elimina a possibilidade de viés de seleção.

B) Métodos quase-experimentais requerem o uso de instrumentos estatísticos para a randomização de indivíduos, garantindo a validade externa do estudo.

C) Tanto os experimentos aleatorizados quanto os quase-experimentos são igualmente eficazes na identificação de efeitos causais, independentemente do contexto.

D) Os quase-experimentos são preferíveis aos RCTs em todas as situações, pois proporcionam resultados mais rapidamente e a um custo inferior.

E) RCTs são inadequados para estudos em larga escala, ao passo que os métodos quase-experimentais são mais adequados para avaliar políticas públicas implementadas em grandes populações.

",A,"

Alternativa A: RCTs, ou ensaios controlados randomizados, de fato são considerados o padrão-ouro (gold standard) quando se fala em validade interna, pois a randomização é uma ferramenta poderosa para eliminar ou reduzir o viés de seleção. Ao atribuir aleatoriamente indivíduos a grupos de tratamento e controle, os efeitos de variáveis confundidoras não observadas tendem a ser balanceados entre os grupos.

Alternativa B: Esta afirmativa está incorreta, pois os métodos quase-experimentais não dependem necessariamente do uso de instrumentos estatísticos para randomização, já que, muitas vezes, não há randomização em quase-experimentos. Eles se baseiam na identificação de cenários naturais ou administrativos que se assemelham a uma situação experimental, mas sem randomização intencional. A validade externa, por outro lado, está ligada à generalização dos resultados para populações maiores.

Alternativa C: Esta alternativa é incorreta porque, embora tanto experimentos aleatorizados quanto quase-experimentos possam ser projetados para identificar efeitos causais, a eficácia de cada método pode variar significativamente de acordo com o contexto e a questão de pesquisa. Não se pode afirmar que são igualmente eficazes independentemente do contexto.

Alternativa D: A preferência por um método em relação ao outro depende do contexto da pesquisa, das perguntas de pesquisa e de considerações éticas ou práticas. RCTs podem, em muitos casos, fornecer estimativas mais precisas de efeitos causais, mas podem ser impraticáveis ou antiéticos em determinadas situações. Em contrapartida, quase-experimentos podem ser mais viáveis em tais circunstâncias, mas podem sofrer com problemas de validade interna.

Alternativa E: Esta opção está equivocada, pois RCTs podem ser adaptados e implementados em larga escala, e também são utilizados em avaliações de políticas públicas. O desafio em estudos de grande escala não é exclusivo aos RCTs, e os métodos quase-experimentais também têm limitações, particularmente em termos de generalização dos resultados (validade externa) e controle de variáveis confundidoras.",1500764
tópico 1,Banco de dados NoSQL,"Questão:
Considere uma aplicação de redes sociais de grande escala que implementa um serviço de mensagens instantâneas entre seus usuários. Com o crescimento exponencial do número de usuários e mensagens trocadas diariamente, a equipe de tecnologia da informação está considerando migrar o sistema de gerenciamento de banco de dados para uma solução NoSQL mais adequada à natureza distribuída e à demanda de escalabilidade em tempo real do serviço. Qual das seguintes opções representa o tipo de banco de dados NoSQL mais apropriado para acomodar essa necessidade?

A) Bancos de Dados Relacionais
B) Bancos de Dados de Grafos
C) Bancos de Dados de Documentos
D) Bancos de Dados Colunares
E) Key-Value Stores

",C,"

Explicação dos itens:

A) Bancos de Dados Relacionais - São inadequados para cenários descritos, pois podem apresentar limitações de escalabilidade horizontal e flexibilidade em termos de esquema, que são fundamentais em uma aplicação de redes sociais com crescimento exponencial.

B) Bancos de Dados de Grafos - Embora possam ser úteis para modelar relações complexas entre usuários, como redes de amizade, não são a melhor escolha para armazenar mensagens instantâneas, que não se beneficiam tanto da modelagem gráfica.

C) Bancos de Dados de Documentos - São ideais para armazenamento de mensagens instantâneas por oferecerem flexibilidade de esquemas, acomodação de grandes volumes de dados semi-estruturados e facilidade para escalabilidade horizontal. Exemplos populares incluem MongoDB e Couchbase, que são amplamente utilizados em aplicações de redes sociais.

D) Bancos de Dados Colunares - São mais adequados para análises de grandes volumes de dados, onde operações de leitura e agregação são frequentes, como em sistemas de Business Intelligence. Eles não são a melhor escolha para o cenário de mensagens instantâneas, cujo foco está no armazenamento e recuperação rápida de dados.

E) Key-Value Stores - Proporcionam rápida leitura e escrita de pares de chave-valor, mas são muito simples e não suportam a complexidade estrutural que um sistema de mensagens instantâneas geralmente precisa, como o armazenamento de metadados associados a cada mensagem.",5859717
tópico 0,"Arquitetura de cloud computing para ciência de dados (AWS, Azure, GCP)","Questão:
Na arquitetura de cloud computing, a escolha do serviço correto para hospedar um processo de ciência de dados é crucial para o desempenho e escalabilidade da solução. Considere que uma empresa planeja implementar um sistema de aprendizado de máquina que requer o treinamento de modelos complexos usando grandes conjuntos de dados. A empresa deseja uma solução que gerencie a infraestrutura de computação subjacente e que possa escalar automaticamente os recursos conforme a demanda. Com base nesses requisitos, qual das seguintes opções de serviços de cloud computing é a mais adequada?

A) AWS EC2 Spot Instances
B) Azure HDInsight
C) Google Cloud Dataproc
D) AWS SageMaker
E) Azure Virtual Machines

",D,"

Explicação dos itens:

A) AWS EC2 Spot Instances: São instâncias de computação que podem ser adquiridas em leilão por um preço inferior ao padrão, porém podem ser interrompidas pela AWS com um curto aviso prévio caso a demanda aumente. São ideais para jobs que podem ser interrompidos, mas não para algo que requer alta disponibilidade e gerenciamento automático da infraestrutura.

B) Azure HDInsight: É um serviço de nuvem para processamento de big data, que fornece clusters gerenciados de Hadoop, Spark, entre outros. Embora adequado para processamento de grandes conjuntos de dados, não se destaca pelo gerenciamento automático de escalabilidade de recursos para treinamento de modelos de aprendizado de máquina.

C) Google Cloud Dataproc: É um serviço de gerenciamento para execução de tarefas de processamento de dados em frameworks como Hadoop e Spark na nuvem do Google. Similar ao Azure HDInsight, é mais focado em processamento de dados e não especificamente no treinamento de modelos de aprendizado de máquina.

D) AWS SageMaker: Este é um serviço de machine learning totalmente gerenciado que permite aos cientistas de dados e desenvolvedores treinar e implantar modelos de aprendizado de máquina facilmente. Ele oferece a capacidade de escalar recursos automaticamente, atendendo assim às necessidades da empresa descritas no cenário.

E) Azure Virtual Machines: São instâncias de máquinas virtuais que podem ser personalizadas conforme necessário. Enquanto elas oferecem a flexibilidade para configurar o ambiente de computação, não oferecem a gestão automática da infraestrutura e da escalabilidade que o AWS SageMaker oferece.",4642862
tópico 1,"Banco de dados relacional: SQL Server, PostgreSQL, MySQL","Questão:
A resiliência e o desempenho em ambientes de banco de dados relacionais são críticos para a operação de sistemas empresariais de grande porte. Considerando a utilização de índices em bancos de dados relacionais como SQL Server, PostgreSQL e MySQL, marque a opção que apresenta afirmações verdadeiras sobre o uso de índices nesses SGBDs:

A) No SQL Server, um índice clustered reorganiza fisicamente as linhas da tabela, enquanto no PostgreSQL e no MySQL, os índices são sempre criados de forma separada dos dados, sem alterar sua organização física.

B) O índice full-text só está disponível no MySQL e é utilizado para melhorar a performance de buscas em colunas de texto grande.

C) O PostgreSQL não suporta a criação de índices parciais enquanto o SQL Server suporta índices filtrados, que são essencialmente o mesmo recurso.

D) Índices hash são suportados em todas as plataformas mencionadas (SQL Server, PostgreSQL e MySQL) e são idealmente utilizados para operações de igualdade.

E) No MySQL, o uso de índices em consultas com JOINs é inadequado e pode levar a uma degradação significativa do desempenho, ao contrário do SQL Server e PostgreSQL, onde o otimizador de consulta pode se beneficiar efetivamente de índices em tais operações.

",A,"

Explicações dos itens:

A) No SQL Server, um índice clustered de fato altera a ordem física das linhas para corresponder ao índice. No PostgreSQL, o índice físico separado é chamado de ""Índice HEAP"", e no MySQL, índices secundários apontam para os registros na tabela original, mas não alteram sua ordem física, o que torna esta afirmação correta.

B) O índice full-text está disponível em todos os SGBDs mencionados, não somente no MySQL, o que faz desta afirmação incorreta.

C) O PostgreSQL suporta índices parciais, que são úteis para economizar espaço e melhorar a performance para consultas que acessam apenas uma porção dos dados. Esse recurso é semelhante aos índices filtrados do SQL Server, portanto, a afirmativa está incorreta.

D) Índices hash estão disponíveis em PostgreSQL e em algumas versões do MySQL (dependendo do engine de armazenamento), mas no SQL Server, índices hash não são usados para índices de armazenamento de dados, focando em índices de memória (Memory-Optimized Tables), o que torna a afirmação incorreta.

E) No MySQL, índices são efetivamente utilizados em consultas com JOINs para melhorar o desempenho, tal como no SQL Server e PostgreSQL. A afirmação é falsa e sugere uma visão contrária ao que é praticado.",6143557
tópico 1,Álgebra relacional e SQL (padrão ANSI),"Questão: Considere duas tabelas em um banco de dados relacional, Cliente(Codigo, Nome, Email) e Pedido(Numero, Data, CodigoCliente, Valor), onde CodigoCliente é uma chave estrangeira que referencia Codigo na tabela Cliente. Suponhamos que queremos listar todos os clientes que fizeram ao menos um pedido acima de R$1000.00. Qual das seguintes opções de consultas em SQL está correta e também representa o equivalente na Álgebra Relacional para essa extração de dados?

A) SELECT DISTINCT Cliente.Nome FROM Cliente JOIN Pedido ON Cliente.Codigo = Pedido.CodigoCliente WHERE Pedido.Valor > 1000.00

B) SELECT Cliente.Nome FROM Cliente WHERE Cliente.Codigo IN (SELECT CodigoCliente FROM Pedido WHERE Valor > 1000.00)

C) SELECT Cliente.Nome FROM Cliente, Pedido WHERE Cliente.Codigo = Pedido.CodigoCliente AND Pedido.Valor > 1000.00

D) SELECT Nome FROM Cliente WHERE EXISTS (SELECT * FROM Pedido WHERE Pedido.CodigoCliente = Cliente.Codigo AND Pedido.Valor > 1000.00)

E) SELECT Nome FROM Cliente WHERE Codigo NOT IN (SELECT CodigoCliente FROM Pedido WHERE Valor <= 1000.00)

",B," 
A alternativa correta é a B) pois apresenta a consulta SQL correta e eficiente para extrair os dados requeridos utilizando uma subconsulta. O equivalente na Álgebra Relacional seria a expressão:

π_Nome(σ_Valor > 1000 (Pedido) ⨝_Codigo = CodigoCliente Cliente)

onde π é o operador de projeção (selecionando a coluna Nome), σ é o operador de seleção (onde o Valor é maior que 1000), e ⨝ é o operador de junção natural (combinando as tabelas Cliente e Pedido onde o Codigo corresponde ao CodigoCliente).

- A alternativa A) também retorna o resultado desejado mas não descreve explicitamente que essa consulta é um equivalente na Álgebra Relacional.
- Na alternativa C), a consulta ainda funciona mas não utiliza a sintaxe JOIN explícita, que é uma prática recomendada no SQL ANSI.
- A alternativa D) demonstra o uso correto do comando EXISTS que checa a existência de pelo menos um pedido que satisfaça as condições, mas novamente não fornece a expressão equivalente na Álgebra Relacional.
- A alternativa E) não retorna os clientes corretos porque seleciona clientes que nunca fizeram um pedido de valor igual ou inferior a 1000.00, que não é a solicitação da questão.

Portanto, B) é a única alternativa que corresponde diretamente ao pedido da questão tanto na linguagem de consulta SQL quanto na notação da Álgebra Relacional.",6264410
tópico 2,Contexto de IA: Tratamento de outliers e agregações,"Questão:
A Inteligência Artificial (IA) é um vasto campo de estudo e aplicação que demanda uma abordagem cuidadosa aos dados que alimentam os modelos algorítmicos. Durante a etapa de pré-processamento, o tratamento de outliers e a aplicação de técnicas de agregação são passos cruciais para assegurar a qualidade e a relevância dos dados. Considere um conjunto de dados com uma variável quantitativa X, onde se observam valores significativamente distantes da média. Qual das seguintes alternativas melhor representa uma abordagem apropriada para o tratamento dos outliers, combinada com uma técnica de agregação para análise subsequente?

A) Substituir todos os outliers por valores ausentes e aplicar a mediana para agregar os dados de X.

B) Excluir todos os registros contendo outliers sem análise adicional e utilizar a média aritmética para agregar os dados de X.

C) Aplicar a transformação logarítmica nos outliers para reduzir a variação, seguido pela aplicação da média geométrica para agregar os dados de X.

D) Manter os outliers no conjunto de dados e utilizar a soma como método de agregação para os dados de X.

E) Utilizar um método de imputação baseado no modelo, como k-NN ou Árvore de Decisão, para substituir os outliers, e empregar a moda como técnica de agregação.

",C,"  
A alternativa C é correta, pois a transformação logarítmica é uma técnica comumente utilizada para diminuir a influência de outliers em variáveis numéricas, restabelecendo uma distribuição mais próxima à normalidade. Se a variável X for positiva e os outliers forem números muito grandes, a transformação logarítmica ajuda a diminuir a discrepância entre esses e os valores mais frequentes. Além disso, a média geométrica é uma forma de agregação que é menos influenciada por valores extremos quando comparada à média aritmética, o que a torna mais apropriada em contextos onde a transformação logarítmica é aplicada.

Item A está incorreto, pois simplesmente substituir os outliers por valores ausentes pode distorcer a análise, especialmente se a quantidade de outliers for significativa. A mediana poderia ser uma agregação adequada para dados com outliers, mas descartar informações pode não ser o melhor caminho.

Item B está incorreto, pois excluir todos os registros com outliers é uma medida drástica e pode resultar em perda importante de informações. A média aritmética também é suscetível à influência de outliers, o que a torna uma má escolha para agregação em presença de valores extremos.

Item D está incorreto, porque, ao manter os outliers sem tratamento adequado, a análise pode ser fortemente enviesada por eles. A soma como método de agregação pouco diz sobre a distribuição dos dados e pode ser enganosa em presença de outliers.

Item E está incorreto, pois embora a imputação via k-NN ou Árvore de Decisão possa ser uma estratégia válida para tratar outliers, a utilização da moda como técnica de agregação não é apropriada para dados quantitativos contínuos, onde pode ser que não existam valores repetidos ou a modalidade não tenha sentido prático.",5761972
tópico 1,Banco de dados NoSQL,"Questão:
Considere o contexto dos bancos de dados NoSQL, que são projetados para superar as limitações dos bancos de dados relacionais em determinados tipos de aplicações modernas, possibilitando a manipulação de grandes volumes de dados, realizações de consultas complexas em tempo hábil e garantindo alta disponibilidade. Dentre as categorias de bancos NoSQL existentes, qual das opções a seguir descreve corretamente um banco de dados orientado a documentos?

A) Utiliza uma estrutura tabular de linhas e colunas, suportando relacionamentos entre as tabelas por meio de chaves estrangeiras.

B) Armazena os dados em pares chave-valor, sendo uma opção eficiente para aplicações que necessitam de armazenamento e recuperação rápida de dados.

C) Organiza os dados como documentos, geralmente no formato JSON ou BSON, permitindo estruturas aninhadas e a indexação eficiente de campos dentro desses documentos.

D) Baseia-se em grafos para representar e armazenar dados, com nós, arestas e propriedades para representar e armazenar informações sobre as relações entre os elementos.

E) Divide os dados em uma distribuição horizontal, conhecida como sharding, para distribuir a carga por múltiplos hosts, melhorando a performance e escalabilidade.

",C,"

Alternativa A descreve um banco de dados relacional, que utiliza tabelas para armazenar os dados. Alternativa B refere-se aos bancos de dados chave-valor, que são otimizados para cenários onde a rapidez na recuperação de dados é crucial. Alternativa C é a correta, pois descreve um banco de dados orientado a documentos, que armazena os dados em estruturas de documentos e é comum o uso de JSON ou BSON. Alternativa D está descrevendo bancos de dados orientados a grafos, que são usados para armazenar dados interconectados. Por fim, a alternativa E menciona o conceito de sharding, uma técnica comum em diversos tipos de bancos de dados NoSQL para distribuir os dados entre diferentes servidores, mas não é uma descrição específica de bancos de dados orientados a documentos.",1380466
tópico 1,Álgebra relacional e SQL (padrão ANSI),"Questão:
Na álgebra relacional e SQL (padrão ANSI), considere a existência das relações 'Funcionario' (codigoF, nomeF, departamento) e 'Projeto' (codigoP, nomeP, departamento). Suponha que um analista de banco de dados precise buscar o nome e o código de todos os funcionários junto com o nome de todos os projetos em que eles trabalham. Contudo, deve-se garantir que, mesmo os funcionários que não estão alocados em nenhum projeto sejam listados, juntamente com o valor NULL na coluna do nome do projeto. Assumindo que ambas as tabelas compartilham a coluna 'departamento' como chave estrangeira para a realização de um JOIN, qual das seguintes instruções SQL irá fornecer o resultado desejado?

A) SELECT F.nomeF, F.codigoF, P.nomeP
   FROM Funcionario F INNER JOIN Projeto P ON F.departamento = P.departamento;

B) SELECT F.nomeF, F.codigoF, P.nomeP
   FROM Funcionario F LEFT JOIN Projeto P ON F.departamento = P.departamento;

C) SELECT F.nomeF, F.codigoF, P.nomeP
   FROM Funcionario F RIGHT JOIN Projeto P ON F.departamento = P.departamento;

D) SELECT F.nomeF, F.codigoF, P.nomeP
   FROM Funcionario F, Projeto P
   WHERE F.departamento = P.departamento;

E) SELECT F.nomeF, F.codigoF, P.nomeP
   FROM Funcionario F FULL OUTER JOIN Projeto P ON F.departamento = P.departamento;

",B,"

Explicação dos itens:

A) Esta opção utiliza um INNER JOIN, o que significa que apenas os funcionários que têm um projeto correspondente no departamento serão selecionados. Funcionários sem projetos correspondentes não serão listados. Portanto, não atende ao requisito da questão.

B) Esta é a opção correta. O LEFT JOIN garante que todos os funcionários serão listados, e para aqueles que não estiverem alocados em nenhum projeto, a coluna referente ao nome do projeto terá o valor NULL.

C) O RIGHT JOIN é o oposto do LEFT JOIN e iria listar todos os projetos, mesmo aqueles que não têm um funcionário correspondente no departamento, e não é o que queremos.

D) Esta opção representa um produto cartesiano seguido de um filtro, que comporta-se como um INNER JOIN quando usado dessa forma. Isso listará apenas os funcionários que têm um projeto correspondente em seu departamento, não incluindo aqueles sem projetos.

E) O FULL OUTER JOIN listaria todas as combinações de funcionários e projetos, incluindo os funcionários sem projetos e também os projetos sem funcionários correspondentes. Embora inclua os funcionários sem projetos, também incluiria projetos sem funcionários, o que não é requerido pela questão.",8628797
tópico 6,Métodos e técnicas de identificação causal: Métodos experimentais RCT e de identificação quase-experimental,"Questão:
Avaliar a causalidade em estudos econômicos é fundamental para a compreensão de como variáveis econômicas e políticas afetam diferentes resultados. Dentre os métodos disponíveis, os experimentos controlados randomizados (RCT - Randomized Controlled Trials) e os métodos de identificação quase-experimental são frequentemente utilizados. Sobre estes métodos, é correto afirmar que:

A) Os RCTs eliminam completamente a possibilidade de qualquer viés de seleção, pois os participantes são designados aleatoriamente ao grupo de tratamento ou controle.
B) Métodos quase-experimentais, como a regressão descontínua (RDD - Regression Discontinuity Design), requerem um alto grau de manipulação por parte do pesquisador para criar grupos de controle e tratamento.
C) Uma das principais vantagens dos RCTs é que eles não necessitam de uma linha de base ou informações antes do tratamento para comparar os grupos de tratamento e controle.
D) Métodos quase-experimentais como o pareamento baseado em escore de propensão (PSM - Propensity Score Matching) dependem fortemente de variáveis instrumentais para a atribuição aleatória no processo de formação dos grupos.
E) Em métodos quase-experimentais, o pesquisador procura explorar eventos ou políticas que geram uma diferenciação exógena na atribuição do tratamento, como em Estudos de Diferenças em Diferenças (DiD - Difference in Differences).

",E,"

Explicação dos itens:
A) Não é verdade que os RCTs eliminem completamente qualquer viés de seleção, uma vez que podem ocorrer problemas como não conformidade e efeitos de spillover que podem introduzir viés no estudo.
B) Embora métodos quase-experimentais exijam que o pesquisador encontre alguma forma de separação entre grupos que se assemelham a uma situação experimental, eles não necessariamente requerem alta manipulação; em muitos casos, eles exploram variações naturais ou políticas para criar grupos de comparação.
C) RCTs geralmente requerem informações de linha de base para garantir que a randomização criou grupos comparáveis e para aumentar a precisão das estimativas de efeito.
D) O PSM é uma técnica usada para construir um grupo de controle que se pareça com o grupo de tratamento em termos de variáveis observadas; ela não depende de variáveis instrumentais, que são empregadas em outra abordagem quase-experimental.
E) Esta afirmação é correta e capta a essência dos métodos quase-experimentais. Eles se baseiam em eventos ou intervenções que afetam apenas parte da população de estudo de maneira exógena, permitindo comparações que se aproximam de um experimento randomizado.",342841
tópico 2,Contexto de IA: Tratamento de dados ausentes,"Questão:
Em um projeto de análise de dados com aplicação de Inteligência Artificial, uma equipe de cientistas de dados se depara com o problema de dados ausentes em um conjunto significativo de observações de sua base de dados. Considerando as abordagens para o tratamento desses dados faltantes, qual é a estratégia que NÃO é recomendada para lidar com essa situação?

A) Imputação de média/média condicional, onde os dados ausentes são substituídos com a média da variável observada ou de um subconjunto condicional da variável.

B) Eliminação de casos, removendo todas as observações onde há pelo menos um valor ausente, o que pode ser útil quando os dados ausentes são aleatoriamente distribuídos.

C) Imputação de dados por meio de algoritmos preditivos, como k-vizinhos mais próximos (k-NN), que utilizam padrões dos dados para estimar valores ausentes.

D) Uso de técnicas com base em modelos de regressão múltipla ou modelos baseados em árvore para a imputação de dados ausentes, tirando vantagem da correlação entre variáveis.

E) Substituição unívoca, na qual todos os dados ausentes em uma variável são substituídos por um único valor fixo, ignorando a estrutura subjacente e a distribuição dos dados.

",E,"

Explicação dos itens:

A) A imputação de média é uma técnica padrão para tratar dados ausentes quando são numericamente razoáveis e não distorce significativamente a distribuição da variável.

B) A eliminação de casos (listwise deletion) é uma opção recomendada se os dados ausentes são poucos e acredita-se que não introduzam viés na análise, embora possa reduzir o tamanho da amostra.

C) A imputação por meio de algoritmos preditivos é uma abordagem sofisticada e comum utilizada para preencher os dados ausentes, aproveitando as relações entre os dados.

D) A imputação baseada em modelos de regressão ou árvores é uma estratégia muito utilizada e pode fornecer estimativas plausíveis para os valores ausentes, sobretudo quando há uma clara relação teórica ou empírica entre as variáveis.

E) A substituição unívoca consiste em preencher todos os dados ausentes com um único valor, como zero ou a média, sem considerar a variabilidade dos dados. Isso pode levar a conclusões errôneas e distorcer análises estatísticas, sendo por isso a opção não recomendada.",4950797
tópico 4,Medidas de tendência central e dispersão e correlação,"Questão:
A análise de dados é uma parte fundamental do processo de tomada de decisão em diversos campos do conhecimento. Em um estudo estatístico, um pesquisador coletou uma amostra de dados relativos a duas variáveis X e Y. Com base nos cálculos preliminares, identificou-se que as medidas de tendência central e dispersão apresentaram os seguintes valores: a média de X é 48, a média de Y é 75, o desvio padrão de X é 12 e o desvio padrão de Y é 20. Além disso, a correlação entre X e Y foi computada e encontrou-se um coeficiente de correlação de Pearson (r) de 0,85.

Com base nessas informações, assinale a alternativa que apresenta uma interpretação correta dos dados:

A) As variáveis X e Y possuem uma correlação perfeita, visto que o coeficiente de correlação é maior que 0,8.
B) A variável X apresenta maior variabilidade que a variável Y, dada a relação entre seus desvios padrão.
C) A correlação positiva forte entre X e Y indica que, para aumentos nos valores de X, são esperados aumentos nos valores de Y.
D) A média de X ser menor que a de Y necessariamente indica que todos os valores de X são menores que os de Y.
E) O coeficiente de correlação de Pearson igual a 0,85 indica que 85% da variação em Y é explicada pela variação em X.

",C," 
A) Esta alternativa está incorreta porque, embora o coeficiente seja alto, uma correlação perfeita seria indicada por um coeficiente igual a 1 ou -1.

B) Esta alternativa está incorreta porque o desvio padrão é uma medida de dispersão que indica a variabilidade dos dados. O fato de X ter um desvio padrão de 12 e Y de 20 sugere que Y possui maior dispersão ou variabilidade comparada a X, não o contrário.

C) Esta alternativa está correta porque um coeficiente de correlação de Pearson de 0,85 indica uma correlação positiva forte. Isso significa que, de maneira geral, um aumento em X tende a estar associado a um aumento em Y.

D) Esta alternativa está incorreta porque a média é uma medida de tendência central e não determina a relação de cada ponto de dados individualmente. A média menor para X indica somente que o valor central da distribuição de X é menor do que o de Y, mas não fornece informações sobre a relação entre os valores individuais de X e Y.

E) Esta alternativa está incorreta porque o coeficiente de correlação de Pearson indica o grau e a direção da relação linear entre duas variáveis, mas não pode ser diretamente interpretado como a porcentagem de variância explicada de uma variável pela outra. Para isso, seria necessário calcular o coeficiente de determinação (r^2).",5774048
tópico 1,"Banco de dados relacional: SQL Server, PostgreSQL, MySQL","Questão:
Considere os seguintes trechos de código SQL correspondentes a duas operações distintas em sistemas de gestão de bancos de dados relacionais diferentes (SQL Server, PostgreSQL e MySQL). Identifique as afirmações corretas com base nos trechos apresentados.

I. No SQL Server, para identificar a versão corrente do servidor de banco de dados, o seguinte comando é utilizado:
```sql
SELECT @@VERSION;
```

II. No PostgreSQL, é possível retornar a configuração atual do servidor utilizando o comando abaixo:
```sql
SHOW server_version;
```

III. No MySQL, para aumentar o tempo limite de uma transação antes de ser automaticamente revertida, pode-se usar o comando seguinte:
```sql
SET innodb_lock_wait_timeout = 120;
```

Assinale a alternativa que corresponde às afirmações corretas.

A) Apenas I.
B) Apenas II.
C) Apenas I e II.
D) Apenas I e III.
E) I, II e III.

",E,"

Explicação dos itens:

Item I: É uma instrução verdadeira. No SQL Server, ""SELECT @@VERSION;"" é um comando comum para retornar informações sobre a versão e ambiente no qual o servidor de banco de dados está sendo executado.

Item II: Este item também é verdadeiro. O comando ""SHOW server_version;"" é uma instrução válida no PostgreSQL e retorna a versão do servidor de banco de dados que está em execução.

Item III: No MySQL, ""SET innodb_lock_wait_timeout = 120;"" é uma instrução correta e é utilizada para definir o tempo de espera para bloqueios de linha em transações InnoDB, com o valor sendo especificado em segundos. Essa configuração define o tempo máximo que uma transação irá esperar para obter um bloqueio antes de ser revertida.

Dessa forma, todas as afirmações I, II e III são verdadeiras e correspondem corretamente aos comandos indicados para cada um dos sistemas de gestão de bancos de dados relacionais mencionados.",2519886
tópico 3,Programação orientada a objetos,"Questão:
Na Programação Orientada a Objetos (POO), um dos seus quatro pilares fundamentais é o Polimorfismo. Este conceito é amplamente aplicado em diversas situações de design de software e implementação de sistemas. Considerando o polimorfismo, analise as afirmativas a seguir e assinale a opção correta.

I. O polimorfismo permite que um objeto seja referenciado de várias formas, principalmente através de interfaces ou classes abstratas, permitindo assim que um mesmo método possa ser utilizado por objetos de tipos diferentes.

II. Em linguagens como Java e C#, o polimorfismo exclusivamente se manifesta em tempo de compilação, sendo também conhecido como polimorfismo estático.

III. O polimorfismo de sobreposição, também conhecido como polimorfismo de inclusão, ocorre quando métodos com o mesmo nome em classes do mesmo ramo de herança têm comportamentos diferentes.

IV. O operador de sobrecarga é um exemplo de polimorfismo ad-hoc, que permite a definição de múltiplos métodos com o mesmo nome, mas com assinaturas diferentes.

Está correto apenas o que se afirma em:

A) I e II.

B) I e III.

C) I, III e IV.

D) II e IV.

E) Todos estão corretos.

",C,"

Explicação dos itens:

I. Correto. O polimorfismo permite que diferentes tipos de objetos sejam tratados de forma unificada, quando compartilham uma interface comum ou uma classe base comum, permitindo que métodos com o mesmo nome sejam chamados indistintamente, promovendo a flexibilidade e reutilização de código.

II. Incorreto. O polimorfismo pode ocorrer tanto em tempo de compilação como em tempo de execução. O polimorfismo estático em tempo de compilação é comumente chamado de sobrecarga de método ('method overloading'), enquanto o polimorfismo dinâmico em tempo de execução é chamado de sobreposição de método ('method overriding').

III. Correto. O polimorfismo de sobreposição ou de inclusão acontece quando subclasses redefinem métodos da superclasse, permitindo que objetos dessas subclasses tenham comportamentos especificados de maneira diferente da original. Esse é um exemplo de polimorfismo dinâmico.

IV. Correto. A sobrecarga (overloading) de operadores ou métodos, que permite a existência de múltiplas funções com o mesmo nome porém com parâmetros diferentes, é um exemplo de polimorfismo ad-hoc.

E. Incorreto. A segunda afirmativa está incorreta, portanto não todos estão corretos.",1938299
tópico 1,Banco de dados NoSQL,"Questão: Em sistemas de banco de dados NoSQL, a escalabilidade e a flexibilidade são aspectos fundamentais para lidar com grandes volumes de dados e solicitações de serviços de forma eficiente. Dentre os tipos de bancos de dados NoSQL, qual das opções a seguir apresenta uma característica que é frequentemente associada a um banco de dados orientado a documentos?

A) Suporte a transações ACID multipassos entre documentos.
B) Uso de grafos para representação dos relacionamentos entre os dados.
C) Armazenamento de dados em tabelas com linhas e colunas bem definidas.
D) Capacidade de armazenar dados semi-estruturados em formatos como JSON ou XML.
E) Alto nível de normalização e integridade referencial entre diferentes conjuntos de dados.

",D," 
Explicação dos itens:

A) Bancos de dados orientados a documentos, como MongoDB, em geral, oferecem suporte a transações ACID, mas não necessariamente multipassos entre documentos. Isso seria mais típico em bancos de dados relacionais.
B) Uso de grafos para a representação dos relacionamentos entre os dados é uma característica marcante de bancos de dados orientados a grafos, como Neo4j, e não dos orientados a documentos.
C) Armazenamento de dados em tabelas com linhas e colunas bem definidas é uma característica dos bancos de dados relacionais tradicionais (SQL), não de bancos de dados NoSQL.
D) Bancos de dados orientados a documentos são conhecidos por sua capacidade de armazenar dados semi-estruturados em formatos flexíveis como JSON ou XML, o que permite uma grande flexibilidade no armazenamento de diferentes tipos de dados.
E) Alto nível de normalização e integridade referencial são aspectos fundamentais de bancos de dados relacionais e não característicos dos bancos de dados NoSQL, que geralmente optam por estruturas mais denormalizadas para otimizar o desempenho e escalabilidade.",4063894
tópico 6,"Diagramas causais: gráficos acíclicos dirigidos; variáveis confundidoras, colisoras e de mediação","Questão: Em estudos epidemiológicos, os diagramas causais são ferramentas fundamentais para a representação e compreensão das relações entre variáveis. Considere um gráfico acíclico dirigido (DAG) em um estudo que avalia o efeito do consumo de uma determinada substância (X) no desenvolvimento de uma doença (Y). Contudo, uma variável socioeconômica (Z) é tanto um preditor de X quanto de Y, atuando como uma variável confundidora na análise.

Dada essa configuração e sabendo-se que as variáveis confundidoras podem introduzir uma associação espúria entre a exposição e o desfecho, qual das seguintes estratégias é mais adequada para controlar confundimento na avaliação do efeito de X sobre Y?

A) Estratificar a análise pelas categorias da variável Z.

B) Ajustar um modelo de regressão múltipla incluindo X, Y e Z como variáveis independentes.

C) Incluir no modelo uma variável W, que é efeito de Y e também causa de X, para controlar o confundimento entre X e Y.

D) Não ajustar por Z, pois a análise univariada é suficiente para estimar o efeito de X sobre Y.

E) Remover do dataset todas as observações onde a variável Z está presente.

",A," 

Explicações:

A) Correto. Estratificar a análise pelas categorias da variável Z permite controlar o confundimento, uma vez que compara indivíduos com níveis semelhantes de Z, reduzindo a associação espúria entre X e Y devido à confusão.

B) Incorreto. Ajustar um modelo de regressão múltipla incluindo X, Y e Z como variáveis independentes poderia levar a interpretações errôneas, já que Y não é uma variável independente, mas sim uma variável dependente que se pretende explicar a partir de X e Z.

C) Incorreto. Incluir uma variável W, que é efeito de Y e também causa de X, no modelo para controlar confundimento introduziria um caminho de colisão ou efeito enviesado, pois ao ajustar pela variável colidora (W), abriria-se uma nova rota de associação espúria entre X e Y.

D) Incorreto. Não ajustar por Z é inadequado, pois a variável confundidora Z é capaz de criar uma associação espúria entre X e Y, e sua omissão pode resultar em estimativas viesadas do efeito de X sobre Y.

E) Incorreto. Remover do dataset todas as observações onde a variável Z está presente não é uma estratégia prática, pois além de reduzir o tamanho da amostra e o poder estatístico do estudo, não resolve o problema do confundimento; simplesmente ignora a existência de um potencial viés.",2499605
tópico 3,"R ou Python: Classes de objetos e suas propriedades (vetores, listas, data.frames)","Questão:

Considere os seguintes trechos de código em R, cada um criando uma estrutura de dado diferente.

I. `a <- c(1, 2, 3, 4, 5)`

II. `b <- list(1, ""a"", TRUE, 3.14)`

III. `df <- data.frame(Nome=c(""Ana"", ""Bruno"", ""Carlos""), Idade=c(23, 25, 28))`

Sobre as estruturas criadas acima, analise as afirmativas a seguir e assinale a opção correta.

a) Somente as estruturas I e III podem ser acessadas por índices numéricos.

b) Todas as estruturas permitem armazenar elementos de tipos diferentes.

c) A estrutura III não permite a manipulação de suas colunas individualmente.

d) A estrutura II permite armazenar mais de um tipo de dado, enquanto I e III são homogêneas em tipo.

e) A estrutura III, além de ser acessada por índices numéricos, permite a manipulação de suas colunas por meio de seus nomes.

",E,"

Explicação dos itens:

a) Incorreto. Todas as estruturas podem ser acessadas por índices numéricos.

b) Incorreto. A estrutura I, que é um vetor, não permite armazenar elementos de tipos diferentes. R irá realizar uma coerção de tipo e converterá todos os elementos para o tipo mais flexível dentre os presentes.

c) Incorreto. É possível manipular as colunas de um data.frame (estrutura III) individualmente, seja por meio de índices numéricos, seja por meio dos nomes das colunas.

d) Incorreto. A afirmativa está parcialmente correta ao dizer que a estrutura II permite armazenar diferentes tipos de dados. No entanto, também é incorreta ao sugerir que o data.frame (estrutura III) é homogêneo em tipo. Um data.frame pode conter colunas de diferentes tipos, porém cada coluna em si deve ser homogênea em tipo.

e) Correto. A estrutura III, que corresponde a um data.frame em R, pode ser acessada tanto por índices numéricos como pelos nomes das colunas, além de permitir a manipulação individual das suas colunas, seja adicionando, removendo ou modificando dados.",4022278
tópico 3,"Visualização de dados ggplot, matplotlib","Questão:
A visualização de dados é uma ferramenta essencial para a compreensão e comunicação de informações complexas. Duas bibliotecas amplamente utilizadas para visualização de dados em linguagens de programação diferentes são ggplot2 e Matplotlib. O ggplot2 é uma biblioteca de visualização de dados para a linguagem de programação R, enquanto Matplotlib é uma biblioteca para Python.

Considere que um cientista de dados está trabalhando com um conjunto de dados que contém informações sobre vendas de produtos em diferentes regiões e deseja criar um gráfico de dispersão que mostre a relação entre o número de unidades vendidas e o preço de venda dos produtos, diferenciando as regiões por cores.

Qual das seguintes alternativas indica corretamente a forma como o cientista de dados deve proceder em ambas as bibliotecas para alcançar tal visualização?

A) Em ggplot2: utilizar a função `qplot(x='unidades_vendidas', y='preco_venda', data=df, color='regiao')`; em Matplotlib: utilizar a função `plt.scatter('unidades_vendidas', 'preco_venda', c=df['regiao'])`
B) Em ggplot2: utilizar a função `ggplot(df, aes(x=unidades_vendidas, y=preco_venda)) + geom_point(aes(color=regiao))`; em Matplotlib: utilizar a função `plt.scatter(df['unidades_vendidas'], df['preco_venda'], c='regiao')`
C) Em ggplot2: utilizar a função `ggplot(df, aes(x=unidades_vendidas, y=preco_venda, color=regiao)) + geom_point()`; em Matplotlib: utilizar a função `plt.scatter(df['unidades_vendidas'], df['preco_venda'], c=df['regiao'])`
D) Em ggplot2: utilizar a função `ggplot(df) + geom_point(mapping=aes(x='unidades_vendidas', y='preco_venda'), color='regiao')`; em Matplotlib: utilizar a função `plt.scatter(x='unidades_vendidas', y='preco_venda', data=df, c='regiao')`
E) Em ggplot2: utilizar a função `ggplot(aes(x=unidades_vendidas, y=preco_venda, col=regiao)) + geom_point(data=df)`; em Matplotlib: utilizar a função `plt.scatter('unidades_vendidas', 'preco_venda', c=df.regiao)`

",C," 
Explicação dos itens:
A) Incorreto. Em ggplot2, a função `qplot` é uma forma simplificada de criar gráficos e não utiliza strings para nomear variáveis. Além disso, a função `plt.scatter` em Matplotlib não aceita a passagem de strings diretamente para argumentos x e y.
B) Incorreto. Enquanto o uso de `aes` e `geom_point` em ggplot2 é correto, em Matplotlib a cor não deve ser passada como uma string 'regiao', mas sim o vetor de cores correspondente.
C) Correto. Em ggplot2, `aes` é usado para mapear variáveis ​​aos eixos e atributos estéticos do gráfico, tal como a cor diferenciando por 'regiao'. Em Matplotlib, a função `plt.scatter` aceita vetores ou colunas de um DataFrame diretamente, e o argumento c pode ser usado para passar as cores correspondentes às categorias da variável 'regiao'.
D) Incorreto. Embora a parte do ggplot2 utilize a função `geom_point`, o mapeamento em `aes` deveria estar dentro do ggplot, não como um argumento separado de geom_point. Em Matplotlib, 'x' e 'y' devem ser dados como vetores e não como strings, e 'data' não é um argumento válido para a função `plt.scatter`.
E) Incorreto. No ggplot2 a função `ggplot` requer que o dataframe seja especificado, a função `aes` deve estar dentro do chamado `ggplot`, e a cor deve ser referida como `color`, e não `col`. No Matplotlib, a função `plt.scatter` não aceita strings diretamente para os argumentos x e y da mesma maneira que a biblioteca ggplot2.",1464050
tópico 1,"Banco de dados relacional: SQL Server, PostgreSQL, MySQL","Questão:

Em um ambiente de banco de dados relacional, profissionais de TI devem escolher o sistema de gestão de banco de dados (SGBD) mais adequado para as necessidades da organização. Considerando os SGBDs SQL Server, PostgreSQL e MySQL, analise as seguintes afirmativas:

I - O SQL Server, um SGBD proprietário da Microsoft, oferece uma integração avançada com outros produtos Microsoft, como o .NET Framework e o Microsoft Azure.

II - PostgreSQL é conhecido por sua conformidade com os padrões SQL e por oferecer suporte a um conjunto mais amplo de recursos do SQL padrão em comparação aos seus concorrentes.

III - MySQL é um SGBD de código aberto amplamente utilizado para aplicações web, mas possui algumas limitações no que diz respeito à implementação completa de triggers e stored procedures em comparação com o SQL Server e PostgreSQL.

É correto o que se afirma em:

A) I, II e III.  
B) I e II, apenas.  
C) II e III, apenas.  
D) I e III, apenas.  
E) II, apenas.

",B,"

Explicação dos itens:

A) Esta alternativa é incorreta porque afirma que todas as afirmativas estão corretas. Entretanto, a afirmativa III possui uma informação desatualizada, já que as versões recentes do MySQL têm melhorado consideravelmente no que diz respeito a triggers e stored procedures.

B) Esta é a alternativa correta porque as afirmativas I e II estão corretas. O SQL Server possui integração avançada com outros produtos Microsoft, e o PostgreSQL é reconhecido pela sua conformidade com os padrões SQL e extensão de funcionalidades que muitas vezes ultrapassam seus concorrentes.

C) Esta alternativa é incorreta porque inclui a afirmativa III, que contém informações desatualizadas sobre o MySQL.

D) Esta alternativa é incorreta porque, apesar da afirmativa I estar correta, a inclusão da afirmativa III a torna incorreta pelo mesmo motivo que a alternativa C.

E) Esta alternativa é incorreta porque exclui a afirmativa I, que está correta.",117920
tópico 0,Ingestão de dados em streaming,"Questão: A empresa Alfa desenvolveu uma aplicação para processar grandes volumes de dados em tempo real, gerados por diversos dispositivos IoT (Internet of Things) espalhados por uma metrópole. Os dados são heterogêneos e chegam em um fluxo constante, sendo essencial para o negócio da Alfa a análise e reação a esses dados de maneira ágil e eficaz. Para tanto, a empresa decidiu utilizar uma arquitetura de processamento de dados em streaming. Considerando as tecnologias e estratégias disponíveis, qual seria a opção mais apropriada para atender às necessidades da empresa Alfa?

A) Utilizar um banco de dados relacional tradicional, com triggers configuradas para lidar com os dados em tempo real conforme eles chegam.

B) Empregar um sistema de mensageria como o RabbitMQ para enfileirar os dados e um conjunto de workers para processá-los de forma síncrona.

C) Implementar uma solução baseada em Apache Kafka para ingestão de dados em streaming, em conjunto com o Apache Flink para processamento em tempo real e análise.

D) Configurar um data lake utilizando o Hadoop HDFS para armazenar os dados brutos e processamento batch periódico por meio do MapReduce.

E) Utilizar um serviço de nuvem como o AWS S3 para armazenamento dos dados e disparar funções Lambda para processamento de cada evento individualmente.

",C," 

Explicação dos itens:

A) Um banco de dados relacional tradicional não é projetado para lidar com grandes volumes de dados em streaming, nem para análises em tempo real de fluxos contínuos de dados. Triggers podem não ser suficientes para o processamento necessário e podem gerar gargalos.

B) O RabbitMQ é uma ferramenta de mensageria que pode ser usada para enfileirar mensagens, porém não é otimizado para alta vazão de dados como os gerados por dispositivos IoT em uma grande cidade. Além disso, o processamento síncrono de workers pode não ser rápido o suficiente para cenários de dados em tempo real.

C) Apache Kafka é uma plataforma de streaming distribuído designada especificamente para lidar com volumes massivos de dados em tempo real. Em conjunto com o Apache Flink, que é uma engine de processamento de stream, podem fornecer a análise em tempo real necessária para a aplicação descrita.

D) Hadoop HDFS é uma solução eficaz para armazenar grandes volumes de dados, mas é mais adequado para situações onde o processamento batch (em lotes) é preferível. Não atenderia à necessidade de processamento em tempo real que a empresa Alfa está procurando.

E) Armazenamento de dados em AWS S3 e processamento utilizando funções Lambda pode ser uma solução para situações de processamento baseado em eventos, mas pode não ser ideal para grandes volumes de dados em streaming devido a questões de escalabilidade e latência.",3314142
tópico 6,Métodos e técnicas de identificação causal: Métodos experimentais RCT e de identificação quase-experimental,"Questão: A avaliação de impacto de políticas públicas requer metodologias que permitam a mensuração dos efeitos causais de determinada intervenção. Dentre as abordagens metodológicas disponíveis, encontram-se os métodos experimentais e os quase-experimentais. Sobre os métodos de identificação causal, avalie as afirmativas abaixo e assinale a opção correta:

I. Randomized Controlled Trials (RCTs) são considerados o padrão ouro para determinação de causalidade, pois o processo de randomização minimiza a probabilidade de viés de seleção, garantindo a comparabilidade entre os grupos de tratamento e controle.

II. Em métodos quase-experimentais, a randomização não é possível ou viável e, por isso, são utilizadas técnicas como difference-in-differences, regression discontinuity ou propensity score matching para criar uma situação comparável ao experimento randomizado e isolar o efeito causal da intervenção.

III. A técnica de propensity score matching utilizada em estudos quase-experimentais assume que há randomização dentro dos pares formados por observações tratadas e não tratadas com características observáveis similares, garantindo resultados tão robustos quanto os de um RCT.

IV. Um dos desafios dos RCTs é a possível violação das condições de internalidade, onde o efeito observado não pode ser atribuído exclusivamente à variável de interesse, comprometendo a validade do estudo.

Está correto o que se afirma em:

A) I e II, apenas.
B) II e III, apenas.
C) I, II e IV, apenas.
D) I, III e IV, apenas.
E) I, II, III e IV.

",C," 

Explicação dos itens:

I. Correto. Os RCTs são de fato considerados o padrão ouro para estabelecer a relação causal, pois a randomização tende a igualar os grupos em relação a variáveis observáveis e não observáveis, reduzindo o risco de viés de seleção.

II. Correto. Os métodos quase-experimentais são utilizados quando a randomização não é possível, e as técnicas mencionadas (difference-in-differences, regression discontinuity, propensity score matching) são algumas das estratégias para simular um grupo de controle e isolar o efeito causal.

III. Incorreto. Embora o propensity score matching crie pares de observações tratadas e não tratadas com características semelhantes, isso não garante resultados tão robustos quanto um RCT porque a técnica não consegue controlar para variáveis não observáveis que poderiam afetar o resultado, o que a randomização de um RCT tenta fazer.

IV. Correto. Apesar do RCT ser uma metodologia de alto padrão para estabelecer causalidade, ele não é imune a problemas como violações de internalidade, que podem ocorrer devido a mecanismos de difusão do tratamento, não adesão ao protocolo, entre outros fatores, que podem comprometer a atribuição causal direta ao tratamento em questão.",5794084
tópico 1,"Banco de dados relacional: SQL Server, PostgreSQL, MySQL","Questão: Em sistemas de banco de dados relacionais como SQL Server, PostgreSQL e MySQL, a otimização de consultas é fundamental para garantir a eficiência na recuperação de dados. Considerando o cenário onde um banco de dados possui uma grande tabela denominada `vendas`, com milhões de registros e colunas como `id`, `produto_id`, `cliente_id`, `data_venda`, e `valor`, um administrador de banco de dados deseja criar um índice para melhorar a performance de consultas que frequentemente filtram e ordenam os dados baseados na `data_venda`. Qual das seguintes estratégias de indexação é a mais apropriada para otimizar tais consultas?

A) Criar um índice clusterizado na coluna `id`.
B) Criar um índice não clusterizado na coluna `data_venda`.
C) Utilizar um índice full-text na coluna `valor`.
D) Criar um índice hash na coluna `cliente_id`.
E) Criar um índice bitmap na coluna `produto_id`.

",B,"

Explicação dos itens:
A) Criar um índice clusterizado na coluna `id` - Normalmente, o índice clusterizado é criado na chave primária, que costuma ser a coluna `id` em muitos bancos de dados. No entanto, essa estratégia não otimizaria as consultas baseadas em `data_venda`, pois o índice clusterizado organiza fisicamente a tabela com base na coluna indexada.

B) Criar um índice não clusterizado na coluna `data_venda` - Essa é a estratégia correta, pois um índice não clusterizado em `data_venda` permitiria que o SGBD localize rapidamente as linhas com base nas datas, sem a necessidade de reorganizar fisicamente toda a tabela, o que é ideal para uma coluna frequentemente utilizada em filtros e ordenações.

C) Utilizar um índice full-text na coluna `valor` - Índices full-text são utilizados para otimizar buscas por texto livre dentro de colunas de texto grande e não são apropriados para colunas numéricas como `valor`.

D) Criar um índice hash na coluna `cliente_id` - Índices hash são bons para operações de igualdade, como encontrar todas as vendas de um determinado cliente. Contudo, essa estratégia não ajudaria nas consultas que dependem da ordenação ou filtragem por `data_venda`.

E) Criar um índice bitmap na coluna `produto_id` - Índices bitmap são eficientes para colunas com poucos valores distintos, como colunas de status ou tipo. Dada a grande quantidade de possíveis produtos, um índice bitmap em `produto_id` pode não ser eficiente e, de qualquer forma, não atenderia à necessidade de otimizar consultas baseadas na `data_venda`.

Portanto, a alternativa B é a mais apropriada para otimizar consultas baseadas na coluna `data_venda`.",9540943
tópico 6,"Diagramas causais: gráficos acíclicos dirigidos; variáveis confundidoras, colisoras e de mediação","Questão: Em estudos observacionais, a análise de causa e efeito é muitas vezes obscurecida pela presença de variáveis confundidoras, colisoras e de mediação. Um pesquisador está interessado em avaliar o efeito da exposição à poluição do ar (X) sobre a incidência de asma em crianças (Y). Ele também está ciente de que o status socioeconômico (Z) pode influenciar tanto a exposição à poluição (visto que famílias de baixa renda podem viver em áreas mais poluídas) quanto a incidência de asma (devido a diferenças no acesso à saúde, entre outros fatores).

Nesse cenário, ao utilizar gráficos acíclicos dirigidos (DAGs) para representar as relações causais, como Z seria classificado em relação ao efeito de X sobre Y?

A) Z é uma variável confundidora que deve ser controlada na análise.
B) Z é irrelevante e deve ser ignorada na análise causal.
C) Z é uma variável colidora que deve ser condicionada na análise.
D) Z atua como uma variável de mediação, mediando a relação entre X e Y.
E) Z é uma variável instrumental para a relação entre X e Y.

",A,"

Alternativa A: Correta. Z é uma variável confundidora porque afeta tanto a exposição à poluição (X) quanto o desfecho saúde, neste caso, a incidência de asma (Y). Logo, Z deve ser controlada na análise para isolar o efeito real de X sobre Y.

Alternativa B: Incorreta. Z não é irrelevante; é uma variável importante que influencia tanto a exposição quanto o desfecho e, por isso, deve ser considerada na análise.

Alternativa C: Incorreta. Z é uma variável confundidora, não colidora. Uma variável colidora é afetada por pelo menos duas outras variáveis no diagrama que são causas de um efeito comum. Neste caso, Z é uma causa comum e não um efeito comum.

Alternativa D: Incorreta. Não há informação suficiente no cenário apresentado que indique que Z está no caminho causal entre X e Y. Para ser uma variável de mediação, Z deveria ser consequência da variável independente (X) e ao mesmo tempo influenciar a variável dependente (Y).

Alternativa E: Incorreta. Uma variável instrumental é uma variável que está associada à variável de exposição e afeta o desfecho apenas por meio dessa exposição. Não há evidência no enunciado de que Z satisfaça esses requisitos.",39013
tópico 0,Ingestão de dados em streaming,"Questão:
Qual das seguintes afirmações sobre técnicas de ingestão de dados em streaming é verdadeira?

A) A ingestão de dados em streaming é normalmente realizada em lotes para otimizar o uso da largura de banda, uma vez que cada lote pode ser compactado antes da transmissão.

B) Sistemas de ingestão de dados em streaming, como o Apache Kafka, não suportam tolerância a falhas e alta disponibilidade, pois são focados exclusivamente em baixa latência nas operações.

C) A ingestão de dados em streaming requer estratégias de processamento complexas, como a computação em janela de tempo, para tratar fluxos de dados que são gerados de forma contínua e em tempo real.

D) Sistemas de ingestão de dados em streaming não precisam ser escaláveis, pois a quantidade de dados gerados pelos dispositivos de Internet das Coisas (IoT) tende a ser constante e previsível.

E) Armazenamento de dados é uma função crítica da ingestão de dados em streaming, e por isso a maioria dos sistemas de streaming foca primariamente em otimizar o armazenamento em longo prazo dos dados, em detrimento da análise em tempo real.

",C," 
A ingestão de dados em streaming é o processo de captura e importação de fluxos de dados que são gerados continuamente por diversas fontes. As afirmações erradas são:

A) A ingestão de dados em streaming é focada no processamento de dados em tempo real, e não em lotes. O processamento em tempo real exige que os dados sejam capturados e processados assim que são gerados, diferentemente do processamento em lotes, que coleta dados durante um período de tempo antes de processá-los.

B) Sistemas de ingestão de dados em streaming, como o Apache Kafka, são efetivamente projetados para suportar tolerância a falhas e alta disponibilidade. Eles usam técnicas como replicação de dados e particionamento para garantir que os sistemas sejam resilientes a falhas e que os dados não sejam perdidos.

D) Sistemas de ingestão de dados em streaming devem ser altamente escaláveis para acomodar o crescimento variável e, muitas vezes, imprevisível do volume de dados gerados, especialmente com o aumento dos dispositivos conectados na IoT.

E) Enquanto o armazenamento de dados é importante, a função primária da ingestão de streaming é facilitar o processamento e a análise de dados em tempo real. Sistemas de streaming são otimizados para processar e possivelmente reagir aos dados à medida que chegam, e não apenas armazená-los para análise posterior.",2128667
tópico 3,"R ou Python: Classes de objetos e suas propriedades (vetores, listas, data.frames)","Questão: Em linguagens de programação como R e Python, estruturas de dados são fundamentais para o armazenamento e manipulação de informações. Considere as afirmativas a seguir sobre classes de objetos em R:

I. Em R, os vetores são coleções homogêneas de elementos, o que significa que todos os elementos devem ser do mesmo tipo. Caso diferentes tipos de dados sejam combinados em um vetor, eles serão coagidos para o tipo de dado mais complexo presente.

II. Listas em R são objetos que podem conter elementos de diferentes tipos e classes, incluindo outros objetos complexos como data.frames e até outras listas, o que as torna heterogêneas.

III. Data.frames em R são semelhantes a tabelas de banco de dados, onde cada coluna pode conter diferentes tipos de dados, mas todas as colunas devem ter o mesmo número de linhas.

É correto o que se afirma em:

A) I, apenas.
B) II, apenas.
C) III, apenas.
D) I e II, apenas.
E) I, II e III.

",E,"

Explicação:

A) Incorreta, pois não é apenas I que está correta.
B) Incorreta, pois não é apenas II que está correta.
C) Incorreta, pois não é apenas III que está correta.
D) Incorreta, pois não são apenas I e II que estão corretas.
E) Correta, pois todas as afirmações sobre as estruturas de dados em R estão corretas:
    - I está correta: em R, vetores são homogêneos e haverá coerção de tipos se forem misturados.
    - II está correta: listas podem conter diferentes tipos de dados, tornando-as heterogêneas.
    - III está correta: data.frames são similares a tabelas de banco de dados onde colunas representam variáveis de diferentes tipos, mas todas devem ter o mesmo comprimento.",973270
tópico 6,Tipos de viés no processo gerador dos dados e soluções: Sampling bias; Selection bias; Attrition bias; Reporting bias; Measurement bias.,"Questão:

A análise de dados é uma ferramenta essencial para a tomada de decisões em diversos campos, como na pesquisa científica, na indústria e em políticas públicas. Entretanto, a qualidade das inferências e previsões que se pode fazer a partir desses dados está significativamente ligada à presença ou ausência de vieses no processo de geração e coleta dos dados. Nesse contexto, considere os seguintes tipos de viés:

I. Sampling bias: Ocorre quando a amostra coletada não representa adequadamente a população de interesse, levando a estimativas que não refletem as características verdadeiras da população.

II. Selection bias: Manifesta-se quando os critérios de seleção dos participantes de um estudo criam diferenças sistemáticas entre os grupos de interesse, afetando a generalização dos resultados.

III. Attrition bias: Refere-se à perda sistemática de participantes durante o curso de um estudo, o que compromete a representatividade da amostra final em relação à amostra inicial.

IV. Reporting bias: Envolve a tendência de reportar apenas resultados favoráveis ou significativos, ignorando achados que não suportam a hipótese de interesse.

V. Measurement bias: Acontece quando erros sistemáticos são introduzidos no processo de medição dos dados, afetando a precisão e a validade dos resultados.

Considerando esses cinco tipos de viés, qual das seguintes ações é apropriada para mitigar o viés de seleção?

A) Utilizar critérios de inclusão e exclusão claros e rigorosos na seleção dos participantes.
B) Realizar testes estatísticos apropriados após a coleta de dados para ajustar diferenças entre grupos.
C) Garantir que o processo de coleta de dados seja cego, de modo que os pesquisadores não saibam a qual grupo o participante pertence.
D) Aumentar a amostra periodicamente para compensar a perda de participantes ao longo do estudo.
E) Adotar procedimentos de randomização na alocação dos sujeitos aos diferentes grupos do estudo.

",E," 
A alternativa E é a correta pois a adoção de procedimentos de randomização na alocação dos sujeitos aos diferentes grupos do estudo é uma das formas mais efetivas de mitigar o viés de seleção, garantindo que os grupos sejam comparáveis em relação a fatores potencialmente confundidores. Ao contrário de simples critérios de inclusão e exclusão (alternativa A) que definem quem poderá participar do estudo, a randomização busca balancear os grupos de modo que as diferenças observadas sejam mais provavelmente atribuíveis ao tratamento ou condição em estudo, e não a características pré-existentes dos participantes.

- A alternativa A é inadequada porque enquanto critérios de inclusão e exclusão são importantes, eles sozinhos não asseguram que o viés de seleção será mitigado, já que os participantes que cumprem esses critérios ainda podem diferir de formas que afetem os resultados do estudo.
- A alternativa B aborda uma ação a ser tomada após a coleta de dados e não visa diretamente a mitigação do viés de seleção no processo de coleta.
- A alternativa C é relevante para evitar biases que podem surgir da interação dos pesquisadores com os participantes, mas não é uma ação direcionada ao viés de seleção.
- A alternativa D visa mitigar o attrition bias, não o selection bias, e não aborda a questão da sistemática diferença entre grupos que caracteriza o viés de seleção.
",3016379
tópico 1,Álgebra relacional e SQL (padrão ANSI),"Questão: Considere o schema de uma base de dados relacional composto pelas seguintes relações:

Funcionarios (cpf, nome, cargo, salario)
Departamentos (depto_id, nome_depto, localizacao)
Projetos (proj_id, nome_proj, budget, depto_id)

A relação ""Funcionarios"" contém informações sobre os funcionários de uma empresa, incluindo seu CPF, nome, cargo e salário. A relação ""Departamentos"" guarda os dados dos departamentos, incluindo ID do departamento, nome e localização. E a relação ""Projetos"" contém informações sobre os projetos em andamento, incluindo o ID do projeto, o nome do projeto, o orçamento (budget) e o ID do departamento responsável pelo projeto.

Suponha que você queira listar o nome, o cargo e o salário dos funcionários que trabalham em departamentos localizados em 'São Paulo' e que estão envolvidos em projetos com orçamento superior a R$ 1.000.000. Considerando o uso de Álgebra Relacional e SQL padrão ANSI, qual das seguintes opções melhor representa a operação a ser realizada?

A) π_nome, cargo, salario(σ_localizacao='São Paulo' ∧ budget>1000000(Funcionarios ⨝ Departamentos ⨝ Projetos))

B) SELECT nome, cargo, salario
   FROM Funcionarios, Departamentos, Projetos
   WHERE Departamentos.localizacao = 'São Paulo'
   AND Projetos.budget > 1000000
   AND Funcionarios.depto_id = Departamentos.depto_id
   AND Departamentos.depto_id = Projetos.depto_id

C) SELECT nome, cargo, salario
   FROM Funcionarios F
   INNER JOIN Departamentos D ON F.depto_id = D.depto_id
   INNER JOIN Projetos P ON D.depto_id = P.depto_id
   WHERE D.localizacao = 'São Paulo'
   AND P.budget > 1000000

D) SELECT F.nome, F.cargo, F.salario
   FROM Funcionarios F
   JOIN Departamentos D ON F.cpf = D.depto_id
   JOIN Projetos P ON D.depto_id = P.proj_id
   WHERE D.localizacao = 'São Paulo'
   AND P.budget > 1000000

E) π_nome, cargo, salario(σ_localizacao='São Paulo' ∧ budget>1000000(Funcionarios ⨝ depto_id=depto_id Departamentos ⨝ depto_id=depto_id Projetos))

",C,"

As respostas são avaliadas da seguinte maneira:

A) Incorreto. A expressão de Álgebra Relacional não especifica corretamente as condições de junção entre as relações.

B) Incorreto. Esta é uma abordagem clássica de SQL, mas não faz uso da cláusula JOIN explicitamente e usa uma sintaxe antiga e potencialmente propensa a erros (junção de tabelas sem especificar as relações de chave estrangeira), o que poderia levar a um resultado de produto cartesiano incorreto.

C) Correto. Esta é a opção correta, pois utiliza a sintaxe ANSI de SQL com INNER JOIN para relacionar as tabelas de acordo com as chaves estrangeiras e especifica claramente as condições de seleção no WHERE.

D) Incorreto. Esta opção contém um erro nas condições de JOIN, relacionando CPF do funcionário com o ID do departamento e o ID do departamento com o ID do projeto, o que é ilógico e não representa corretamente a relação entre tabelas.

E) Incorreto. Apesar de tentar expressar a operação em Álgebra Relacional, esta opção também não especifica corretamente as igualdades nas junções entre as relações, falhando em estabelecer as devidas correlações entre as chaves estrangeiras.",4833488
tópico 4,"Principais distribuições de probabilidade discretas e contínuas: distribuição uniforme, distribuição binomial, distribuição Poisson e distribuição normal","Questão: 
Seja X uma variável aleatória que segue uma distribuição dentre as distribuições de probabilidade discretas e contínuas mencionadas. Considere as seguintes informações sobre X:

I. A média e a variância de X são ambas iguais a λ, um valor positivo.

II. A probabilidade de um evento ocorrer dentro de um intervalo fixo é constante e não depende da ocorrência do evento em intervalos diferentes.

III. O número de eventos que ocorrem em um intervalo é independente do número de eventos em outro intervalo, dado que os intervalos não se sobrepõem.

Com base nessas informações, qual distribuição de probabilidade mais provavelmente descreve a variável aleatória X?

A) Distribuição Uniforme
B) Distribuição Binomial
C) Distribuição Poisson
D) Distribuição Normal

",C,"

Explicação dos itens:

A) Distribuição Uniforme - Esta distribuição é caracterizada por ter uma probabilidade constante em seu intervalo de definição, mas sua média não é necessariamente igual à sua variância. Portanto, o item I já indica que não é a distribuição uniforme.

B) Distribuição Binomial - A distribuição binomial possui média np e variância np(1-p), onde n é o número de ensaios e p é a probabilidade de sucesso em um ensaio. Portanto, a média e a variância não são iguais na distribuição binomial, descartando esta opção com base no critério I.

C) Distribuição Poisson - Esta distribuição é adequada para descrever o número de eventos que ocorrem num intervalo de tempo ou espaço se os eventos ocorrem independentemente e a uma taxa constante. De acordo com as informações fornecidas, caracteriza-se pela igualdade entre a média e a variância (λ), e pela independência dos eventos em intervalos não sobrepostos, o que está alinhado com as propriedades II e III. Portanto, esta é a distribuição que melhor se encaixa na descrição dada.

D) Distribuição Normal - A distribuição normal é simétrica em torno da média, e sua variância é uma medida de dispersão em torno desta média. A distribuição normal tem uma forma de sino e não possui as propriedades descritas no enunciado, especificamente a igualdade entre média e variância, e também não é apropriada para contar o número de eventos em intervalos fixos de tempo ou espaço como no critério III. Logo, também não se aplica à descrição de X.",7449410
tópico 3,"R ou Python: Classes de objetos e suas propriedades (vetores, listas, data.frames)","Questão: Em relação às linguagens de programação R e Python, é correto afirmar que vetores, listas e data.frames (ou DataFrames no Python) representam estruturas de dados fundamentais em análise de dados. Considere as seguintes afirmações sobre suas propriedades:

I. Em R, vetores são estruturas homogêneas que podem conter elementos de diferentes tipos de dados, como números e strings, na mesma estrutura.
II. As listas, tanto em R quanto em Python, são estruturas heterogêneas que permitem armazenar elementos de diferentes tipos, incluindo outras listas ou estruturas de dados complexas.
III. Em Python, DataFrames são oferecidos pela biblioteca pandas e permitem a manipulação de dados tabulares em duas dimensões com diferentes tipos de dados em suas colunas.
IV. Data.frames em R não suportam operações de agrupamento e sumarização de dados, tais funcionalidades estão restritas a estruturas mais avançadas como tibbles ou DataFrames no Python.

Assinale a opção que contém apenas as afirmações corretas.

A) I e II
B) II e III
C) I, II e IV
D) II, III e IV
E) III e IV

",B,"

Explicação dos itens:

I. Esta afirmação é incorreta. Em R, vetores são estruturas homogêneas e todos os elementos devem ser do mesmo tipo de dados. Se diferentes tipos são combinados, a coerção de tipos ocorre, e todos os elementos são convertidos para o mesmo tipo.

II. Esta afirmação é correta. Tanto em R quanto em Python, as listas podem conter elementos de diferentes tipos, e isso inclui números, strings, outras listas e até objetos mais complexos.

III. Esta afirmação é correta. Em Python, a biblioteca pandas introduz o objeto DataFrame, que é altamente utilizado para manipulação de dados tabulares e permite diferentes tipos de dados por coluna, semelhante às planilhas em software de tabelas como Microsoft Excel ou Google Sheets.

IV. Esta afirmação é incorreta. Em R, data.frames suportam uma grande variedade de operações, incluindo agrupamento e sumarização. Bibliotecas complementares, como dplyr, podem facilitar e expandir essas funcionalidades, mas não são as únicas maneiras de realizar tais operações.",3898449
tópico 0,"Arquitetura de cloud computing para ciência de dados (AWS, Azure, GCP)","Questão: Em uma organização que utiliza intensivamente serviços de computação em nuvem para projetos de ciência de dados, está sendo planejada a migração de um workflow analítico tradicional para uma arquitetura em cloud a fim de otimizar processos e recursos. A equipe de TI está avaliando as opções AWS, Azure e GCP, considerando este cenário:

1. A necessidade de um serviço de armazenamento de objetos altamente escalável.
2. A utilização de máquinas virtuais personalizáveis para processamento de grandes volumes de dados.
3. Um serviço gerenciado de Big Data para executar jobs de análise e machine learning sem a gestão direta da infraestrutura.
4. Ferramentas integradas para o desenvolvimento de modelos de machine learning e experimentação.

Considerando as especificações acima, qual das seguintes opções melhor atenderia aos requisitos da organização?

A) AWS com Amazon S3 para armazenamento, EC2 para máquinas virtuais, EMR para Big Data e SageMaker para modelos de ML.
B) Azure com Azure Blob Storage para armazenamento, Azure Virtual Machines para processamento, HDInsight para Big Data e Azure Machine Learning Studio para ML.
C) GCP com Google Cloud Storage para armazenamento, Compute Engine para máquinas virtuais, BigQuery para Big Data e AI Platform para modelos de ML.
D) Azure com Azure File Storage para armazenamento, Azure Functions para processamento, Data Lake Analytics para Big Data e Azure Machine Learning para modelos de ML.
E) AWS com Amazon Glacier para armazenamento a longo prazo, Lambda para computação serverless, Redshift para warehousing de dados e Deep Learning AMIs para ML.

",A," 
- A opção A oferece uma solução completa e otimizada com os serviços da AWS: Amazon S3 para armazenamento de objetos escalável; EC2 fornece máquinas virtuais personalizáveis; EMR é uma solução gerenciada para Big Data, adequada para processamento de análises de grandes volumes de dados e machine learning; e SageMaker é uma plataforma para facilmente construir, treinar e implantar modelos de machine learning.
- A opção B também fornece um conjunto de soluções adequadas com os serviços do Azure; no entanto, ela não é a alternativa escolhida pois essa questão tende a não ter mais de uma resposta correta e a alternativa A já foi selecionada como correta.
- A opção C propõe uma combinação de serviços do GCP que atenderia aos requisitos, mas não é a opção selecionada devido ao mesmo motivo que a alternativa B.
- A opção D inclui o Azure File Storage, que não é o serviço de armazenamento de objetos escalável mais sugerido quando comparado ao Azure Blob Storage. Além disso, Azure Functions é mais focado em computação serverless do que em processamento de grandes volumes de dados para ciência de dados.
- A opção E seleciona alguns serviços da AWS que não são os mais indicados para os requisitos dados: Amazon Glacier é usado para o armazenamento a longo prazo e não para acesso frequente aos dados; Lambda é voltado para computação serverless e pode não ser a melhor opção para processamento de grandes volumes de dados; Redshift é uma ótima solução para data warehousing, mas não é um serviço gerenciado de Big Data; Deep Learning AMIs são instâncias de máquina virtual com frameworks de deep learning, mas não são uma plataforma integrada de desenvolvimento de modelos de machine learning.",714640
tópico 6, Modelos probabilísticos gráficos: cadeias de Markov; filtros de Kalman; Redes bayesianas,"Questão:
A análise probabilística de sistemas complexos pode ser realizada através de diferentes modelos gráficos, os quais permitem a representação de dependências estocásticas entre variáveis aleatórias. Sobre os modelos probabilísticos gráficos, analise as seguintes afirmativas e assinale a opção correta.

I. Cadeias de Markov são modelos que expressam a propriedade de que o estado futuro depende apenas do estado presente, não do histórico de estados passados, caracterizando a propriedade de ""sem memória"" ou Markovianidade.

II. Filtros de Kalman são utilizados para estimar o estado de um sistema dinâmico em tempo discreto na presença de ruídos nas medições, por meio de um conjunto recursivo de fórmulas matemáticas que preveem o estado futuro baseado nas estimativas passadas e na observação atual.

III. As Redes Bayesianas são estruturas representacionais para variáveis aleatórias que podem ser descritas como acíclicas, porém não lidam explicitamente com a temporalidade ou a sequência em que os eventos ocorrem.

IV. As Cadeias de Markov de tempo contínuo são mais adequadas para representar sistemas em que as transições de estados ocorrem em intervalos regulares de tempo pré-determinados, diferentemente das cadeias de tempo discreto que se adaptam melhor a modelos com transições em intervalos irregulares.

A) Apenas as afirmativas I e III estão corretas.
B) Apenas as afirmativas I e II estão corretas.
C) Apenas as afirmativas II e III estão corretas.
D) Apenas as afirmativas I, II e III estão corretas.
E) Todas as afirmativas estão corretas.

",D,"

Explicação dos itens:

I. Correta. As Cadeias de Markov são caracterizadas pelo princípio de que a probabilidade de transição para o próximo estado só depende do estado atual e não do histórico de estados anteriores, o que é conhecido como propriedade de Markov ou a suposição de ""sem memória"".

II. Correta. Filtros de Kalman são aplicados para estimar o estado de sistemas lineares dinâmicos em presença de ruídos. Eles são um conjunto de equações matemáticas que proporcionam uma maneira eficiente de inferir o estado de um processo de maneira recursiva, ou seja, o estado atual é estimado a partir do estado anterior e da nova medição.

III. Correta. Redes Bayesianas são modelos gráficos que representam variáveis e suas dependências condicionais por meio de um grafo direcionado acíclico (DAG). Eles são ótimos para modelar incertezas e raciocinar sobre variáveis aleatórias, mas não modelam explicitamente a evolução temporal.

IV. Incorreta. A afirmação IV está incorreta pois inverte a aplicabilidade das cadeias de Markov de tempo contínuo e discreto. Cadeias de Markov de tempo contínuo são mais adequadas para modelar sistemas em que as transições de estados não ocorrem em intervalos de tempo fixos e são descritas por taxas de transição, enquanto cadeias de Markov de tempo discreto lidam com sistemas que têm transições em passos de tempo claros e distintos.",8575997
tópico 6,"Diagramas causais: gráficos acíclicos dirigidos; variáveis confundidoras, colisoras e de mediação","Questão: Em uma pesquisa sobre os efeitos do estilo de vida no desenvolvimento de doenças cardiovasculares, um epidemiologista decide construir um diagrama causal para compreender as relações entre as diversas variáveis. O pesquisador identifica que a prática regular de exercícios (Exercício), a alimentação saudável (Alimentação), o nível de estresse (Estresse) e o histórico familiar de doenças cardíacas (Histórico Familiar) são fatores importantes a serem considerados. Ele também considera indispensável incluir no modelo a pressão arterial (Pressão Arterial) como um possível intermediário entre os fatores de risco e as doenças cardiovasculares (Doença Cardiovascular). Diante disso, é correto afirmar que no diagrama causal:

A) A variável ""Exercício"" é uma variável confundidora na relação entre ""Alimentação"" e ""Doença Cardiovascular"".

B) A variável ""Histórico Familiar"" é uma variável colisora na relação entre ""Pressão Arterial"" e ""Doença Cardiovascular"".

C) A variável ""Estresse"" é uma variável confundidora na relação entre ""Pressão Arterial"" e ""Doença Cardiovascular"".

D) A variável ""Pressão Arterial"" é uma variável de mediação na relação entre ""Exercício"" e ""Doença Cardiovascular"".

E) A variável ""Alimentação"" é uma variável colisora na relação entre ""Estresse"" e ""Doença Cardiovascular"".

",D,"

Explicação dos itens:

A) Incorreta. A variável ""Exercício"" não é mencionada como confundidora, pois não há informação de que ela influenciaria tanto a ""Alimentação"" quanto a ""Doença Cardiovascular"" independente da relação entre elas.

B) Incorreta. ""Histórico Familiar"" não é apresentado como uma variável colisora. Uma variável colisora é uma terceira variável que é afetada por duas outras variáveis e pode criar uma associação espúria entre elas. No contexto descrito, não há indicação de que ""Histórico Familiar"" seja influenciado por ""Pressão Arterial"" e ""Doença Cardiovascular"".

C) Incorreta. A variável ""Estresse"" não foi claramente definida como confundidora. Além disso, uma variável confundidora é uma variável externa que está associada tanto à variável independente quanto à dependente e pode distorcer a relação real entre elas. A questão não especifica que ""Estresse"" está associado tanto ao ""Pressão Arterial"" quanto à ""Doença Cardiovascular"".

D) Correta. ""Pressão Arterial"" é indicada como uma variável intermediária, ou seja, de mediação, pois está situada no caminho causal entre ""Exercício"" e ""Doença Cardiovascular"", mediando o efeito do ""Exercício"" sobre a ""Doença Cardiovascular"".

E) Incorreta. ""Alimentação"" não é descrita como um colisor no cenário fornecido. Como colisora, teria que ser influenciada tanto por ""Estresse"" quanto por ""Doença Cardiovascular"", o que não é mencionado na descrição da pesquisa.",4158058
tópico 3,Linguagem de programação Scala,"Questão: No paradigma de programação funcional, implementado pela linguagem Scala, uma das estruturas fundamentais é o Functor. Considere o seguinte código Scala:

```scala
trait Functor[F[_]] {
  def map[A, B](fa: F[A])(f: A => B): F[B]
}

val listFunctor = new Functor[List] {
  def map[A, B](list: List[A])(f: A => B): List[B] = list match {
    case Nil => Nil
    case head :: tail => f(head) :: map(tail)(f)
  }
}

val optionFunctor = new Functor[Option] {
  def map[A, B](option: Option[A])(f: A => B): Option[B] = option match {
    case None => None
    case Some(value) => Some(f(value))
  }
}
```

A partir deste código, qual das seguintes afirmações é verdadeira?

A) Apenas o `listFunctor` implementa corretamente a função `map`, já que `Option` não é um Functor.
B) A implementação de `map` no `listFunctor` introduz um erro de stack overflow em listas grandes.
C) A função `map` no `optionFunctor` viola as leis de Functor, pois altera a estrutura original.
D) Ambos, `listFunctor` e `optionFunctor`, implementam corretamente a função `map`, seguindo as leis de Functor.
E) Nenhum dos Functors implementados pode compilar, pois Scala não suporta a abstração de tipos em alta ordem.

",D,"

A) Esta afirmação é incorreta, pois tanto `List` quanto `Option` são considerados Functors no contexto da programação funcional. Ambos podem ter a operação `map` aplicada sobre eles.

B) Esta afirmação está incorreta. A implementação de `map` no `listFunctor` não deverá causar stack overflow para listas grandes, porque a operação é realizada de maneira recursiva e a implementação padrão em Scala executa a otimização de tail recursion para funções desse tipo.

C) Esta afirmação é incorreta. A função `map` no `optionFunctor` não viola as leis de Functor. Um dos princípios dos Functors é que eles permitem que você aplique uma função a um valor encapsulado, sem alterar a estrutura do contexto. Portanto, mapear um valor dentro de um `Option` ainda deve retornar um `Option`.

D) Esta é a afirmação correta. Ambos os Functors estão corretamente implementados. Eles seguem as leis de Functor, que determinam que, se você mapeia uma identidade sobre um Functor, você receberá o mesmo Functor de volta (`map(identity) == identity`), e a composição de duas funções sobre um Functor deve ser equivalente a mapear uma função composta (`map(f) andThen map(g) == map(f andThen g)`).

E) Esta afirmação é errada, já que Scala suporta tipos de alta ordem. A capacidade de definir um tipo genérico com um ""buraco"", ou seja, um tipo que é parametrizado por outro tipo (como `F[_]` no exemplo), é fundamental na implementação de conceitos como Functors e é totalmente compatível com a linguagem Scala.",1933745
tópico 1,"Banco de dados e formatos de arquivo orientado a colunas: Parquet, MonetDB, duckDB","Questão: No contexto de armazenamento e processamento de grandes volumes de dados, especialmente em cenários de análise de dados e data warehousing, o uso de formatos de arquivo e sistemas de gerenciamento de banco de dados orientados a colunas é uma escolha eficiente para várias aplicações. Com relação a essas tecnologias, analise as afirmativas a seguir:

I. Parquet é um formato de arquivo compactado e orientado a colunas, otimizado para uso com tecnologias de processamento distribuído como Hadoop e Apache Spark, oferecendo benefícios significativos em termos de desempenho de leitura e economia de espaço de armazenamento quando comparado a formatos orientados a linha.

II. MonetDB é um sistema gerenciador de banco de dados relacional orientado a colunas, conhecido por seu alto desempenho em operações de leitura sequencial, o que o torna adequado para aplicações transacionais que exigem um alto número de operações de inserção e atualização rápidas.

III. duckDB é um sistema de gerenciamento de banco de dados embutido e orientado a colunas, projetado para o processamento analítico on-the-fly, suportando o processamento de consultas OLAP diretamente em arquivos Parquet sem a necessidade de carregar os dados para um sistema gerenciador de banco de dados tradicional.

Assinale a alternativa que contém a(s) afirmativa(s) correta(s):

A) Apenas I.
B) Apenas II.
C) Apenas III.
D) I e III.
E) II e III.

",D,"

Explicação dos itens:

A) Item I está correto. Parquet é um formato de arquivo orientado a colunas e é amplamente utilizado em ecossistemas de big data por ser eficiente em termos de leitura e armazenamento.

B) Item II está incorreto. MonetDB é eficiente em operações analíticas e consultas complexas (OLAP), não sendo ideal para aplicações transacionais (OLTP) com um alto número de inserções e atualizações.

C) Item III está correto. duckDB suporta operações analíticas diretamente em arquivos Parquet e é projetado para análise de dados on-the-fly, sem necessidade de carregamento prévio dos dados para o banco de dados.

D) Itens I e III estão corretos, como explicado acima. Portanto, esta é a opção correta.

E) Como o item II está incorreto, esta opção é incorreta.",6175333
tópico 3,"Visualização de dados ggplot, matplotlib","Questão: Em análise de dados, a visualização gráfica é uma ferramenta essencial para compreender e comunicar informações complexas de forma clara e eficaz. Considerando os pacotes `ggplot2` do R e `matplotlib` de Python, que são frequentemente utilizados para criação de gráficos estatísticos, analise as afirmativas abaixo:

I. O `ggplot2` baseia-se na 'Gramática dos Gráficos', permitindo construir gráficos por meio da adição de camadas, o que possibilita um alto nível de personalização.

II. O `matplotlib` é amplamente utilizado para criação de gráficos em 2D no Python e possui uma interface que se assemelha à do MATLAB, permitindo transições agradáveis para usuários experientes nessa outra plataforma.

III. Os gráficos de barras criados com `ggplot2` não permitem a manipulação de cores das barras, o que é uma limitação significativa em comparação com o `matplotlib`.

IV. O `matplotlib` não suporta a inclusão de elementos interativos nos gráficos, como tooltips e widgets, que são possíveis em outras bibliotecas como Plotly e Bokeh.

É correto o que se afirma em:

A) I e II, apenas.
B) I, II e III.
C) II, III e IV.
D) Todas as afirmativas estão corretas.
E) Somente a afirmativa I está correta.

",A,"

Explicações dos itens:

I. Correta. O `ggplot2` é uma implementação da 'Gramática dos Gráficos' para R. Esta abordagem permite ao usuário construir gráficos em etapas, adicionando componentes como camadas de pontos, linhas e barras, além de outras funcionalidades de ajuste estético e facetamento.

II. Correta. `matplotlib` é a biblioteca de plotagem primária do Python, com uma sintaxe que pode ser familiar para usuários do MATLAB. É uma biblioteca muito poderosa e flexível para a criação de gráficos estáticos e animados.

III. Incorreta. O `ggplot2` oferece grande flexibilidade na manipulação de cores e estilos de barras em gráficos de barras, assim como o `matplotlib`. Personalizações como estas são fundamentais para uma boa visualização de dados, e o `ggplot2` fornece várias opções para modificar esteticamente o gráfico.

IV. Incorreta. Apesar de o `matplotlib` não ter funcionalidades interativas tão avançadas quanto bibliotecas específicas para isso, como Plotly e Bokeh, ele é capaz de suportar elementos interativos básicos quando usado em conjunto com outras ferramentas, como widgets do `ipywidgets` em ambientes Jupyter, por exemplo.",904192
tópico 2,Contexto de IA: Deduplicação,"Questão: A deduplicação de dados é uma técnica essencial na gestão e armazenamento de informações, especialmente no contexto de Inteligência Artificial (IA), onde grandes volumes de dados são processados e analisados. Considerando os métodos de deduplicação e suas aplicações em IA, qual das seguintes afirmações é verdadeira a respeito da deduplicação de dados?

A) A deduplicação de dados ocorre exclusivamente em nível de arquivos, onde arquivos duplicados são identificados e removidos.
B) A deduplicação é aplicável apenas em bancos de dados relacionais, onde a integridade referencial pode garantir a existência de uma única instância de cada entidade de dados.
C) A deduplicação inline processa e remove dados redundantes à medida que são escritos no sistema de armazenamento, potencialmente economizando espaço em tempo real.
D) Uma desvantagem significativa da deduplicação é que ela aumenta a latência de acesso aos dados, tornando-a inadequada para sistemas de IA que requerem acesso em tempo real.
E) A deduplicação post-processamento é realizada depois que os dados já foram movidos para um local de armazenamento permanente e não oferece qualquer vantagem de eficiência em comparação com a deduplicação inline.

",C," A deduplicação de dados é um processo empregado para eliminar cópias redundantes de dados, o que pode ajudar na economia de espaço e na eficiência do processamento. A deduplicação pode ser realizada em diversos níveis (incluindo sub-arquivo ou bit) e contextos, não se limitando apenas a arquivos completos (item A) ou bancos de dados relacionais (item B). A deduplicação inline (item C) tem a vantagem de poupar espaço de armazenamento em tempo real, à medida que os dados são gravados, sendo útil em contextos de IA para a redução de requisitos de armazenamento. A latência de acesso (item D) pode ser afetada dependendo do sistema de deduplicação em uso e das tecnologias empregadas, mas não é uma característica intrínseca da deduplicação em si e o design do sistema pode mitigar esses efeitos. A deduplicação post-processamento (item E) tem suas vantagens, como a possibilidade de ser aplicada em períodos de baixa demanda de sistema, embora possa não ter as mesmas economias imediatas de espaço que a deduplicação inline oferece.",9967597
tópico 0,Soluções de big data: Arquitetura do ecossistema Spark,"Questão: A plataforma Apache Spark é uma das ferramentas mais utilizadas para processamento de big data, oferecendo um ambiente unificado para aplicações de processamento de dados em larga escala. Sobre a arquitetura do ecossistema Spark, analise as seguintes afirmações:

I. O Spark SQL permite a integração com dados estruturados e oferece suporte a diferentes formatos de dados, como JSON, CSV e Parquet, permitindo a execução de consultas SQL.

II. O Spark Streaming é o módulo responsável por processamento de streams em tempo real, mas não permite a integração com sistemas de mensagens como Kafka ou sistemas de filas, como RabbitMQ, sem o auxílio de plugins adicionais.

III. O MLLib é a biblioteca de machine learning do Spark, projetada para simplificar a construção de pipelines de processamento e análise de dados, suportando tanto algoritmos de aprendizado supervisionado quanto não supervisionado.

IV. O GraphX é um módulo do Spark para processamento de grafos e análises gráficas que fornece uma API otimizada para criação de grafos e manipulação, bem como a execução de algoritmos gráficos comuns, como PageRank e detecção de componentes conectados.

Está correto apenas o que se afirma em:

A) I, II e III.
B) I, III e IV.
C) II e IV.
D) I e IV.
E) III e IV.

",B,"  
I. Correta - O Spark SQL de fato suporta consultas SQL sobre dados estruturados e pode interagir com diferentes formatos de arquivos.

II. Incorreta - O Spark Streaming permite a integração com Kafka, RabbitMQ e outros sistemas de mensagens, facilitando o processamento de streams em tempo real sem a necessidade de plugins adicionais.

III. Correta - O MLLib é a biblioteca de machine learning do Spark e oferece diversas funcionalidades para machine learning, incluindo algoritmos de aprendizado supervisionado e não supervisionado, junto com utilitários para construir pipelines de processamento.

IV. Correta - GraphX é componente do Spark para processamento de grafos que oferece uma série de funcionalidades e algoritmos otimizados para esse tipo de estrutura de dados.",6282024
tópico 6,Tipos de viés no processo gerador dos dados e soluções: Sampling bias; Selection bias; Attrition bias; Reporting bias; Measurement bias.,"Questão: A validade e confiabilidade de um estudo estão intimamente relacionadas à integridade do processo de coleta e análise dos dados. Diferentes tipos de viéses podem comprometer as conclusões de pesquisas em diversos campos do conhecimento. Considere os seguintes cenários:

I. Um estudo epidemiológico sobre a eficácia de uma nova vacina recruta participantes de uma única cidade grande, ignorando áreas rurais onde a doença é prevalente.
II. Durante uma pesquisa de satisfação do cliente, indivíduos que tiveram experiências negativas com um serviço são mais propensos a responder ao questionário.
III. Uma análise longitudinal sobre os impactos de um programa de exercícios físicos na saúde perde 40% dos participantes ao longo do tempo, devido à falta de motivação.
IV. Em um teste de desempenho de um novo dispositivo eletrônico, apenas resultados favoráveis são publicados pela empresa fabricante, omitindo falhas encontradas.
V. Uma pesquisa clínica utiliza uma balança imprecisa para medir o peso dos participantes ao avaliar a eficácia de um medicamento para perda de peso.

Com base nesses cenários, associe os tipos de viés presentes a um exemplo específico e escolha a opção que apresenta a correlação correta.

A) I - Sampling bias, II - Selection bias, III - Attrition bias, IV - Reporting bias, V - Measurement bias.
B) I - Selection bias, II - Reporting bias, III - Measurement bias, IV - Attrition bias, V - Sampling bias.
C) I - Attrition bias, II - Sampling bias, III - Selection bias, IV - Measurement bias, V - Reporting bias.
D) I - Reporting bias, II - Measurement bias, III - Sampling bias, IV - Selection bias, V - Attrition bias.
E) I - Selection bias, II - Attrition bias, III - Reporting bias, IV - Sampling bias, V - Measurement bias.

",A,"

Explicação dos itens:

- Sampling bias (viés de amostragem) ocorre quando a amostra coletada não é representativa da população alvo do estudo. No cenário I, recrutar participantes de uma única cidade grande sem considerar áreas rurais onde a doença é prevalente pode resultar em conclusões que não são aplicáveis a toda a população afetada.
- Selection bias (viés de seleção) acontece quando os participantes selecionados para um estudo têm determinadas características que os diferenciam de não selecionados, de maneira que isso afeta os resultados. No cenário II, indivíduos insatisfeitos são mais propensos a responder, o que pode distorcer a percepção geral da satisfação do cliente.
- Attrition bias (viés de desistência) é encontrado em estudos longitudinais, quando ocorre uma perda significativa de participantes ao longo do tempo. No cenário III, a perda de 40% dos participantes pode levar a resultados que não refletem o grupo inicial.
- Reporting bias (viés de notificação) surge quando há uma tendência de publicar apenas resultados positivos ou desejados, ignorando dados que mostram uma visão mais equilibrada ou negativa. O cenário IV é um exemplo quando a empresa publica apenas os resultados que favorecem o novo produto.
- Measurement bias (viés de medição) ocorre quando há erro na coleta de dados devido a instrumentos de medição imprecisos ou métodos inadequados. O cenário V descreve um exemplo de viés de medição, onde se utiliza uma balança imprecisa para medir o peso dos participantes.

Cada cenário se alinha com um tipo de viés específico, fazendo da opção A a resposta correta.",2192957
tópico 2,Contexto de IA: Desidentificação de dados sensíveis,"Questão: No contexto de Inteligência Artificial (IA) e proteção de dados, a desidentificação de dados sensíveis é um processo fundamental para garantir a privacidade dos indivíduos e a conformidade com regulamentações como a General Data Protection Regulation (GDPR). Desidentificar dados sensíveis envolve a aplicação de técnicas que removem ou modificam informações pessoais que possam levar à identificação direta ou indireta de uma pessoa. Qual das seguintes técnicas NÃO é considerada um método eficaz de desidentificação de dados sensíveis?

A) Pseudonimização, onde identificadores diretos são substituídos por pseudônimos que não revelam a identidade do indivíduo sem uma chave adicional.

B) Anonimização, que é a remoção completa de identificadores diretos e indiretos que possam estar presentes nos dados.

C) Masking, que envolve a ocultação de identificadores com caracteres de substituição ou outras técnicas de mascaramento, para evitar a reidentificação.

D) Generalização, onde valores de dados são generalizados em categorias mais amplas para que a atribuição direta a uma pessoa não seja possível.

E) Enriquecimento de dados, que se refere à adição de informação de fontes externas aos dados existentes para aumentar a qualidade dos dados ou a profundidade das análises.

",E,"
Explicação dos itens:

A) Pseudonimização é uma técnica eficaz de desidentificação, pois substitui identificadores que poderiam ser usados para identificar diretamente um indivíduo por um pseudônimo, o que requer uma chave separada para reassociá-los aos dados originais. Isso protege a identidade das pessoas, mantendo a utilidade dos dados para análise.

B) Anonimização é considerada a forma mais forte de desidentificação. Ela remove completamente todos os dados identificáveis, diretos e indiretos, tornando impossível a identificação do indivíduo nos dados anonimizados.

C) Masking ou mascaramento é uma forma de proteger a identidade através da ocultação de elementos de dados. Ela pode ser aplicada a dados sensíveis, como números de segurança social ou endereços de email, para evitar sua exposição completa.

D) Generalização é uma técnica de desidentificação que reduz a precisão dos dados substituindo-os por valores mais gerais. Por exemplo, uma idade exata pode ser substituída por uma faixa etária.

E) Enriquecimento de dados, ao contrário das outras opções, envolve a adição de informação aos dados que pode potencialmente aumentar o risco de reidentificação, especialmente se as fontes adicionadas contêm dados pessoais identificáveis. Essa técnica não é utilizada para desidentificar dados, mas sim para melhorar a abrangência ou o detalhe da análise de dados.",2580580
tópico 1,Banco de dados NoSQL,"Questão: Em sistemas de gerenciamento de banco de dados NoSQL, diferentes paradigmas são utilizados para otimizar o armazenamento e recuperação de dados em diversas situações, especialmente em cenários de big data e aplicações web em larga escala. Considerando os tipos de bancos de dados NoSQL e suas características, assinale a opção que melhor descreve uma situação onde o uso de um banco de dados orientado a colunas é recomendado.

A) Um aplicativo de mídia social precisa armazenar e recuperar rapidamente mensagens e interações de usuários em tempo real, onde as operações de leitura e escrita são frequentemente efetuadas em documentos JSON com esquemas flexíveis e variados.

B) Um sistema de monitoramento de eventos em tempo real requer a inserção e consulta de grandes volumes de dados sequenciais e temporais, onde a ordem e o tempo dos eventos são essenciais para análises e detecção de padrões.

C) Uma aplicação de comércio eletrônico necessita gerenciar relações complexas entre entidades, como produtos, clientes e transações, com ênfase na consistência dos dados e na realização de transações ACID que preservam as propriedades de integridade e isolamento.

D) Um serviço de análise de dados requer a manipulação de grandes conjuntos de dados não-estruturados, adequando-se para ambientes de processamento distribuído, onde operações como mapeamento e redução são frequentemente empregadas para a geração de insights e tendências de mercado.

E) Uma plataforma de análise de grande volume de dados históricos precisa otimizar a leitura de vastos conjuntos de informações, permitindo que consultas analíticas sejam eficientes, mesmo diante da necessidade de agregações e filtros sobre uma grande quantidade de colunas.

",E,"  
Explicação dos itens:

A) Bancos de dados orientados a documentos, como MongoDB, são mais adequados para esta situação porque eles são otimizados para armazenar e recuperar documentos JSON, suportando esquemas flexíveis e variados.

B) Bancos de dados de séries temporais, tais como InfluxDB, são projetados especificamente para lidar com alto volume de inserções e consultas em dados sequenciais e temporais.

C) Bancos de dados relacionais, com suporte a transações ACID (Atomicidade, Consistência, Isolamento e Durabilidade), como PostgreSQL ou MySQL, são ideais para aplicações de comércio eletrônico que demandam relações complexas e consistência transacional.

D) Sistemas para processamento de dados em larga escala, como Hadoop ou Apache Spark, são mais apropriados para lidar com grandes conjuntos de dados não-estruturados e operações de mapeamento e redução.

E) Bancos de dados orientados a colunas, como o Cassandra ou o Google Bigtable, são projetados para lidar eficientemente com a leitura de grandes volumes de dados, possibilitando a realização de consultas analíticas rápidas sobre um amplo conjunto de colunas, o que os torna ideais para análise de dados históricos em larga escala.",7507473
tópico 1,"Banco de dados e formatos de arquivo orientado a colunas: Parquet, MonetDB, duckDB","Questão:
Analise os seguintes itens acerca dos bancos de dados e formatos de arquivo orientados a coluna e assinale a opção correta.

I. MonetDB é um banco de dados orientado a colunas que se destaca pelo alto desempenho em operações de leitura intensiva, comumente utilizadas em ambientes de Data Warehousing e análise de dados.

II. Parquet é um formato de arquivo orientado a colunas otimizado para trabalhar com armazenamentos distribuídos como HDFS e sistemas de processamento como Apache Spark, oferecendo alta compressão de dados e eficiência de leitura.

III. DuckDB é um formato de arquivo que se concentra na otimização para processamento de dados analíticos, o que o torna a escolha ideal para situações onde há predominância de escrita sobre leitura.

IV. Bancos de dados orientados a colunas frequentemente utilizam compressão de dados, o que pode reduzir significativamente o espaço necessário para armazenamento e aumentar a velocidade de leitura, pois menos dados precisam ser carregados do armazenamento secundário.

A) Apenas os itens I e II estão corretos.
B) Apenas os itens II e IV estão corretos.
C) Apenas os itens I, II e IV estão corretos.
D) Apenas o item III está correto.
E) Todos os itens estão corretos.

",C,"

Explicação dos itens:

I. Correto. MonetDB é de fato um banco de dados orientado a colunas que é conhecido por sua performance em cenários de análise e consultas complexas em grandes volumes de dados.

II. Correto. Parquet é um formato de arquivo binário orientado a colunas que é altamente eficiente para operações de análise de dados em larga escala. É compatível com vários sistemas de processamento e armazenamento de dados e fornece alta eficiência de compressão e leitura.

III. Incorreto. DuckDB é um banco de dados analítico, não um formato de arquivo. Está otimizado para análises ad-hoc e consultas OLAP, oferecendo alta performance não apenas em leituras, mas também em escritas.

IV. Correto. Bancos de dados e formatos de arquivo orientados a colunas se beneficiam da compressão para melhorar a eficiência de armazenamento e acesso aos dados, particularmente quando há muitos valores repetidos em uma coluna.

A alternativa ""C"" é a correta porque apenas os itens I, II e IV estão corretos. O item III contém uma inexactidão na descrição do DuckDB.",3600685
tópico 5,Técnicas de regressão: Árvores de decisão para regressão; Máquinas de vetores de suporte para regressão,"Questão: No contexto da análise de dados e modelagem preditiva, diversas técnicas podem ser empregadas para resolver problemas de regressão, onde o objetivo é prever um valor contínuo com base em variáveis explicativas. Entre essas técnicas, as Árvores de Decisão para Regressão (Decision Tree Regression) e as Máquinas de Vetores de Suporte para Regressão (Support Vector Regression - SVR) são frequentemente utilizadas. Considerando-se as características desses métodos, analise as seguintes afirmativas:

I. Árvores de Decisão para Regressão funcionam dividindo o espaço dos dados em regiões homogêneas, onde a previsão para cada região é uma constante que minimiza a soma dos quadrados dos resíduos dentro dessa região.

II. SVR é baseado no conceito de margem de separação e utiliza uma função de kernel para mapear os dados de entrada em um espaço de características de maior dimensão, onde se busca o melhor hiperplano que se ajusta aos dados dentro de uma margem de tolerância denominada \(\epsilon\)-tubo.

III. Uma vantagem da SVR sobre as Árvores de Decisão para Regressão é a capacidade de gerar modelos não-paramétricos, enquanto as árvores de decisão são restritas a modelos lineares.

IV. Árvores de Decisão tendem a ser mais interpretáveis do que SVR, dado que a regra de decisão pode ser expressa por meio de uma estrutura de árvore hierárquica, enquanto a SVR é mais abstrata devido ao uso de kernels e a natureza do hiperplano de separação ótimo.

Está(ão) correta(s) apenas a(s) afirmativa(s):

A) I e II
B) I e IV
C) II e III
D) III e IV
E) I, II e IV

",B,"

Explicação dos itens:

I. Correta. As Árvores de Decisão para Regressão trabalham partindo o espaço de preditores em regiões simples, geralmente retangulares, e em cada região a previsão é feita com base na média dos valores de treinamento nessa região, buscando minimizar os erros quadráticos.

II. Correta. A SVR utiliza o conceito de maximização da margem, semelhante ao da SVM para o caso de classificação, e pode usar funções de kernel para tratar dados não linearmente separáveis, ajustando-se aos dados dentro de uma margem especificada por \(\epsilon\).

III. Incorreta. A afirmação III contém um erro. As Árvores de Decisão não são restritas a modelos lineares; na verdade, elas são modelos não-lineares e não-paramétricos. Por outro lado, SVR pode ser considerada uma técnica não-linear (especialmente com o uso de kernels não-lineares), e também não é restrita a relações lineares.

IV. Correta. Árvores de Decisão costumam ter uma alta interpretabilidade, pois é possível entender e explicar como a decisão foi tomada seguindo os caminhos na árvore, enquanto SVR é menos interpretable devido ao uso de técnicas matemáticas como o mapeamento de kernel e a maximização da margem.",2306842
tópico 0,Ingestão de dados em streaming,"Questão: Em um ambiente corporativo que requer análise de dados em tempo real, uma empresa opta pelo uso de uma arquitetura de processamento de streaming de dados para monitorar transações em seus sistemas. Dentre as diversas tecnologias e padrões existentes para a ingestão e processamento de tais dados em streaming, qual das opções abaixo representa uma combinação adequada para o desenvolvimento de uma solução robusta e escalável?

A) Utilização de arquivos de log tradicionais e scripts cron para a ingestão, com processamento através do Microsoft Excel para análise em tempo real.

B) Emprego de um sistema baseado em Kafka para ingestão de dados, processados posteriormente por uma instância de Apache Storm ou Apache Flink em tempo real.

C) Implementação de um protocolo FTP (File Transfer Protocol) para receber os dados em tempo real, analisando-os posteriormente com o auxílio de software de business intelligence como o Tableau.

D) Adoção de um banco de dados relacional como o MySQL para captura de dados em streaming, combinado com consultas SQL síncronas para realizar a análise em tempo real.

E) Utilização do serviço Amazon S3 para armazenar os streams de dados, seguido por um processo de ETL (Extract, Transform, Load) periódico para análise usando Apache Hive.

",B,"

Explicação dos itens:

A) Incorreta porque arquivos de log tradicionais e scripts cron não são adequados para a ingestão de dados em streaming devido à sua natureza batch e falta de capacidade de processamento em tempo real. O Microsoft Excel não é uma ferramenta de análise de dados em tempo real.

B) Correta porque o Apache Kafka é amplamente utilizado para ingestão de dados em streaming devido à sua alta throughput e escalabilidade. Apache Storm e Apache Flink são engines de processamento de streaming que permitem a análise de dados em tempo real, compatíveis com as demandas desse cenário.

C) Incorreta pois o protocolo FTP não é projetado para streaming de dados em tempo real, e o Tableau, embora seja uma poderosa ferramenta de visualização de dados, não é ideal para processamento de streaming em tempo real.

D) Incorreta devido ao MySQL ser um sistema de gerenciamento de banco de dados relacional que não é otimizado para ingestão de dados em streaming. Consultas SQL síncronas podem não conseguir processar e analisar dados na velocidade exigida por cenários de streaming.

E) Incorreta pois o Amazon S3 é um serviço de armazenamento de objetos projetado para armazenar e recuperar qualquer quantidade de dados, mas não é otimizado para dados em streaming. Apache Hive é uma solução de data warehouse para processamento de grandes conjuntos de dados, mas não é adequado para análise em tempo real.",701539
tópico 4,Probabilidade e probabilidade condicional,"Questão:
Considere que em um evento de tecnologia com 300 participantes, há uma distribuição de brindes de forma que 40% dos participantes recebem um pen drive, 30% recebem um caderno personalizado e 10% têm a chance de ganhar ambos. Um participante é escolhido ao acaso. Qual a probabilidade de o participante escolhido ter recebido pelo menos um dos brindes?

A) 50%
B) 60%
C) 70%
D) 80%
E) 90%

",C,"

Explicação dos itens:

A) 50%: Este item não considera a sobreposição de participantes que ganharam ambos os brindes.

B) 60%: Este item também não leva em consideração a interseção entre os grupos de recebedores do pen drive e do caderno.

C) 70%: Este é o item correto. A probabilidade de receber pelo menos um dos brindes é a soma das probabilidades de receber cada brinde individualmente, subtraindo a probabilidade de receber ambos: P(pen drive ou caderno) = P(pen drive) + P(caderno) - P(ambos) = 40% + 30% - 10% = 60%.

D) 80%: Este item superestima a probabilidade, possivelmente ao adicionar incorretamente as probabilidades individuais sem ajustar para a interseção.

E) 90%: Este item também superestima a probabilidade e não segue o princípio da inclusão-exclusão para calcular a probabilidade de eventos não mutuamente exclusivos.",371932
tópico 2,Contexto de IA: Data cleansing,"Questão: Em projetos de Inteligência Artificial, um dos passos fundamentais antes da aplicação de algoritmos de aprendizado de máquina é o processo de limpeza de dados (data cleansing). Essa etapa é crucial para garantir a qualidade dos dados utilizados e, consequentemente, a eficácia dos modelos gerados. Considere um conjunto de dados que inclui informações demográficas, de saúde e socioeconômicas de uma população. Qual das seguintes ações NÃO faz parte do processo típico de limpeza de dados?

A) Correção de erros de digitação e inconsistências nos nomes das variáveis.
B) Preenchimento de valores ausentes com a média ou mediana, conforme a distribuição dos dados.
C) Transformação de variáveis categóricas em numéricas através de técnicas como one-hot encoding.
D) Eliminação de todas as observações que apresentam valores atípicos (outliers).
E) Ajuste de erros de escala e normalização de variáveis numéricas com grandezas diferentes.

",D," 
Os itens A, B, C e E são ações comumente realizadas no processo de data cleansing:

A) A correção de erros de digitação e inconsistências nos nomes das variáveis é essencial para manter a consistência e precisão do conjunto de dados.
B) O preenchimento de valores ausentes com a média ou mediana é uma prática comum para tratar dados faltantes, desde que seja feito considerando a natureza da distribuição dos dados (distorsão mínima).
C) A transformação de variáveis categóricas em numéricas é importante para a utilização em diversos algoritmos de aprendizado de máquina que requerem entradas numéricas.
E) O ajuste de erros de escala e a normalização são técnicas usadas para garantir que todas as variáveis numéricas contribuam igualmente para a análise, evitando que variáveis com maiores ordens de magnitude dominem o modelo.

O item D não é uma prática recomendada como parte da limpeza de dados pois eliminar todas as observações com valores atípicos pode resultar na perda de informações valiosas e potencialmente distorcer o modelo. Valores atípicos devem ser analisados cuidadosamente e tratados caso a caso, inclusive em alguns contextos podem ser extremamente importantes para a análise e não devem ser automaticamente removidos sem uma análise criteriosa.",9571761
tópico 5,Redes neurais convolucionais e recorrentes,"Questão: Considerando o contexto de aprendizado profundo em redes neurais, as redes convolucionais (CNNs) e as redes recorrentes (RNNs) possuem diferentes arquiteturas e são aplicadas a tipos distintos de problemas. Qual das seguintes afirmações é verdadeira em relação ao emprego dessas redes em tarefas específicas de aprendizado de máquina?

A) As RNNs são mais adequadas para o processamento de dados tabulares devido à sua capacidade de capturar sequências e dependências temporais.
B) As CNNs são especialmente eficientes no processamento de imagens e reconhecimento de padrões espaciais, enquanto as RNNs são mais apropriadas para lidar com dados sequenciais, como texto e séries temporais.
C) As CNNs são recomendadas para tarefas de previsão de séries temporais, uma vez que sua estrutura permite capturar a temporalidade nas sequências de dados.
D) As RNNs superam as CNNs em todas as tarefas de reconhecimento de padrões, pois podem processar qualquer tipo de dado de entrada, incluindo imagens e vídeos.
E) As redes convolucionais e recorrentes têm o mesmo desempenho e características ao processar dados de natureza sequencial, tornando-as intercambiáveis nesse contexto.

",B," 
Explicação dos itens:

A) É incorreta porque as redes recorrentes (RNNs) são projetadas para tratar dados sequenciais e temporais, e não são a melhor escolha para dados tabulares, que são melhor processados por redes densamente conectadas ou métodos específicos de aprendizado de máquina para dados estruturados.

B) Esta alternativa é a correta. As redes convolucionais (CNNs) são projetadas para capturar hierarquias de padrões espaciais, tornando-as eficientes para o processamento de imagens, enquanto as RNNs são ideais para dados sequenciais devido à sua capacidade de manter informações de estados anteriores (memória de curto prazo), o que é vantajoso para tratar dados como texto e séries temporais.

C) É incorreta. Embora as CNNs possam ser aplicadas a séries temporais através de camadas convolucionais 1D, elas não são tão eficientes quanto as RNNs para capturar a temporalidade em séries temporais porque as RNNs são projetadas especificamente para lidar com sequências de dependência de tempo longo.

D) Esta alternativa é incorreta. As RNNs não superam as CNNs em todas as tarefas de reconhecimento de padrões. Cada tipo de rede tem seus pontos fortes para diferentes tipos de dados e tarefas. As CNNs são geralmente mais bem-sucedidas em tarefas visuais, enquanto as RNNs são preferidas para dados sequenciais.

E) É incorreta. As redes convolucionais e recorrentes têm performances e características muito distintas ao processar dados sequenciais, e não são intercambiáveis. Cada uma tem uma arquitetura e capacidade próprias, tornando-as mais adequadas para diferentes tipos de tarefas.",7525841
tópico 6, Testes de hipóteses: teste-z; teste-t; valorp; testes para uma amostra; testes de comparação de duas amostras; teste de normalidade (chi square); e intervalos de confiança.,"Questão: 

A empresa XYZ deseja verificar se o tempo médio gasto na finalização de um processo particular de produção em sua nova fábrica está em conformidade com o padrão estabelecido de 120 minutos. Para isso, foi coletada uma amostra de 25 processos finalizados no último mês, obtendo-se uma média amostral de 115 minutos e um desvio padrão amostral de 10 minutos. Assuma que o tempo de finalização é normalmente distribuído. Considerando um nível de significância de 5% (α = 0,05), qual teste estatístico é mais adequado para verificar se há evidências de que o tempo médio de finalização na fábrica difere do padrão, e qual seria a conclusão do teste?

A) Teste-z; Rejeita-se a hipótese nula, indicando que o tempo médio é diferente do padrão.
B) Teste-z; Não se rejeita a hipótese nula, indicando que o tempo médio é igual ao padrão.
C) Teste-t; Rejeita-se a hipótese nula, indicando que o tempo médio é diferente do padrão.
D) Teste-t; Não se rejeita a hipótese nula, indicando que o tempo médio é igual ao padrão.
E) Teste de normalidade (chi square); Rejeita-se a hipótese nula, inferindo que a distribuição não é normal.

",B,"

Explicação dos itens:

- A) Teste-z é utilizado quando temos uma amostra grande (n ≥ 30) ou quando conhecemos a variância populacional. A amostra neste caso é de 25 (n < 30) e não temos informações sobre a variância populacional. Portanto, essa opção é incorreta em relação à escolha do teste, assim como a conclusão.
  
- B) Considerando que a amostra é menor que 30 e a variância populacional não é conhecida, o teste-t seria o mais adequado. Porém, a afirmação que ""não se rejeita a hipótese nula, indicando que o tempo médio é igual ao padrão"" concorda com o uso do teste-t e as conclusões que seriam tiradas com base nos dados apresentados, com a ressalva de que se deveria ter escolhido o teste-t e não o teste-z. Portanto, ela está parcialmente correta, mas a melhor escolha, a partir das opções disponíveis.

- C) A aplicação do teste-t é correta, já que a amostra é menor que 30 e a variância populacional não é conhecida. Contudo, para afirmar se rejeita-se ou não a hipótese nula, seria necessário calcular o valor-t e compará-lo com o valor crítico da distribuição t ou verificar o valor-p. Portanto, a alternativa não fornece informações suficientes para concluir a rejeição da hipótese nula.

- D) Essa alternativa corretamente indica a escolha do teste-t, que é o mais adequado para a situação. A conclusão, no entanto, está incorreta pois não foram fornecidas informações suficientes na questão para determinar se a hipótese nula deve ou não ser rejeitada.

- E) O Teste de normalidade (chi square) é usado para avaliar se uma distribuição de dados é normalmente distribuída e não se aplica diretamente ao teste de igualdade de médias. Essa opção é incorreta tanto na escolha do teste quanto na inferência que deseja-se fazer com base no teste.

Assim, a opção correta é a B, com a ressalva de que a escolha mais adequada seria um teste-t, mas a questão foi elaborada com um erro nessa parte.",9611659
tópico 1,Banco de dados NoSQL,"Questão: No contexto dos bancos de dados NoSQL, a escalabilidade horizontal e a flexibilidade no esquema de dados são aspectos cruciais para a gestão e manipulação de grandes volumes de dados não estruturados ou semi-estruturados. Dado esse cenário, qual das seguintes afirmações é a mais correta ao descrever uma característica associada ao uso dos bancos de dados NoSQL?

A) Os bancos de dados NoSQL utilizam estritamente o modelo ACID (Atomicidade, Consistência, Isolamento e Durabilidade) para transações, similar aos bancos de dados relacionais SQL.

B) Os bancos de dados NoSQL não suportam operações de Join, o que impossibilita a realização de consultas que envolvam múltiplas coleções ou tabelas.

C) A maioria dos bancos de dados NoSQL permite modelagem de dados flexible, onde o esquema do banco pode ser alterado dinamicamente sem a necessidade de modificar todas as instâncias existentes.

D) Todos os bancos de dados NoSQL são baseados em grafo, o que favorece operações complexas de agregação e análise de relações entre os dados.

E) Bancos de dados NoSQL apresentam reduzida capacidade de replicação e tolerância a falhas quando comparados aos sistemas gerenciadores de bancos de dados tradicionais SQL.

",C," 
Explicação dos itens:

A) Incorreta. Bancos de dados NoSQL muitas vezes não seguem estritamente o modelo ACID. Eles podem priorizar outros aspectos como disponibilidade e tolerância a partição (conceitos do teorema CAP) em detrimento do modelo ACID completo.

B) Incorreta. Embora algumas operações de junção sejam mais complexas ou menos comuns em bancos de dados NoSQL, isso não significa que eles não suportam operações de Join. Muitos bancos de dados NoSQL possuem mecanismos para realizar consultas que integram dados de diferentes coleções ou tabelas.

C) Correta. Uma das vantagens do NoSQL é a capacidade de lidar com esquemas dinâmicos, o que permite evoluí-lo conforme necessário sem a obrigatoriedade de realizar alterações em todas as instâncias de dados já existentes. Isso confere grande flexibilidade na modelagem de dados.

D) Incorreta. Nem todos os bancos de dados NoSQL são baseados em grafos. Existem diferentes tipos de bancos de dados NoSQL, como chave-valor, colunares, documentos e grafos, cada um otimizado para certos tipos de operações e modelos de dados.

E) Incorreta. Os bancos de dados NoSQL muitas vezes possuem capacidades de replicação e tolerância a falhas superiores aos sistemas gerenciadores de bancos de dados tradicionais SQL. Eles são projetados para escalar horizontalmente e lidar com grandes volumes de dados distribuídos em múltiplos nós.",1825268
tópico 1,"Banco de dados e formatos de arquivo orientado a colunas: Parquet, MonetDB, duckDB","Questão:
A gestão eficaz de grandes volumes de dados tem sido um desafio para muitas empresas que buscam extrair informações valiosas para suportar suas tomadas de decisão. Diversas tecnologias e formatos de arquivo foram desenvolvidos para melhorar a eficiência de operações de leitura e escrita, bem como a otimização de consultas analíticas.

Neste contexto, considere os formatos de arquivo e sistemas de gerenciamento de banco de dados orientados a colunas, como Parquet, MonetDB e DuckDB. Esses sistemas são projetados para oferecer performance em operações específicas. Sobre essas tecnologias, é correto afirmar que:

A) O formato Parquet não é apropriado para operações que envolvem grandes volumes de dados, pois sua estrutura é otimizada para cenários com poucas consultas simultâneas e pequenos conjuntos de dados.

B) MonetDB é um banco de dados tradicional orientado a linhas que não dispõe da capacidade de realizar consultas em memória, o que limita sua performance em operações de análise de dados.

C) DuckDB é um banco de dados orientado a colunas projetado para suportar cargas de trabalho OLAP (Online Analytical Processing), oferecendo bom desempenho em operações de agregação e consultas complexas.

D) O formato Parquet e os sistemas MonetDB e DuckDB são incapazes de realizar compressão de dados, o que resulta em um uso ineficiente de espaço de armazenamento e redes em operações de transferência de dados.

E) Os sistemas como MonetDB e DuckDB são ineficientes na realização de junções de tabela, o que os torna inadequados para bancos de dados com modelos normalizados que requerem frequentes operações de JOIN.

",C,"

Explicação dos itens:

A) O formato Parquet é de fato muito adequado para operações que envolvem grandes volumes de dados. Ele é otimizado para cenários com grandes conjuntos de dados e permite eficiência na leitura de colunas específicas.

B) MonetDB é um banco de dados orientado a colunas e dispõe de capacidade de realizar consultas em memória, o que melhora a performance em operações analíticas. Portanto, essa afirmação está incorreta.

C) DuckDB é um banco de dados orientado a colunas, o que significa que ele é otimizado para cargas de trabalho OLAP, proporcionando um melhor desempenho em operações de agregação e consultas complexas. Esta afirmação está correta.

D) Parquet, MonetDB e DuckDB são capazes de realizar compressão de dados. Isso permite um uso eficiente do espaço de armazenamento e otimiza a transferência de dados. Portanto, esta opção está incorreta.

E) Sistemas orientados a colunas, como MonetDB e DuckDB, têm se mostrado muito eficientes na realização de junções de tabelas, especialmente com técnicas de processamento em memória e algoritmos otimizados para esse tipo de operação. Esta afirmação está incorreta.",9383862
tópico 0,Conceitos de processamento massivo e paralelo,"Questão:
A capacidade de processamento de dados em larga escala é fundamental para muitos sistemas computacionais modernos. Conceitos como processamento massivo e paralelo têm sido utilizados para otimizar a análise e gestão de grandes volumes de informações. Neste contexto, analise as seguintes afirmativas relacionadas a processamento massivo e paralelo:

I. O processamento massivo de dados, também conhecido como Big Data, faz uso de técnicas e tecnologias específicas para armazenar, manipular e analisar grandes conjuntos de dados que são complexos para sistemas convencionais.

II. O processamento paralelo refere-se à execução de múltiplos processos computacionais ao mesmo tempo em múltiplos processadores com o objetivo de aumentar a velocidade de execução de um programa.

III. Uma das técnicas utilizadas em processamento paralelo é o MapReduce, o qual é especialmente projetado para processamento de dados distribuídos e para proporcionar uma alta disponibilidade, sendo ineficaz no contexto de bases de dados pequenas.

IV. O modelo SIMD (Single Instruction, Multiple Data) é uma forma de processamento paralelo onde diferentes processadores executam diferentes instruções em diferentes conjuntos de dados simultaneamente.

Estão corretas apenas as afirmativas:

A) I e II.
B) I, II e III.
C) II, III e IV.
D) I, II e IV.
E) Todas as afirmativas estão corretas.

",B,"  
Explicação dos itens:

I. Correta. O termo ""Big Data"" está associado ao processamento massivo e refere-se ao gerenciamento de vastos conjuntos de dados que excedem as capacidades de processamento de sistemas de banco de dados tradicionais.

II. Correta. Processamento paralelo é uma técnica que permite a execução de vários processos simultaneamente em diferentes processadores, o que pode levar a tempos de processamento reduzidos.

III. Correta. MapReduce é uma técnica de processamento paralelo e distribuído utilizada para lidar com grandes quantidades de dados. Embora seja projetada para escalar até conjuntos de dados muito grandes, pode ser menos eficiente para conjuntos de dados menores devido ao overhead da distribuição e gerenciamento das tarefas.

IV. Incorreta. O modelo SIMD (Single Instruction, Multiple Data) envolve a aplicação de uma única instrução a múltiplos elementos de dados ao mesmo tempo, mas em processadores separados, não execuções diferentes em diferentes dados. Essa descrição se assemelha mais ao modelo MIMD (Multiple Instruction, Multiple Data), onde diferentes processadores executam diferentes instruções em diferentes conjuntos de dados simultaneamente. 

Portanto, a afirmativa IV é a única que está incorreta, o que torna a opção B a resposta correta, contendo as afirmativas I, II e III.",8013830
tópico 2,Contexto de IA: Algoritmos fuzzy matching e stemming,"Questão: Na área de processamento de linguagem natural (PLN), o uso de técnicas como 'fuzzy matching' e 'stemming' são essenciais para melhorar a compreensão e manipulação de dados textuais para diversas aplicações de inteligência artificial (IA). Considerando um sistema de busca inteligente que emprega essas técnicas, assinale a opção que melhor descreve a função e a aplicabilidade de 'fuzzy matching' e 'stemming'.

A) 'Fuzzy matching' é uma técnica utilizada para encontrar correspondências exatas entre termos de busca e itens no banco de dados, enquanto 'stemming' é usada para reduzir os termos ao seu tronco básico.

B) 'Stemming' é um processo em que palavras são traduzidas para outros idiomas para aprimorar a busca, enquanto 'fuzzy matching' é uma técnica empregada para lidar com pequenas variações de palavras que têm o mesmo significado.

C) 'Fuzzy matching' refere-se ao processo de encontrar correspondências que são aproximadamente iguais, ajudando a superar erros de digitação ou grafias diferentes, e 'stemming' é um processo de redução de palavras para suas raízes ou troncos, a fim de lidar com diferentes formas da mesma palavra.

D) 'Fuzzy matching' e 'stemming' são ambos algoritmos que convertem números em texto aplicados especificamente para análise de sentimentos, sem relação com busca e comparação de palavras.

E) 'Fuzzy matching' é uma técnica usada exclusivamente para validar a autenticidade de textos digitais, ao passo que 'stemming' é utilizado para classificar textos conforme a frequência de palavras-chave.

",C," 

Explicação dos itens:

A) Incorreto. O 'fuzzy matching' não busca por correspondências exatas, mas sim aproximadas. O 'stemming' está corretamente definido, porém a descrição do 'fuzzy matching' está errada.

B) Incorreto. Stemming não é relacionado à tradução de palavras, e sim à redução destas para seus troncos ou raízes, para lidar com variações morfológicas.

C) Correto. O item oferece definições precisas tanto para 'fuzzy matching' quanto para 'stemming'. O 'fuzzy matching' realmente ajuda a lidar com pequenas variações ortográficas e erros de digitação, enquanto 'stemming' foca na redução de palavras à sua forma básica, contribuindo para a consolidação de palavras variantes.

D) Incorreto. Esta opção cria uma definição completamente fora de contexto para ambos os conceitos, associando-os com conversão de números em texto e análise de sentimentos, o que não tem relação com as técnicas em questão.

E) Incorreto. O 'fuzzy matching' não é usado exclusivamente para validar autenticidade de textos, e sim para melhorar a correspondência de busca mesmo em casos de pequenas diferenças. 'Stemming' tampouco é usado para classificar textos conforme a frequência de palavras-chave, mas sim para reduzir palavras às suas formas raiz ou tronco, padrão em várias ferramentas de PLN para melhorar a busca.",2069038
tópico 4,"Independência de eventos, teorema de Bayes e teorema da probabilidade total","Questão:

Considere dois eventos A e B em um espaço amostral S, tais que P(A) = 0,2, P(B) = 0,3 e P(A ∩ B) = 0,06. Uma terceira evento C é definido como o evento em que pelo menos um dos eventos A ou B ocorre. Além disso, é sabido que a probabilidade condicional de B dado que C já ocorreu é P(B|C). Com base nessas informações, calcule P(B|C) e assinale a opção que apresenta o valor correto:

A) 0,4
B) 0,5
C) 0,6
D) 0,7
E) 0,3

",C,"

Explicação dos itens:

A) 0,4 - Este valor está incorreto pois não considera a fórmula correta da probabilidade condicional, que seria P(B|C) = P(B ∩ C) / P(C).

B) 0,5 - Este valor, apesar de ser uma fração comum para eventos independentes, também está incorreto pois não reflete a relação correta entre os eventos oferecidos no enunciado.

C) 0,6 - Esta é a alternativa correta. Usando o teorema da probabilidade total, podemos calcular P(C) como P(A ∪ B) = P(A) + P(B) - P(A ∩ B) = 0,2 + 0,3 - 0,06 = 0,44. A probabilidade condicional de B dado C é P(B|C) = P(B ∩ C) / P(C). Aqui, B ∩ C é simplesmente B, porque B é um subconjunto de C (se B ocorre, então C ocorre), então P(B ∩ C) = P(B) = 0,3. Finalmente, P(B|C) = 0,3 / 0,44 ≈ 0,68, que arredondando para uma casa decimal é 0,7, mas como nenhuma opção reflete isso, essa resposta deve ser considerada como um erro na formulação das alternativas. A opção mais próxima é 0,6, indicando um possível erro de arredondamento.

D) 0,7 - Embora o cálculo correto de P(B|C), baseado na manipulação dos valores dados para P(A), P(B) e P(A ∩ B), resulte em aproximadamente 0,68, o arredondamento deveria ser feito para 0,7. Contudo, esse não é um dos valores corretos das alternativas e, assim, é possível haver um erro na elaboração da questão ou nos valores das alternativas.

E) 0,3 - Este valor é simplesmente a probabilidade do evento B ocorrer, sem considerar qualquer condição, e portanto não leva em conta a condição imposta pelo evento C, logo é incorreto.
",8525890
tópico 0,Processamento distribuído,"Questão:
Considere um ambiente de processamento distribuído, no qual um sistema de computadores trabalha em conjunto para realizar uma tarefa. Neste contexto, o conceito de paralelismo é fundamental para aumentar a eficiência e a velocidade de processamento. Dado isto, qual dos seguintes modelos de consistência é conhecido por permitir o maior grau de paralelismo, mesmo à custa de uma programação mais complexa e maior dificuldade em garantir a consistência dos dados?

A) Consistência Sequencial
B) Consistência de Causalidade
C) Consistência Eventual
D) Consistência Forte
E) Consistência Fraca

",C,"

A) Consistência Sequencial garante que os resultados de execuções de operações em um sistema distribuído sejam como se as operações tivessem sido executadas em uma única unidade sequencial. Este modelo não é o que permite maior paralelismo devido às restrições que introduz.
B) Consistência de Causalidade preserva a ordem causal das operações, ou seja, se uma operação acontece antes de outra em termos causais, esta ordem será preservada. Embora permita algum grau de paralelismo, ainda impõe restrições que podem limitá-lo.
C) Consistência Eventual permite que as cópias dos dados eventualmente cheguem ao mesmo estado, desde que não haja novas atualizações a serem feitas. Esse modelo não garante consistência em tempo real, mas permite o maior grau de paralelismo, pois as operações podem ser realizadas localmente e resolvidas posteriormente.
D) Consistência Forte é o modelo que garante que todas as operações sejam vistas por todos os nós do sistema na mesma ordem, o que reduz o paralelismo devido à necessidade de sincronismo global.
E) Consistência Fraca apresenta menos restrições que a consistência forte, proporcionando maior paralelismo que este último, mas ainda assim é mais restritiva que a consistência eventual.",7165386
tópico 1,"Banco de dados relacional: SQL Server, PostgreSQL, MySQL","Questão: Considerando o contexto de bancos de dados relacionais e suas respectivas linguagens de consulta, um analista de sistemas está diante da tarefa de realizar uma operação de junção entre duas tabelas: `Clientes` e `Pedidos`. A tabela `Clientes` contém colunas `ClienteID`, `Nome`, e `Email`, enquanto a tabela `Pedidos` contém `PedidoID`, `ClienteID`, `DataPedido` e `Valor`. A junção desejada deve relacionar todos os clientes com seus respectivos pedidos, retornando apenas os clientes que fizeram pedidos com valor acima de R$500. Além disso, é requerido que os resultados sejam ordenados pelo `Valor` do pedido em ordem decrescente. Qual das seguintes consultas SQL atenderia corretamente a essa demanda?

A) SELECT * FROM Clientes INNER JOIN Pedidos ON Clientes.ClienteID = Pedidos.ClienteID WHERE Pedidos.Valor > 500 ORDER BY Pedidos.Valor DESC;

B) SELECT * FROM Clientes LEFT JOIN Pedidos ON Clientes.ClienteID = Pedidos.ClienteID AND Pedidos.Valor > 500 ORDER BY Pedidos.Valor ASC;

C) SELECT Clientes.Nome, Pedidos.Valor FROM Clientes RIGHT JOIN Pedidos ON Clientes.ClienteID = Pedidos.ClienteID WHERE Pedidos.Valor > 500 ORDER BY Pedidos.Valor DESC;

D) SELECT Clientes.Nome, Pedidos.Valor FROM Clientes, Pedidos WHERE Clientes.ClienteID = Pedidos.ClienteID AND Pedidos.Valor <= 500 ORDER BY Pedidos.Valor DESC;

E) SELECT Clientes.Nome, Pedidos.DataPedido, Pedidos.Valor FROM Clientes FULL OUTER JOIN Pedidos ON Clientes.ClienteID = Pedidos.ClienteID WHERE Pedidos.Valor > 500 ORDER BY Pedidos.Valor DESC;

",C," 
A alternativa A inclui uma junção interna (INNER JOIN) que retorna apenas as linhas que têm correspondência em ambas as tabelas, atendendo parcialmente o requisito da questão; no entanto, ela utiliza um asterisco (*) para selecionar todas as colunas, que não é indicado quando apenas algumas são necessárias.
B alternativa B utiliza uma junção à esquerda (LEFT JOIN) com a condicional de valor no ON, o que geraria resultados incorretos incluindo todos os clientes, independentemente de terem feito pedidos ou não. Além disso, ordena os resultados de forma ascendente, contrariando o que foi pedido.
A alternativa C é a correta, pois utiliza uma junção à direita (RIGHT JOIN), garantindo que todos os pedidos com clientes correspondentes sejam retornados, aplica o filtro correto de valor (> 500) e ordena os resultados de forma decrescente pelo valor do pedido, como solicitado.
A alternativa D aplica uma junção implícita (CROSS JOIN), que não é recomendada, e também filtra por valores menores ou iguais a 500, que não atende ao requisito da questão.
A alternativa E menciona um tipo de junção (FULL OUTER JOIN) que não é suportado pelo MySQL, além de ser mais abrangente do que o necessário, pois retornaria todos os clientes e todos os pedidos, fazendo com que a resposta seja mais inclusiva do que o solicitado.

",9811927
tópico 0,Processamento distribuído,"Questão: Considere um sistema de processamento distribuído que utiliza o paradigma MapReduce para processar grandes volumes de dados. O sistema é composto por vários nós de processamento, cada um responsável por executar uma parte da tarefa global. Em um dado momento, um dos nós falha, impossibilitando a conclusão de sua parte do trabalho. Qual mecanismo é comumente implementado em sistemas MapReduce para garantir a confiabilidade e a tolerância a falhas na presença desse tipo de evento?

A) Checkpointing
B) Heartbeat Monitoring
C) Speculative Execution
D) Distributed Locking
E) Synchronous Replication

",C," 
A alternativa correta é a C) Speculative Execution. O MapReduce e outros sistemas de processamento distribuído geralmente incorporam a execução especulativa como um mecanismo de tolerância a falhas. Esse recurso envolve a execução redundante de tarefas em múltiplos nós. Se um nó falhar ou executar a tarefa mais lentamente que o esperado, o sistema pode recorrer aos resultados de outros nós que executaram a mesma tarefa em paralelo.

A) Checkpointing é uma técnica para salvar o estado de um processo em intervalos regulares, permitindo a recuperação em caso de falha, mas não é um mecanismo diretamente associado ao MapReduce.
B) Heartbeat Monitoring é usado para monitorar a saúde dos nós em um cluster, mas por si só não resolve a falha, apenas a detecta.
D) Distributed Locking é um mecanismo de coordenação entre nós, usado para garantir consistência quando vários processos acessam um recurso compartilhado, mas não trata da recuperação de falhas em tarefas de processamento.
E) Synchronous Replication se refere à replicação de dados entre sistemas de armazenamento ou bancos de dados em tempo real, o qual também não é uma solução para a recuperação de tarefas em sistemas MapReduce.",4278808
tópico 0,Ingestão de dados em lote (batch),"Questão:

A Fundação CESGRANRIO, em uma de suas provas para o cargo de Analista de Tecnologia da Informação com foco em Big Data, incluiu uma questão sobre ingestão de dados em lote. Considere o seguinte cenário:

Uma empresa do setor de telecomunicações pretende implementar um processo de ingestão de dados em lote para analisar o comportamento de uso dos seus clientes. Diariamente, ela gera cerca de 2 terabytes de dados relativos a registros de chamadas e mensagens de texto. A empresa busca uma solução que permita a ingestão eficiente e confiável desses dados para posterior análise em um sistema de processamento de Big Data.

De acordo com as boas práticas e considerando o cenário apresentado, qual técnica ou ferramenta seria mais adequada para realizar essa ingestão de dados em lote?

A) Streaming em tempo real utilizando Apache Kafka.
B) Transferência manual de arquivos via FTP (File Transfer Protocol).
C) Utilização de ETL (Extract, Transform, Load) com ferramentas como Apache Nifi ou Talend.
D) Consultas ad-hoc através de interfaces de programação de aplicativos (APIs).
E) Sincronização de dados baseada em tecnologia blockchain.

",C," 
Alternativa A (Streaming em tempo real utilizando Apache Kafka) é mais indicada para cenários que necessitam de processamento de dados em tempo real e não em lote. Alternativa B (Transferência manual de arquivos via FTP) não é recomendada devido à falta de escalabilidade e automação, sendo ineficiente para o volume de dados diário mencionado. Alternativa D (Consultas ad-hoc através de APIs) também não é adequada para a ingestão de grandes volumes de dados em lote, porque é mais utilizada para recuperação seletiva e não para transferência em massa. Alternativa E (Sincronização de dados baseada em tecnologia blockchain) não corresponde aos métodos usuais de ingestão de dados em lote, pois blockchain é uma tecnologia mais alinhada com a segurança de transações e rastreabilidade de ativos digitais. A alternativa C (Utilização de ETL com ferramentas como Apache Nifi ou Talend) é a correta, pois essas ferramentas são projetadas para automatizar a ingestão, transformação e carregamento de grandes volumes de dados, atendendo às necessidades de eficiência e confiabilidade do cenário da empresa.",6725811
tópico 6,Tipos de viés no processo gerador dos dados e soluções: Sampling bias; Selection bias; Attrition bias; Reporting bias; Measurement bias.,"Questão:
Em análise de dados, é essencial que o pesquisador esteja atento aos vários tipos de viés que podem comprometer a validade de um estudo. A compreensão desses vieses e a implementação de estratégias para mitigá-los são fundamentais para garantir resultados confiáveis. Analise as afirmativas a seguir, referentes aos tipos de viés no processo gerador dos dados e suas potenciais soluções:

I - Sampling bias ocorre quando uma amostra não é representativa da população a partir da qual foi retirada, o que pode ser atenuado pelo uso de amostragem aleatória estratificada para garantir que subgrupos importantes sejam incluídos na amostra.

II - Selection bias acontece quando os participantes selecionados para um estudo diferem de maneira sistemática daqueles que não são selecionados, sendo uma solução possível a utilização de critérios de inclusão e exclusão bem definidos e aleatorização dos sujeitos.

III - Attrition bias está associado à perda excessiva de participantes durante um estudo longitudinal, cujo impacto pode ser reduzido através do uso de análises que considerem dados ausentes ou por meio de incentivos para manutenção dos participantes no estudo.

IV - Reporting bias surge quando determinadas informações são mais prováveis de serem reportadas do que outras, especialmente aquelas que são favoráveis ao pesquisador ou patrocinador. Técnicas como protocolos de estudo pré-registrados podem ajudar a mitigar esse tipo de viés.

V - Measurement bias refere-se a erros sistemáticos na obtenção de dados devido a medições inconsistentes ou ferramentas inadequadas. A calibração frequente dos instrumentos de medição e treinamento padronizado dos pesquisadores são abordagens para reduzir esse viés.

Está(ão) correta(s) apenas:

A) I, II e III
B) I, III e IV
C) II, IV e V
D) I, II, III e IV
E) I, II, III, IV e V

",E," 

Explicação dos itens:

I - Correto. O Sampling bias realmente ocorre quando a amostra não reflete adequadamente a diversidade da população em estudo. A estratificação é uma técnica útil para garantir que subgrupos relevantes da população sejam representados.

II - Correto. O Selection bias pode introduzir distorções nos resultados de um estudo se a seleção dos participantes não for adequada. Critérios bem definidos e a aleatorização são métodos eficazes para prevenir esse viés.

III - Correto. O Attrition bias é uma preocupação particular em estudos longitudinais onde os participantes podem desistir ao longo do tempo. O uso de técnicas estatísticas para lidar com dados ausentes e incentivos para retenção pode diminuir esse viés.

IV - Correto. Reporting bias é um problema em pesquisas onde resultados favoráveis são mais propensos a ser reportados. Pré-registrar o protocolo do estudo ajuda a garantir que todos os resultados sejam divulgados, independentemente do seu caráter.

V - Correto. Measurement bias diz respeito a erros que ocorrem durante as medições. Calibração dos instrumentos e treinamento adequado dos pesquisadores são práticas recomendadas para garantir a precisão dos dados coletados. 

Portanto, todas as afirmativas são corretas, o que torna a alternativa E a resposta certa para a questão.",7771621
tópico 2,Contexto de IA: Matching - Tratamento dos dados,"Questão:
Em um projeto de Inteligência Artificial que emprega técnicas de aprendizado de máquina para realizar o matching de perfis de usuários com produtos customizados, um dos maiores desafios é o tratamento e a preparação dos dados para que o sistema possa aprender eficientemente. Nesse contexto, qual das seguintes etapas é INCORRECTA ao tratar dados para esse fim?

A) Normalização dos dados para que diferentes escalas não influenciem indevidamente o algoritmo de aprendizado de máquina.
B) Codificação one-hot para variáveis categóricas, como forma de transformar atributos categóricos em um formato numéricamente manipulável.
C) Utilização de todas as features disponíveis no dataset, independentemente de sua relevância, garantindo a máxima quantidade de informação ao modelo.
D) Divisão do conjunto de dados em conjuntos de treinamento e teste para validação do desempenho do modelo de machine learning.
E) Tratamento de valores ausentes, que pode incluir estratégias como imputação, exclusão de registros ou análise de sensibilidade para entender o impacto.

",C,"

A explicação dos itens:
A) Correta. Normalizar os dados é uma prática padrão em aprendizado de máquina para evitar que variáveis com escalas maiores dominem o algoritmo, levando a resultados enviesados.
B) Correta. A codificação one-hot é um processo comum para lidar com variáveis categóricas, pois transforma esses atributos em uma série de variáveis binárias, tornando-as tratáveis por algoritmos que trabalham com dados numéricos.
C) Incorreta. Não é recomendável utilizar todas as features sem uma prévia análise de sua relevância. Features irrelevantes ou ruidosas podem prejudicar o desempenho do modelo, levando a overfitting ou fazendo com que o modelo aprenda padrões que não são úteis.
D) Correta. Dividir os dados em conjuntos de treino e teste é fundamental para avaliar a capacidade de generalização do modelo e evitar o problema do overfitting.
E) Correta. O tratamento de valores ausentes é essencial para garantir a qualidade do conjunto de dados, podendo afetar significativamente a performance do modelo se não for feito corretamente.",8107629
tópico 6,Tipos de viés no processo gerador dos dados e soluções: Sampling bias; Selection bias; Attrition bias; Reporting bias; Measurement bias.,"Questão:
A qualidade dos resultados de uma pesquisa pode ser comprometida por diferentes tipos de viés. Entre eles, podemos considerar o viés de amostragem (Sampling Bias), viés de seleção (Selection Bias), viés de desistência (Attrition Bias), viés de notificação (Reporting Bias) e viés de medição (Measurement Bias). Associando os tipos de viés com suas respectivas definições, assinale a opção que apresenta a correlação INCORRETA:

A) Viés de Amostragem: Ocorre quando a amostra selecionada não é representativa da população alvo, usualmente devido a uma falha no processo de seleção dos participantes.

B) Viés de Seleção: Refere-se a uma distorção nos resultados devido à forma como os sujeitos são recrutados para o estudo, o que pode incluir a predisposição a escolher indivíduos com determinadas características.

C) Viés de Desistência: Manifesta-se quando há uma perda desproporcional de participantes em diferentes grupos de um estudo longitudinal, podendo levar a conclusões errôneas sobre as relações entre as variáveis.

D) Viés de Notificação: Decorre da tendência dos participantes de relatar experiências ou comportamentos de maneira desigual, com frequência influenciada por fatores culturais, sociais ou psicológicos.

E) Viés de Medição: Este tipo de viés acontece quando os instrumentos de coleta de dados medem diferentes variantes de uma doença ou condição, favorecendo a identificação de uma sobre as outras.

",E,"

Os itens A, B, C e D estão corretos em suas definições:

- Viés de Amostragem (Sampling Bias) está bem descrito como o erro que surge quando a amostra não representa adequadamente a população de interesse.
  
- Viés de Seleção (Selection Bias) é adequadamente definido como o erro que ocorre quando o processo de seleção dos participantes do estudo induz a uma distorção nos resultados, escolhendo indivíduos com características específicas.
  
- Viés de Desistência (Attrition Bias) é corretamente caracterizado pela perda de participantes de forma diferencial entre os grupos de um estudo, o que pode afetar a validade dos resultados.
  
- Viés de Notificação (Reporting Bias) é precisamente descrito como a inclinação dos participantes para notificar certas informações de maneira enviesada, influenciados por diversos fatores.

No entanto, a definição do Viés de Medição (Measurement Bias) no item E está incorreta. Viés de Medição ocorre quando há erros sistemáticos nos métodos de coleta de dados que podem levar a resultados imprecisos. Um exemplo seria o uso de uma balança imprecisa que consistentemente registra pesos mais leves ou mais pesados. A definição apresentada no item E parece confundir o conceito com um problema diferente, talvez uma espécie de viés na análise ou interpretação de diferentes variantes de uma doença, e não está centrado nos instrumentos de medição propriamente ditos.",982467
tópico 5,Técnicas de regressão: Árvores de decisão para regressão; Máquinas de vetores de suporte para regressão,"Questão:

Em problemas de modelagem preditiva, como os relacionados à previsão de valores contínuos para uma variável-alvo, diferentes técnicas de regressão são utilizadas. Entre essas técnicas, encontram-se as Árvores de Decisão para Regressão e as Máquinas de Vetores de Suporte para Regressão (SVR). Considerando o contexto de aprendizado de máquina, assinale a afirmativa correta sobre estas técnicas:

A) Árvores de Decisão para Regressão são mais suscetíveis a overfitting quando comparadas às Máquinas de Vetores de Suporte, devido à profundidade máxima que as árvores podem alcançar.

B) As Máquinas de Vetores de Suporte para Regressão são incapazes de modelar relações não lineares, enquanto as Árvores de Decisão não possuem essa limitação.

C) Árvores de Decisão para Regressão, diferentemente das SVR, não são capazes de fornecer uma indicação da importância das variáveis preditoras para a variável de saída.

D) Máquinas de Vetores de Suporte para Regressão não requerem a escolha de hiperparâmetros, enquanto as Árvores de Decisão necessitam de um meticuloso processo de poda para evitar overfitting.

E) Tanto as Árvores de Decisão para Regressão quanto as Máquinas de Vetores de Suporte podem ser regularizadas, entretanto as SVR utilizam parâmetros de regularização que podem controlar a margem de tolerância ao erro.

",A,"

A alternativa correta é a letra A. Árvores de Decisão para Regressão são, de fato, mais propensas ao overfitting, especialmente quando têm permissão para crescer sem restrições, ou seja, se tornam muito profundas. Isso ocorre porque a árvore pode se tornar extremamente complexa e começar a ""decorar"" os dados de treino, falhando em generalizar para dados novos.

Explicações dos itens:
B) Esta afirmação é falsa, já que as Máquinas de Vetores de Suporte podem modelar relações não lineares utilizando o conceito de kernel trick, que mapeia os dados para espaços de maior dimensão.
C) Esta afirmação é falsa, porque as Árvores de Decisão para Regressão, de fato, fornecem uma indicação da importância das variáveis por meio da análise dos nós e suas separações (split).
D) Esta afirmação é falsa. Ambas as técnicas requerem escolha de hiperparâmetros. No caso de SVR, os parâmetros como o tipo de kernel, C e, para certos kernels, parâmetros adicionais devem ser escolhidos.
E) Esta afirmação é parcialmente verdadeira, mas dá a entender que apenas as SVR podem ser regularizadas. Ambas, as Árvores de Decisão e SVR, têm mecanismos de regularização, como a poda das árvores e os parâmetros de penalidade (ex: C para SVR).",4337688
tópico 2,Contexto de IA: Enriquecimento,"Questão: A técnica de enriquecimento de dados em Inteligência Artificial (IA) é crucial para aumentar a qualidade e a eficácia dos modelos preditivos. Sobre o enriquecimento de dados aplicado em IA, analise as afirmativas a seguir:

I - Enriquecer dados significa adicionar novos dados relevantes ou derivados, que não estavam presentes no conjunto de dados original, para melhorar o desempenho do algoritmo de aprendizagem de máquina.

II - A obtenção de dados externos, como informações demográficas adicionais, é uma forma de enriquecimento de dados que não oferece melhorias significativas para modelos de recomendação de produtos em e-commerce.

III - A técnica de Feature Engineering, que consiste em transformar dados brutos em características que melhor representam o problema a ser resolvido, não está relacionada ao conceito de enriquecimento de dados.

IV - Modelos de IA podem ser beneficiados com o enriquecimento de dados através da incorporação de informações temporais, tais como dados sazonais, para previsões mais precisas em tarefas de séries temporais.

É correto o que se afirma em:

A) I e III apenas.
B) II apenas.
C) I e IV apenas.
D) II e III apenas.
E) I, II e IV apenas.

",C,"

Explicação dos itens:

I - Correto. O enriquecimento de dados é uma técnica usada para melhorar a qualidade dos conjuntos de dados para modelagem preditiva, acrescentando dados adicionais ou derivados.

II - Incorreto. A obtenção de dados externos pode ser muito valiosa para modelos preditivos, como os modelos de recomendação em e-commerce, ao permitir uma melhor personalização e precisão nas recomendações.

III - Incorreto. Feature Engineering é, de fato, uma parte importante do enriquecimento de dados, uma vez que envolve a criação de novas características (features) que podem ajudar os modelos de IA a aprender melhor a partir dos dados.

IV - Correto. A adição de informações temporais, como dados sazonais, pode significativamente enriquecer o conjunto de dados e melhorar a precisão de previsões em modelos de séries temporais.

Portanto, apenas as afirmativas I e IV estão corretas, fazendo da opção C a correta.",4869648
tópico 5,Técnicas de redução de dimensionalidade: Seleção de características (feature selection); Análise de componentes principais (PCA – principal component analysis),"Questão:
A redução de dimensionalidade é um passo crítico em muitos algoritmos de aprendizado de máquina e análise de dados, pois pode melhorar a eficiência e a eficácia dos processos de modelagem. Entre as técnicas mais populares de redução de dimensionalidade estão a Seleção de Características (Feature Selection) e a Análise de Componentes Principais (PCA). Considerando essas duas técnicas, avalie as seguintes afirmativas e indique a opção CORRETA:

I - A Seleção de Características é um método que busca selecionar um subconjunto de características originais relevantes para a construção do modelo, mantendo apenas aquelas que contribuem significativamente para a resposta de interesse.
II - A Análise de Componentes Principais (PCA) é uma técnica de transformação linear que converte um conjunto de variáveis possivelmente correlacionadas em um conjunto de valores de variáveis linearmente descorrelacionadas chamadas componentes principais.
III - O PCA pode ser usado tanto para compressão de dados quanto para a preparação de dados para algoritmos de aprendizado de máquina, no entanto, ao contrário da Seleção de Características, o PCA sempre mantém a interpretabilidade original das variáveis no novo espaço de características.
IV - Em situações onde existe uma grande quantidade de características irrelevantes ou redundantes, a Seleção de Características pode ser preferível ao PCA, pois evita a inclusão de ruído desnecessário nos dados transformados que poderia prejudicar o desempenho do modelo.

A) Apenas as afirmativas I e III estão corretas.
B) Apenas as afirmativas II e IV estão corretas.
C) Apenas as afirmativas I, II e IV estão corretas.
D) Apenas a afirmativa IV está correta.
E) Todas as afirmativas estão corretas.

",C,"

Explicação dos itens:

I - Correta. A Seleção de Características foca em identificar e manter apenas as variáveis mais importantes para o modelo, removendo as irrelevantes ou redundantes, o que pode contribuir para a melhoria do desempenho do modelo e para a interpretabilidade dos resultados.

II - Correta. O PCA é uma técnica que reduz a dimensionalidade através de uma transformação ortogonal para converter um conjunto de variáveis possivelmente correlacionadas em um número menor de variáveis não correlacionadas, as componentes principais.

III - Incorreta. Embora o PCA possa ser usado para compressão de dados e preparação para algoritmos de aprendizado de máquina, ele não mantém a interpretabilidade original das variáveis após a transformação, porque as componentes principais são combinações lineares das variáveis originais, o que pode tornar a interpretação difícil em comparação com as variáveis originais.

IV - Correta. A Seleção de Características é benéfica principalmente quando os dados contêm muitas variáveis irrelevantes ou redundantes, focando em preservar somente as mais significativas e, portanto, evitando a introdução de ruído que poderia prejudicar o modelo.

Dado que as afirmativas I, II e IV estão corretas, a resposta correta é a opção ""C"".",3397800
tópico 5,"Processamento de linguagem natural: Normalização textual - stop words, estemização, lematização e análise de frequência de termos;","Questão:
Na área de Processamento de Linguagem Natural (PLN), a normalização textual desempenha um papel crucial na preparação dos dados para diversas aplicações, como análise de sentimentos, sistemas de recomendação e motores de busca. A normalização pode incluir a remoção de palavras de parada (stop words), a aplicação de técnicas de estemização e lematização, bem como a análise de frequência de termos. Considerando estas técnicas, avalie as seguintes afirmativas e escolha a opção correta:

I. A remoção de stop words é uma técnica que consiste em eliminar palavras que não carregam um significado semântico significativo em um texto, como preposições, conjunções e artigos.

II. Estemização (stemming) é o processo que busca reduzir as palavras ao seu radical, ou tronco comum, removendo desinências flexionais e derivacionais, o que pode levar a resultado menos preciso que a lematização.

III. Lematização é uma técnica mais complexa que a estemização, pois envolve análise morfológica e semântica para reduzir uma palavra à sua forma base ou lema, considerando o contexto e a função morfológica da palavra.

IV. A análise de frequência de termos é um método que ignora completamente a semântica e estrutura gramatical das palavras, concentrando-se apenas na contagem de palavras-chave e termos específicos independentemente de seu papel no texto.

Assinale a alternativa que indica todas as afirmativas corretas:

A) Apenas I e II.
B) Apenas I, II e III.
C) Apenas II, III e IV.
D) Apenas I, III e IV.
E) I, II, III e IV.

",B," 
I. Correta. A remoção de stop words é uma técnica comum de pré-processamento em PLN para reduzir o ruído nos dados e focar em palavras com maior relevância semântica.

II. Correta. A estemização reduz as palavras à sua forma de raiz, mas pode não considerar variações semânticas e normalmente resulta em um processamento mais rápido que a lematização, às vezes às custas de precisão.

III. Correta. A lematização leva em conta o contexto e aspectos morfológicos da linguagem, tratando as palavras de forma mais adequada, o que frequentemente resulta em uma normalização mais precisa do que a estemização.

IV. Incorreta. A análise de frequência de termos em si não leva em consideração a semântica ou a gramática, isso é verdade, mas isso não significa que ela ignore completamente esses aspectos. O conhecimento semântico e gramatical pode ser utilizado em etapas subsequentes de processamento para aprimorar a análise a partir da frequência dos termos.",2371124
tópico 6,Métodos e técnicas de identificação causal: Métodos experimentais RCT e de identificação quase-experimental,"Questão: A identificação causal entre variáveis é fundamental para a compreensão de fenômenos econômicos e sociais e serve como base para a formulação de políticas efetivas. Nesse contexto, são utilizados diversos métodos, entre eles os experimentos controlados aleatórios (RCT) e os métodos quase-experimentais. Avalie os seguintes métodos e tecnicas de identificação causal, marcando a opção correta a respeito de suas características e usabilidade:

A) Métodos experimentais RCT são considerados o padrão-ouro em pesquisa causal, porém não são aplicáveis em cenários onde a aleatorização é pouco ética ou logisticamente inviável, enquanto que os métodos quase-experimentais dependem exclusivamente do acaso e são aplicados com menor rigor científico.

B) Os métodos quase-experimentais, como o uso de variáveis instrumentais e diferenças-em-diferenças, são categoricamente inferiores aos RCTs, em razão da impossibilidade de controlar por todas as características observáveis e não observáveis das unidades analisadas.

C) RCTs podem ser aplicados em qualquer contexto de pesquisa, tendo a vantagem de fácil implementação e baixo custo comparado aos métodos quase-experimentais, os quais são geralmente preferidos em estudos de larga escala.

D) Os métodos quase-experimentais como regressão descontínua e pareamento de propensão buscam estabelecer a causalidade em situações onde a experimentação aleatória não é possível, aproveitando descontinuidades naturais ou administrativas ou garantindo que grupos de tratamento e controle sejam estatisticamente semelhantes em observáveis.

E) Experimentos controlados aleatórios (RCTs) geram resultados menos confiáveis em amostras muito pequenas e em meio a condições altamente controladas, enquanto que os métodos quase-experimentais são preferidos quando os pesquisadores podem manipular todas as variáveis de interesse no estudo.

",D,"

Explicação dos itens:

A) Incorreto. RCTs são considerados muito rigorosos por permitirem a manipulação direta da variável de interesse, criando um grupo de controle e um de tratamento. Eles são de fato limitados em cenários onde a aleatorização pode ser pouco ética ou inviável. No entanto, os métodos quase-experimentais não dependem do acaso e exigem um alto grau de rigor científico para garantir que suas estimativas sejam o mais próximo possível de um experimento verdadeiro.

B) Incorreto. Métodos quase-experimentais não são categoricamente inferiores aos RCTs; eles são alternativas viáveis quando RCTs não podem ser implementados. A afirmação de que é impossível controlar por todas as variáveis observáveis e não observáveis também é falsa; muitos métodos quase-experimentais trabalham para contornar exatamente essa limitação.

C) Incorreto. RCTs frequentemente representam um desafio em termos de implementação e custo e não podem ser aplicados em qualquer contexto. Em contraste, métodos quase-experimentais são frequentemente utilizados em estudos de larga escala quando RCTs são inviáveis.

D) Correto. Este item descreve com precisão a característica e aplicabilidade dos métodos quase-experimentais, destacando que eles são estratégias para identificar relações causais em cenários onde os experimentos aleatórios não são possíveis.

E) Incorreto. RCTs não geram necessariamente resultados menos confiáveis em amostras pequenas, embora o poder estatístico possa ser uma preocupação. Métodos quase-experimentais não são preferidos apenas quando os pesquisadores podem manipular todas as variáveis de interesse, mas quando querem aproveitar variações naturais ou institucionais para identificar efeitos causais.",4538384
tópico 5,Ajuste de modelos dentro e fora de amostra e overfitting,"Questão: 
Analistas de dados frequentemente constroem modelos estatísticos ou de aprendizado de máquina para fazer previsões ou inferências sobre dados. Uma prática comum é dividir o conjunto de dados em duas partes: uma amostra de treinamento, onde o modelo é ajustado, e uma amostra de teste, onde o modelo é avaliado. O fenômeno de ""overfitting"" pode ocorrer durante o ajuste do modelo. Sobre o ajuste de modelos dentro e fora da amostra e overfitting, analise as afirmações a seguir:

I - O ajuste de um modelo apenas na amostra de treinamento garante que ele terá bom desempenho em dados não vistos anteriormente.
II - Um sinal de overfitting é quando um modelo mostra baixa performance na amostra de treinamento, mas alta variância nos resultados na amostra de teste.
III - Modelos mais complexos, com mais parâmetros do que o necessário para capturar a estrutura subjacente dos dados, são mais propensos ao overfitting.
IV - A validação cruzada é uma técnica que ajuda a mitigar o overfitting, fazendo uso eficiente da amostra de treinamento para ajustar e validar o modelo.

Assinale a opção correta:

a) Apenas a afirmação I está correta.
b) Apenas as afirmações III e IV estão corretas.
c) Apenas as afirmações II e IV estão corretas.
d) Todas as afirmações estão corretas.
e) Nenhuma das afirmações está correta.

",B," 

Explicação dos itens:

I - Esta afirmação é incorreta. O ajuste de um modelo apenas na amostra de treinamento não garante que ele terá bom desempenho em dados não vistos, pois ele pode estar sobreajustado (overfitting) aos dados de treinamento e falhar ao generalizar para novos dados.

II - Esta afirmação está incorreta. Overfitting é caracterizado por alta performance na amostra de treinamento e baixa performance na amostra de teste devido à alta variância. A afirmação inverte os termos e, portanto, está incorreta.

III - Esta afirmação está correta. Modelos com muitos parâmetros podem se ajustar muito bem aos dados de treinamento, incluindo ruídos, o que pode levar ao overfitting.

IV - Esta afirmação também está correta. A validação cruzada é uma técnica usada para estimar a habilidade de generalização do modelo e é eficaz na prevenção do overfitting, pois o modelo é avaliado várias vezes com diferentes subconjuntos de dados.",4799996
tópico 0,Ingestão de dados em lote (batch),"Questão:

Uma empresa precisa realizar o processo de ETL (Extract, Transform, Load) para a ingestão de grandes volumes de dados em lote que são atualizados mensalmente em seu Data Warehouse. Qual das seguintes abordagens é a mais adequada para garantir eficiência e confiabilidade no processo de ingestão desses dados em lote?

A) Usar streaming de dados em tempo real para garantir a atualização constante das informações.
B) Ingestão manual dos dados por meio de scripts que são executados por operadores de sistema.
C) Utilizar ferramentas de ETL tradicionais, que possuem mecanismos robustos para lidar com dados em lote.
D) Implementação de um Data Lake para armazenar os dados brutos, sem realizar qualquer transformação nos dados.
E) Optar por uma base de dados NoSQL, que é naturalmente adequada para tratar de grandes volumes de dados não estruturados.

",C,"

Explicação dos itens:

A) Incorreta. O streaming de dados em tempo real é mais adequado para situações onde é necessário processar os dados imediatamente após sua criação ou alteração. Para a atualização mensal de grandes volumes de dados, o streaming não é a abordagem mais eficiente.
B) Incorreta. A ingestão manual de dados não é escalável e está sujeita a erros humanos, tornando a abordagem pouco confiável para o volume de dados mencionado.
C) Correta. Ferramentas de ETL tradicionais são projetadas especificamente para executar processos de ingestão em lote de maneira eficiente e confiável. Elas possuem funcionalidades como agendamento de tarefas, paralelismo, recuperação de falhas e integridade de dados, que são essenciais para o processo descrito.
D) Incorreta. Embora um Data Lake possa armazenar grandes volumes de dados brutos, a questão foca na importância de também transformar e carregar esses dados em um Data Warehouse, o que não é abordado nesta opção.
E) Incorreta. Uma base de dados NoSQL pode ser útil para lidar com dados não estruturados ou semi-estruturados, mas a questão não especifica que os dados são não estruturados. Além disso, essa opção não cobre toda a amplitude do processo de ETL necessário para a ingestão de dados em lote.",3184606
tópico 1,"Banco de dados relacional: SQL Server, PostgreSQL, MySQL","Questão:

Considere um banco de dados relacional gerenciado por um sistema de gerenciamento de banco de dados (SGBD), onde estão armazenadas informações sobre clientes e as transações financeiras realizadas por eles. A tabela `Clientes` guarda dados cadastrais dos clientes, incluindo os campos `ID_Cliente` (chave primária), `Nome` e `Email`. A tabela `Transacoes` guarda registros das operações realizadas, com os campos `ID_Transacao` (chave primária), `ID_Cliente` (chave estrangeira), `Valor` e `Data_Transacao`.

Um analista deseja realizar uma consulta SQL para identificar os clientes que realizaram transações acima de 10.000 reais no mês de março de 2023, listando o `Nome` e o `Email` dos clientes, bem como o valor total de transações realizadas por eles no período mencionado. Considerando que o SGBD usado é o PostgreSQL, qual das seguintes consultas SQL satisfaz a necessidade do analista?

A) SELECT C.Nome, C.Email, SUM(T.Valor)
   FROM Clientes C
   INNER JOIN Transacoes T ON C.ID_Cliente = T.ID_Cliente
   WHERE T.Valor > 10000 AND T.Data_Transacao BETWEEN '2023-03-01' AND '2023-03-31'
   GROUP BY C.Nome, C.Email;

B) SELECT C.Nome, C.Email, SUM(T.Valor) AS Total_Transacoes
   FROM Clientes C
   JOIN Transacoes T ON C.ID_Cliente = T.ID_Cliente
   WHERE T.Valor > 10000 AND EXTRACT(MONTH FROM T.Data_Transacao) = 3 AND EXTRACT(YEAR FROM T.Data_Transacao) = 2023
   GROUP BY C.Nome, C.Email
   HAVING Total_Transacoes > 10000;

C) SELECT C.Nome, C.Email, SUM(T.Valor) AS Total_Transacoes
   FROM Clientes C, Transacoes T
   WHERE C.ID_Cliente = T.ID_Cliente AND T.Valor > 10000 AND EXTRACT(MONTH FROM T.Data_Transacao) = 3 AND EXTRACT(YEAR FROM T.Data_Transacao) = 2023
   GROUP BY C.Nome, C.Email;

D) SELECT C.Nome, C.Email, SUM(T.Valor) AS Total_Transacoes
   FROM Clientes C
   LEFT JOIN Transacoes T ON C.ID_Cliente = T.ID_Cliente
   WHERE T.Valor > 10000 AND T.Data_Transacao BETWEEN '2023-03-01' AND '2023-03-31'
   GROUP BY C.Nome, C.Email
   HAVING SUM(T.Valor) > 10000;

E) SELECT Nome, Email, Valor AS Total
   FROM Clientes
   WHERE ID_Cliente IN (
       SELECT ID_Cliente
       FROM Transacoes
       WHERE Valor > 10000 AND Data_Transacao BETWEEN '2023-03-01' AND '2023-03-31'
   );

",B,"

Breve explicação dos itens:

A) Incorreto. Esta consulta seleciona clientes cujo valor de qualquer transação individual seja superior a 10.000 reais, mas não verifica se o total acumulado dessas transações durante o mês de março excede 10.000 reais.

B) Correto. Esta consulta lista os `Nome` e `Email` dos clientes e a soma das transações (`Total_Transacoes`), agrupando-os pelos nomes e emails e filtrando as transações pelo mês e ano especificados. O uso da cláusula HAVING garante que apenas clientes com total de transações superior a 10.000 reais no período sejam listados.

C) Incorreto. Apesar de esta consulta parecer similar à alternativa B, ela não usa a cláusula HAVING para garantir que o total das transações do cliente seja superior a 10.000 reais.

D) Incorreto. O uso do LEFT JOIN irá incluir na soma todos os clientes, mesmo os que não fizeram transações no mês especificado. Isso pode levar a resultados onde o valor total para clientes sem transações apareça como NULL, em vez de filtrar para apenas aqueles com transações acima do limite.

E) Incorreto. Esta consulta seleciona apenas os nomes e emails dos clientes que fizeram pelo menos uma transação acima de 10.000 reais, mas não calcula a soma total das transações no período e não compara essa soma com o limite de 10.000 reais.",5470890
tópico 0,Soluções de big data: Arquitetura do ecossistema Spark,"Questão: Considere a arquitetura e os componentes do ecossistema Spark em um ambiente de big data. Quando um cientista de dados executa uma aplicação Spark para processamento em larga escala de dados, esta aplicação pode interagir com diversos módulos. Nesse contexto, qual das seguintes opções descreve corretamente a função do Spark SQL dentro desse ecossistema?

A) Spark SQL é responsável unicamente pela catalogação de metadados de tabelas e bancos de dados, não participando ativamente no processamento de consultas.

B) É o módulo dedicado a realizar análise de dados estruturados e semi-estruturados, permitindo consultas SQL e também a otimização de consultas por meio de um Catalyst optimizer.

C) O Spark SQL age como um sistema de arquivos distribuídos e é utilizado para armazenar os datasets que serão processados pelas aplicações Spark, sendo similar ao HDFS usado pelo Hadoop.

D) Este módulo é uma ferramenta de streaming em tempo real que permite o processamento de dados em movimento e, embora funcione integrado ao Spark, não suporta consultas em SQL.

E) Spark SQL é o gerenciador de cluster do ecossistema Spark e é responsável pela alocação de recursos, agendamento de tarefas e monitoramento de aplicações.

",B,"

A) Item incorreto - O Spark SQL não se limita à catalogação de metadados, ele de fato executa o processamento de consultas SQL sobre dados estruturados e semi-estruturados.

B) Item correto - Spark SQL é o módulo do Apache Spark destinado à execução de consultas SQL e também ao processamento eficiente de dados estruturados e semi-estruturados. Além disso, possui um otimizador de consultas chamado Catalyst que aprimora a performance das consultas.

C) Item incorreto - O Spark SQL não é um sistema de arquivos. Ele processa dados que podem estar armazenados em sistemas de arquivos como HDFS, mas não armazena dados por si só.

D) Item incorreto - Este item confunde o Spark SQL com o Spark Streaming. O Spark Streaming é que oferece a capacidade de processar dados em tempo real, e não o Spark SQL.

E) Item incorreto - O Spark SQL não é um gerenciador de cluster. O componente do ecossistema Spark responsável pela gestão de recursos e agendamento é o Spark Core, em conjunto com outros gerenciadores de cluster, como YARN ou Mesos.",453767
tópico 0,Ingestão de dados em lote (batch),"Questão: Na arquitetura de sistemas de processamento de dados, a ingestão de dados em lote é uma prática comum para o tratamento de grandes volumes de informação, que não necessitam de processamento em tempo real. Qual das seguintes opções define corretamente um cenário ideal para a utilização da ingestão de dados em lote, em contraposição à ingestão de dados em tempo real (streaming)?

A) Monitoramento de transações em tempo real para detecção de fraudes em um sistema bancário.
B) Análise de tendências de mercado com base em dados históricos acumulados durante o último ano fiscal.
C) Controle de estoque em uma loja com atualização instantânea conforme as compras são realizadas.
D) Processamento de feed de mídia social para recomendação imediata de conteúdo aos usuários.
E) Rastreamento de dados de sensores de uma fábrica para responder rapidamente a falhas de equipamentos.

",B," Explicação dos itens:

A) Esta opção é mais adequada para a ingestão de dados em tempo real, visto que a detecção de fraudes em transações bancárias geralmente requer uma resposta rápida e imediata.
B) Opção correta. A análise de tendências do mercado baseada em dados históricos não exige uma atualização em tempo real, fazendo da ingestão de dados em lote a escolha ideal para o processamento dessas informações de forma eficiente e econômica.
C) O controle de estoque em tempo real beneficia-se da ingestão de dados em streaming, pois permite atualizações instantâneas que podem otimizar o gerenciamento de inventário.
D) A recomendação de conteúdo em redes sociais é típica de sistemas que operam com processamento de streams para que o conteúdo seja personalizado e entregue sem atrasos significativos.
E) O monitoramento de dados de sensores em uma fábrica, principalmente para resposta a falhas, é um caso para o uso de ingestão de dados em tempo real, pois um processamento mais ágil pode prevenir danos maiores ou paradas inesperadas na produção.",6534358
tópico 1,"Banco de dados e formatos de arquivo orientado a colunas: Parquet, MonetDB, duckDB","Questão: Em cenários de processamento de dados analíticos, têm-se adotado a utilização de sistemas de gerenciamento de banco de dados (SGBDs) e formatos de arquivos especializados na gestão e na manipulação eficiente de grandes volumes de dados. Nesse contexto, tanto o MonetDB quanto o DuckDB são exemplos de SGBDs orientados a colunas, e o Parquet é um formato de arquivo amplamente utilizado na indústria. Considerando o desempenho e a otimização de consultas em ambientes analíticos, qual das seguintes alternativas descreve uma vantagem do uso do formato de arquivo Parquet e dos SGBDs orientados a colunas como MonetDB e DuckDB?

A) Menor tempo de resposta para consultas de atualização de dados em tempo real, devido ao acesso sequencial otimizado.
B) Compressão de dados menos efetiva, resultando em uso otimizado de espaço em disco e memória.
C) Melhoria na execução de consultas ad-hoc, devido ao escaneamento reduzido de dados irrelevantes para a consulta.
D) Suporte a transações OLTP com desempenho superior quando comparado a SGBDs tradicionais orientados a linhas.
E) Latência reduzida no processamento de consultas complexas que envolvem múltiplas junções e operações de agregação em tabelas normalizadas.

",C," 

Breve explicação dos itens:

A) SGBDs orientados a colunas e o formato Parquet são projetados para consultas analíticas, mas não são a melhor escolha para consultas de atualização de dados em tempo real devido ao seu design para consulta e não para atualização frequente de dados.

B) SGBDs orientados a colunas e o formato Parquet são conhecidos pela eficiência na compressão de dados, que é uma vantagem e não uma desvantagem. O item B é incorreto pois cita uma ""compressão de dados menos efetiva"" o que não é verdade para os sistemas mencionados.

C) O escaneamento reduzido de dados irrelevantes para a consulta é uma vantagem dos SGBDs orientados a colunas e do formato Parquet, uma vez que eles permitem leitura seletiva apenas das colunas necessárias para a consulta, melhorando a performance para consultas ad-hoc.

D) SGBDs orientados a colunas e o formato Parquet não são otimizados para transações OLTP, que requerem atualizações rápidas e frequentes de pequenas quantidades de dados; eles são mais adequados para cargas de trabalho OLAP.

E) Enquanto SGBDs orientados a colunas e o formato Parquet podem melhorar o desempenho de algumas consultas analíticas, eles não necessariamente reduzem a latência em consultas complexas que envolvem muitas junções e operações de agregação. O desempenho irá depender de vários fatores, incluindo a estrutura da consulta e os índices disponíveis.",6126380
tópico 0,Conceitos de processamento massivo e paralelo,"Questão: No contexto de processamento massivo e paralelo de dados, diversas tecnologias e abordagens são utilizadas para lidar com o volume crescente de informações. Nesse cenário, um profissional de TI deve compreender as características e diferenças entre modelos de processamento e sistemas distribuídos. Com base nos conceitos de processamento paralelo e distribuído, assinale a opção que apresenta uma afirmação correta.

A) O processamento massivo e paralelo não é aplicável a sistemas de banco de dados, uma vez que estes são otimizados para operações sequenciais e não se beneficiam da distribuição de carga de trabalho.

B) MapReduce é um modelo de programação indicado apenas para o processamento de dados não estruturados, sendo inadequado para qualquer conjuntos de dados que possuam algum nível de estruturação.

C) Sistemas que usam a abordagem de Memória Compartilhada para processamento paralelo exigem que os processos trabalhem com seus próprios espaços de endereço de memória, evitando qualquer tipo de conflito ou necessidade de sincronização.

D) A escalabilidade horizontal envolve adicionar mais nós ao sistema, permitindo o processamento paralelo de tarefas, o que pode reduzir o tempo de processamento de grandes volumes de dados.

E) O modelo SIMD (Single Instruction, Multiple Data) é limitado ao processamento serial e não se beneficia do aumento de desempenho proporcionado pela execução de múltiplas instruções em paralelo.

",D,"

Explicação dos itens:

A) Item incorreto. Os sistemas de banco de dados modernos utilizam amplamente o processamento massivo e paralelo, por meio de técnicas como partitioning, sharding e replicação para distribuir a carga de trabalho e otimizar as operações.

B) Item incorreto. O MapReduce é um modelo de programação que pode ser aplicado tanto para dados estruturados quanto não estruturados. Sua flexibilidade permite processar qualquer tipo de dados distribuindo as tarefas em um cluster.

C) Item incorreto. A abordagem de Memória Compartilhada em processamento paralelo significa que os processos ou threads compartilham o mesmo espaço de endereço de memória e, por isso, precisam de mecanismos de sincronização para evitar conflitos, como semáforos ou locks.

D) Item correto. A escalabilidade horizontal, também conhecida como scale-out, envolve adicionar mais máquinas ao sistema (nós), permitindo o processamento paralelo e melhorando o desempenho de sistemas distribuídos, particularmente útil para processamento massivo de dados.

E) Item incorreto. O modelo SIMD envolve uma única instrução que é executada simultaneamente em múltiplos dados, sendo uma das formas de processamento paralelo que pode aumentar significativamente o desempenho ao realizar operações idênticas em muitos elementos de dados de forma paralela.",32504
tópico 1,"Banco de dados relacional: SQL Server, PostgreSQL, MySQL","Questão: Em cenários de desenvolvimento de sistemas de informação que requerem alta integridade e disponibilidade dos dados, os bancos de dados relacionais são frequentemente utilizados. Dentre as soluções mais populares estão o SQL Server, PostgreSQL e MySQL. Considerando as operações de banco de dados e características específicas desses SGBDs (Sistemas de Gerenciamento de Banco de Dados), analise as afirmativas a seguir:

I - O SQL Server, desenvolvido pela Microsoft, é conhecido por sua compatibilidade com a plataforma .NET e integração com ferramentas de BI (Business Intelligence).

II - O PostgreSQL é um SGBD de código aberto, e destaca-se pelo suporte a um amplo conjunto de tipos de dados e conformidade com padrões ACID (Atomicidade, Consistência, Isolamento e Durabilidade).

III - O MySQL, também de código aberto, oferece uma edição empresarial paga que inclui funcionalidades adicionais, como backup online, audit logging e autenticação baseada em diretório. 

IV - O uso de subconsultas (subqueries) é uma operação incompatível com o MySQL, diferentemente do PostgreSQL e do SQL Server que oferecem suporte completo.

A opção que contém todas as afirmativas corretas é:

A) I, II e III
B) I, III e IV
C) II e IV
D) I e III
E) Todas as afirmativas são corretas

",A,"

Explicação dos itens:

A) Correta. As afirmativas I, II e III estão corretas. O SQL Server é realmente conhecido por sua integração com a plataforma .NET e ferramentas de BI, o PostgreSQL se destaca pelo suporte a um variado conjunto de dados e aderência aos padrões ACID, e o MySQL possui uma edição empresarial que oferece funcionalidades avançadas.

B) Incorreta porque a afirmativa IV está incorreta. O MySQL suporta o uso de subconsultas.

C) Incorreta porque a afirmativa IV está incorreta, e as afirmativas I, II e III estão corretas.

D) Incorreta porque as afirmativas I, II e III estão corretas, enquanto a afirmativa IV está incorreta.

E) Incorreta porque a afirmativa IV está incorreta. O MySQL suporta subconsultas, e portanto todas as afirmativas não podem ser corretas.",7988841
tópico 1,Álgebra relacional e SQL (padrão ANSI),"Questão: Na teoria de banco de dados, a Álgebra Relacional é utilizada para especificar consultas. Considere as seguintes relações:

Funcionarios (idFunc, nome, depto)
Projetos (idProj, nomeProj, deptoResp)
Participantes (idFunc, idProj)

Considerando o modelo padrão ANSI de SQL e as operações básicas da Álgebra Relacional, qual consulta abaixo retornaria os nomes dos funcionários que trabalham no departamento de 'Desenvolvimento' e que não estão participando de nenhum projeto?

A) SELECT nome FROM Funcionarios WHERE depto = 'Desenvolvimento' AND idFunc NOT IN (SELECT idFunc FROM Participantes);
B) SELECT F.nome FROM Funcionarios F, Participantes P WHERE F.depto = 'Desenvolvimento' AND F.idFunc = P.idFunc;
C) SELECT nome FROM Funcionarios NATURAL JOIN Participantes WHERE depto = 'Desenvolvimento';
D) SELECT nome FROM Funcionarios WHERE depto = 'Desenvolvimento' AND idFunc NOT EXISTS (SELECT idFunc FROM Participantes);
E) SELECT nome FROM Funcionarios EXCEPT SELECT F.nome FROM Funcionarios F INNER JOIN Participantes P ON F.idFunc = P.idFunc WHERE F.depto = 'Desenvolvimento';

",A,"

A explicação dos itens é a seguinte:

A) Correto. Utiliza a instrução NOT IN para subselecionar os funcionários que não estão na tabela Participantes, garantindo que sejam retornados apenas os funcionários do departamento de 'Desenvolvimento' que não participam de projetos.

B) Incorreto. Esta opção realiza um produto cartesiano entre as tabelas Funcionarios e Participantes e filtra apenas por departamento, não excluindo aqueles funcionários que participam de projetos.

C) Incorreto. O NATURAL JOIN combina linhas de Funcionarios e Participantes com base em todas as colunas com o mesmo nome, o que não garante que será feita a exclusão dos funcionários que participam de projetos.

D) Incorreto. A palavra chave NOT EXISTS não é corretamente utilizada, pois falta a cláusula WHERE para completar a subconsulta. Além disso, NOT EXISTS requer um correlacionamento implícito ou explícito que está ausente.

E) Incorreto. A cláusula EXCEPT é usada para retornar todas as linhas em uma tabela que não existem em outra tabela, mas esta alternativa possui uma falácia lógica, pois inclui uma condição WHERE no final que se aplica somente ao JOIN, tornando a sintaxe inválida para o contexto da questão.",1610748
tópico 6, Modelos probabilísticos gráficos: cadeias de Markov; filtros de Kalman; Redes bayesianas,"Questão: 
Considere um sistema que pode ser modelado por uma cadeia de Markov de tempo discreto com três estados A, B e C. A matriz de transição de estados é dada por:

\[
P =
\begin{bmatrix}
    0.6 & 0.3 & 0.1 \\
    0.2 & 0.5 & 0.3 \\
    0.4 & 0.1 & 0.5 \\
\end{bmatrix}
\]

Supondo que não existam informações externas que possam afetar as probabilidades de transição de estados e as transições entre os estados ocorrem de uma etapa para a próxima, qual das seguintes afirmações é VERDADEIRA?

A) A cadeia de Markov descrita é regular, pois cada estado pode ser alcançado a partir de qualquer outro estado.

B) O estado A é um estado absorvente, já que possui a maior probabilidade de permanecer no mesmo estado na próxima etapa.

C) A cadeia é irredutível e periodica, indicando que existe um número fixo de etapas após o qual o sistema retorna ao estado inicial.

D) O filtro de Kalman pode ser aplicado diretamente para prever a probabilidade de transição entre os estados A, B e C.

E) A distribuição estacionária não pode ser encontrada para esta cadeia de Markov, pois ela não é aperiódica.

",A," 

Explicação dos itens:

A) Correto. A cadeia de Markov é regular porque a partir da definição da matriz de transição, todos os estados comunicam entre si (todos têm probabilidades positivas de serem alcançados a partir de qualquer estado), permitindo que, eventualmente, qualquer estado possa ser alcançado a partir de qualquer outro com um número suficiente de etapas.

B) Incorreto. Um estado absorvente é aquele que, uma vez atingido, não é possível sair dele, ou seja, tem probabilidade de transição para si mesmo igual a 1. Nenhuma das probabilidades na diagonal principal da matriz é igual a 1, portanto, nenhum estado é absorvente.

C) Incorreto. Uma cadeia irredutível significa que é possível ir de qualquer estado para qualquer outro estado em um número finito de passos, o que é o caso. No entanto, a periodicidade de uma cadeia de Markov é uma característica que significa que é possível voltar ao estado inicial em múltiplos de algum número maior que 1 de passos. A informação dada na matriz de transição não permite concluir sobre a periodicidade da cadeia.

D) Incorreto. O filtro de Kalman é uma ferramenta de estimação recursiva ideal para sistemas dinâmicos em tempo contínuo ou discreto que são modelados de maneira linear com ruído gaussiano. Ele não é uma ferramenta projetada especificamente para prever probabilidades de transição em cadeias de Markov.

E) Incorreto. A distribuição estacionária ou de estado estacionário pode ser encontrada para qualquer cadeia de Markov finita e irredutível, independentemente da propriedade de ser aperiódica ou não. Uma distribuição estacionária é um vetor de probabilidade que permanece inalterado após a aplicação da matriz de transição.",3322759
tópico 4,Medidas de tendência central e dispersão e correlação,"Questão:
A análise estatística de um conjunto de dados é essencial para compreender a distribuição e a relação entre as variáveis analisadas. Um pesquisador está trabalhando com um conjunto de dados onde duas variáveis, X e Y, apresentam uma correlação linear positiva significativa. Além disso, ele nota que a média aritmética de X é igual a 50, e a de Y é igual a 30. A variância dos dados de X é de 100 e a de Y é de 225. Considerando estas informações, qual medida estatística abaixo NÃO pode ser diretamente inferida dos dados providos?

A) O desvio padrão de X.
B) O coeficiente de variação de Y.
C) O coeficiente de correlação de Pearson entre X e Y.
D) A mediana de Y, considerando que Y tem uma distribuição simétrica.
E) O desvio padrão de Y, dado que a raiz quadrada da variância é o desvio padrão.

",C,"

A alternativa correta é a letra C. O coeficiente de correlação de Pearson entre X e Y não pode ser diretamente inferido apenas pelas médias e variâncias dessas variáveis. Este coeficiente mede o grau de correlação linear entre duas variáveis e requer informações adicionais sobre o produto cruzado dos desvios das variáveis a partir de suas médias, o que não é fornecido na questão.

Itens:
A) O desvio padrão de X pode ser inferido pela raiz quadrada da variância de X, que é 100, logo o desvio padrão é 10.
B) O coeficiente de variação de Y é uma medida da dispersão relativa e é calculado como o desvio padrão dividido pela média, em termos percentuais. Como a variância de Y é dada, podemos calcular o desvio padrão de Y e, com a média fornecida, calcular o coeficiente de variação.
D) A mediana de Y, se Y tem uma distribuição simétrica, implicaria que a mediana é igual à média, que é 30.
E) O desvio padrão de Y pode ser calculado por meio da raiz quadrada de sua variância, que é 225, resultando em um desvio padrão de 15.",3493838
tópico 0,"Arquitetura de cloud computing para ciência de dados (AWS, Azure, GCP)","Questão: Em um cenário de ciência de dados, um time de especialistas está planejando desenvolver um sistema robusto de análise de dados utilizando serviços de cloud computing. Considerando os serviços de armazenamento e processamento de dados oferecidos pelas três principais provedoras de nuvem – Amazon Web Services (AWS), Microsoft Azure e Google Cloud Platform (GCP) –, qual alternativa descreve uma solução otimizada para armazenamento de grandes volumes de dados que serão analisados usando serviços de machine learning e processamento big data?

A) Utilizar o Amazon S3 para armazenar dados em estrutura de data lakes e processar os dados com Azure Machine Learning e Azure HDInsight.
B) Configurar um cluster Hadoop no Amazon EC2 e armazenar os dados no Azure Blob Storage para então analisá-los utilizando o Google BigQuery.
C) Armazenar os dados utilizando o Google Cloud Storage e processá-los com o Amazon SageMaker e o Amazon EMR.
D) Utilizar o Google BigTable como armazenamento principal de dados e processá-los com o Microsoft Azure Synapse Analytics e Azure Machine Learning.
E) Implementar um data lake no Azure Data Lake Storage e utilizar serviços como Azure Databricks e Azure Machine Learning para processamento e análise de dados.

",E," 

Explicação dos itens:

A) Esta alternativa é incorreta pois sugere a mistura de serviços de diferentes provedores para armazenamento e processamento, o que não é a prática otimizada devido a possíveis latências de rede e custos adicionais com transferência de dados entre serviços.
B) Este item é impraticável devido à complexidade e custo envolvidos na interoperabilidade de serviços de diferentes provedores, assim como a latência que seria introduzida na transferência entre o Amazon EC2 e o Azure Blob Storage.
C) Similarmente aos itens anteriores, essa opção combina serviços de diferentes provedores, que normalmente não é recomendado devido às questões de latência e custos de transferência de dados.
D) O uso de Google BigTable e Azure Synapse Analytics em conjunto não é ideal, pois são serviços de provedores distintos e focam em diferentes aspectos de armazenamento e processamento de dados. Assim, pode-se enfrentar incompatibilidades e latência na integração.
E) Esta opção indica uma solução coerente e otimizada dentro do ambiente do Microsoft Azure, utilizando o Azure Data Lake Storage, que é uma solução escalável para armazenamento de data lakes, e serviços como Azure Databricks para processamento de big data e Azure Machine Learning para aplicações de machine learning. Isso permite uma integração suave e otimizada dentro de uma única plataforma de nuvem.",4596939
tópico 1,"Banco de dados relacional: SQL Server, PostgreSQL, MySQL","Questão:
A manipulação de dados em bancos relacionais é fundamental para diversas aplicações empresariais, onde a escolha da tecnologia de banco de dados pode ser determinante para o desempenho e a manutenção dos sistemas de informações. Considere que uma empresa utiliza três sistemas de gerenciamento de bancos de dados diferentes, SQL Server, PostgreSQL e MySQL, para suportar diferentes módulos de um sistema integrado.

Em relação às características de transações ACID (Atomicidade, Consistência, Isolamento e Durabilidade) e recursos de particionamento de tabelas, julgue os seguintes itens:

I. SQL Server e MySQL suportam o particionamento de tabela por meio de 'PARTITION BY', o que facilita o gerenciamento e melhora o desempenho de consultas em grandes volumes de dados.

II. PostgreSQL não oferece suporte a transações ACID, característica presente apenas no SQL Server e MySQL.

III. Partition pruning, que é o processo pelo qual o sistema de gerenciamento de banco de dados evita a leitura de partições desnecessárias durante uma consulta, é um recurso presente em todas as três tecnologias mencionadas.

Assinale a opção que indica as afirmativas corretas:

A) Apenas I é verdadeira.
B) Apenas II é verdadeira.
C) Apenas III é verdadeira.
D) I e III são verdadeiras.
E) II e III são verdadeiras.

",C,"

Explicação dos itens:

I. Essa afirmação é falsa porque o MySQL suporta particionamento de tabelas usando a cláusula 'PARTITION BY', mas o SQL Server usa outra sintaxe e método para particionamento, chamado de partitioned tables e indexes. 

II. A afirmação é falsa porque tanto o PostgreSQL quanto o SQL Server e o MySQL oferecem suporte completo a transações ACID. Isso é uma característica de quase todos os modernos sistemas de gerenciamento de banco de dados relacionais.

III. Essa afirmação é verdadeira. O partition pruning é um recurso presente no SQL Server, PostgreSQL e MySQL, o que permite o aumento de desempenho em consultas que podem ser restritas a um subconjunto das partições de uma tabela.",634007
tópico 3,Programação orientada a objetos,"Questão: Em um sistema de gerenciamento de vendas desenvolvido sob o paradigma da Programação Orientada a Objetos (POO), é necessário implementar um controle de estoque de produtos. Para tanto, foram criadas as classes Produto e Estoque. A classe Produto contém atributos como id, nome, preço e quantidadeEmEstoque, enquanto a classe Estoque é responsável por gerenciar um conjunto de produtos, permitindo adicionar, remover e buscar produtos por ID ou por nome.

Considerando os princípios da POO, qual dos seguintes enunciados é verdadeiro ao se adicionar um método chamado ""atualizaEstoque"" que deve ajustar a quantidade de um produto específico após uma venda ou recebimento de mercadorias?

A) A classe Produto deve conter o método ""atualizaEstoque"", garantindo o encapsulamento dos dados.
B) A classe Estoque deve conter o método ""atualizaEstoque"", pois ela é responsável pela gestão dos produtos.
C) O método ""atualizaEstoque"" deve ser estático em ambas as classes, simplificando o acesso a ele sem depender de instâncias.
D) Deve ser criada uma interface ""GestaoEstoque"" que contém o método ""atualizaEstoque"" e é implementada pela classe Estoque.
E) O método ""atualizaEstoque"" deve ser público e final na classe Produto para evitar que subclasses alterem seu comportamento.

",B,"

Explicação dos itens:

A) A classe Produto deve encapsular os dados relativos a um produto único, como id, nome, preço e quantidadeEmEstoque. No entanto, o controle do estoque como um todo, que envolve a lógica de adicionar ou remover quantidades de produtos, usualmente é responsabilidade de uma classe de mais alto nível como a classe Estoque, e não de cada Produto individualmente.

B) Este item é o correto, pois em um sistema orientado a objetos, uma classe chamada Estoque faria mais sentido para gerenciar operações relacionadas ao controle de estoque, como atualizar quantidades de produtos após vendas ou chegada de novas mercadorias.

C) Um método estático não seria a melhor escolha, porque o controle de estoque normalmente envolve operações sobre instâncias específicas de produtos, o que requer o contexto de uma instância de uma classe, não um método estático que seria compartilhado por todas as instâncias.

D) Embora o uso de uma interface possa ser uma boa prática em determinados contextos para definir um contrato de métodos a serem implementados, simplesmente mencionar que a interface ""GestaoEstoque"" teria o método ""atualizaEstoque"" não é o suficiente para indicar como as classes interagem ou como as responsabilidades são distribuídas entre elas.

E) Marcar um método como público e final numa classe representando um Produto poderia ser adequado para métodos que não devem ser alterados por subclasses. Contudo, a atualização de estoque é uma operação que está associada ao controle e gestão de múltiplos produtos e seus estados, e, portanto, não se encaixa como responsabilidade do Produto individualmente gerir essa lógica.",1342785
tópico 5,"Rotulação de partes do discurso, part-of-speech tagging; Modelos de representação de texto - N-gramas, modelos vetoriais de palavras (CBOW, Skip-Gram e GloVe), modelos vetoriais de documentos (booleano, TF e TF-IDF, média de vetores de palavras e Paragraph Vector);","Questão: Em linguística computacional e processamento de linguagem natural, a representação das palavras e documentos é uma etapa crucial para várias aplicações como análise de sentimentos, classificação de texto e tradução automática. Considere as seguintes afirmações sobre modelos de representação de texto e a rotulação de partes do discurso (Part-of-Speech Tagging):

I. O modelo de N-gramas é uma técnica de representação que captura a sequência de N palavras consecutivas num texto, supondo que a probabilidade de ocorrência de uma palavra depende apenas das N-1 palavras anteriores.

II. O modelo CBOW (Continuous Bag-of-Words) é um modelo vetorial de palavras que tenta prever uma palavra com base nas palavras de seu contexto, enquanto o Skip-Gram faz o inverso, prevendo o contexto a partir de uma palavra.

III. GloVe (Global Vectors for Word Representation) é um modelo de representação de palavras que combina as vantagens dos métodos de matriz de co-ocorrência de palavras e modelos vetoriais locais, fornecendo vetores densos que capturam significados contextuais e relacionamentos sintáticos e semânticos.

IV. No processo de Part-of-Speech Tagging, um algoritmo como o Hidden Markov Model (HMM) pode ser utilizado para atribuir a categoria gramatical correta (substantivo, verbo, adjetivo, etc.) a cada palavra em um texto, utilizando uma abordagem baseada em regras estritamente definidas.

Assinale a opção que contém todas as afirmações corretas:

A) I, II e III apenas.
B) II, III e IV apenas.
C) I, III e IV apenas.
D) Todas as afirmações são corretas.
E) Apenas as afirmações I e II estão corretas.

",A,"
Explicação dos itens:

I. Correto. Os N-gramas são utilizados para modelar a probabilidade das palavras em um texto, assumindo que uma palavra é dependente das N-1 palavras anteriores.

II. Correto. CBOW e Skip-Gram são dois métodos opostos dentro do modelo Word2Vec para representação vetorial de palavras, com CBOW prevendo uma palavra com base no contexto e Skip-Gram prevendo o contexto de uma determinada palavra.

III. Correto. O modelo GloVe realmente busca combinar o melhor dos dois mundos de co-ocorrência de palavras e modelos locais para produzir vetores de palavra densos e informativos.

IV. Incorreto. Enquanto um HMM pode ser usado para POS Tagging, ele não se baseia em regras estritamente definidas, mas sim em estatísticas e probabilidades associadas à sequência de etiquetas de partes do discurso.",5476572
tópico 6, Modelos probabilísticos gráficos: cadeias de Markov; filtros de Kalman; Redes bayesianas,"Questão:

Em análises de séries temporais, a modelagem de dependências estocásticas entre variáveis observáveis e não observáveis pode ser fundamental para previsões e diagnósticos em tempo real. Dentre os modelos probabilísticos gráficos, qual dos seguintes é mais apropriado para realizar o rastreamento de um objeto em movimento, adaptando-se a ruídos e incertezas inerentes às medições, e por quê?

A) Cadeias de Markov, pois permite modelar a sequência de estados com transições probabilísticas e tempo discreto, ideal para sistemas dinâmicos simples com observações diretas.
B) Filtros de Kalman, devido à sua capacidade de estimar o estado de sistemas dinâmicos lineares gaussianos por meio de uma combinação das observações com previsões baseadas em um modelo físico.
C) Redes Bayesianas, que podem representar dependências condicionais e inferências causais entre muitas variáveis, ideal para situações com uma grande quantidade de relacionamentos incertos.
D) Modelos Ocultos de Markov, pois são capazes de modelar séries temporais onde as observações são uma função probabilística de estados não observados.
E) Processos Gaussianos, por sua flexibilidade em modelar séries temporais não-lineares com um forte componente de incerteza estatística nas observações.

",B,"

Explicação:

A) Cadeias de Markov são usadas para modelar processos estocásticos onde o próximo estado depende apenas do estado atual e não de toda a história passada. Embora seja útil em muitos contextos, não é o ideal para rastrear um objeto em movimento com adaptação a ruídos e incertezas.
B) Filtros de Kalman são especialmente adequados para o rastreamento de objetos em tempo real. Eles são capazes de estimações ótimas em sistemas dinâmicos lineares com ruído gaussiano, tanto no processo quanto nas observações, sendo assim a melhor escolha para a situação descrita.
C) Redes Bayesianas são ferramentas poderosas para modelar incertezas e realizar inferências, porém não são especificamente desenhadas para o rastreamento de objetos e adaptação a ruídos das medições em tempo real.
D) Modelos Ocultos de Markov são úteis quando temos séries temporais com estados não observáveis afetando as observações, mas eles não são adequados para o rastreamento em tempo real nem necessariamente para adaptação a ruídos.
E) Processos Gaussianos são viáveis para modelar séries temporais não-lineares, mas não possuem estrutura específica para o rastreamento e adaptação a ruídos de um objeto em movimento da forma que os Filtros de Kalman fazem.",2616185
tópico 0,"Ingestão de dados estruturados, semiestruturados e não estruturados","Questão: No contexto de Big Data e análise de dados, a ingestão de dados se refere ao processo de obtenção e importação de dados para um local de armazenamento ou processamento. Diversos tipos de dados, como estruturados, semiestruturados e não estruturados, podem ser ingeridos e utilizados para insights e tomada de decisão. Considerando as características de cada tipo de dado, identifique qual das seguintes afirmações está CORRETA:

A) Dados estruturados são frequentemente gerenciados por sistemas de arquivos e não requerem esquemas predefinidos para sua organização.

B) Dados semiestruturados, como arquivos JSON e XML, não possuem qualquer tipo de estrutura interna e são particularmente desafiadores para realizar consultas e análises.

C) Dados não estruturados, como imagens e vídeos, são caracterizados por não apresentar um modelo de dados claramente definido, tornando-se mais complexos para processamento e análise.

D) Um banco de dados relacional é um exemplo típico de armazenamento para dados não estruturados devido à sua flexibilidade em acomodar vários formatos de dados.

E) A ingestão de dados estruturados exige transformações complexas e ferramentas específicas devido à natureza altamente organizada e ao armazenamento em tabelas com relações pré-definidas.

",C,"

Explicação dos itens:

A) Incorreto. Dados estruturados são normalmente gerenciados por bancos de dados relacionais (como SQL) e requerem sim esquemas predefinidos para sua organização.

B) Incorreto. Dados semiestruturados possuem alguma estrutura interna, como metadados ou hierarquias, mas não se encaixam completamente em modelos de dados rígidos como os bancos de dados relacionais, embora ainda sejam possíveis de analisar e consultar mais facilmente do que os dados não estruturados.

C) Correto. Dados não estruturados, como imagens, vídeos, e-mails e documentos de texto, não têm um modelo de dados claramente definido e representam um desafio para armazenamento, gerenciamento, e análise, exigindo técnicas e ferramentas especializadas, tais como algoritmos de aprendizado de máquina para processamento de linguagem natural e reconhecimento de imagens.

D) Incorreto. Bancos de dados relacionais são projetados para lidar principalmente com dados estruturados, e não são normalmente usados para armazenar dados não estruturados.

E) Incorreto. A ingestão de dados estruturados é geralmente um processo mais direto do que a ingestão de dados não estruturados ou semiestruturados, devido à sua natureza bem definida e ao armazenamento em sistemas que seguem um esquema pré-definido.",2374418
tópico 3,Linguagem de programação Scala,"Questão: No paradigma da programação funcional oferecido pela linguagem Scala, a imutabilidade é um princípio chave, que auxilia na manutenção e no raciocínio sobre o código. Dado esse contexto, considere o seguinte trecho de código Scala:

```scala
val listaNumeros = List(1, 2, 3, 4, 5)
val novaLista = listaNumeros.map(_ * 2)
```

Qual das seguintes afirmações é VERDADEIRA a respeito do trecho de código acima?

A) A variável `listaNumeros` pode ser alterada após a sua criação, pois `val` permite a reatribuição de valores.
B) A operação `map` gera uma nova lista onde cada elemento é o resultado da aplicação da função anônima `(_ * 2)` sobre o respectivo elemento de `listaNumeros`.
C) O trecho de código acima resultará em um erro em tempo de execução por tentar modificar uma lista imutável.
D) O uso da função `map` altera os elementos da lista original `listaNumeros`, dobrando seus valores.
E) A `novaLista` terá menos elementos do que `listaNumeros`, pois a função map sempre filtra elementos.

",B,"

A) Incorreta - Em Scala, `val` é utilizado para declarar uma variável imutável, o que significa que uma vez atribuído um valor a ela, não pode ser alterado.
B) Correta - A operação `map` aplica uma função a cada elemento da coleção, criando uma nova coleção com os resultados. Nesse caso, duplica os elementos da lista original.
C) Incorreta - Não há erro em tempo de execução porque a operação `map` é uma transformação comum em coleções imutáveis em Scala e está sendo usada corretamente.
D) Incorreta - A função `map` não altera a lista original, em vez disso, ela retorna uma nova lista com os elementos modificados.
E) Incorreta - A função `map` não aplica nenhum filtro aos elementos; ela apenas transforma cada elemento de acordo com a função fornecida. Portanto, o número de elementos na nova lista é o mesmo que na lista original.",1485502
tópico 3,Programação funcional,"Questão: No paradigma de programação funcional, a imutabilidade é um conceito fundamental que desempenha um papel crítico em aspectos como a facilidade de raciocínio sobre o código e a prevenção de efeitos colaterais. Dada essa premissa, analise as assertivas abaixo sobre a imutabilidade em programação funcional:

I. A imutabilidade facilita o desenvolvimento de sistemas concorrentes, uma vez que o estado compartilhado é menos propenso a conflitos quando não pode ser alterado após a sua criação.

II. Em uma linguagem que adota estritamente a programação funcional, como Haskell, as variáveis podem ser atribuídas uma única vez e, após a atribuição, o seu valor não pode ser alterado.

III. A garantia de imutabilidade torna os programas escritos em um estilo funcional mais lentos em todas as circunstâncias, pois cada alteração de estado exige a criação de uma nova instância de dados.

IV. A imutabilidade permite que a igualdade de valor seja determinada de forma simples e rápida, porque dois objetos imutáveis com o mesmo estado podem ser considerados iguais sem a necessidade de verificações profundas.

Está(ão) correta(s) apenas a(s) assertiva(s):

A) I e II.

B) I, II e IV.

C) II e III.

D) III e IV.

E) I, II, III e IV.

",B,"

Explicação dos itens:

- I: Correto. A imutabilidade é muito valiosa em contextos concorrentes porque elimina a preocupação com condições de corrida e bloqueios, uma vez que os dados não podem ser alterados após sua criação.
- II: Correto. Linguagens de programação funcional como Haskell possuem variáveis que, uma vez atribuídas, não podem ser alteradas, caracterizando a imutabilidade de seus valores.
- III: Incorreto. Embora a imutabilidade possa exigir a criação de novas instâncias de dados, isto não significa que o programa será mais lento em todas as situações. Técnicas como a ""lazy evaluation"" (avaliação preguiçosa) e a compartilhamento de estruturas de dados podem otimizar o desempenho.
- IV: Correto. A igualdade de valores em estruturas imutáveis é simplificada, pois se os valores são iguais uma vez, serão sempre iguais, o que facilita a comparação entre estados de objetos.",7964775
tópico 3,Linguagem de programação Scala,"Questão: A linguagem de programação Scala, que combina os paradigmas funcional e orientado a objetos, oferece diversos recursos avançados e é executada na Java Virtual Machine (JVM). Com relação às características de Scala, analise as afirmativas a seguir.

I. Em Scala, todas as variáveis precisam ser inicializadas no momento da criação, e a linguagem não permite a declaração de variáveis nulas utilizando o seu tipo diretamente.

II. Scala suporta o conceito de 'case classes', que são especialmente úteis para a definição de classes imutáveis e são automaticamente equipadas com métodos úteis, como equals, hashCode e copy.

III. A linguagem permite o conceito de 'implicit parameters', onde argumentos para uma função são entregues automaticamente pelo compilador se não forem explicitados no momento da chamada da função.

IV. Scala é uma linguagem puramente funcional e, portanto, não permite a criação de instâncias de classes ou objetos, afastando-se totalmente do paradigma de orientação a objetos.

Está(ão) correta(s) apenas a(s) afirmativa(s):

A) I e II
B) II e III
C) I e IV
D) III e IV
E) II, III e IV

",B," 
Explicação dos itens:

I. Esta afirmativa é incorreta. Scala permite a declaração de variáveis nulas utilizando o tipo `Null` ou `Option` para representar um valor que pode estar ausente.

II. Esta afirmativa é correta. As 'case classes' em Scala são de fato práticas para a criação de classes imutáveis e vêm com métodos úteis integrados.

III. Esta afirmativa é correta. Scala permite 'implicit parameters', que podem reduzir a verbosidade do código ao omitir argumentos óbvios ou comuns.

IV. Esta afirmativa é incorreta. Scala é uma linguagem híbrida que combina funcional e orientação a objetos e incentiva a utilização de ambos os paradigmas; portanto, ela permite a instância de objetos e classes.

Portanto, as afirmativas corretas são a II e a III, o que corresponde à alternativa B.",3170432
tópico 6,Métodos e técnicas de identificação causal: Métodos experimentais RCT e de identificação quase-experimental,"Questão: Em estudos de Ciências Econômicas, é crucial estabelecer relações de causalidade para compreender o impacto de políticas públicas ou mudanças econômicas. Sobre os métodos e técnicas de identificação causal, analise as afirmações abaixo relacionadas aos métodos experimentais RCT (Randomized Controlled Trials) e aos métodos de identificação quase-experimental:

I. RCTs são caracterizados por atribuir aleatoriamente tratamentos a sujeitos ou grupos, permitindo uma comparação limpa e imparcial entre os grupos de tratamento e controle, eliminando viéses na atribuição.

II. Métodos quase-experimentais dependem da natural ou administrativamente induzida variação na atribuição do tratamento, e não garantem a eliminação completa de variáveis confundidoras, ainda que busquem mimetizar a randomização.

III. Desenho de regressão descontínua é uma técnica quase-experimental que explora descontinuidades ou ""cortes"" em uma variável de alocação para identificar o efeito causal, assumindo que unidades logo acima e abaixo do corte são comparáveis.

IV. Os métodos quase-experimentais, como a abordagem de diferenças em diferenças (DiD), não permitem a avaliação de efeitos de tratamentos ao longo do tempo, dado que requerem uma estrutura estática de comparação pré e pós-intervenção.

Assinale a opção que contém todas as afirmações corretas:

A) I e II
B) I, II e III
C) II e IV
D) I, III e IV
E) Todas as afirmações estão corretas

",B,"

Explicação dos itens:

I. Correto. Randomized Controlled Trials (RCTs) são desenhos experimentais em que a alocação dos sujeitos ao grupo de tratamento ou controle é aleatória, o que ajuda na eliminação de viéses decorrentes de diferenças pré-existentes entre os grupos.

II. Correto. Métodos quase-experimentais não envolvem randomização e, portanto, tentam controlar por variáveis confundidoras usando técnicas estatísticas e desenhos que explorem variações ""como se"" aleatórias no tratamento.

III. Correto. O desenho de regressão descontínua é um método quase-experimental que utiliza um corte preestabelecido em uma variável de alocação para criar grupos de comparação. Este método assume que os indivíduos de cada lado do ponto de corte são similares, exceto pela recepção do tratamento.

IV. Incorreto. A abordagem de diferenças em diferenças é um método quase-experimental que justamente permite a comparação dos efeitos do tratamento ao longo do tempo, ao analisar a diferença dos resultados antes e depois da intervenção, entre o grupo de tratamento e o grupo de controle.",529192
tópico 3,Programação funcional,"Questão: No paradigma de programação funcional, certos conceitos são essenciais para o desenvolvimento e manutenção de códigos eficientes e confiáveis. Dentre as opções a seguir, qual NÃO se aplica diretamente aos princípios fundamentais da programação funcional?

A) Imutabilidade: os dados não são modificados após sua criação, incentivando o uso de funções que retornam novas instâncias ao invés de alterar as existentes.

B) Efeitos colaterais limitados: funções devem ter impactos mínimos fora de seu escopo, visando não alterar estados externos e serem previsíveis em seus resultados.

C) Primeira classe: funções são tratadas como valores de primeira classe, podendo ser atribuídas a variáveis, passadas como argumentos, ou retornada por outras funções.

D) Herança prototípica: objetos podem ter como protótipo outros objetos, permitindo a reutilização de propriedades de maneira similar à herança em programação orientada a objetos.

E) Pureza: as funções devem ser puras, ou seja, o mesmo conjunto de entradas deve sempre resultar no mesmo conjunto de saídas, sem depender ou alterar estados externos.

",D,"

A alternativa D é incorreta no contexto de programação funcional. Herança prototípica é um conceito associado com a programação orientada a objetos, mais especificamente em linguagens como JavaScript, onde objetos podem herdar propriedades de outros objetos através de uma cadeia de protótipos. Esse não é um conceito primário na programação funcional, a qual enfatiza a imutabilidade de dados, funções como cidadãos de primeira classe e funções puras para evitar efeitos colaterais. As demais alternativas (A, B, C e E) são características fundamentais da programação funcional.",1726160
tópico 0,Armazenamento de big data,"Questão:
Analise as afirmativas a seguir relacionadas ao armazenamento de big data e as tecnologias associadas:

I. Sistemas de arquivos distribuídos, como o Hadoop Distributed File System (HDFS), são projetados para armazenar e processar grandes volumes de dados de forma eficiente, permitindo a escalabilidade horizontal e a redundância de dados.

II. Bancos de dados NoSQL, como Cassandra e MongoDB, oferecem flexibilidade no armazenamento de dados não estruturados ou semi-estruturados, mas não suportam transações ACID, comprometendo a integridade dos dados em ambientes distribuídos.

III. O armazenamento em data warehouses é ideal para big data devido à alta normalização e otimização para consultas ad-hoc complexas, embora esse tipo de solução seja menos eficaz para lidar com dados não estruturados ou em fluxo contínuo.

IV. Soluções como o Google BigTable e Amazon DynamoDB são exemplos de sistemas de armazenamento projetados para oferecer alta disponibilidade e desempenho para aplicações que requerem acesso rápido a grandes volumes de dados.

Está correto o que se afirma em:

A) I, II e III apenas.
B) I, III e IV apenas.
C) I e IV apenas.
D) II e III apenas.
E) I, II, III e IV.

",C," 

Explicação dos itens:

I. Correta - O HDFS é um sistema de arquivos distribuídos especialmente projetado para lidar com big data e é uma fundamental parte do ecossistema Hadoop, permitindo escalabilidade e redundância.

II. Incorreta - Muitos bancos de dados NoSQL suportam, em algum grau, características das transações ACID (Atomicidade, Consistência, Isolamento, Durabilidade), embora possam oferecer garantias mais fracas em algum destes aspectos quando comparados com bancos de dados SQL tradicionais, em favor de flexibilidade e desempenho em ambientes distribuídos.

III. Incorreta - Data warehouses são tipicamente otimizados para dados estruturados e armazenamento de grandes volumes de dados históricos para análise. Eles não são necessariamente ideais para big data não estruturado ou dados em tempo real, onde soluções como data lakes ou plataformas de stream processing seriam mais apropriadas.

IV. Correta - O Google BigTable e Amazon DynamoDB são exemplos de sistemas de armazenamento NoSQL que fornecem alta disponibilidade e desempenho para acessar e gerenciar big data em aplicações web e móveis em larga escala.",5026578
tópico 1,Banco de dados NoSQL,"Questão: Considerando as características dos sistemas de banco de dados NoSQL, avalie as seguintes afirmativas referentes à sua concepção e aplicabilidade:

I. Bancos de dados NoSQL são projetados para otimizar a escala horizontal, oferecendo assim alta disponibilidade e distribuição geográfica dos dados, o que é especialmente útil em ambientes de computação em nuvem.
II. Os sistemas NoSQL abdicam completamente do uso de esquemas de dados, operando exclusivamente com dados não estruturados.
III. Uma das principais vantagens dos bancos de dados NoSQL sobre os sistemas de banco de dados relacionais é a capacidade de armazenar e processar grande volume de dados diversas vezes caracterizados pelas três V's: volume, velocidade e variedade.

É correto o que se afirma em:

A) I, apenas.
B) I e II, apenas.
C) I e III, apenas.
D) II e III, apenas.
E) I, II e III.

",C,"

Explicação dos itens:

A) Incorreta. A alternativa sugere que apenas a afirmativa I é verdadeira, porém, a afirmativa III também é correta.

B) Incorreta. A afirmativa II é falsa porque os bancos de dados NoSQL podem utilizar esquemas flexíveis e até esquemas fixos em alguns casos, não operando exclusivamente com dados não estruturados.

C) Correta. A afirmativa I está correta porque os sistemas NoSQL são de fato projetados para escala horizontal e alta disponibilidade. Já a afirmativa III está correta ao indicar que os bancos NoSQL são adequados para lidar com grandes volumes de dados com alta velocidade e variedade.

D) Incorreta. Como explicado anteriormente, a afirmativa II é falsa, o que invalida esta opção.

E) Incorreta. Esta alternativa inclui a afirmativa II, que é falsa, como parte das assertivas corretas, portanto não pode ser a resposta certa.",224918
tópico 5,Técnicas de regressão: Árvores de decisão para regressão; Máquinas de vetores de suporte para regressão,"Questão:
Analise as seguintes afirmações relacionadas às técnicas de regressão envolvendo Árvores de Decisão e Máquinas de Vetores de Suporte (Support Vector Machines - SVM):

I. Árvores de decisão para regressão são modelos flexíveis que podem capturar relações não lineares sem a necessidade de transformações explícitas dos dados de entrada.

II. Uma vantagem das Máquinas de Vetores de Suporte para regressão (SVR) é a sua capacidade de lidar eficientemente com dimensões elevadas de input, devido ao uso de funções de kernel.

III. O ajuste de uma Árvore de Decisão frequentemente requer poda para evitar overfitting, enquanto que nas SVM para regressão o risco de overfitting é naturalmente controlado pelo parâmetro de regularização C.

Está correto o que se afirma em:

A) I, apenas.
B) II, apenas.
C) I e II, apenas.
D) II e III, apenas.
E) I, II e III.

",E,"

Explicação dos itens:

I. Correto. As Árvores de Decisão para regressão não requerem que as relações entre variáveis de entrada e saída sejam lineares ou que sigam uma forma específica, sendo capazes de modelar relações não lineares de forma inerente à sua estrutura de particionamento do espaço de entrada.

II. Correto. As SVMs são conhecidas pela capacidade de gerenciar grandes conjuntos de dados e muitas dimensões (alta dimensionalidade) sem maiores dificuldades. As funções de kernel permitem que as SVMs operem em um espaço de características de maior dimensão sem a necessidade de calcular explicitamente as novas dimensões, devido ao truque do kernel.

III. Correto. Árvores de decisão para regressão podem ser propensas ao overfitting, especialmente quando são muito profundas. A técnica de poda é usada para reduzir a complexidade da árvore e assim mitigar o overfitting. As Máquinas de Vetores de Suporte, por sua vez, utilizam o parâmetro de regularização C para equilibrar a margem de erro com a complexidade do modelo, ajudando a prevenir o overfitting independentemente da complexidade do modelo adotado.",2485571
tópico 5,Técnicas de regressão: Árvores de decisão para regressão; Máquinas de vetores de suporte para regressão,"Questão: 
Considere um cenário onde um cientista de dados foi encarregado de desenvolver um modelo preditivo para estimar o valor de imóveis com base em um amplo conjunto de características, incluindo a localização, área construída, número de quartos, idade do imóvel, entre outras variáveis explicativas. O objetivo é selecionar uma técnica de regressão que seja capaz de lidar com a complexidade do problema e que forneça resultados interpretáveis. Com base nas seguintes opções, qual seria a técnica de regressão mais adequada para atender a ambos os critérios, interpretabilidade e capacidade de lidar com a complexidade dos dados?

A) Regressão Linear Simples
B) Regressão por Árvores de Decisão
C) Máquinas de Vetores de Suporte (SVR) com kernel linear
D) Máquinas de Vetores de Suporte (SVR) com kernel radial (RBF)
E) Redes Neurais Artificiais

",B," 
A alternativa correta é a letra B, Regressão por Árvores de Decisão. A explicação dos itens é a seguinte:

A) Regressão Linear Simples - Não seria adequada, pois ela é limitada a relacionamentos lineares e simples entre as variáveis explicativas e a variável dependente, o que provavelmente não será suficiente para capturar a complexidade das relações no cenário fornecido.

B) Regressão por Árvores de Decisão - Esta é a técnica mais apropriada entre as opções fornecidas, devido à sua capacidade de modelar relações não-lineares e complexas entre as características e a variável resposta. Além disso, as árvores de decisão são relativamente fáceis de interpretar, pois podem ser visualizadas e compreendidas através de um formato de árvore.

C) Máquinas de Vetores de Suporte (SVR) com kernel linear - Embora possam ser úteis para grandes volumes de dados e variáveis, o uso de um kernel linear é mais indicado para relações linearmente separáveis, o que pode não ser o caso nas relações complexas entre as características do imóvel e seu valor.

D) Máquinas de Vetores de Suporte (SVR) com kernel radial (RBF) - Esta técnica é capaz de capturar relações complexas e não-lineares graças ao uso do kernel RBF. No entanto, a interpretação do modelo se torna mais difícil, uma vez que a transformação do espaço de entrada em um espaço de características de alta dimensão complica a compreensão dos efeitos e das relações entre as variáveis.

E) Redes Neurais Artificiais - Redes neurais são poderosas e capazes de modelar interações complexas entre variáveis, porém elas apresentam uma interpretabilidade muito limitada. A ""caixa-preta"" associada às redes neurais torna difícil entender como as variáveis de entrada estão afetando a saída do modelo, o que não cumpre o critério de interpretabilidade desejado.",4567656
tópico 4,Regra empírica (regra de três sigma) da distribuição normal,"Questão: Em estatística, a Regra Empírica, também conhecida como regra dos três sigmas, é utilizada para descrever a distribuição normal de dados. Assumindo que os dados estão devidamente distribuídos, a regra afirma certas porcentagens dos dados estão a uma, duas e três desvios padrões da média. Considerando uma distribuição normal perfeita, quais as porcentagens de dados que se encontram a até um, dois e três desvios padrão da média, respectivamente?

A) Aproximadamente 68%, 90% e 99,7%
B) Aproximadamente 34%, 68% e 95%
C) Aproximadamente 68%, 95% e 99,7%
D) Aproximadamente 50%, 75% e 90%
E) Aproximadamente 95%, 99% e 99,9%

",C," 

Explicação dos itens: 

A) Incorreta. Reflete uma compreensão equivocada da segunda e terceira porcentagens.
B) Incorreta. A primeira porcentagem está correta para um lado da curva, mas as duas seguintes são incorretas.
C) Correta. De acordo com a Regra Empírica, aproximadamente 68% dos dados estão a um desvio padrão da média, cerca de 95% estão a dois desvios padrões, e aproximadamente 99,7% estão a três desvios padrões da média em uma distribuição normal.
D) Incorreta. Subestima significativamente a porcentagem de dados dentro de um, dois, e três desvios padrões da média.
E) Incorreta. Superestima as porcentagens, especialmente no caso de um e dois desvios padrões da média.",2995420
tópico 1,Banco de dados NoSQL,"Questão:

A tecnologia NoSQL foi criada como uma resposta à necessidade de armazenar e processar grandes volumes de dados não estruturados ou semi-estruturados, que não se encaixam adequadamente nos moldes dos sistemas de banco de dados relacionais tradicionais. Dentre as características dos bancos de dados NoSQL, uma delas se destaca pela capacidade de se adaptar a diferentes tipos de estruturas de dados sem a necessidade de schemas fixos. Este aspecto é particularmente essencial para o desenvolvimento de aplicações que necessitam de flexibilidade e escalabilidade. Qual das opções a seguir descreve de forma correta esta característica específica dos bancos de dados NoSQL?

A) Indexação Automática: Todos os dados inseridos no banco NoSQL são automaticamente indexados, o que permite consultas e operações de busca extremamente rápidas mesmo em volumes altíssimos de informação.

B) Transações ACID: Os sistemas NoSQL implementam transações que seguem estritamente as propriedades ACID (Atomicidade, Consistência, Isolamento, Durabilidade), garantindo um alto nível de integridade dos dados.

C) Schema-less: Os bancos de dados NoSQL muitas vezes são 'schema-less', isto é, não requerem a definição de um esquema rígido de banco de dados antes do armazenamento dos dados. Isso permite a adição de novos campos sem a necessidade de modificar todos os registros existentes.

D) Normalização de Dados: Os bancos de dados NoSQL seguem o princípio da normalização, que é a organização de dados em tabelas que minimizam a redundância e dependências.

E) SQL Support: Bancos de dados NoSQL expandiram suas capacidades para suportarem a linguagem SQL tradicional, para que possam tirar proveito do conhecimento já estabelecido dos desenvolvedores em SQL.

",C," 
A alternativa correta é a letra C - Schema-less. Os bancos de dados NoSQL são conhecidos por serem 'schema-less', ou seja, não exigem um esquema definido previamente, o que oferece flexibilidade ao permitir a adição de novos campos ou tipos de dados sem a necessidade de alterar a estrutura existente. Isso difere dos bancos de dados relacionais tradicionais, que requerem um esquema fixo que define as tabelas, colunas e os tipos de dados antes que os dados possam ser armazenados. Vejamos as outras alternativas:

A) Indexação Automática: Apesar de muitos bancos de dados NoSQL oferecerem formas de indexação eficazes, a indexação automática não é o fator que proporciona a adaptabilidade de estrutura de dados.

B) Transações ACID: Muitos bancos de dados NoSQL não suportam completamente as propriedades ACID, especialmente em ambientes distribuídos, pois frequentemente priorizam a disponibilidade e a tolerância a partições (conforme o Teorema CAP) em vez da consistência estrita.

D) Normalização de Dados: A normalização é um conceito mais associado a bancos de dados relacionais e não se aplica da mesma forma aos bancos de dados NoSQL, que podem armazenar dados de maneiras menos estruturadas.

E) SQL Support: Nem todos os bancos de dados NoSQL suportam a linguagem SQL, pois foram concebidos para operar com consultas em formatos que podem ser muito diferentes da linguagem SQL tradicional, embora algumas soluções NoSQL tenham começado a introduzir compatibilidade com SQL.",6526684
tópico 6,Tipos de viés no processo gerador dos dados e soluções: Sampling bias; Selection bias; Attrition bias; Reporting bias; Measurement bias.,"Questão: No contexto de pesquisas e coleta de dados, o controle de vieses é fundamental para garantir a validade e confiabilidade dos resultados obtidos. Dentre os diversos tipos de viés conhecidos, certos padrões podem afetar a representatividade da amostra e a integridade das inferências realizadas a partir dos dados coletados. Considere os seguintes cenários:

I. Um estudo longitudinal sobre alimentação e saúde cardiovascular perde progressivamente participantes ao longo do tempo, principalmente aqueles com dietas menos saudáveis, que abandonam o estudo por falta de interesse em mudar seus hábitos.

II. Uma pesquisa sobre hábitos de consumo de mídia digital utiliza uma amostra que inclui somente usuários de uma rede social específica, sem considerar indivíduos que não a utilizam.

III. Durante a coleta de dados sobre eficácia de um novo medicamento, pacientes são selecionados a partir de um hospital que atende majoritariamente indivíduos de alta renda, deixando de incluir a diversidade socioeconômica da população.

IV. Um estudo sobre prevalência de cárie dentária coleta dados apenas durante horário comercial, excluindo trabalhadores que não podem participar de consultas neste horário.

V. Uma pesquisa sobre satisfação de clientes em uma loja solicita avaliações de forma não anônima, levando a uma tendência de os clientes darem respostas mais positivas para evitar possíveis constrangimentos.

Os cenários I, II, III, IV e V exemplificam, respectivamente, os seguintes vieses:

A) Attrition bias; Selection bias; Selection bias; Reporting bias; Measurement bias.

B) Attrition bias; Sampling bias; Selection bias; Sampling bias; Reporting bias.

C) Selection bias; Sampling bias; Selection bias; Measurement bias; Reporting bias.

D) Attrition bias; Sampling bias; Selection bias; Sampling bias; Measurement bias.

E) Measurement bias; Selection bias; Sampling bias; Attrition bias; Reporting bias.

",D," 

Explicação dos itens:

I. Attrition bias: Ocorre devido à perda de participantes ao longo de um estudo longitudinal, o que pode afetar a representatividade dos resultados ao final do estudo.

II. Sampling bias: Surgido ao selecionar uma amostra não-representativa da população total, no caso, apenas usuários de uma rede social específica.

III. Selection bias: Refere-se à escolha de participantes de forma a não refletir corretamente a população visada. Neste caso, ao selecionar participantes somente de alta renda.

IV. Sampling bias: Quando a coleta de dados é feita de maneira a excluir sistematicamente parta da população, como no exemplo dos trabalhadores que não podem participar de consultas durante o horário comercial.

V. Measurement bias: Esse viés ocorre quando o método de coleta de dados influencia os resultados, como no caso de pesquisas não anônimas que podem levar a respostas enviesadas para evitar constrangimentos.",7965104
tópico 6,Tipos de viés no processo gerador dos dados e soluções: Sampling bias; Selection bias; Attrition bias; Reporting bias; Measurement bias.,"Questão: Em um estudo longitudinal sobre os efeitos de uma nova droga para redução do colesterol, uma equipe de pesquisadores recrutou 1000 pacientes de um total de 5000 possíveis candidatos, sem levar em conta a distribuição geográfica dos indivíduos. Após dois anos de estudo, aproximadamente 300 pacientes abandonaram a pesquisa, a maioria deles justificando o abandono pelo surgimento de efeitos colaterais não previstos. Além disso, os dados coletados sobre os níveis de colesterol foram relatados pelos próprios pacientes via telefone, e não por profissionais de saúde. Dado este cenário, identifique quais vieses podem ter afetado os resultados do estudo e escolha a alternativa que melhor representa uma combinação desses vieses.

A) Sampling bias e Selection bias
B) Attrition bias e Measurement bias
C) Selection bias, Attrition bias e Measurement bias
D) Reporting bias e Measurement bias
E) Sampling bias, Attrition bias e Reporting bias

",C," 
- Sampling Bias: Não foi mencionado diretamente, mas o fato de os recrutadores não considerarem a distribuição geográfica na seleção dos participantes pode sugerir que um viés de amostragem ocorreu se certas áreas foram mais representadas do que outras.
- Selection Bias: Não houve indicação direta de que os pacientes incluídos no estudo foram selecionados de maneira que não fossem representativos da população mais ampla dos candidatos.
- Attrition Bias: Esse viés está presente, pois uma grande proporção dos participantes abandonou o estudo, o que pode levar a uma distorção dos resultados se aqueles que saíram eram sistematicamente diferentes daqueles que permaneceram.
- Reporting Bias: Não foi mencionado que houve preferência na divulgação de resultados positivos ou negativos; portanto, reporting bias pode não ser aplicável neste contexto.
- Measurement Bias: Esse viés foi claramente introduzido, pois os dados sobre os níveis de colesterol foram relatados pelos pacientes ao invés de serem medidos por profissionais de saúde, o que pode comprometer a precisão das informações coletadas.

Portanto, embora possa haver potencial para Sampling e Reporting bias dependendo de detalhes adicionais que não são fornecidos na questão, os vieses mais claramente indicados aqui são o Attrition bias, devido à grande perda de participantes, e o Measurement bias, devido à metodologia de coleta de dados. Assim, a alternativa correta é a C, que contempla o Selection bias, Attrition bias e Measurement bias, com base nos detalhes fornecidos.",4861529
tópico 6, Modelos probabilísticos gráficos: cadeias de Markov; filtros de Kalman; Redes bayesianas,"Questão:
A análise de modelos probabilísticos gráficos é fundamental no campo da estatística e aprendizado de máquina para representar distribuições de probabilidade complexas e realizar inferências. Dentre esses modelos, as cadeias de Markov, filtros de Kalman e redes bayesianas têm aplicações específicas dependendo da natureza dos dados e do problema em estudo.

Considere que você é um cientista de dados e está trabalhando com um sistema de localização em tempo real, que requer a estimação da posição e velocidade de um objeto em movimento, baseando-se em leituras ruidosas dos sensores. Avalie os seguintes modelos probabilísticos gráficos e escolha a opção que melhor se adequa a esse cenário.

A) Cadeias de Markov.
B) Filtros de Kalman.
C) Redes Bayesianas.
D) Todas as opções são igualmente adequadas.
E) Nenhuma das opções se ajusta a este cenário.

",B,"

Explicação dos itens:

A) Cadeias de Markov são modelos estocásticos que descrevem sistemas de estados que transitam de um para outro com probabilidades definidas. Embora úteis para modelar sequências de eventos dependentes somente do estado anterior, não são ideais para lidar com a estimativa em tempo real necessária para a localização baseada em dados ruidosos de sensores.

B) Filtros de Kalman são algoritmos que fornecem estimativas eficientes e recursivas do estado interno de um sistema dinâmico a partir de medições observadas ao longo do tempo que possuem ruídos. Eles são amplamente utilizados em sistemas de controle e navegação, como o sistema de localização em tempo real mencionado, que precisa filtrar o ruído das medições dos sensores e estimar com precisão a posição e velocidade do objeto.

C) Redes Bayesianas são representações gráficas de dependências condicionais entre variáveis aleatórias, e são particularmente úteis para representar conhecimento incerto, fazer inferências e aprender a partir de dados. Embora sejam poderosas para uma gama de problemas de modelagem e inferência, não são especificamente projetadas para lidar com as necessidades de filtragem e previsão temporal que são características dos filtros de Kalman.

D) Esta opção é incorreta porque, apesar de todos os modelos serem importantes em seus respectivos contextos, nem todos são indicados para o problema específico de estimativa em tempo real de posição e velocidade a partir de medidas ruidosas.

E) Esta opção é incorreta porque o filtro de Kalman (opção B) é de fato uma técnica adequada para o cenário proposto.",3022598
tópico 6, Modelos probabilísticos gráficos: cadeias de Markov; filtros de Kalman; Redes bayesianas,"Questão:

A análise de modelos probabilísticos gráficos é uma ferramenta importante no campo de aprendizado de máquina e inteligência artificial para representar dependências entre variáveis aleatórias. Cadeias de Markov, filtros de Kalman e Redes Bayesianas são exemplos desses modelos. Uma característica importante das cadeias de Markov é a propriedade de ""falta de memória"", também conhecida como propriedade de Markov. Considere que uma cadeia de Markov é utilizada para modelar o estado do tempo, onde o tempo pode estar 'ensolarado', 'nublado' ou 'chuvoso'. Sabendo que o clima do dia atual depende somente do clima do dia anterior, qual das seguintes afirmações é verdadeira sobre a cadeia de Markov aplicada a este modelo?

A) A probabilidade de estar 'ensolarado' hoje depende da sequência completa de estados de tempo dos dias anteriores.

B) Os filtros de Kalman são uma alternativa melhor para modelar o clima, dado que são baseados em estados contínuos, enquanto cadeias de Markov lidam com estados discretos.

C) A probabilidade de estar 'ensolarado' hoje depende unicamente do estado do tempo de ontem, não importando o estado em dias anteriores a este.

D) Redes Bayesianas não podem ser utilizadas para modelar este tipo de problema, pois não conseguem representar dependências condicionais entre variáveis.

E) A utilização de cadeias de Markov é inadequada para este tipo de modelagem, pois não é possível estimar as probabilidades de transição entre estados.

",C,"

A explicação dos itens:

A) Esta alternativa é incorreta porque contraria a propriedade de Markov, que estabelece que a probabilidade de transição para um estado futuro depende apenas do estado presente, e não de uma sequência de eventos passados.

B) Esta alternativa é incorreta, embora os filtros de Kalman sejam utilizados para estados contínuos e as cadeias de Markov para estados discretos, não significa que um seja necessariamente melhor que o outro para modelagem do clima; eles são utilizados em contextos diferentes de acordo com a natureza dos dados.

C) Esta alternativa é correta, pois reflete a propriedade de Markov, na qual o estado futuro ('ensolarado' hoje) depende apenas do estado presente (estado do tempo de ontem), sem memória de estados anteriores.

D) Esta alternativa é incorreta, pois Redes Bayesianas são capazes de representar dependências condicionais complexas entre variáveis aleatórias e podem perfeitamente modelar a dependência do estado do tempo baseado em estados passados.

E) Esta alternativa é incorreta, pois cadeias de Markov podem ser bastante adequadas para este tipo de modelagem, e as probabilidades de transição entre estados podem ser estimadas a partir de dados históricos do clima.",5644728
tópico 5,Técnicas de redução de dimensionalidade: Seleção de características (feature selection); Análise de componentes principais (PCA – principal component analysis),"Questão: Em um projeto de aprendizado de máquina que envolve um grande número de variáveis, um cientista de dados pode se deparar com dificuldades relacionadas à alta dimensionalidade dos dados, o que pode levar a problemas como a maldição da dimensionalidade e overfitting. Considere as seguintes técnicas de redução de dimensionalidade: seleção de características (feature selection) e análise de componentes principais (PCA – principal component analysis). Com base na compreensão destas técnicas, assinale a opção que melhor descreve um cenário ideal para a aplicação de PCA ao invés de seleção de características.

A) Quando a interpretabilidade do modelo é primordial e a contribuição individual de cada variável original precisa ser compreendida.
B) Quando existe a necessidade de reduzir o tempo de computação e os recursos, mantendo a maior parte da variabilidade dos dados.
C) Quando é essencial manter apenas as variáveis que possuem maior correlação com a variável de resposta, independentemente das outras características.
D) Quando se quer manter a estrutura original dos dados, sem criar combinações lineares das variáveis existentes.
E) Quando cada característica é fundamental para a previsão e não se deseja combinar as variáveis de forma linear.

",B,"

Explicação dos itens:

A) Incorreto. O PCA cria componentes principais que são combinações lineares das variáveis originais, o que torna mais difícil compreender a contribuição individual de cada variável.
B) Correto. O PCA é uma técnica de redução de dimensionalidade que transforma as variáveis originais em um conjunto de novas variáveis ortogonais (componentes principais) que explicam a maior parte da variância dos dados, sendo útil para reduzir o custo computacional e ainda manter a maior parte da informação.
C) Incorreto. Seleção de características é mais apropriada nesse cenário, pois se busca identificar as variáveis mais relevantes à variável de resposta sem transformá-las.
D) Incorreto. O PCA altera a estrutura original dos dados através da combinação linear das variáveis originais para gerar os componentes principais.
E) Incorreto. O PCA combina as variáveis de maneira linear para criar os componentes principais, o que não é desejável se cada característica individual for considerada essencial.",3830121
tópico 6, Modelos probabilísticos gráficos: cadeias de Markov; filtros de Kalman; Redes bayesianas,"Questão: Em relação aos modelos probabilísticos gráficos, considere as seguintes afirmações sobre cadeias de Markov, filtros de Kalman e Redes Bayesianas:

I. Uma cadeia de Markov é um modelo estocástico que descreve uma sequência de possíveis eventos onde a probabilidade de cada evento depende apenas do estado imediatamente anterior na sequência.
II. O filtro de Kalman é uma técnica algorítmica que oferece uma solução eficiente para a estimação do estado linear dinâmico de sistemas em tempo discreto sujeitos a ruídos gaussianos.
III. As Redes Bayesianas são um tipo de modelo probabilístico gráfico que representa as dependências condicionais entre variáveis aleatórias por meio de um grafo acíclico direcionado, utilizadas para inferência estatística.

Qual(is) afirmação(ões) está(ão) correta(s)?

A) Apenas I e II.
B) Apenas II e III.
C) Apenas I e III.
D) I, II e III.
E) Nenhuma das afirmações é correta.

",D,"

Explicação:
I. Correta. Uma cadeia de Markov é, de fato, um modelo estocástico que não possui memória, o que significa que a probabilidade de transição para o próximo estado depende apenas do estado atual e não da sequência de eventos que ocorreram anteriormente. Este princípio é conhecido como propriedade de Markov ou ""sem memória"".

II. Correta. O filtro de Kalman é amplamente usado para a estimação em tempo real do estado interno de um sistema linear dinâmico a partir de uma série de medições que estão sujeitas a ruídos. Ele é ótimo para sistemas sujeitos a ruídos gaussianos e é empregado em diversas áreas, como engenharia aeroespacial para navegação e controle de aeronaves e satélites.

III. Correta. Redes Bayesianas são modelos gráficos que representam variáveis aleatórias em um domínio e suas dependências condicionais por meio de um grafo acíclico direcionado (DAG). Esses modelos são eficientes para realizar inferências e tomar decisões sob incerteza.

Todas as três afirmações são corretas e, portanto, a opção D é a alternativa que responde corretamente à pergunta.",5306377
tópico 2,Contexto de IA: Deduplicação,"Questão: Em um cenário onde uma empresa de grande porte decide implementar um mecanismo de Inteligência Artificial (IA) para otimização de sua base de dados, uma das estratégias adotadas é a deduplicação de dados. Considerando que a deduplicação envolve a identificação e remoção de duplicatas, incrementando a eficiência no armazenamento e processamento de dados, qual técnica de IA NÃO é tipicamente utilizada no processo de deduplicação?

A) Aprendizagem de máquina para classificação de instâncias duplicadas.
B) Redes neurais para reconhecimento de padrões em grandes conjuntos de dados.
C) Processamento de linguagem natural para identificar semelhanças semânticas em textos.
D) Algoritmos genéticos para a evolução de soluções de problemas de otimização.
E) Sistemas especialistas baseados em regras para identificar não conformidades.

",D," 

Explicação dos itens:

A) Incorreta. A aprendizagem de máquina é frequentemente utilizada para classificar instâncias em dados, identificando possíveis duplicatas em função de suas características.
B) Incorreta. Redes neurais são uma ferramenta poderosa para reconhecimento de padrões e, portanto, podem ser aplicadas para identificar duplicatas dentro de grandes conjuntos de dados.
C) Incorreta. O processamento de linguagem natural pode ser utilizado para entender o contexto e a semântica de textos, auxiliando na identificação de duplicações especialmente em bases de dados que contém informações textuais.
D) Correta. Enquanto algoritmos genéticos são essenciais para problemas de otimização, eles não são tipicamente utilizados no contexto de deduplicação de dados, uma vez que seu foco está na evolução iterativa de soluções e não na identificação direta de duplicatas.
E) Incorreta. Sistemas especialistas baseados em regras podem ser aplicados para estabelecer critérios lógicos a fim de identificar não conformidades, que poderiam incluir a presença de dados duplicados.",3173559
tópico 5,"Avaliação de modelos de classificação: treinamento, teste, validação; validação cruzada; métricas de avaliação - matriz de confusão, acurácia, precisão, revocação, F1-score e curva ROC","Questão: Em modelagem preditiva, um cientista de dados está desenvolvendo um modelo de classificação binária para prever a ocorrência de uma doença rara em um conjunto de pacientes. Devido à raridade da doença, o conjunto de dados possui uma distribuição desbalanceada com um número significativamente menor de casos positivos (pessoas com a doença) do que negativos (pessoas sem a doença). O cientista decidiu usar a validação cruzada para melhor avaliar o desempenho do modelo e tem em mãos várias métricas obtidas a partir dos resultados do modelo. Qual das seguintes métricas seria a mais informativa para garantir que o modelo tenha uma boa performance na classificação do evento raro corretamente?

A) Acurácia

B) Precisão

C) Revocação

D) F1-score

E) Área sob a curva ROC (AUC-ROC)

",C," 
Explicação:

A) Acurácia: Esta alternativa não é a mais adequada para o cenário descrito, porque em casos de classes desbalanceadas, a acurácia pode ser enganosa, indicando um alto valor mesmo se o modelo simplesmente prever a classe majoritária (negativos, neste caso) para todos os casos.

B) Precisão: Mede a proporção de predições positivas corretas em relação a todas as predições positivas. Embora seja uma métrica importante, a precisão sozinha não considera a quantidade de casos positivos reais que o modelo deixou de identificar (falsos negativos).

C) Revocação: Também conhecida como sensibilidade ou taxa de verdadeiro positivo, a revocação é a proporção de casos positivos reais que foram identificados corretamente pelo modelo. Em um cenário com uma doença rara, é fundamental que o modelo tenha uma boa revocação para garantir que a maioria dos verdadeiros casos positivos seja detectada.

D) F1-score: É a média harmônica entre precisão e revocação. Enquanto é uma métrica que leva em consideração tanto precisão quanto revocação, ela não prioriza uma em detrimento da outra e pode não ser a melhor escolha se o objetivo é assegurar a detecção de uma condição rara com o menor número de falsos negativos possível.

E) Área sob a curva ROC (AUC-ROC): Esta métrica reflete o desempenho geral do modelo em todas as probabilidades de classificação, mas como a AUC-ROC leva em consideração tanto a taxa de verdadeiro positivo quanto a de falso positivo, ela pode ainda dar uma visão demasiado otimista em casos de desbalanceamento extremo.

Portanto, a métrica mais informativa para este cenário é a Revocação (C), pois é crucial que o modelo identifique corretamente os casos positivos da doença rara, mesmo que isso custe uma maior quantidade de falsos positivos.",3236507
tópico 2,Contexto de IA: Tratamento de dados ausentes,"Questão: Em um projeto de Inteligência Artificial (IA), a fase de pré-processamento dos dados é crucial para garantir a qualidade do modelo preditivo a ser construído. Durante essa fase, um dos problemas frequentemente enfrentados é o tratamento de dados ausentes. Considerando um conjunto de dados com variáveis numéricas e categóricas, qual das seguintes técnicas NÃO é recomendada para tratar de dados ausentes em variáveis categóricas?

A) Imputação utilizando a moda das variáveis categóricas.
B) Utilização de algoritmos de Machine Learning que suportam dados ausentes.
C) Substituição dos valores ausentes pelo valor médio da variável categórica.
D) Aplicação de técnicas de imputação múltipla.
E) Criação de uma categoria adicional para representar os dados ausentes.

",C," 

Explicação:

A) A imputação utilizando a moda das variáveis categóricas é uma prática comum e pode ser adequada quando se presume que a razão para os dados estarem ausentes é aleatória e que a moda representa bem a categoria mais frequente.
B) Alguns algoritmos de Machine Learning, como árvores de decisão e florestas aleatórias, podem lidar com dados ausentes diretamente sem a necessidade de imputação prévia, o que os torna uma opção viável.
C) A substituição dos valores ausentes pelo valor médio é uma técnica apropriada para variáveis numéricas e não categóricas, devido à natureza qualitativa destas. Utilizar média para dados categóricos não faz sentido matemático ou estatístico, tornando a opção incorreta, por isso é a resposta a esta questão.
D) Técnicas de imputação múltipla criam diversas ""completas"" versões do conjunto de dados, cada uma com diferentes imputações para os valores ausentes, para representar a incerteza sobre qual imputação seria a mais correta. É uma técnica avançada e aceitável para lidar com dados ausentes.
E) Criar uma categoria adicional pode ser uma boa estratégia, pois permite ao modelo aprender com os dados que originalmente contém valores ausentes, sem assumir que se assemelham arbitrariamente a qualquer outra categoria existente.",2829239
tópico 6, Testes de hipóteses: teste-z; teste-t; valorp; testes para uma amostra; testes de comparação de duas amostras; teste de normalidade (chi square); e intervalos de confiança.,"Questão: Uma pesquisa foi realizada para investigar a eficácia de um novo medicamento para reduzir a pressão arterial em pacientes diagnosticados com hipertensão arterial. Uma amostra de 40 pacientes foi selecionada, e a redução média da pressão arterial após o uso do medicamento foi de 5 mmHg, com um desvio padrão de 10 mmHg. A pressão arterial dos pacientes antes do tratamento tinha média conhecida de 150 mmHg. O pesquisador deseja testar se a redução observada na amostra é estatisticamente significativa em um nível de significância de 5% (α = 0,05).

Para realizar o teste de hipóteses apropriado, qual dos seguintes testes deve ser usado?

A) Teste-z para uma amostra
B) Teste-t para duas amostras independentes
C) Valor-p para amostras emparelhadas
D) Teste de normalidade (chi-square)
E) Teste-t para uma amostra

",E," 

Alternativa A (Teste-z para uma amostra) não é a correta, pois apesar de ser um teste aplicável quando conhecemos a média populacional e temos grande amostra, o teste-z é recomendado apenas quando a variância populacional é conhecida, o que não é o caso.

Alternativa B (Teste-t para duas amostras independentes) não é adequada, porque estamos interessados em comparar a pressão antes e depois do tratamento no mesmo grupo de pacientes, e não entre dois grupos independentes.

Alternativa C (Valor-p para amostras emparelhadas) é um conceito incorrelatamente expresso. O valor-p é uma métrica que resulta de realizar o teste de hipótese e não um teste por si só.

Alternativa D (Teste de normalidade - chi-square) não é o correto, pois seu uso é para verificar a aderência de uma distribuição a uma distribuição teórica (normal, nesse caso), e não para testar a diferença de médias.

Alternativa E (Teste-t para uma amostra) é a correta. A média populacional antes do tratamento é conhecida, mas a variância populacional não é. Utiliza-se o teste-t quando a variância populacional é desconhecida, mesmo com uma amostra grande (n=40 é suficientemente grande para que a distribuição t tenda para a distribuição normal). O teste verifica se a média de redução observada na amostra (5 mmHg) é significativamente diferente da média populacional (0 mmHg, assumindo que não houve redução) a um nível de significância de 5%.",3245619
tópico 2,Contexto de IA: Deduplicação,"Questão: A deduplicação de dados é uma técnica avançada utilizada para reduzir o espaço necessário para armazenamento de informações em sistemas de TI, através da eliminação de cópias de dados repetidos. No contexto de Inteligência Artificial (IA) e machine learning, a deduplicação apresenta particular importância em processos de pré-processamento de dados. Qual das seguintes afirmações melhor descreve o impacto da deduplicação nas etapas de treinamento de modelos de IA?

A) A deduplicação é irrelevante para o treinamento de modelos de IA, pois a redundância de dados pode auxiliar na estabilidade do algoritmo durante sua fase de aprendizagem.
B) A deduplicação pode prejudicar a performance do modelo de IA, já que a remoção de dados duplicados pode resultar na perda de informações cruciais para o reconhecimento de padrões.
C) A deduplicação melhora significativamente a acurácia dos modelos de IA ao remover ruídos e informações dispensáveis que poderiam levar a resultados enviesados.
D) A deduplicação de dados é somente recomendada para conjuntos de dados pequenos, onde o espaço de armazenamento e poder de processamento são limitados.
E) A deduplicação, quando aplicada corretamente, contribui para a eficiência computacional e minimiza o risco de sobreajuste (overfitting) do modelo, ao garantir que apenas informações únicas sejam utilizadas no treinamento.

",E,"  
Explicação dos itens:

A) Esta alternativa é incorreta porque, embora em alguns casos a redundância de dados possa ajudar na estabilidade do modelo, na maioria das situações a deduplicação é um processo valioso para melhorar o desempenho e a qualidade do treinamento de IA.
B) Embora a perda de informação seja uma preocupação válida, a deduplicação não elimina informações cruciais, mas sim cópias exatas de entradas já existentes. Isso não afeta a capacidade do modelo de reconhecer padrões, tornando essa opção incorreta.
C) A deduplicação pode melhorar a qualidade dos dados, mas afirmar que ela melhora significativamente a acurácia dos modelos é uma superestimação, pois outros fatores também influenciam na acurácia. Ademais, a afirmação de que ela remove ruídos e informações dispensáveis é demasiadamente ampla, pois a deduplicação foca especificamente em duplicatas exatas e não em ruídos gerais.
D) Esta afirmação é falsa, pois a deduplicação é relevante e útil independentemente do tamanho do conjunto de dados. Em grandes conjuntos de dados, especialmente, pode resultar em economias significativas de armazenamento e poder de processamento.
E) Esta afirmação é a mais correta, pois reconhece dois benefícios principais da deduplicação: a eficiência computacional e a minimização do risco de sobreajuste. Isso se deve ao fato de que a deduplicação assegura que o modelo não vai aprender repetidamente dos mesmos exemplos, o que poderia fazer com que se ajustasse demais às particularidades do conjunto de treino, em vez de aprender a generalizar a partir de um conjunto de dados mais diversificado.",3361148
tópico 5,Técnicas de regressão: Árvores de decisão para regressão; Máquinas de vetores de suporte para regressão,"Questão: 
Uma equipe de cientistas de dados está trabalhando em um projeto de modelagem preditiva para prever os preços de imóveis com base em diversas características, como localização, área construída, número de quartos, entre outras. Para isso, estão considerando o uso de Árvores de Decisão para Regressão (Decision Tree Regression) e Máquinas de Vetores de Suporte para Regressão (Support Vector Regression - SVR). Com base nas características do problema e das técnicas citadas, analise as afirmativas abaixo e assinale a opção correta.

I. Árvores de Decisão para Regressão podem capturar relacionamentos não lineares entre os recursos de entrada e a variável de saída, mas podem sofrer de sobreajuste se não forem adequadamente podadas ou regularizadas.

II. Embora as Máquinas de Vetores de Suporte para Regressão possam modelar relações não lineares por meio do uso de kernels, elas exigem um tempo de treinamento consideravelmente maior em comparação às Árvores de Decisão, especialmente em grandes conjuntos de dados.

III. Máquinas de Vetores de Suporte para Regressão, com a escolha adequada do parâmetro de regularização, tendem a ser menos sensíveis a outliers do que as Árvores de Decisão para Regressão.

Está correto o que se afirma em:

A) I, apenas.

B) II, apenas.

C) I e II, apenas.

D) II e III, apenas.

E) I, II e III.

",E,"

Explicação:

Item I: Esta afirmativa está correta, pois as Árvores de Decisão para Regressão realmente podem captar relações não lineares e são suscetíveis a sobreajuste (overfitting) se as árvores tornarem-se muito complexas sem a devida poda ou regularização.

Item II: Esta afirmativa também está correta. As Máquinas de Vetores de Suporte (SVMs) podem levar mais tempo para treinar em comparação às Árvores de Decisão, particularmente em conjuntos de dados de grande escala, devido à complexidade computacional do processo de otimização envolvido na busca da margem máxima.

Item III: Outra afirmativa correta, pois as Máquinas de Vetores de Suporte para Regressão tendem a ser mais robustas a outliers quando adequadamente regularizadas, já que o objetivo principal do SVR é minimizar o erro dentro de um certo limite de tolerância, sem dar demasiado peso a erros muito grandes (outliers).

Todas as três afirmativas são verdadeiras e aplicam-se ao contexto dado da previsão de preços de imóveis utilizando Árvores de Decisão para Regressão e Máquinas de Vetores de Suporte para Regressão.",9301489
tópico 6,Tipos de viés no processo gerador dos dados e soluções: Sampling bias; Selection bias; Attrition bias; Reporting bias; Measurement bias.,"Questão: Em estudos observacionais e experimentais, o viés pode comprometer a validade dos resultados e as conclusões tiradas a partir de dados coletados. Qual dos seguintes tipos de viés ocorre quando certos grupos são sub-representados ou super-representados devido à maneira como as amostras são coletadas, e como ele pode ser mitigado?

A) Sampling bias; mitigado pela utilização de estratificação na seleção da amostra.
B) Selection bias; mitigado pela inclusão de todos os elementos do população alvo.
C) Attrition bias; mitigado pela garantia de que todos os participantes sejam monitorados por igual.
D) Reporting bias; mitigado pela implementação de medidas para garantir que todos os eventos sejam relatados.
E) Measurement bias; mitigado pelo treinamento rigoroso dos pesquisadores na coleta de dados.

",A," 

Explicação dos itens:

A) Correto. Sampling bias, também conhecido como viés de amostragem, ocorre quando algumas partes da população alvo têm menos probabilidade de serem incluídas na amostra do que outras. Isso pode ser mitigado pela utilização de técnicas de estratificação, que garantem que diferentes subgrupos da população sejam proporcionalmente representados na amostra.

B) Incorreto. Selection bias, ou viés de seleção, ocorre quando os indivíduos selecionados para participar de um estudo não são representativos de toda a população alvo, mas este viés normalmente é resultado de como os participantes são escolhidos para o estudo, não pela sub ou super-representação devido a técnicas de amostragem.

C) Incorreto. Attrition bias, ou viés de desistência, acontece em estudos longitudinais quando há uma perda diferencial de participantes ao longo do tempo, o que pode afetar a representatividade da amostra final. A mitigação envolve procedimentos para minimizar a desistência e garantir que o acompanhamento seja igual para todos os participantes.

D) Incorreto. Reporting bias, ou viés de relato, ocorre quando determinados tipos de eventos ou resultados são sistematicamente sub ou super-relatados. Medidas para garantir a adequada notificação de todos os eventos ajudam a mitigar esse viés.

E) Incorreto. Measurement bias, ou viés de medição, é resultante de erros sistemáticos na maneira como as informações são coletadas ou medidas. O treinamento dos pesquisadores para coletar dados de modo consistente e rigoroso é um método para mitigar esse tipo de viés.",5701428
tópico 1,Banco de dados NoSQL,"Questão: Considerando o modelo NoSQL, que tem se popularizado no contexto de Big Data e aplicações que demandam escalabilidade horizontal e flexibilidade de esquema, analise as seguintes afirmações referentes às suas características e tipos:

I. Bancos de dados NoSQL do tipo chave-valor armazenam os dados em um esquema que não requer uma estrutura fixa, permitindo que cada entrada tenha um conjunto único de chaves e valores associados.
II. Bancos de dados orientados a documento, como o MongoDB, são exemplos de NoSQL que suportam indexação e buscas complexas, com a vantagem adicional de armazenar os dados de forma semelhante aos documentos JSON, facilitando o trabalho com APIs e serviços web.
III. Sistemas NoSQL de bancos de dados em colunas, a exemplo do Cassandra, são uma boa escolha para aplicações que precisam realizar agregações complexas e joins frequentes, similares aos bancos de dados relacionais.
IV. Os bancos de dados orientados a grafos, como o Neo4j, não são recomendados para aplicativos que necessitam de relacionamentos entre os dados, pois essa categoria de NoSQL não suporta operações que envolvem a análise de interconexões entre os dados.

Qual das afirmações acima está INCORRETA?

A) I e II
B) II e III
C) III e IV
D) Apenas IV
E) Apenas III

",C,"

Explicações dos itens:

I. Esta afirmação é verdadeira. Os sistemas chave-valor, como o Redis, realmente permitem esquemas flexíveis, o que é uma característica notável dos bancos de dados NoSQL.

II. Esta afirmação é verdadeira. Bancos de dados orientados a documentos são projetados para armazenar, recuperar e gerenciar estruturas de documentos semiestruturados, e o MongoDB é um exemplo clássico de suporte a indexação e buscas complexas.

III. Esta afirmação é falsa. Bancos de dados em colunas são otimizados para leituras e escritas rápidas de grandes volumes de dados e não para agregações complexas e joins, que são mais comuns em sistemas relacionais.

IV. Esta afirmação é falsa. Bancos de dados orientados a grafos foram criados exatamente para lidar com relações complexas entre os dados, com alto desempenho em operações que envolvem a análise de interconexões.",403757
tópico 0,Soluções de big data: Arquitetura do ecossistema Spark,"Questão: Dentro do ecossistema de soluções de Big Data, a arquitetura do Apache Spark destaca-se por sua capacidade de processamento rápido e seu conjunto abrangente de ferramentas. Analise as afirmações a seguir relativas à arquitetura e aos componentes do Apache Spark:

I. Spark SQL permite a execução de consultas SQL. No entanto, não é possível combinar SQL com programação funcional usando outras APIs do Spark.

II. Spark Streaming adiciona capacidade de processamento de streams de dados ao Spark, com a qual podem ser processados dados em tempo real partindo de diferentes fontes como Kafka, Flume e Kinesis.

III. A principal abstração de dados no Spark é o RDD (Resilient Distributed Dataset), que permite operações como map, filter e reduce de maneira distribuída e tolerante a falhas.

IV. O MLib é uma biblioteca do Spark criada exclusivamente para o processamento de grafos e otimização de consultas.

Assinale a opção que contém apenas as afirmações corretas sobre o Apache Spark:

A) I e IV
B) II e III
C) I, II e IV
D) II, III e IV
E) I, II e III

",B,"

Explicação:

Item I: Incorreto. O Spark SQL permite sim combinar consultas SQL com outras APIs do Spark, como o Spark DataFrames e Datasets, possibilitando o uso misto de SQL e programação funcional.

Item II: Correto. Spark Streaming é um componente do Spark que permite o processamento em tempo real (streaming) a partir de diferentes fontes de dados, como Kafka, Flume e Kinesis, integrando-se facilmente com o restante do ecossistema Spark.

Item III: Correto. O RDD é a principal abstração de dados do Spark. Ele é um conjunto de dados distribuído que pode ser processado em paralelo, e possui características de resiliência a falhas e operações de alto nível como map, filter e reduce.

Item IV: Incorreto. O MLib é uma biblioteca do Spark para machine learning e não é especializada exclusivamente no processamento de grafos. Para o processamento de grafos, o Spark oferece o GraphX. O MLib fornece diversas ferramentas de aprendizagem de máquina, como classificação, regressão, clustering e filtragem colaborativa, entre outras.

Assim, apenas as afirmações II e III estão corretas, o que corresponde à opção B.",4122120
tópico 2,Contexto de IA: Desidentificação de dados sensíveis,"Questão: Na implementação de sistemas de Inteligência Artificial que lidam com o processamento de dados pessoais, é fundamental assegurar a privacidade e a proteção contra a identificação indevida de indivíduos. Considerando as técnicas de desidentificação de dados sensíveis aplicadas para garantir o anonimato dos dados dos usuários, avalie as afirmações abaixo e assinale a opção correta:

I. A pseudonimização é uma técnica de desidentificação que substitui os identificadores diretos por pseudônimos, de forma que não seja possível atribuir os dados a uma pessoa específica sem o uso de informações adicionais mantidas separadamente.

II. A agregação dos dados envolve a combinação de dados em grupos de tal maneira que os dados individuais não possam mais ser distinguidos, garantindo a desidentificação completa e irreversível dos dados.

III. A técnica de k-anonimato assegura que cada registro não possa ser distinguido de pelo menos k-1 outros registros no banco de dados, reduzindo o risco de identificação ao associar os dados desidentificados com outras fontes de informação.

IV. A criptografia dos dados é considerada uma forma eficaz de desidentificação, já que torna a informação inacessível para quem não detém a chave criptográfica.

Está correto apenas o que se afirma em:

A) I e II
B) I e III
C) II e IV
D) I, III e IV
E) I, II, III e IV

",B,"

Explicação dos itens:

I. Correta. A pseudonimização é uma técnica de proteção de dados pessoais que substitui identificadores para impedir a associação direta com uma pessoa sem informações adicionais, compatível com o descrito no enunciado.

II. Incorreta. A agregação de dados reduz o risco de identificação individual, mas não garante a desidentificação completa ou irreversível, pois, em alguns casos, pode ser possível reidentificar os indivíduos através de técnicas avançadas de análise de dados ou combinação com outras fontes de informação.

III. Correta. A k-anonimato é uma técnica de privacidade de dados que busca proteger a identidade dos indivíduos assegurando que cada registro em um banco de dados seja indistinguível de pelo menos outros k-1 registros, dificultando a reidentificação.

IV. Incorreta. A criptografia é um método de proteção de dados que impede o acesso não autorizado à informação, mas não é uma técnica de desidentificação per se. Os dados criptografados ainda são considerados identificáveis porque a descriptografia é possível com a chave adequada.",311000
tópico 6,"Diagramas causais: gráficos acíclicos dirigidos; variáveis confundidoras, colisoras e de mediação","Questão: A análise causal é essencial em estudos epidemiológicos, econômicos, e de diversas outras áreas do conhecimento, com o intuito de compreender a ligação entre variáveis e estabelecer relações de causa e efeito. Diagramas causais, especialmente gráficos acíclicos dirigidos (DAGs), são ferramentas valiosas para representar essas relações.

Considere que um estudo está examinando a relação entre atividade física (A) e saúde cardiovascular (S). Para tal, um conjunto de variáveis é levantado: idade (I), dieta (D), e histórico familiar de doenças cardíacas (F). Em um DAG corretamente construído que considera todas essas variáveis, qual das seguintes estruturas poderia representar adequadamente uma variável confundidora?

A) Uma seta dirigida de I para A e outra seta dirigida de I para S.
B) Uma seta dirigida de A para D e outra seta dirigida de D para S.
C) Uma seta dirigida de A para F e outra seta dirigida de F para S.
D) Uma seta dirigida de S para D e outra seta dirigida de D para A.
E) Uma seta dirigida de D para I e outra seta dirigida de I para F.

",A,"

Explicação dos itens:

A) Correta. A idade (I) influencia tanto a prática de atividade física (A) quanto a saúde cardiovascular (S). Portanto, a idade pode atuar como uma variável confundidora, associada tanto à exposição quanto ao desfecho, sugerindo uma correlação espúria entre A e S caso não seja corretamente controlada.

B) Incorreta. Esse é um exemplo de uma variável mediadora, onde a atividade física (A) pode afetar a dieta (D), que, por sua vez, tem um impacto sobre a saúde cardiovascular (S). A variável mediadora (D) está na cadeia de influência entre a exposição (A) e o desfecho (S).

C) Incorreta. Esse é um exemplo de estrutura onde o histórico familiar (F) seria um resultado da atividade física (A), o que não é plausível, e ainda assim só por isso não constitui uma variável confundidora.

D) Incorreta. Esta estrutura sugere que saúde cardiovascular (S) influencia a dieta (D) que, por sua vez, influencia a atividade física (A). Além de representar um ciclo causal improvável, não configura uma variável confundidora.

E) Incorreta. Embora a idade possa influenciar o tipo de dieta e esta, por sua vez, pode influenciar o histórico familiar, essa cadeia não demonstra uma relação de confusão entre as variáveis principais de interesse, atividade física (A) e saúde cardiovascular (S).",9288318
tópico 5,"métricas de avaliação - matriz de confusão, acurácia, precisão, revocação, F1-score e curva ROC","Questão:

Em problemas de classificação, diversas métricas podem ser utilizadas para avaliar a qualidade do modelo. A Matriz de Confusão é uma ferramenta que permite a visualização do desempenho do algoritmo, onde cada linha da matriz representa as instâncias em uma classe real, enquanto cada coluna representa as instâncias em uma classe prevista pelo modelo. Considere que um modelo de classificação binária foi aplicado em um conjunto de dados para predizer a ocorrência de uma doença. Os termos Verdadeiro Positivo (VP), Falso Positivo (FP), Verdadeiro Negativo (VN) e Falso Negativo (FN) são derivados dessa matriz. Com base nesses valores, é possível calcular métricas como Acurácia, Precisão, Revocação e F1-score. A partir dos dados:

VP = 80
FP = 20
VN = 150
FN = 50

Escolha a opção que apresenta o cálculo correto da Acurácia e do F1-score:

A) Acurácia = 0,92 e F1-score = 0,64
B) Acurácia = 0,82 e F1-score = 0,50
C) Acurácia = 0,76 e F1-score = 0,73
D) Acurácia = 0,77 e F1-score = 0,66
E) Acurácia = 0,69 e F1-score = 0,58

",B,"

Acurácia é calculada como a soma dos verdadeiros positivos e verdadeiros negativos dividida pelo total de casos (VP + VN)/(VP + FN + FP + VN). Com os dados fornecidos, temos (80 + 150)/(80 + 20 + 150 + 50) = 230/300 = 0,76 (aproximadamente 0,77). Nenhuma opção corresponde exatamente a 0,77, portanto é um erro no enunciado ou nas alternativas.

Precisão é calculada como VP/(VP + FP). No nosso caso, teríamos 80/(80 + 20) = 0,80.

Revocação é calculada como VP/(VP + FN). No caso, 80/(80 + 50) = 0,615.

F1-score é o harmônico entre Precisão e Revocação, calculado por 2 * (Precisão * Revocação)/(Precisão + Revocação). Portanto, 2 * (0,80 * 0,615)/(0,80 + 0,615) ≈ 0,695, que podemos arredondar para 0,70. Nenhuma das alternativas corresponde ao valor exato.

A alternativa B é a mais próxima do cálculo teórico, porém a precisão do enunciado pode ser questionada. Nos exames da CESGRANRIO, normalmente espera-se que uma das opções reflita o cálculo exato. Recomenda-se inspeção nas opções de resposta ou corrigir o enunciado para que as alternativas de resposta reflitam os valores calculados corretamente.",3599499
tópico 5,"Métricas de similaridade textual - similaridade do cosseno, distância euclidiana, similaridade de Jaccard, distância de Manhattan e coeficiente de Dice","Questão:

Considere duas strings de texto S1 e S2 e a necessidade de medir a similaridade entre elas para uma aplicação em processamento de linguagem natural (PLN). Entre as métricas listadas abaixo, qual não é apropriada para medir a similaridade textual diretamente e por quê?

A) Similaridade do Cosseno
B) Distância Euclidiana
C) Similaridade de Jaccard
D) Distância de Manhattan
E) Coeficiente de Dice

",B,"

Explicação:

A) Similaridade do Cosseno - Esta métrica é apropriada para medir a similaridade textual, pois compara a orientação dos vetores de palavras, independentemente de seu tamanho, sendo útil em PLN.

B) Distância Euclidiana - Não é apropriada para medir a similaridade textual diretamente uma vez que é sensível ao tamanho dos vetores de texto, o que pode gerar resultados imprecisos devido à diferença no comprimento dos documentos.

C) Similaridade de Jaccard - Apropriada para medir a similaridade textual ao comparar a intersecção e a união dos conjuntos de palavras, fornecendo uma medida de quão similares são dois conjuntos de tokens.

D) Distância de Manhattan - Apesar de poder ser usada em alguns contextos de PLN, ela não é a melhor métrica para similaridade textual, pois considera a soma das distâncias absolutas entre os pontos em um espaço vetorial. No entanto, ela ainda pode ser aplicada, principalmente em espaços vetoriais de alta dimensionalidade, como em PLN.

E) Coeficiente de Dice - Esta métrica é apropriada pois mede a sobreposição entre dois conjuntos de palavras (como a Similaridade de Jaccard), dando ênfase aos casos de grande intersecção de palavras, o que é desejável no contexto de similaridade textual.

Portanto, a opção B é a menos apropriada para medir diretamente a similaridade textual, já que a distância euclidiana pode ser distorcida pelo comprimento dos vetores de texto.",2042740
tópico 5,"Rotulação de partes do discurso, part-of-speech tagging","Questão: Considere uma sequência de palavras extraída de um corpus em processo de anotação para treinamento de algoritmos de aprendizado de máquina aplicados a tarefas de processamento de linguagem natural. A sequência é a seguinte: ""Os investimentos em tecnologia advêm da necessidade de inovação."" Para a correta atribuição de etiquetas de partes do discurso (part-of-speech tagging - POS tagging) a cada palavra da sequência em um modelo baseado no padrão do corpus utilizado, qual das seguintes opções apresenta a sequência correta das etiquetas POS?

A) DET, NOUN, ADP, NOUN, VERB, ADP, NOUN, ADP, NOUN
B) DET, NOUN, ADP, NOUN, NOUN, ADP, NOUN, ADP, NOUN
C) PRON, NOUN, ADP, NOUN, VERB, ADP, NOUN, ADP, NOUN
D) DET, VERB, ADP, ADJ, VERB, ADP, NOUN, ADP, NOUN
E) DET, NOUN, ADP, NOUN, ADV, ADP, ADJ, ADP, NOUN

",A,"

Explicação dos itens:

A) DET (Determinante), NOUN (Substantivo), ADP (Preposição), NOUN (Substantivo), VERB (Verbo), ADP (Preposição), NOUN (Substantivo), ADP (Preposição), NOUN (Substantivo) - Esta é a sequência correta, que identifica corretamente as partes do discurso em questão: ""Os"" é um determinante, ""investimentos"" e ""tecnologia"" são substantivos, ""em"" é uma preposição, ""advêm"" é um verbo, ""da"" é uma combinação de preposição com determinante, ""necessidade"" é um substantivo, ""de"" é uma preposição e ""inovação"" é um substantivo.

B) Esta opção erroneamente classifica ""advêm"" como um substantivo, quando na realidade é um verbo.

C) Esta opção erroneamente classifica ""Os"" como um pronome, quando na realidade é um determinante.

D) Esta sequência contém vários erros, começando com a classificação de ""investimentos"" como um verbo e ""tecnologia"" como um adjetivo, além de ""advêm"" ser classificado incorretamente como verbo novamente.

E) Aqui ""advêm"" está incorretamente classificado como um advérbio e ""necessidade"" como um adjetivo, o que não condiz com as suas funções na frase.",2248309
tópico 5,Técnicas de regressão: Árvores de decisão para regressão; Máquinas de vetores de suporte para regressão,"Questão: Ao aplicar técnicas de regressão em um conjunto de dados complexo e de grande dimensão, um cientista de dados pode optar por usar Árvores de Decisão para Regressão (Decision Tree Regression) ou Máquinas de Vetores de Suporte para Regressão (Support Vector Regression - SVR). Ambas as técnicas têm suas particularidades em termos de modelagem e aplicabilidade. Nesse contexto, analise as afirmativas a seguir e marque a opção que contém a assertiva correta:

I. As Árvores de Decisão para Regressão são modelos não-lineares e não-paramétricos que se ajustam perfeitamente a dados com relações lineares.

II. As Máquinas de Vetores de Suporte para Regressão são capazes de modelar relações complexas e não-lineares entre as variáveis independentes e a resposta, especialmente com o uso de funções de kernel.

III. Uma vantagem das Árvores de Decisão para Regressão é sua capacidade de lidar tanto com variáveis categóricas quanto numéricas sem a necessidade de transformação prévia.

IV. As Máquinas de Vetores de Suporte para Regressão requerem a escolha de um parâmetro de regularização e uma função de kernel apropriada, tornando-se mais complexas na fase de configuração e escolha de parâmetros do que as Árvores de Decisão.

A) Apenas a afirmativa I está correta.
B) Apenas a afirmativa II está correta.
C) Apenas a afirmativa III está correta.
D) As afirmativas II e IV estão corretas.
E) Todas as afirmativas I, II, III e IV estão corretas.

",D,"

Explicação dos itens:

I. Esta afirmativa é incorreta. As Árvores de Decisão para Regressão são modelos não-lineares sim, mas elas não se ajustam perfeitamente apenas a dados com relações lineares. Na verdade, são flexíveis para capturar relações não-lineares e interações complexas entre as variáveis.

II. Esta afirmativa é correta. As SVR são particulares por utilizar funções de kernel para modelar relações complexas e não-lineares. Esse recurso permite que a SVR lide eficazmente com dados em espaços de maior dimensão e descubra padrões complexos.

III. Esta afirmativa é correta parcialmente. Enquanto as Árvores de Decisão para Regressão podem lidar diretamente com variáveis categóricas e numéricas, isso não significa que elas sempre operem sem a necessidade de transformação prévia. Em certos casos, a transformação de variáveis pode melhorar a performance do modelo.

IV. Esta afirmativa é correta. A escolha adequada do parâmetro de regularização e da função de kernel é essencial para o desempenho da SVR. Isso pode tornar a fase de configuração do modelo mais complexa em comparação com as Árvores de Decisão, que geralmente têm uma configuração mais direta.

Assim, apenas as afirmativas II e IV estão corretas, tornando a opção D a resposta correta.",1942119
tópico 5,"Técnicas de agrupamento: Agrupamento por partição, por densidade e hierárquico","Questão:
A análise de agrupamentos é uma técnica exploratória de dados amplamente utilizada para identificar estruturas ou padrões em um conjunto de observações. Dentre os métodos de agrupamento, destaca-se a divisão em categorias como agrupamento por partição, por densidade e hierárquico. Considerando as características intrínsecas de cada um destes métodos, associe cada descrição a seguir ao respectivo método de agrupamento.

I. Este método envolve a construção de uma hierarquia de clusters de maneira iterativa, seja por abordagem divisiva ou aglomerativa, criando um dendrograma que permite observar o processo de formação de grupos.
II. Nesta técnica, o conjunto de dados é dividido em um número pré-definido de clusters, onde cada ponto de dado é atribuído ao grupo com o centroide ou média mais próxima, buscando minimizar a variação intra-cluster e maximizar a variação inter-cluster.
III. Este método identifica regiões de alta densidade que são separadas por regiões de baixa densidade. Os pontos dentro de uma região densa são agrupados, e os pontos em regiões pouco densas são tipicamente considerados ruídos ou outliers.

As técnicas de agrupamento correspondentes a I, II e III são, respectivamente:

A) Agrupamento por partição, por densidade e hierárquico.
B) Agrupamento hierárquico, por densidade e por partição.
C) Agrupamento por densidade, hierárquico e por partição.
D) Agrupamento hierárquico, por partição e por densidade.
E) Agrupamento por partição, hierárquico e por densidade.

",D,"

Explicação dos itens:

I. Descreve o método hierárquico de agrupamento, o qual constrói clusters baseados em uma hierarquia que pode ser representada através de um dendrograma.

II. Detalha o método de agrupamento por partição, como o algoritmo k-means, que divide o conjunto de dados em k grupos, sendo 'k' um número pré-definido, com o objetivo de minimizar a soma dos quadrados dentro dos clusters.

III. Refere-se ao método de agrupamento por densidade, exemplo disso é o algoritmo DBSCAN, que trabalha com a noção de densidade para formar clusters, separando regiões de alta densidade das de baixa densidade e considerando pontos isolados em regiões de baixa densidade como ruídos ou outliers.

Desta forma, é possível relacionar as descrições I, II e III respectivamente aos métodos hierárquico, por partição e por densidade, o que corresponde à alternativa D.",9231957
tópico 5,Redes neurais convolucionais e recorrentes,"Questão: Em relação às Redes Neurais Convolucionais (CNNs) e às Redes Neurais Recorrentes (RNNs), assinale a opção correta que caracteriza de maneira adequada a principal diferença estrutural e a aplicabilidade destes dois tipos de arquiteturas de redes neurais profundas.

A) CNNs e RNNs possuem arquiteturas muito semelhantes, diferindo apenas no número de parâmetros treináveis, sendo que as RNNs são geralmente utilizadas para analisar sequências de dados temporais ou sequenciais.

B) As CNNs são mais adequadas para o reconhecimento de padrões em imagens e vídeos devido à sua capacidade de capturar a localidade espacial e a hierarquia de características, enquanto RNNs são desenhadas para lidar com dados sequenciais, como linguagem natural ou séries temporais, por sua habilidade de manter informações de estados anteriores.

C) As RNNs utilizam filtros convolucionais para extrair características de dados, o que as torna mais eficientes para o processamento de imagens e vídeos do que as CNNs, que são mais apropriadas para sequências de dados onde a ordem dos elementos é crucial.

D) CNNs aplicam sua rede de forma recorrente sobre os dados de entrada, o que as torna adequadas para processamento de texto e fala, em que a relação temporal entre os dados é importante, enquanto as RNNs utilizam camadas densas para processar imagens estáticas.

E) Ambas as redes, CNNs e RNNs, são igualmente eficazes em todas as tarefas de aprendizado de máquina, não havendo diferenças significativas em termos de aplicabilidade e desempenho entre as duas arquiteturas.

",B,"

Explicação dos itens:

A) Esta alternativa está incorreta porque as CNNs e RNNs possuem arquiteturas fundamentalmente diferentes e não diferem apenas no número de parâmetros. As RNNs possuem loops que as tornam distintas e são utilizadas principalmente para dados sequenciais.

B) Esta é a alternativa correta porque resume adequada e distintamente a principal vantagem de cada tipo de rede: as CNNs são eficazes no processamento de imagens devido à sua estrutura que capta a localidade espacial das características, enquanto as RNNs são desenhadas para tratar informações sequenciais, mantendo estado de informações anteriores.

C) Esta alternativa está incorreta porque inverte a aplicabilidade das CNNs e RNNs. As CNNs é que utilizam filtros convolucionais para processamento de imagens, e as RNNs são utilizadas principalmente para dados sequenciais.

D) Esta alternativa também está incorreta porque confunde as funções das CNNs e RNNs. As CNNs não aplicam sua rede de maneira recorrente, e sim utilizam convoluções para processar imagens. As RNNs é que têm uma natureza recorrente e são utilizadas para dado textuais e de fala, entre outros tipos sequenciais de dados.

E) Esta alternativa está incorreta pois ignora as diferenças fundamentais e os pontos de força de cada tipo de rede para tarefas específicas. Existe sim uma diferença significativa em termos de desempenho e aplicabilidade entre as duas arquiteturas, dependendo da natureza do problema.",4025826
tópico 5,Técnicas de redução de dimensionalidade: Seleção de características (feature selection); Análise de componentes principais (PCA – principal component analysis),"Questão:
Em análise de dados multidimensionais, é comum o emprego de técnicas de redução de dimensionalidade para simplificar os conjuntos de dados sem perdas significativas de informação. Sobre as técnicas de seleção de características (feature selection) e análise de componentes principais (PCA – principal component analysis), analise as afirmativas a seguir e marque a opção correta:

I. A seleção de características visa escolher um subconjunto de características originais com o objetivo de reduzir ruídos e melhorar a eficiência de algoritmos de aprendizado de máquina.
II. A análise de componentes principais (PCA) transforma as variáveis originais em um novo conjunto de variáveis chamadas componentes principais, que são ordenadas de modo que as primeiras tenham a maior variação possível.
III. Tanto a seleção de características quanto o PCA podem ser considerados métodos supervisionados de redução de dimensionalidade, uma vez que levam em conta as saídas desejadas para ajustar os parâmetros de seleção ou transformação.

A) Apenas I e II estão corretas.
B) Apenas II e III estão corretas.
C) Apenas I e III estão corretas.
D) Todas as afirmativas estão corretas.
E) Nenhuma das afirmativas está correta.

",A," 

Explicação dos itens:

Item I está correto. A seleção de características é uma técnica que consiste em selecionar as mais importantes características de um conjunto de dados, reduzindo o número de variáveis e eliminando aquelas que pouco contribuem para o modelo.

Item II está correto. PCA é um método estatístico que simplifica a complexidade dos espaços de dados multidimensionais preservando sua variação e estrutura, transformando as variáveis originais em componentes principais de forma ordenada, onde a primeira componente principal tem a maior variação.

Item III está incorreto. A seleção de características pode ser tanto supervisionada quanto não supervisionada, pois pode levar em consideração ou não a variável de saída. PCA, por sua vez, é um método não supervisionado, não levando em consideração as variáveis de saída durante a transformação. Portanto, a afirmação de que ambos os métodos são supervisionados é incorreta.",7811517
tópico 5,"Modelos de representação de texto - N-gramas, modelos vetoriais de palavras (CBOW, Skip-Gram e GloVe)","Questão: A escolha do modelo de representação de texto é um passo crucial em tarefas de Processamento de Linguagem Natural (PLN), uma vez que impacta diretamente a forma como os dados serão interpretados e manuseados por algoritmos. Entre as opções de modelos de representação de texto, temos os N-gramas, que são sequências contíguas de N itens de um dado texto, e os modelos vetoriais de palavras, como o CBOW (Continuous Bag of Words), Skip-Gram e GloVe (Global Vectors for Word Representation).

Suponhamos um cenário onde o contexto das palavras é extremamente importante para a tarefa de PLN em execução, e que haja a necessidade de capturar relações e padrões em um grande corpus textual. Neste cenário, qual dos seguintes modelos de representação de texto é o MAIS apropriado de ser utilizado?

A) N-gramas, dado que eles são capazes de capturar frequências de sequências de palavras e são simples de implementar.

B) CBOW, pois seu objetivo é prever uma palavra com base no contexto e ele se destaca em corpora menores.

C) Skip-Gram, tendo em vista que ele é projetado para prever o contexto a partir de uma palavra e tende a se sair melhor com corpora maiores e com palavras de contexto raro.

D) GloVe, visto que ele combina as vantagens da fatoração de matriz com aprendizado local baseado em janela e é eficiente na captura de relações globais do corpus.

E) Todos os modelos são igualmente apropriados, desde que devidamente configurados e otimizados para a tarefa específica.

",C," 

Explicação dos itens:

A) N-gramas são úteis para modelar a frequência de sequências de palavras e podem capturar contextos locais até certo ponto, mas podem não ser tão eficazes em capturar relações de longo alcance ou padrões mais complexos presentes em um grande corpus textual.

B) O modelo CBOW é eficiente em contextos menores e foca em prever uma palavra central com base nas palavras do contexto que a cercam. No entanto, para corpora maiores e contextos mais ricos, pode não ser o mais eficaz.

C) O modelo Skip-Gram é projetado para prever palavras de contexto a partir de uma palavra central. Ele tende a se sair melhor em corpora de textos maiores, onde há uma vasta gama de contextos possíveis, e consegue lidar de forma eficiente com palavras raras, o que o torna apropriado para o cenário descrito na questão.

D) GloVe é conhecido por sua habilidade em capturar informações globais e relações estatísticas de um grande corpus. Contudo, ele tem um foco mais direcionado para a captura de relações semânticas globais e não necessariamente supera o Skip-Gram em termos de análise de contexto detalhado, especialmente para palavras raras.

E) Embora a customização e a otimização dos modelos possam aumentar seu desempenho, cada um tem características distintas que os tornam mais adequados para certos tipos de tarefas. No cenário proposto, o Skip-Gram é considerado o mais apropriado devido à sua capacidade de manejar grandes corpora e capturar contextos ricos.",9651991
tópico 5,"modelos vetoriais de documentos (booleano, TF e TF-IDF, média de vetores de palavras e Paragraph Vector);","Questão:
A recuperação de informação em grandes repositórios de dados envolve a utilização de modelos para representar documentos de maneira que a computação de semelhanças e relevâncias entre eles e as consultas dos usuários seja viável. Os modelos vetoriais de documentos têm sido amplamente utilizados para essa finalidade, cada um com suas características e aplicações. Considerando os modelos Booleano, TF (Term Frequency), TF-IDF (Term Frequency-Inverse Document Frequency), Média de Vetores de Palavras e Paragraph Vector, é INCORRETO afirmar que:

A) O modelo Booleano opera sob o princípio de lógica binária, onde os termos presentes no documento são representados por 1 e os ausentes por 0, sem levar em conta a frequência do termo no documento.
B) O modelo TF-IDF pondera o número de vezes que um termo aparece no documento (TF) com a inversa da frequência do termo nos documentos do corpus (IDF), aumentando a importância de termos menos comuns nos documentos.
C) O modelo de Média de Vetores de Palavras representa cada documento como o vetor médio das representações vetoriais de todas as palavras no documento, assim, palavras com significado similar tendem a ter representações vetoriais similares.
D) No modelo Paragraph Vector, também conhecido como Doc2Vec, cada documento é mapeado para um único vetor fixo, aprendido de forma não supervisionada, que captura a semântica do documento independentemente do seu tamanho.
E) O modelo TF considera que a relevância de um termo em um documento é diretamente proporcional à frequência de ocorrência do termo em todo o corpus, incentivando assim o uso de termos mais frequentes em toda a coleção de documentos.

",E,"

A alternativa E é INCORRETA porque faz uma afirmação errada sobre o modelo TF (Term Frequency). Contrariamente ao que é mencionado na alternativa, o modelo TF avalia a relevância de um termo com base na sua frequência dentro de um único documento, e não em todo o corpus. O modelo TF não incentiva o uso de termos mais frequentes em toda a coleção de documentos. Pelo contrário, ele simplesmente conta o número de vezes que um termo ocorre em um documento, independentemente de sua frequência no corpus.

As demais alternativas estão corretas:
A) Descreve adequadamente o modelo Booleano, que não considera a frequência dos termos nos documentos.
B) Explica corretamente o cálculo do modelo TF-IDF, que combina o TF com o IDF para ponderar a importância dos termos.
C) Apresenta uma característica verdadeira do modelo de Média de Vetores de Palavras, que utiliza a média dos vetores das palavras.
D) Corretamente descreve o modelo Paragraph Vector (Doc2Vec) como um método que cria uma representação vetorial fixa para cada documento.",5587913
tópico 5,"Processamento de linguagem natural: Normalização textual - stop words, estemização, lematização e análise de frequência de termos;","Questão: No contexto de processamento de linguagem natural (PLN), diversas técnicas são aplicadas para a preparação e análise de dados textuais. Entre elas, temos a normalização textual, que envolve processos como remoção de stop words, estemização, lematização e análise de frequência de termos. Considere as seguintes afirmações sobre esses processos:

I. A remoção de stop words é uma técnica que elimina palavras frequentes em um idioma que, geralmente, oferecem pouco valor semântico ao texto, tais como preposições, conjunções e artigos.

II. Estemização é o processo pelo qual palavras são reduzidas ao seu radical, removendo os sufixos e mantendo intacta a parte do termo que contém seu significado básico. 

III. Lematização envolve a análise morfológica das palavras, resultando na sua forma canônica ou de dicionário, o que nem sempre corresponde ao radical da palavra.

IV. A análise de frequência de termos prescinde de qualquer normalização textual prévia, uma vez que a relevância dos termos é diretamente proporcional à sua recorrência no texto.

Qual(is) afirmação(ões) está(ão) correta(s)?

A) Apenas I e II
B) I, II e III
C) Apenas I, III e IV
D) Todas as afirmações estão corretas
E) Apenas II e III

",B," 

Explicação dos itens:

A) Esta opção está incorreta, porque ignora a afirmação III que é verdadeira sobre lematização.

B) Esta opção está correta, pois as afirmações I, II e III estão corretas. 
   I - A remoção de stop words é uma técnica fundamental na normalização textual para reduzir ruídos nos dados.
   II - A estemização reduz as palavras ao seu radical, mantendo a essência semântica do termo.
   III - Lematização é um processo mais complexo que a estemização, pois considera o contexto e a flexão das palavras para retornar à forma de dicionário ou canônica.
   
C) Esta opção está incorreta, porque inclui a afirmação IV, que é falsa.
   
D) Esta opção está incorreta, pois a afirmação IV é falsa. A análise de frequência de termos geralmente se beneficia de normalização textual para evitar distorções devido a variações ou palavras de baixa relevância semântica.
   
E) Esta opção está incorreta, porque exclui a afirmação I que é verdadeira sobre a remoção de stop words e ignora que a afirmação IV é falsa.",4755312
tópico 5,Ajuste de modelos dentro e fora de amostra e overfitting,"Questão:
Quando se trata de ajustar modelos estatísticos baseados em conjuntos de dados, é fundamental evitar o problema de overfitting, o qual pode levar a um desempenho insatisfatório do modelo quando aplicado a novos dados. Sobre este tema, assinale a afirmativa correta:

A) O ajuste de modelo dentro da amostra sempre garante que o modelo terá um bom desempenho fora da amostra, pois utiliza todos os dados disponíveis para a sua construção.
B) Overfitting é uma característica desejável nos modelos preditivos, pois indica que o modelo foi capaz de capturar todas as nuances dos dados de treino.
C) Cross-validation é uma técnica eficaz na prevenção de overfitting, pois permite uma melhor estimativa do erro de previsão fora da amostra.
D) Modelos com grande quantidade de parâmetros são menos propensos ao overfitting, pois são mais flexíveis e adaptáveis a diferentes tipos de dados.
E) Para evitar o overfitting, é recomendável utilizar o menor conjunto de dados possível para treinar o modelo, minimizando assim a complexidade do modelo.

",C,"

Explicação dos itens:
A) Esta alternativa é incorreta porque um bom ajuste do modelo dentro da amostra não garante necessariamente que o modelo terá um bom desempenho em novos dados. Isso ocorre devido ao fenômeno de overfitting, onde o modelo pode aprender padrões específicos dos dados de treino que não se generalizam para dados desconhecidos.
B) Esta alternativa é incorreta já que overfitting não é desejável. Overfitting significa que o modelo está ajustado demais aos dados de treino e não tem capacidade de generalizar para novos dados, resultando em um desempenho pobre em dados fora da amostra.
C) Esta alternativa é correta. Cross-validation é uma técnica usada para avaliar a habilidade de generalização de modelos estatísticos. Por permitir o uso de diferentes conjuntos de treino e teste, ajuda a estimar o erro fora da amostra e a evitar o overfitting.
D) Esta alternativa é incorreta. Modelos com muitos parâmetros, apesar de serem flexíveis, são justamente mais propensos ao overfitting, pois tendem a se ajustar demais aos dados de treino, capturando o ruído ao invés de apenas os sinais.
E) Esta alternativa é incorreta. Usar o menor conjunto de dados possível pode levar a um modelo muito simples que não capta as complexidades dos dados (underfitting). É importante encontrar um equilíbrio entre a complexidade do modelo e a quantidade de dados para treinar o modelo, evitando tanto overfitting quanto underfitting.",4407342
tópico 5,Técnicas de classificação: Naive Bayes; Árvores de decisão (algoritmos ID3 e C4.5); Florestas aleatórias (random forest); Máquinas de vetores de suporte (SVM – support vector machines); K vizinhos mais próximos (KNN – K-nearest neighbours),"Questão: Em um projeto de ciência de dados para classificação de clientes em potenciais compradores de um novo produto, o cientista de dados tem que escolher entre diferentes algoritmos de aprendizado supervisionado. Considerando que o dataset contém um grande volume de dados, com várias features categóricas e que o objetivo é ter um modelo que possa oferecer uma interpretação clara das regras de classificação para a equipe de marketing, qual dos seguintes algoritmos seria o mais adequado para atender a esses requisitos?

A) Naive Bayes
B) Árvore de decisão (C4.5)
C) Máquinas de vetores de suporte (SVM – support vector machines)
D) K vizinhos mais próximos (KNN – K-nearest neighbours)
E) Florestas aleatórias (random forest)

",B,"

Explicação dos itens:
A) O Naive Bayes é um bom classificador quando se assume independência condicional entre as features, mas ele não é o melhor em termos de interpretabilidade porque não fornece uma representação lógica clara das decisões tomadas.

B) As árvores de decisão, especialmente o algoritmo C4.5, são conhecidas por sua capacidade de gerar modelos com regras de decisão claras e compreensíveis para humanos, o que seria benéfico para a equipe de marketing. Além disso, C4.5 lida bem com variáveis categóricas.

C) As máquinas de vetores de suporte (SVM) são poderosas para encontrar a margem de decisão ótima e lidam bem com datasets de alta dimensão, mas elas não são conhecidas pela interpretabilidade dos seus resultados, que é uma exigência nesse cenário.

D) KNN pode não ser o mais adequado para um grande volume de dados devido ao seu custo computacional elevado em tempo de classificação e porque tende a ser menos interpretável em comparação com as árvores de decisão.

E) Florestas aleatórias são conjuntos de árvores de decisão e, embora possam melhorar a precisão das previsões, elas não são tão fáceis de interpretar quanto uma única árvore de decisão, pois consistem em um grande número de árvores e, portanto, de decisões combinadas.",6213517
tópico 5,"Avaliação de modelos de classificação: treinamento, teste, validação; validação cruzada","Questão: Em um cenário de aprendizado de máquina, a utilização de métodos de avaliação de modelos de classificação é fundamental para garantir a generalização do modelo para novos dados. Dentre os métodos de avaliação, a validação cruzada é amplamente utilizada e reconhecida pela sua eficácia em estimar a capacidade de generalização do modelo. Considerando a técnica de K-fold cross-validation:

Um cientista de dados opta por aplicar a estratégia de 5-fold cross-validation em seu modelo de classificação. Nesse contexto, assinale a opção que descreve corretamente o processo desta técnica de validação cruzada.

A) A base de dados é dividida em 5 conjuntos mutuamente exclusivos, e o modelo é treinado e testado 5 vezes, usando a cada iteração um conjunto diferente como teste e os demais como treinamento.
B) O modelo é treinado 5 vezes utilizando toda a base de dados e então é testado uma única vez em um conjunto de dados completamente novo e externo.
C) É realizada uma única divisão da base de dados, na qual 20% é usado para teste e o restante é utilizado para treinar o modelo 5 vezes consecutivas, sem alteração dos dados.
D) A base de dados é dividida em 5 conjuntos, onde, em cada iteração, 4 destes são usados para treinamento e o quinto é usado repetidamente como um conjunto de validação.
E) A base de dados é dividida aleatoriamente em 5 conjuntos e o modelo é treinado e testado 5 vezes, sendo que em cada iteração, um conjunto diferente é escolhido ao acaso para servir como teste e os restantes como treinamento.

",A,"

Explicação dos itens:

A) Correta. Descreve precisamente o processo de 5-fold cross-validation, onde a base de dados é dividida em 5 partes, e cada parte é usada uma vez como conjunto de teste e as outras quatro vezes como treinamento. Isso permite que cada instância da base de dados seja usada tanto para treinamento quanto para teste, uma vez ao longo do processo.

B) Incorreta. Sugere que o modelo só é testado após os 5 treinamentos, o que não é característico do método K-fold cross-validation, onde o teste ocorre em cada uma das K iterações.

C) Incorreta. Essa opção descreve um processo de validação holdout simples e não uma validação cruzada, onde deveriam ocorrer múltiplas divisões da base para treinamento e teste.

D) Incorreta. Implica incorretamente que o mesmo conjunto de validação é usado repetidamente, o que contraria a ideia da validação cruzada que exige que cada parte seja usada exatamente uma vez como conjunto de teste.

E) Incorreta. Embora descreva a ideia de subdivisão da base de dados e treinamento e teste múltiplos, a referência ao acaso na escolha dos conjuntos de teste a cada iteração não é precisa, já que em K-fold cross-validation cada parte é utilizada uma única vez como teste, sem aleatoriedade nessa escolha.",9220049
tópico 5,"Processamento de linguagem natural: Normalização textual - stop words, estemização, lematização e análise de frequência de termos;","Questão:
Na área de Processamento de Linguagem Natural (PLN), uma das etapas fundamentais para entender e processar textos naturais é a normalização textual. Esta etapa envolve técnicas como a remoção de palavras de parada (stop words), a estemização, a lematização e a análise de frequência de termos. Considerando estas técnicas, analise as seguintes afirmativas:

I. Stop words são palavras comuns em um texto (como preposições, conjunções e artigos) que costumam ser removidas durante a pré-processamento de dados em PLN, pois não contribuem de forma significativa para a compreensão do significado do texto.
II. Estemização refere-se ao processo de reduzir palavras ao seu radical ou forma base, não necessariamente resultando em uma palavra existente na língua, mas facilitando a correlação de palavras com significados semelhantes.
III. Lematização, ao contrário da estemização, busca transformar uma palavra em sua forma canônica ou de dicionário, considerando a sua morfologia e contexto, resultando sempre em uma palavra existente na língua.
IV. A análise de frequência de termos é um processo redundante na normalização textual, dado que tanto a estemização quanto a lematização já simplificam o texto a um conjunto reduzido de termos relevantes.

Qual(is) afirmativa(s) é(são) correta(s) em relação à normalização textual em PLN?

A) I e II apenas
B) I, II e III apenas
C) I, II, III e IV
D) I e III apenas
E) II e IV apenas

",B,"

Explicação dos itens:

A) Incorreta. Afirmativa III também está correta, portanto, resposta incompleta.

B) Correta. Afirmativas I, II e III são verdadeiras e descrevem corretamente os conceitos de stop words, estemização e lematização, respectivamente.

C) Incorreta. A afirmativa IV está errada porque a análise de frequência de termos não é redundante. Ela é uma técnica importante para identificar a importância de termos dentro de um conjunto de dados, mesmo após estemização e lematização.

D) Incorreta. Afirmativa II também está correta, pois descreve de forma acurada o que é estemização.

E) Incorreta. A afirmativa IV está errada conforme já explicado, e a afirmativa I é correta, o que não isenta o erro na IV.",1744308
tópico 5,"Rotulação de partes do discurso, part-of-speech tagging","Questão: Em processamento de linguagem natural, a técnica conhecida como ""Part-Of-Speech Tagging"" (POS Tagging), ou rotulação de partes do discurso, é fundamental para diversas aplicações, como análise sintática, reconhecimento de entidades nomeadas e tradução automática. Considere o seguinte conjunto de tags do Universal POS tags:

1. VERB - verbos (todos os tempos e modos)
2. NOUN - substantivos (comuns e próprios)
3. PRON - pronomes
4. ADJ - adjetivos
5. ADV - advérbios
6. ADP - preposições e pós-posições
7. CONJ - conjunções coordenativas e subordinativas
8. DET - determinantes
9. NUM - numerais
10. PRT - partículas ou palavras de função gramatical
11. X - outros

Ao analisar a sentença ""O pequeno gato dormia tranquilamente sobre o tapete"", que conjunto de tags do Universal POS corretamente identifica as partes do discurso presentes na sentença?

A) DET, ADJ, NOUN, VERB, ADJ, ADP, DET, NOUN
B) DET, VERB, NOUN, VERB, ADV, ADP, DET, NOUN
C) DET, ADJ, ADJ, VERB, ADV, ADP, NOUN, NOUN
D) DET, ADJ, NOUN, ADJ, VERB, ADP, DET, NOUN
E) DET, ADJ, NOUN, VERB, ADV, ADP, DET, NOUN

",E," 

A opção E é a correta, pois identifica de maneira acurada cada parte do discurso da sentença em questão:
- DET ""O"" é um determinante (artigo definido).
- ADJ ""pequeno"" é um adjetivo que qualifica o substantivo seguinte.
- NOUN ""gato"" é um substantivo, o sujeito da sentença.
- VERB ""dormia"" é um verbo que indica a ação realizada pelo sujeito.
- ADV ""tranquilamente"" é um advérbio que modifica o verbo, indicando a maneira como o gato dormia.
- ADP ""sobre"" é uma preposição que introduz a locução prepositiva, indicando a relação de lugar.
- DET ""o"" é um segundo determinante (artigo definido).
- NOUN ""tapete"" é um substantivo que completa o sentido da locução prepositiva.

As demais opções estão incorretas porque atribuem tags que não correspondem às partes do discurso referidas na sentença. Por exemplo, a opção A atribui erradamente a tag ADJ ao verbo ""dormia"", enquanto a opção B rotula incorretamente o adjetivo ""pequeno"" como um verbo, além de ignorar a preposição ""sobre"". A opção C incorretamente identifica dois adjetivos em sequência e não reconhece o advérbio ""tranquilamente"". Por fim, a opção D insere a tag ADJ onde deveria estar a tag VERB, mostrando uma má interpretação do papel de ""dormia"" na sentença.",514155
tópico 5,"Técnicas de agrupamento: Agrupamento por partição, por densidade e hierárquico","Questão:
Analise as seguintes afirmativas sobre as técnicas de agrupamento (clustering) e identifique a opção correta.

I - O agrupamento por partição, como o algoritmo K-Means, divide o conjunto de dados em um número pré-definido de grupos onde cada ponto pertence a um e apenas um grupo, baseado na minimização da soma das distâncias quadráticas de cada ponto ao centroide do grupo.

II - O agrupamento por densidade, tal como o DBSCAN (Density-Based Spatial Clustering of Applications with Noise), depende da definição prévia do número de grupos e é especialmente eficaz para identificar clusters de formatos irregulares, pois baseia-se na densidade de pontos em uma região do espaço de características.

III - O agrupamento hierárquico pode ser dividido em duas categorias: aglomerativo e divisivo. No modo aglomerativo, inicialmente cada ponto é considerado um cluster individual, e os pares de clusters são fundidos à medida que se sobe na hierarquia, enquanto no modo divisivo, inicia-se com um único cluster que progressivamente é dividido em clusters menores.

É correto o que se afirma em:

A) Apenas I
B) Apenas II
C) Apenas III
D) I e III
E) II e III

",D,"

Explicação dos itens:

I - Correto: O K-Means é um exemplo clássico de agrupamento por partição onde o número de grupos (clusters) precisa ser definido a priori e cada ponto é atribuído ao centroide mais próximo.

II - Incorreto: O DBSCAN não requer a especificação prévia do número de grupos, o que torna essa afirmativa incorreta. O método é, de fato, eficaz para identificar clusters com formatos irregulares, já que define clusters com base na densidade de pontos no espaço de características.

III - Correto: O agrupamento hierárquico é geralmente dividido em duas abordagens: aglomerativa, que começa com muitos pequenos clusters e os funde progressivamente, e divisiva, que começa com um único cluster englobando todos os pontos e o divide até que cada ponto seja um cluster individual, ou até atingir o nível de granularidade desejado.",4811085
tópico 5,Técnicas de regressão: Árvores de decisão para regressão; Máquinas de vetores de suporte para regressão,"Questão: No contexto da modelagem preditiva para a estimação de valores contínuos, existem diferentes técnicas que podem ser empregadas. Com relação às Árvores de Decisão para Regressão e Máquinas de Vetores de Suporte para Regressão (SVR - Support Vector Regression), analise as afirmativas a seguir e escolha a opção correta.

I. Árvores de Decisão para Regressão trabalham particionando o espaço de entrada em regiões distintas, onde para cada região é feita uma predição de valor constante.

II. O SVR tem como objetivo encontrar uma função que possua uma margem máxima de erro, permitindo que alguns erros ocorram dentro de um limite definido pela variável slack.

III. Enquanto o SVR é mais sensível a outliers, as Árvores de Decisão para Regressão são completamente imunes a eles, tornando-se a melhor escolha em todos os cenários com dados não filtrados.

IV. Uma vantagem das Árvores de Decisão é o fato de serem facilmente interpretáveis e poderem ser visualizadas graficamente, o que não é uma característica das Máquinas de Vetores de Suporte.

Escolha a alternativa que contém a(s) afirmativa(s) correta(s):

A) I, II e III

B) I, II e IV

C) Apenas II e III

D) Apenas I e IV

E) Todas as afirmativas estão corretas

",B,"

Explicação dos itens:

I. Correta. As Árvores de Decisão para Regressão particionam o espaço de entrada em regiões e fazem previsões de valores constantes para cada uma delas, sendo essa uma característica fundamental desses modelos.

II. Correta. O SVR busca uma função que tenha a maior margem possível, tolerando algumas previsões dentro de uma margem de erro controlada pela variável slack.

III. Incorreta. Embora as Árvores de Decisão para Regressão sejam menos sensíveis a outliers em comparação com algumas outras técnicas, elas não são completamente imunes a eles como a afirmativa sugere. Além disso, não são necessariamente a melhor escolha em todos os cenários, pois a escolha do modelo depende do contexto específico do problema.

IV. Correta. Uma das vantagens das Árvores de Decisão é que elas são modelos facilmente interpretáveis e que podem ser representados visualmente, facilitando a compreensão das decisões do modelo. SVR, sendo uma técnica baseada em geometria de vetores em espaços de alta dimensão, é geralmente menos intuitiva e mais difícil de visualizar.

Portanto, as afirmações I, II e IV estão corretas, tornando a alternativa B a correta.",8648486
tópico 5,Ajuste de modelos dentro e fora de amostra e overfitting,"Questão: Em estatística e aprendizado de máquina, o ajuste de modelos consiste em selecionar um modelo matemático a partir de uma classe de modelos e estimar os parâmetros do modelo usando dados. Quando realizamos o ajuste de um modelo, estamos sujeitos a diversos riscos, dentre eles o overfitting, que ocorre quando o modelo se ajusta excessivamente aos dados de treino, comprometendo sua capacidade de generalização para dados não observados (fora de amostra). Qual das seguintes estratégias é mais eficaz para minimizar o risco de overfitting?

A) Aumentar a complexidade do modelo adicionando mais parâmetros.
B) Utilizar todos os dados disponíveis para o treinamento do modelo, evitando separar uma parte para teste.
C) Selecionar o modelo com o menor erro de predição nos dados de treinamento.
D) Aplicar técnicas de regularização para penalizar a complexidade do modelo.
E) Evitar a validação cruzada, para reduzir a variabilidade na estimativa do erro de teste.

",D,"

Explicação dos itens:

A) Incorreto. Aumentar a complexidade do modelo pode levar a um ajuste excessivo aos dados de treino, exacerbando o problema de overfitting.
B) Incorreto. Sem separar uma parte dos dados para teste, torna-se impossível avaliar adequadamente a capacidade de generalização do modelo.
C) Incorreto. Um baixo erro de predição nos dados de treino não garante que o modelo terá um bom desempenho em dados fora da amostra.
D) Correto. Técnicas de regularização, como o Lasso ou Ridge, ajudam a evitar o overfitting ao penalizar a magnitude dos parâmetros, favorecendo modelos mais simples que têm melhor capacidade de generalização.
E) Incorreto. A validação cruzada é uma ferramenta importante para estimar o erro de teste e a robustez de um modelo, ajudando a evitar o overfitting por meio da avaliação de seu desempenho em vários conjuntos de dados de treino e teste.",2921748
tópico 5,"modelos vetoriais de documentos (booleano, TF e TF-IDF, média de vetores de palavras e Paragraph Vector);","Questão: No campo da recuperação de informação, os modelos vetoriais de documentos são amplamente utilizados para representar e comparar textos. Dentre os modelos abaixo, qual se destaca por incorporar não apenas a frequência de termos no documento, mas também a importância do termo no conjunto de dados:

A) Modelo Booleano
B) Modelo TF (Term Frequency)
C) Modelo TF-IDF (Term Frequency-Inverse Document Frequency)
D) Modelos de Média de Vetores de Palavras
E) Modelo Paragraph Vector

",C," 

Explicação:

A) O Modelo Booleano é baseado na lógica booleana e utiliza operadores como AND, OR e NOT para combinar termos de busca. Não considera a frequência dos termos nem a sua importância no conjunto de dados.

B) O Modelo TF (Term Frequency) leva em conta apenas a frequência dos termos no documento, sem ponderar a sua relevância em relação ao conjunto completo de documentos.

C) O Modelo TF-IDF (Term Frequency-Inverse Document Frequency) é o que melhor representa a importância de um termo considerando não apenas sua frequência no documento (TF), mas também penalizando termos comuns em muitos documentos através do IDF (Inverse Document Frequency). Por isso, ele é o modelo correto nesta questão.

D) Modelos de Média de Vetores de Palavras consideram a representação vetorial das palavras e a média desses vetores para representar o documento. Embora levem em conta aspectos semânticos das palavras, não ponderam diretamente a frequência dos termos e a sua importância em um conjunto maior de documentos.

E) O Modelo Paragraph Vector, também conhecido como Doc2Vec, representa documentos como vetores únicos em um espaço de características, onde é levado em conta o contexto das palavras nos documentos. No entanto, como o TF-IDF, não se foca diretamente na frequência dos termos cruzada com a sua importância no conjunto de dados.",3295832
tópico 5,"Avaliação de modelos de classificação: treinamento, teste, validação; validação cruzada","Questão: Dentre os métodos de avaliação de desempenho de modelos de classificação, a validação cruzada é uma técnica que visa mitigar problemas como o sobreajuste e fornecer uma estimativa mais confiável do poder de generalização do modelo. Considerando o método de validação cruzada k-fold, onde o conjunto de dados é dividido aleatoriamente em k subconjuntos mutuamente exclusivos de aproximadamente mesmo tamanho e o modelo é treinado e validado k vezes, cada vez utilizando um subconjunto diferente como conjunto de validação e os restantes como conjunto de treinamento, qual das opções abaixo descreve corretamente uma característica importante desse método?

A) A validação cruzada k-fold não é apropriada para conjuntos de dados pequenos, pois não permite uma estimativa confiável da performance do modelo.

B) Um dos benefícios da validação cruzada k-fold é que todos os exemplos do conjunto de dados são utilizados para treinamento e teste, garantindo que a avaliação do modelo aproveite o conjunto de dados integralmente.

C) A escolha do número de folds k é arbitrária e não influencia a estimativa da variância do modelo.

D) Quando k é igual ao número de exemplos no conjunto de dados, o método k-fold é equivalente à validação holdout, em que o conjunto é dividido apenas uma vez em conjuntos de treinamento e teste.

E) A validação cruzada k-fold pode ser executada apenas uma vez, pois repetições adicionais do processo não oferecem insights relevantes ao desempenho do modelo.

",B,"

Explicações:

A) Incorreta. A validação cruzada k-fold é, na verdade, bastante apropriada para conjuntos de dados pequenos, uma vez que maximiza o uso dos dados disponíveis para treinamento e teste.

B) Correta. Esta é uma das principais vantagens da validação cruzada k-fold. Cada exemplo será utilizado tanto para treinamento quanto para validação/teste exatamente uma vez, permitindo uma avaliação abrangente do desempenho do modelo.

C) Incorreta. A escolha de k tem um impacto significativo na estimativa do desempenho do modelo. Valores pequenos de k podem levar a uma estimativa enviesada e com alta variância, enquanto valores grandes demais podem ser computacionalmente custosos e levar a estimativas com baixa variância, mas possivelmente enviesadas devido ao menor tamanho de cada conjunto de treinamento.

D) Incorreta. Quando k é igual ao número de exemplos, o método k-fold é conhecido como Leave-One-Out Cross-Validation (LOOCV), não sendo equivalente ao método holdout.

E) Incorreta. Repetições da validação cruzada k-fold podem fornecer uma avaliação mais confiável do desempenho do modelo, especialmente se o conjunto de dados não for homogêneo. Repetições podem ajudar a entender a variabilidade e a estabilidade do modelo.",3451111
tópico 5,Técnicas de redução de dimensionalidade: Seleção de características (feature selection); Análise de componentes principais (PCA – principal component analysis),"Questão: Na análise de grandes conjuntos de dados, frequentemente nos deparamos com a maldição da dimensionalidade, que pode levar a um aumento no tempo de processamento e a dificuldades no desempenho de modelos de aprendizado de máquina. Abordagens como Seleção de Características (Feature Selection) e Análise de Componentes Principais (PCA) são utilizadas para reduzir a dimensionalidade dos dados. Considerando essas técnicas, assinale a alternativa correta:

A) A Seleção de Características é um método supervisionado que consiste em escolher um subconjunto de características relevantes, enquanto PCA é uma técnica não supervisionada que transforma as variáveis originais em novas variáveis ortogonais chamadas componentes principais.

B) PCA é um método supervisionado que pode ser aplicado somente quando as variáveis originais são correlacionadas linearmente e a seleção de características é utilizada quando as variáveis são independentes.

C) A Seleção de Características e PCA têm o mesmo princípio operacional: ambos selecionam as variáveis mais relevantes e descartam as demais com base nos valores dos autovalores da matriz de covariância.

D) PCA é capaz de manter a interpretabilidade dos dados após a transformação, já que as componentes principais são uma combinação linear das variáveis originais, enquanto na Seleção de Características perde-se a interpretabilidade das características selecionadas.

E) Tanto PCA quanto Seleção de Características são capazes de aumentar a acurácia dos modelos de aprendizado de máquina de maneira garantida, independentemente do tipo de dados e da estrutura do problema.

",A,"

Explicação dos itens:

A) Este item está correto. A Seleção de Características é geralmente um processo supervisionado que escolhe um subconjunto de características originais, mantendo apenas as mais relevantes para o problema em questão. Por outro lado, PCA é uma técnica não supervisionada que reduz a dimensionalidade ao projetar os dados em novas direções (componentes principais) que maximizam a variância, sem considerar quaisquer rótulos ou resultados.

B) Este item é incorreto. PCA é uma técnica não supervisionada e não requer que as variáveis originais estejam correlacionadas linearmente; ele procurará as direções que maximizam a variância independentemente. A Seleção de Características pode ser aplicada tanto em variáveis correlacionadas quanto em independentes.

C) Este item é incorreto. Embora o PCA envolva autovalores da matriz de covariância para encontrar direções de máxima variância, isso não é equivalente a selecionar as variáveis mais relevantes. A Seleção de Características funciona de maneira diferente, escolhendo um subconjunto das características originais com base em sua relevância, não em autovalores.

D) Este item é incorreto. Na verdade, após a aplicação do PCA, as novas variáveis (componentes principais) são menos interpretáveis porque são combinações lineares das variáveis originais. Na Seleção de Características, as variáveis selecionadas mantêm sua interpretabilidade original porque o subconjunto consiste nas características originais sem alteração.

E) Este item é incorreto. Não existe garantia de que os métodos de redução de dimensionalidade aumentarão a acurácia dos modelos. A eficácia da redução de dimensionalidade depende muito do tipo de dados e do problema específico. Em alguns casos, a redução pode até prejudicar o desempenho se informações importantes forem perdidas no processo.",345372
tópico 5,Técnicas de classificação: Naive Bayes; Árvores de decisão (algoritmos ID3 e C4.5); Florestas aleatórias (random forest); Máquinas de vetores de suporte (SVM – support vector machines); K vizinhos mais próximos (KNN – K-nearest neighbours),"Questão:

Em aprendizado de máquina, diversas técnicas de classificação são utilizadas para modelar e prever dados categóricos. Entre essas técnicas, temos o Naive Bayes, as Árvores de Decisão com os algoritmos ID3 e C4.5, as Florestas Aleatórias (Random Forest), as Máquinas de Vetores de Suporte (Support Vector Machines - SVM) e o K Vizinhos Mais Próximos (KNN - K-nearest neighbors). Analisando as características dessas técnicas, avalie as afirmativas a seguir:

I - O algoritmo Naive Bayes assume independência condicional entre os preditores e é especialmente eficiente para grandes volumes de dados.
II - Árvores de decisão geradas através do algoritmo ID3 não podem tratar dados contínuos diretamente, exigindo uma etapa de discretização dos dados.
III - Florestas Aleatórias consistem em um conjunto de árvores de decisão independentes, tendo como principal vantagem a redução da variância do modelo em comparação com árvores de decisão isoladas.
IV - O algoritmo SVM é útil para dados de alta dimensão e busca encontrar o hiperplano com a maior margem entre as classes, porém pode ser ineficiente com grandes volumes de dados.
V - KNN é um algoritmo que não requer um modelo explícito para classificação, dependendo significativamente da escolha do parâmetro K e da medida de distância utilizada.

Assinale a opção que contém todas as afirmativas corretas:

a) I, II e III
b) I, III e V
c) II, IV e V
d) I, II, III e V
e) Todas as afirmativas estão corretas

",B,"

Explicação dos itens:

I - Correto. Naive Bayes assume independência entre os preditores (variáveis explicativas) condicionalmente à classe, o que simplifica os cálculos, tornando o algoritmo eficiente, especialmente para grandes volumes de dados.

II - Incorreto. Enquanto o ID3 trabalha com atributos categóricos, o C4.5 (uma evolução do ID3), pode tratar dados contínuos diretamente, convertendo-os em testes categóricos.

III - Correto. Florestas Aleatórias são um ensemble de árvores de decisão, ajudando a melhorar a precisão e controlar o overfitting, reduzindo a variância do modelo através da média das previsões das árvores individuais.

IV - Incorreto. É verdade que o SVM lida bem com dimensões elevadas e procura o hiperplano com a maior margem, mas o desempenho com grandes volumes de dados não é necessariamente ineficiente; depende da implementação e da otimização do algoritmo.

V - Correto. O KNN opera com base na proximidade entre as instâncias de dados e não exige um modelo prévio, a escolha de K e da métrica de distância tem um impacto significativo na performance do algoritmo.",4216241
tópico 5,"métricas de avaliação - matriz de confusão, acurácia, precisão, revocação, F1-score e curva ROC","Questão: Em problemas de classificação no âmbito do aprendizado de máquina, avaliar o desempenho do modelo é fundamental para entender o quão bem ele poderá generalizar para dados não vistos. Um analista de dados utilizou várias métricas de desempenho para avaliar seu modelo de classificação binária. Dado a matriz de confusão a seguir, onde Positivo (P) representa a classe de interesse e Negativo (N) representa a outra classe:

```
               Predito
               P       N
Real P       80       20
     N       30      120
```
Com base nessa matriz de confusão, assinale a opção que CORRETAMENTE apresenta o cálculo da Precisão (ou Precisão Positiva):

A) Precisão = 80 / (80 + 20)

B) Precisão = 80 / (80 + 30)

C) Precisão = (80 + 120) / (80 + 20 + 30 + 120)

D) Precisão = 120 / (30 + 120)

E) Precisão = (80 + 30) / (80 + 120)

",B,"

Explicação dos itens:

A) Incorreto. Esta opção representa uma confusão com a medida de Acurácia, mas aqui o numerador e denominador estão trocados.

B) Correto. Precisão é calculada como o número de verdadeiros positivos (80) dividido pela soma dos verdadeiros positivos e falsos positivos (80 + 30), o que resulta em 80 / (80 + 30).

C) Incorreto. Esta opção corresponde ao cálculo da Acurácia, que consiste no total de predições corretas (verdadeiros positivos e verdadeiros negativos) dividido pelo total de predições.

D) Incorreto. Este é o cálculo da precisão para a classe Negativa, também conhecida como especificidade.

E) Incorreto. Esta opção não representa nenhuma das métricas padrão conhecidas na avaliação de modelos de classificação.",6243463
tópico 5,"Modelos de representação de texto - N-gramas, modelos vetoriais de palavras (CBOW, Skip-Gram e GloVe)","Questão: No processamento de linguagem natural, diferentes modelos de representação de texto têm sido propostos para capturar o significado semântico e as relações contextuais presentes na linguagem humana. Dentre estes, os modelos vetoriais de palavras como CBOW (Continuous Bag of Words), Skip-Gram e GloVe (Global Vectors for Word Representation) são amplamente utilizados. Com base no entendimento desses modelos, considere as seguintes afirmações e assinale a opção correta.

I - O modelo CBOW prevê uma palavra-alvo com base no contexto ao redor, enquanto o Skip-Gram prevê o contexto a partir de uma palavra-alvo.

II - O modelo Skip-Gram tende a ser mais eficiente do que o CBOW para conjuntos de dados menores e palavras com frequência baixa.

III - GloVe objetiva unir a força dos modelos baseados em matrizes de co-ocorrência com a eficiência dos modelos baseados em aprendizado de previsões contextuais, como o CBOW e o Skip-Gram.

IV - Em um modelo Skip-Gram, a janela de contexto se restringe a palavras imediatamente adjacentes à palavra-alvo sem qualquer flexibilidade na quantidade de palavras consideradas.

V - N-gramas e modelos vetoriais compartilham a mesma abordagem na representação semântica de palavras, uma vez que ambos consideram a ordem exata das palavras no texto.

A) Apenas as afirmações I e II estão corretas.
B) Apenas as afirmações I, II e III estão corretas.
C) Apenas as afirmações II, III e IV estão corretas.
D) Apenas as afirmações I, III e IV estão corretas.
E) Todas as afirmações estão corretas.

",B,"

Explicação:

I - Correta. O modelo CBOW tenta prever uma palavra-alvo com base no contexto em sua volta, enquanto o Skip-Gram funciona na direção oposta, tentando prever o contexto, isto é, as palavras ao redor de uma palavra-alvo.

II - Correta. Skip-Gram geralmente é melhor que o CBOW para tratar conjuntos de dados que são menores e contêm palavras raras ou de frequência baixa, pois foca na previsão do contexto de palavras individuais, o que amplifica o sinal de palavras menos frequentes.

III - Correta. O modelo GloVe visa combinar os aspectos positivos da fatorialização de matrizes de co-ocorrência com os modelos de predição de palavras no contexto para produzir representações vetoriais que capturem as relações semânticas numa escala global.

IV - Incorreta. A janela de contexto no modelo Skip-Gram é ajustável e pode ser configurada para considerar várias palavras próximas à palavra-alvo, não apenas as imediatamente adjacentes.

V - Incorreta. N-gramas são baseados na contiguidade e sequência fixa de n palavras, enquanto os modelos vetoriais buscam representar palavras em um espaço contínuo, onde o significado é inferido a partir das relações com outras palavras, não se concentrando na ordem exata em um texto.",9080269
tópico 5,"Métricas de similaridade textual - similaridade do cosseno, distância euclidiana, similaridade de Jaccard, distância de Manhattan e coeficiente de Dice","Questão:
Considere um sistema de recomendação de textos que busca maximizar a relevância dos documentos sugeridos a um usuário com base na similaridade textual entre os documentos e as consultas do usuário. Cinco métricas diferentes de similaridade foram testadas: similaridade do cosseno, distância euclidiana, similaridade de Jaccard, distância de Manhattan e coeficiente de Dice. Qual das seguintes afirmações é correta a respeito dessas métricas, considerando que estamos trabalhando no contexto de espaço vetorial de termos e documentos?

A) A similaridade do cosseno e a distância euclidiana são equivalentes para comparação de textos normalizados, onde a dimensão dos vetores é a frequência dos termos.
B) A distância de Manhattan aumenta conforme a diferença entre os vetores de características aumenta e é mais suscetível a outlier em comparação com a similaridade do cosseno.
C) A similaridade de Jaccard é apropriada apenas para dados binários e não considera a magnitude da frequência com que os termos aparecem nos documentos.
D) O coeficiente de Dice e a similaridade do cosseno tendem a dar resultados idênticos independentemente do tipo de dados ou da ponderação aplicada aos termos nos vetores.
E) A distância euclidiana e a distância de Manhattan são métricas que proporcionam valores de similaridade, onde valores mais altos indicam maior similaridade.

",C,"

Alternativa A: Incorreta porque a similaridade do cosseno avalia o ângulo entre os vetores, enquanto a distância euclidiana mede a distância ""direta"" entre eles, podendo ser afetada pela magnitude dos vetores, mesmo que os ângulos sejam semelhantes.

Alternativa B: Incorreta porque embora a afirmação de que a distância de Manhattan pode ser sensível a outliers seja verdadeira, a comparação com a similaridade do cosseno parece colocar a similaridade do cosseno em desvantagem, que é uma métrica com bom desempenho em alta dimensionalidade.

Alternativa C: Correta, uma vez que a similaridade de Jaccard é calculada com base na interseção sobre a união de dois conjuntos de itens e não considera a frequência com que estes itens ocorrem, sendo portanto aplicável para comparar conjuntos binários (presença/ausência de termos).

Alternativa D: Incorreta porque, apesar do coeficiente de Dice ser semelhante à similaridade do cosseno, eles podem dar resultados diferentes dependendo da ponderação dos termos e do tipo de dados, já que o Dice dá maior ênfase à interseção.

Alternativa E: Incorreta pois tanto a distância euclidiana quanto a distância de Manhattan são métricas de distância, não de similaridade, o que significa que valores menores indicam maior similaridade, e não o contrário.",2112838
tópico 5,Redes neurais convolucionais e recorrentes,"Questão: No contexto do aprendizado profundo, as redes neurais convolucionais (CNNs) e as redes neurais recorrentes (RNNs) são estruturas amplamente utilizadas para lidar com diferentes tipos de dados e tarefas. Qual das seguintes afirmações descreve corretamente uma das principais diferenças entre CNNs e RNNs?

A) As CNNs são mais adequadas para o processamento de sequências temporais devido à sua capacidade de capturar dependências temporais.

B) As RNNs têm a capacidade de tratar dados de entrada de tamanho variável, enquanto as CNNs requerem um tamanho de entrada fixo.

C) As CNNs geralmente são aplicadas em dados tabulares onde as relações entre as colunas precisam ser aprendidas em profundidade.

D) As RNNs são apropriadas para tarefas envolvendo percepção visual, como reconhecimento de imagens e classificação, graças à sua estrutura em camadas.

E) As CNNs são particularmente eficientes para extrair características hierárquicas espaciais em imagens devido à operação de convolução que exploram.

",E," 

Explicações dos itens:

A) As CNNs, ao contrário do que o item sugere, são mais adequadas para processamento de imagens e reconhecimento visual, enquanto as RNNs são mais utilizadas para sequências temporais devido à sua natureza recorrente.

B) As RNNs, de fato, têm a capacidade de lidar com dados de entrada de tamanho variável porque levam em conta a sequência, independentemente do seu comprimento. Contudo, as CNNs também podem ser adaptadas para tratar entradas de tamanho variável usando técnicas como redes totalmente convolucionais.

C) As CNNs não são tipicamente aplicadas a dados tabulares. Esse tipo de dado frequentemente não possui a estrutura espacial que as CNNs são projetadas para explorar. As redes neurais densas ou redes específicas para dados tabulares, como as redes neurais baseadas em transformadores, são mais apropriadas para essa tarefa.

D) Esse item inverte as aplicações de CNNs e RNNs. As RNNs são mais adequadas para tarefas como modelagem de linguagem e sequências temporais, enquanto as CNNs são utilizadas para percepção visual devido à operação de convolução que é boa para identificar padrões locais em imagens.

E) Correto. As CNNs são especialmente eficazes para tarefas de visão computacional, como reconhecimento de imagem e classificação, pois conseguem extrair características hierárquicas espaciais devido às suas operações de convolução e pooling.",7426778
tópico 5,"Modelos de representação de texto - N-gramas, modelos vetoriais de palavras (CBOW, Skip-Gram e GloVe)","Questão: Em relação aos modelos de representação de texto em processamento de linguagem natural, considere as seguintes afirmações sobre os métodos N-gramas e modelos vetoriais de palavras:

I. O modelo N-grama considera uma sequência de N itens como representativa do contexto textual, onde N pode ser o número de palavras ou caracteres que formam a sequência.

II. Continuous Bag of Words (CBOW) e Skip-Gram são modelos vetoriais provenientes da arquitetura do Word2Vec, utilizados para capturar o contexto semântico das palavras em um espaço de baixa dimensão.

III. CBOW prevê a palavra atual com base nas palavras de contexto, enquanto Skip-Gram prediz as palavras de contexto a partir da palavra atual.

IV. GloVe opera unicamente em escala local, aprendendo padrões de co-ocorrência específicos para a janela de contexto em questão, sem considerar a estatística global do corpus.

Com base nas afirmações acima, assinale a opção correta:

A) Todas as afirmações estão corretas.
B) Apenas as afirmações I, II e III estão corretas.
C) Apenas as afirmações I, II e IV estão corretas.
D) Apenas as afirmações I e III estão corretas.
E) Apenas as afirmações II e IV estão corretas.

",B,"

Explicação:

Item I: Correto. Os modelos N-gramas são baseados em sequências contíguas de N itens (sendo esses itens palavras, letras, sílabas ou fonemas) extraídos de um texto. O contexto da sequência de palavras é considerado para tentar prever ou entender o próximo item ou conjunto de itens na sequência.

Item II: Correto. CBOW e Skip-Gram são dois modelos introduzidos pelo framework Word2Vec. Eles funcionam ao aprender representações vetoriais de palavras de modo a prever palavras com base em seu contexto (CBOW) ou prever o contexto com base em uma palavra (Skip-Gram), resultando em vetores de baixa dimensão.

Item III: Correto. O modelo CBOW usa um conjunto de palavras de contexto para prever uma palavra-alvo, enquanto o modelo Skip-Gram faz o inverso, utilizando uma palavra-alvo para prever as palavras de contexto.

Item IV: Incorreto. O modelo Global Vectors for Word Representation (GloVe) é projetado para combinar as vantagens dos métodos baseados em contagem de co-ocorrências globais (estatística de todo o corpus) e os métodos de predição local (janela de contexto imediata). Portanto, o GloVe opera nos níveis local e global.",7152326
tópico 5,Redes neurais convolucionais e recorrentes,"Questão:
A aplicação de Redes Neurais Artificiais (RNAs) tem se intensificado em diversos campos da ciência e da tecnologia, sendo particularmente notável em áreas como visão computacional e processamento de linguagem natural. Redes Neurais Convolucionais (CNNs) e Redes Neurais Recorrentes (RNNs) são dois subtipos de RNAs que possuem características e aplicações específicas. Considerando as peculiaridades de cada arquitetura, analise as seguintes assertivas e assinale a opção correta:

I. CNNs são especialmente adequadas para processamento e análise de imagens, uma vez que sua estrutura é projetada para capturar a hierarquia espacial de características dentro de um quadro.
II. RNNs são capazes de processar sequências de dados de comprimento variável e manter um estado interno que reflete a informação sequencial, sendo ideais para tarefas como tradução de linguagem ou previsão de séries temporais.
III. A presença de unidades de memória de longo prazo, como as LSTM (Long Short-Term Memory), não é relevante para o desempenho das RNNs no processamento de sequências longas e na manutenção de informações relevantes ao longo do tempo.
IV. CNNs frequentemente requerem mais dados de treino que as RNNs devido à maior complexidade das tarefas de visão computacional quando comparadas às de processamento de sequência de texto.

A) Apenas I e II estão corretas.
B) Apenas II e III estão corretas.
C) Apenas I, II e IV estão corretas.
D) Apenas II e IV estão corretas.
E) Todas estão corretas.

",A,"

Explicação dos itens:
I. Correto. As CNNs possuem uma arquitetura que é projetada especificamente para processar dados com uma grade bidimensional, como imagens, e são capazes de preservar as relações espaciais e hierárquicas entre os pixels.

II. Correto. As RNNs são projetadas para trabalhar com sequências de dados e possuem a característica de manter um estado interno ou ""memória"" que captura informações sobre os elementos anteriores da sequência, tornando-as adequadas para tarefas como reconhecimento de fala e análise de texto.

III. Incorreto. A presença de mecanismos como LSTM nas RNNs é de fato relevante e melhora sua capacidade de aprender dependências de longo prazo em sequências de dados, o que é crucial para aplicações como previsão de séries temporais e modelagem de linguagem.

IV. Incorreto. Não é possível afirmar categoricamente que CNNs requerem mais dados que as RNNs. A quantidade de dados de treino necessária depende da complexidade da tarefa e da arquitetura específica usada. Ambos os tipos de rede podem demandar grandes conjuntos de dados dependendo do problema a ser resolvido.

Logo, as afirmativas I e II estão corretas, fazendo da opção A a alternativa correta.",9502777
tópico 5,Técnicas de redução de dimensionalidade: Seleção de características (feature selection); Análise de componentes principais (PCA – principal component analysis),"Questão: Em aprendizado de máquina, as técnicas de redução de dimensionalidade são cruciais para melhorar a eficiência dos modelos e lidar com o problema da maldição da dimensionalidade. Dentre essas técnicas, a Seleção de Características e a Análise de Componentes Principais (PCA) são bastante conhecidas. Acerca dessas técnicas, avalie as assertivas abaixo e selecione a opção correta:

I. A Seleção de Características consiste em identificar e selecionar um subconjunto das características mais relevantes do conjunto original de dados, reduzindo assim a complexidade do modelo sem alteração significativa do espaço de características original.

II. PCA é um método de transformação linear que projeta os dados em um novo espaço de características, criando assim componentes não correlacionados que explicam a máxima variação nos dados.

III. Enquanto a Seleção de Características é uma técnica de redução de dimensionalidade que busca preservar a interpretabilidade das variáveis originais, PCA gera novos atributos que muitas vezes não possuem uma interpretação direta em termos das variáveis originais.

IV. PCA pode ser aplicada a qualquer tipo de dado, independente da presença de correlação entre as características, uma vez que não se baseia em correlação para a redução de dimensionalidade.

Está correto o que se afirma em:

A) I e III, apenas.
B) I, II e III, apenas.
C) II e IV, apenas.
D) I, II, III e IV.
E) I e II, apenas.

",B,"

Explicação dos itens:

I. Correto. A Seleção de Características realmente busca identificar as características mais relevantes do conjunto original, com o objetivo de reduzir o número de variáveis e simplificar o modelo sem perder informações críticas.

II. Correto. A PCA busca as direções de maior variância nos dados e projeta os dados nessas direções, gerando componentes principais que são ortogonais (não correlacionados) entre si.

III. Correto. A Seleção de Características mantém as variáveis originais, tornando a interpretação mais direta. PCA, por outro lado, cria uma nova representação dos dados com componentes que são combinações lineares das variáveis originais e, em geral, perde-se a interpretação direta das variáveis originais.

IV. Incorreto. PCA é um método que assume que há correlação entre as características para identificar as direções de maior variância. Se não houver correlação ou se a correlação for fraca, os primeiros componentes principais podem não capturar uma quantidade significativa da variabilidade dos dados, tornando a PCA menos efetiva.",1528354
tópico 5,"Rotulação de partes do discurso, part-of-speech tagging","Questão: Em processamento de linguagem natural (PLN), a rotulação de partes do discurso, comumente referida como part-of-speech tagging (POS tagging), é uma etapa fundamental na análise de texto. Esta técnica envolve a atribuição de categorias gramaticais a cada palavra em um texto, como substantivos, verbos, adjetivos, etc. Considerando os algoritmos utilizados para POS tagging, é correto afirmar que:

A) Modelos baseados em gramáticas prescritivas são os mais utilizados atualmente devido à sua alta precisão nas línguas naturais.
B) Algoritmos de aprendizado de máquina não supervisionados não são aplicáveis para POS tagging, pois exigem anotações manuais prévias das categorias nas sentenças.
C) Abordagens baseadas em regras podem falhar em contextos ambíguos, onde uma palavra pode pertencer a mais de uma categoria gramatical.
D) Métodos estatísticos, como as cadeias de Markov ocultas (HMM), não utilizam informações contextuais para determinar as categorias gramaticais.
E) Técnicas de deep learning, como as Redes Neurais Recorrentes (RNN), são inadequadas para tarefas de POS Tagging por não conseguirem captar sequências longas de dependências textuais.

",C," 
Modelos baseados em regras funcionam bem até certo ponto, mas enfrentam dificuldades com a complexidade inerente e a ambiguidade das línguas naturais, onde uma palavra pode ter múltiplas categorias gramaticais dependendo do contexto em que é utilizada. Por exemplo, ""gosto"" pode ser um substantivo (""O gosto do doce é bom"") ou um verbo (""Eu gosto de doce""). Alternativa A é incorreta pois, embora as gramáticas prescritivas sejam precisas, elas não são as mais utilizadas devido à sua inflexibilidade e à dificuldade de cobrir todas as nuances da linguagem. Alternativa B é incorreta já que algoritmos de aprendizado de máquina não supervisionados como word embeddings podem ser utilizados em POS tagging sem anotações manuais prévias. Alternativa D está erroneamente indicando que HMMs não usam informações contextuais, mas eles de fato levam em conta o contexto ao atribuir categorias gramaticais. Alternativa E subestima as RNNs, uma vez que elas são eficazes para capturar dependências de longo alcance e têm sido aplicadas com sucesso em tarefas de POS tagging.",3897372
tópico 5,Técnicas de regressão: Árvores de decisão para regressão; Máquinas de vetores de suporte para regressão,"Questão:
Numa análise comparativa de modelos para previsão de demanda energética de um setor industrial, dois tipos de técodos de regressão foram considerados: Árvores de Decisão para Regressão e Máquinas de Vetores de Suporte para Regressão (SVR - Support Vector Regression). Avaliando as características destes modelos, qual dos seguintes itens CORRETAMENTE descreve uma vantagem do uso de Máquinas de Vetores de Suporte para Regressão (SVR) em relação às Árvores de Decisão para Regressão?

A) SVR geralmente possui um tempo de treinamento mais rápido devido à simplicidade do modelo.
B) SVR tende a ser menos susceptível a sobreajustes em comparação com Árvores de Decisão.
C) Árvores de Decisão para Regressão são mais indicadas que SVR quando o conjunto de dados possui muitos recursos (features) esparsos.
D) SVR requer menos pré-processamento dos dados quando comparado com Árvores de Decisão para Regressão.
E) Árvores de Decisão capacitam modelos a serem atualizados facilmente com novos dados, o que não é possível com SVR.

",B,"

Alternativas:
A) Incorreta. Máquinas de Vetores de Suporte para Regressão (SVR) são baseadas na otimização quadrática, o que pode levar a um tempo de treinamento mais longo, especialmente em grandes conjuntos de dados.
B) Correta. Um dos pontos fortes do SVR é a sua capacidade de generalizar bem e, portanto, ser menos propenso a sobreajuste (overfitting), graças à sua natureza de maximização da margem e ao uso de regularização.
C) Incorreta. Essa afirmação está ao contrário; geralmente, SVR lida melhor com conjuntos de dados de alta dimensionalidade devido à aplicação de técnicas como o truque do kernel.
D) Incorreta. Tanto SVR quanto Árvores de Decisão podem requerer pré-processamento, como normalização dos dados. No entanto, SVR pode ser mais sensível à escala dos dados devido ao seu princípio de otimização.
E) Incorreta. Essa afirmação está ao contrário; Árvores de Decisão são, de fato, mais fáceis de atualizar com novos dados, pois permitem a inserção incremental de pontos de dados, enquanto que SVR geralmente requer um re-treino do modelo completo.",2966294
tópico 5,Ajuste de modelos dentro e fora de amostra e overfitting,"Questão:
 
A técnica de ajuste de modelos em aprendizado de máquina e estatística envolve a otimização dos parâmetros do modelo de modo a melhor capturar a relação existente entre as variáveis independentes e a variável dependente. No entanto, um problema comum encontrado é o overfitting, no qual o modelo ajusta-se perfeitamente aos dados da amostra (in-sample) mas não generaliza bem para novos dados (out-of-sample). Com base nesse conhecimento, considere as seguintes afirmativas a respeito do ajuste de modelos e identifique a opção correta:

I. A validação cruzada é uma técnica que pode ajudar a mitigar o overfitting, permitindo que o modelo seja testado com múltiplos conjuntos de treinamento e validação extraídos da mesma amostra. 
II. Um modelo com alto overfitting pode ter um desempenho notavelmente bom fora da amostra, pois suas especificações complexas permitem uma adaptação precisa a qualquer conjunto de dados.
III. A complexidade do modelo deve ser regulada de forma a equilibrar o erro de treinamento e o erro de validação, evitando assim o trade-off entre viés e variância.
IV. Um modelo com overfitting é caracterizado por um baixo erro de treinamento e um alto erro de validação, sinalizando que o modelo pode não ser capaz de generalizar bem para dados não vistos.

A opção que contém todas as afirmativas verdadeiras é:

a) I e II
b) I e III
c) II e IV
d) III e IV
e) I e IV

",B," 

Explicação dos itens:

I. Verdadeira. A validação cruzada é uma técnica amplamente utilizada para avaliar a capacidade de generalização de um modelo, utilizando diferentes partes da amostra em iterações distintas.

II. Falsa. Overfitting caracteriza-se por uma alta performance na amostra de treinamento, mas uma performance pobre em dados não vistos (fora da amostra). Isso ocorre porque o modelo se torna excessivamente complexo e captura o ruído específico dos dados de treinamento, ao invés das verdadeiras tendências subjacentes.

III. Verdadeira. O ajuste de modelo ideal envolve encontrar um ponto ótimo entre viés (underfitting) e variância (overfitting), o que é muitas vezes referido como o trade-off viés-variância.

IV. Verdadeira. A discrepância entre um baixo erro de treinamento e um alto erro de validação é um indicativo clássico de overfitting, sugerindo que o modelo está memorizando os dados de treinamento específicos, em vez de aprender padrões generalizáveis.

Assim, a resposta correta é o item b), que lista as afirmativas I e III como verdadeiras e descarta a afirmativa II, que está incorreta. O item IV também é verdadeiro e está correto ao mencionar as características de um modelo com overfitting.",4631738
tópico 5,Técnicas de classificação: Naive Bayes; Árvores de decisão (algoritmos ID3 e C4.5); Florestas aleatórias (random forest); Máquinas de vetores de suporte (SVM – support vector machines); K vizinhos mais próximos (KNN – K-nearest neighbours),"Questão: Uma empresa está implementando um sistema de classificação para automatizar a filtragem de e-mails, identificando e separando as mensagens que são consideradas spam. Para isso, ela está considerando diferentes algoritmos de aprendizado de máquina para criar um modelo eficiente. Considerando as características de cada técnico de classificação, escolha a opção correta que destaca adequadamente um algoritmo e uma característica única ou vantajosa para a tarefa em questão:

A) O algoritmo Naive Bayes seria inadequado para tal tarefa, pois assume independência condicional entre as características, o que não é realista em textos.
B) As árvores de decisão, criadas pelos algoritmos ID3 e C4.5, são uma escolha ruim devido à sua incapacidade de lidar com atributos contínuos, sendo aplicáveis apenas a categorias discretas.
C) As florestas aleatórias (random forest) não seriam eficientes no processo de classificação de spam devido à sua natureza de ensemble, que exige mais poder computacional e é mais apropriada para conjuntos de dados de grandes dimensões.
D) Máquinas de vetores de suporte (SVM) são altamente recomendadas para a classificação de textos, como e-mails, por conta de sua eficiência em espaços de alta dimensão e capacidade de lidar com margens de separação não lineares.
E) O método de K vizinhos mais próximos (KNN) não seria prático para a filtragem de spam, uma vez que esse algoritmo requer a comparação direta com todos os exemplos no conjunto de dados, o que é inviável para uma base de e-mails em constante crescimento devido à demanda de tempo para classificação.

",D,"

Explicação dos itens:
A) Incorreto. Na verdade, o algoritmo Naive Bayes é frequentemente utilizado para a classificação de textos, incluindo filtragem de spam, justamente por ser eficiente mesmo com a suposição de independência condicional.
B) Incorreto. As árvores de decisão podem sim lidar com atributos contínuos, e os algoritmos como ID3 e C4.5 são projetados para tratar tanto de características discretas quanto contínuas, usando métodos como a binarização.
C) Incorreto. As florestas aleatórias são eficazes e robustas para tarefas de classificação, incluindo spam. O aumento no poder computacional pode ser compensado pela precisão e pela redução do sobreajuste proporcionados por essa técnica.
D) Correto. SVM é considerado um dos algoritmos de melhor desempenho para a categorização de textos, devido à sua eficácia em altas dimensões e à flexibilidade em encontrar fronteiras de decisão ótimas entre as diferentes classes.
E) Incorreto. Enquanto o KNN pode ser uma escolha inviável para conjuntos de dados muito grandes devido à sua computação intensiva na fase de classificação, o método não é descartado categoricamente; a questão está mais relacionada à escalabilidade e eficiência conforme o conjunto de dados cresce.",4651144
tópico 5,"Avaliação de modelos de classificação: treinamento, teste, validação; validação cruzada","Questão: A respeito da avaliação de modelos de classificação em aprendizado de máquina, uma etapa crucial é a validação do modelo, que visa garantir que o modelo generalize bem para novos dados, evitando problemas como overfitting ou underfitting. Diferentes técnicas são empregadas para alcançar este objetivo. Supondo que você esteja trabalhando com um conjunto de dados limitado e deseje uma estimativa robusta da performance do seu modelo de classificação, qual das seguintes técnicas de validação abaixo seria mais apropriada para utilizar?

A) Utilizar um único split de treinamento e teste, aplicando o modelo apenas uma vez nos dados de teste.
B) Empregar uma validação cruzada leave-one-out, onde cada exemplo é usado uma vez como dado de teste enquanto os outros formam o conjunto de treinamento.
C) Realizar apenas o treinamento do modelo no conjunto completo de dados, sem separar um conjunto de teste.
D) Subdividir os dados em um conjunto de treinamento e um conjunto de validação, sem incluir um conjunto de teste separado.
E) Alternar semanalmente entre diferentes conjuntos de treinamento e teste para avaliar a performance do modelo.

",B," 

Explicação dos itens: 

A) Esta alternativa não é ideal pois forneceria apenas uma estimativa da performance do modelo com base em uma única divisão de dados, podendo não ser representativa da capacidade do modelo de generalizar.

B) A validação cruzada leave-one-out é uma abordagem adequada especialmente para conjuntos de dados limitados, pois maximiza o uso dos dados para treinamento e teste, oferecendo uma estimativa mais robusta da performance do modelo.

C) Treinar um modelo sem separar um conjunto de teste pode levar ao overfitting, pois não há verificação independente da performance do modelo em dados não vistos durante o treinamento.

D) A divisão em treinamento e validação sem a separação de um conjunto de teste pode não ser suficiente para garantir que o modelo generalize bem, pois o conjunto de validação é usado para ajustar hiperparâmetros, podendo levar a um otimismo enviesado da estimativa de desempenho.

E) Alternar conjuntos de treinamento e teste sem um método sistemático como a validação cruzada pode resultar em estimativas de performance inconsistentes e irregulares, que dependem do conjunto de dados específico escolhido em cada semana.",9524597
tópico 5,"métricas de avaliação - matriz de confusão, acurácia, precisão, revocação, F1-score e curva ROC","Questão: Em um projeto de pesquisa na área de aprendizado de máquina, um cientista de dados está avaliando um classificador binário que foi treinado para identificar a ocorrência de uma certa doença rara. A priori, a prevalência da doença no conjunto de dados é baixa. O cientista de dados deseja escolher uma métrica de desempenho que considere tanto a precisão quanto a revocação do modelo, já que é de suma importância não apenas predizer corretamente as condições positivas, mas também não ignorar os verdadeiros casos positivos, dada a gravidade da doença. Qual das seguintes métricas de avaliação o cientista de dados deveria priorizar para esse propósito?

A) Acurácia
B) Precisão
C) Revocação
D) F1-Score
E) Área sob a curva ROC (AUC-ROC)

",D,"

A) Acurácia não é a métrica ideal para essa situação, pois ela pode ser enganosa em datasets desbalanceados, onde a classe de interesse tem uma baixa prevalência.

B) Precisão é importante para garantir que as predições positivas sejam confiáveis, mas por si só, não considera a taxa de verdadeiros positivos que foram corretamente identificados dentre todos os casos reais da doença.

C) Revocação é essencial para um cenário onde é crítico detectar todos os verdadeiros casos da doença, porém, se usada isoladamente, pode permitir um alto número de falsos positivos, o que pode ser custoso em termos de seguimento de testes e tratamentos.

D) F1-Score é a métrica mais adequada neste caso porque é a média harmônica entre precisão e revocação, oferecendo um equilíbrio entre as duas, o que é crucial quando ambas são importantes para o contexto da aplicação.

E) Área sob a curva ROC (AUC-ROC) é uma métrica robusta que mede a capacidade do modelo de distinguir entre as classes, mas ela não enfatiza o equilíbrio entre precisão e revocação; portanto, o F1-Score é ainda mais adequado para este cenário específico.",615759
tópico 5,"modelos vetoriais de documentos (booleano, TF e TF-IDF, média de vetores de palavras e Paragraph Vector);","Questão: Em sistemas de recuperação de informação, diversos modelos vetoriais são aplicados para representação e comparação de documentos. Considerando os modelos vetoriais Booleano, Frequência do Termo (TF), Frequência do Termo-Inverso da Frequência do Documento (TF-IDF), Média de Vetores de Palavras (Word Embeddings) e Paragraph Vector, avalie as seguintes assertivas quanto a sua aplicabilidade e características:

I. O modelo Booleano utiliza operadores lógicos como AND, OR e NOT para combinar termos de consulta, retornando documentos que satisfazem exatamente a expressão booleana especificada, sem contudo, prover um ranqueamento baseado em relevância dos documentos.

II. O cálculo de TF é útil para destacar a importância de um termo em um documento no corpus. No entanto, não leva em consideração a frequência com que o termo aparece em outros documentos do corpus, o que poderia inflar a importância de termos comuns.

III. O modelo TF-IDF melhora a representação de TF ao diminuir o peso de termos que aparecem frequentemente em muitos documentos do corpus, considerando-os menos informativos e, portanto, menos importantes para a caracterização da singularidade de um documento.

IV. A técnica de Média de Vetores de Palavras combina a representação de todas as palavras em um documento por meio da média dos vetores de palavras pré-treinados, contudo, essa abordagem não leva em conta a ordem em que as palavras aparecem no documento.

V. O Paragraph Vector, também conhecido como Doc2Vec, é uma extensão do Word2Vec, que supera as limitações da Média de Vetores de Palavras ao aprender representações fixas de pedaços de texto de qualquer tamanho, incorporando a sequência das palavras e a semântica do documento.

Estão corretas as assertivas:

A) I, II e III apenas.
B) II, IV e V apenas.
C) I, III e IV apenas.
D) I, III e V apenas.
E) I, II, III, IV e V.

",D,"

A explicação dos itens é a seguinte:

I. Correta. O modelo booleano é um dos mais primitivos e opera com base em operadores lógicos, resultando em uma pesquisa que ou retorna o documento como relevante ou não, sem um ranqueamento de relevância.

II. Incorreta. Essa assertiva estaria correta se estivesse se referindo ao modelo TF-IDF, e não ao TF sozinho. O TF, por si só, apenas evidencia a importância do termo dentro de um documento individual.

III. Correta. O modelo TF-IDF é amplamente utilizado justamente por sua capacidade de diminuir o peso dos termos frequentes em todo o corpus, refletindo a importância dos termos para um documento específico.

IV. Correta. A média dos vetores de palavras não considera a posição ou sequência das palavras, apenas sua presença no documento, o que pode ser uma desvantagem quando a sequência é importante para o significado do texto.

V. Correta. O Paragraph Vector, ou Doc2Vec, considera a ordem e contexto das palavras e é capaz de gerar representações vetoriais para textos de comprimentos variáveis, sendo uma melhoria em relação ao modelo de média de vetores de palavras.",2950167
tópico 5,"Processamento de linguagem natural: Normalização textual - stop words, estemização, lematização e análise de frequência de termos;","Questão:

A normalização textual é um passo crítico no processamento de linguagem natural, que tem como objetivo a preparação dos textos para análises subsequentes, como classificação de texto, análise de sentimentos ou recuperação de informações. Considerando as técnicas de normalização textual, analise as afirmações a seguir:

I. A remoção de stop words tem por objetivo eliminar palavras que são consideradas irrelevantes para a análise de significado de um texto, como preposições, conjunções e artigos.

II. A estemização reduz as palavras à sua raiz, muitas vezes resultando em um termo que não necessariamente existe como uma palavra com significado completo, mas que preserva o sentido básico do termo original.

III. A lematização, diferentemente da estemização, busca transformar uma palavra em sua forma canônica ou lema, que geralmente é uma palavra válida no idioma e que tem o significado mais próximo possível da palavra original.

IV. A análise de frequência de termos é uma técnica que atribui mais importância a palavras que aparecem com menor frequência no texto, já que estas tendem a ser mais relevantes para o contexto.

Qual(is) afirmação(ões) está(ão) correta(s)?

A) Apenas I e II
B) Apenas III
C) Apenas I, II e III
D) Apenas I, III e IV
E) Todas as afirmações estão corretas

",C,"

Explicação dos itens:

A afirmação I está correta, pois a remoção de stop words é um procedimento padrão na normalização textual para reduzir o ruído e focar em palavras que são potencialmente mais significativas para a análise.

A afirmação II também é correta. A estemização é um método que busca simplificar as palavras para a sua forma raiz ou ""stem"", que pode não ser uma palavra com significado independente, mas ajuda a consolidar diferentes variações da mesma palavra.

A afirmação III é verdadeira. A lematização é uma técnica mais sofisticada do que a estemização, pois leva em consideração a morfologia do idioma para reduzir a palavra à sua forma lema, que é uma palavra válida e com significado próprio.

A afirmação IV é incorreta. A análise de frequência de termos (term frequency) geralmente considera que palavras que aparecem mais frequentemente são mais relevantes para o texto, ao contrário do afirmado. No entanto, em algumas técnicas de avaliação de importância de termos, como o TF-IDF (term frequency-inverse document frequency), a relevância de um termo aumenta proporcionalmente à sua frequência em um documento, mas é ponderada inversamente pela sua frequência nos demais documentos do corpus.",4571779
tópico 5,"Métricas de similaridade textual - similaridade do cosseno, distância euclidiana, similaridade de Jaccard, distância de Manhattan e coeficiente de Dice","Questão: Em problemas de processamento de linguagem natural (PLN), avaliar a similaridade entre textos é fundamental para diversas aplicações, como detecção de plágio, clusterização de documentos e sistemas de recomendação. Dentre as métricas de similaridade e distância textual, algumas são amplamente empregadas devido às suas propriedades únicas. Dado um conjunto de documentos textuais, qual das seguintes métricas seria MAIS apropriada para determinar o grau de similaridade entre eles, considerando a importância relativa de diferentes termos nos documentos?

A. Distância Euclidiana, por considerar a raiz quadrada da soma dos quadrados das diferenças entre as coordenadas correspondentes.
B. Similaridade do Cosseno, por medir o cosseno do ângulo entre dois vetores em um espaço de características, dando ênfase à direção dos vetores em vez de sua magnitude.
C. Distância de Manhattan, por somar as diferenças absolutas das coordenadas correspondentes, favorecendo textos que compartilham muitos termos comuns.
D. Similaridade de Jaccard, por medir a interseção dividida pela união dos conjuntos de termos, enfatizando a presença e ausência de termos distintos.
E. Coeficiente de Dice, por calcular duas vezes o número de itens comuns, dividido pela soma total de itens nos dois conjuntos.

",B,"

Explicação dos itens:
A. A Distância Euclidiana pode não ser a melhor escolha para medir a similaridade textual em PLN, porque não considera a importância diferenciada que alguns termos podem ter nos documentos (como a importância dada por pesos TF-IDF, por exemplo).
B. A Similaridade do Cosseno é frequentemente utilizada em PLN porque destaca a orientação dos vetores de termos, e não sua magnitude, o que é importante quando a frequência dos termos varia grandemente entre os documentos. Isto permite identificar textos que compartilham contextos ou tópicos semelhantes, mesmo com diferenças de magnitude nos vetores de características.
C. A Distância de Manhattan pode não ser ideal para textos porque não leva em conta a direção dos vetores, apenas a soma das diferenças das coordenadas, o que pode não refletir adequadamente a similaridade de conteúdo quando há termos de diferentes graus de importância.
D. A Similaridade de Jaccard não considera a frequência com que os termos aparecem, apenas sua presença ou ausência, o que pode ser limitado em algumas aplicações de PLN onde a frequência é importante.
E. O Coeficiente de Dice é semelhante à Similaridade de Jaccard no sentido de que favorece a sobreposição de termos, mas como a Similaridade de Jaccard, não leva em consideração a frequência dos termos nos documentos, o que pode ser uma limitação em certas aplicações.",9654010
tópico 5,"Técnicas de agrupamento: Agrupamento por partição, por densidade e hierárquico","Questão: Em análise de dados, técnicas de agrupamento são essenciais para identificar a estrutura oculta nos dados e agrupar objetos similares em um mesmo conjunto. Diferentes técnicas podem ser aplicadas dependendo das características específicas dos dados e dos objetivos de análise. Considerando as técnicas de agrupamento por partição, por densidade e hierárquico, analise as seguintes afirmações:

I. Agrupamento por partição, como o algoritmo K-means, tenta dividir o conjunto de dados em um número pré-definido de grupos, onde cada objeto pertence a exatamente um grupo baseado na minimização da soma dos quadrados das distâncias entre os objetos e o centróide do seu grupo.

II. O agrupamento por densidade, exemplificado pelo algoritmo DBSCAN, define clusters com base na densidade de pontos de dados, expandindo clusters ao redor de pontos rotulados como centrais que excedem um limiar de densidade mínimo e conectando áreas de alta densidade que são separadas por regiões esparsas.

III. Agrupamento hierárquico constrói uma hierarquia de clusters de uma forma aglomerativa ou divisiva, em que o método aglomerativo começa com cada ponto como um cluster único e mescla-os passo a passo, enquanto o método divisivo inicia com um único cluster contendo todos os pontos e os divide progressivamente.

Quais afirmações estão corretas?

A) Apenas I e II.
B) Apenas I e III.
C) Apenas II e III.
D) I, II e III.

",B,"

Explicação dos itens:

A alternativa A é incorreta porque apesar de as afirmações I e II estarem corretas, a alternativa descarta a corretude da afirmação III, a qual também está correta.

A alternativa B é a correta, pois todas as afirmações I, II e III estão corretas. O agrupamento por partição, por densidade e hierárquico são explicados de maneira coerente, correspondendo a suas definições e funcionalidades.

A alternativa C é incorreta porque supõe que somente as afirmações II e III estariam corretas, no entanto, a afirmação I também está correta.

A alternativa D é uma opção que parece correta inicialmente, mas de acordo com a forma em que as alternativas devem ser avaliadas, é uma alternativa incorreta, pois a questão pede explicitamente para identificar quais afirmações estão corretas, no plural, e a alternativa correta deve ser apresentada no singular, seguindo a lógica aplicada a esse tipo de prova.",1811774
tópico 5,Ajuste de modelos dentro e fora de amostra e overfitting,"Questão:
A aplicação de técnicas de ajuste de modelos estatísticos em finanças quantitativas é uma das tarefas cruciais para a previsão de preços de ativos e a gestão de riscos. Entretanto, ao realizar o ajuste de modelos, é importante compreender os conceitos de ajuste dentro e fora da amostra, assim como o fenômeno do overfitting, que pode comprometer a eficácia preditiva do modelo. Sobre esses conceitos, é CORRETO afirmar que:

A) O ajuste de modelos dentro da amostra refere-se ao processo de otimização do modelo estatístico utilizando todos os dados disponíveis, sem reservar uma parte deles para testar a eficácia do modelo.

B) Overfitting é um problema que ocorre quando um modelo estatístico é ajustado de forma muito genérica, não capturando as particularidades dos dados de treinamento.

C) Um modelo bem ajustado fora da amostra demonstra boa capacidade de generalização quando consegue prever de forma precisa novos dados, testando sua eficácia em uma amostra diferente daquela utilizada na fase de treinamento.

D) A validação cruzada é uma técnica que garante o ajuste perfeito do modelo tanto dentro quanto fora da amostra, eliminando por completo o risco de overfitting.

E) Overfitting é desejável em modelos estatísticos, pois indica que o modelo tem alta complexidade e pode capturar todas as nuances dos dados de treinamento.

",C," 

Explicação dos itens:

A) Incorreta. O ajuste de modelos dentro da amostra utiliza parte dos dados disponíveis para o treinamento do modelo, enquanto uma outra parte, chamada conjunto de validação, é reservada para testar a eficácia do modelo. Este item descreve incorretamente o processo.

B) Incorreta. Overfitting ocorre quando um modelo estatístico é ajustado de forma excessivamente específica aos dados de treinamento, o que pode levar a um desempenho pobre em novos dados, pois o modelo captura também o ""ruído"" ou variações aleatórias dos dados de treinamento ao invés de apenas as relações sistemáticas.

C) Correta. A definição oferecida neste item caracteriza corretamente um ajuste de modelo bem sucedido fora da amostra e aponta para a importância da capacidade de generalização de um modelo estatístico.

D) Incorreta. Enquanto a validação cruzada é uma técnica que visa reduzir o risco de overfitting e melhorar a capacidade de generalização do modelo, ela não garante o ajuste perfeito dentro e fora da amostra. Existem sempre limitações na predição devido a incertezas e variações nos dados.

E) Incorreta. Overfitting não é desejável, pois, apesar de indicar uma alta capacidade de ajuste aos dados de treinamento, geralmente resulta em pobre desempenho preditivo em relação a dados novos, pois o modelo aprende os erros específicos ou o ruído presente na amostra de treinamento em vez das verdadeiras relações subjacentes.",4161281
tópico 5,"Métricas de similaridade textual - similaridade do cosseno, distância euclidiana, similaridade de Jaccard, distância de Manhattan e coeficiente de Dice","Questão: Uma empresa está implementando um sistema de processamento de linguagem natural para comparar e agrupar documentos similares de acordo com seu conteúdo textual. Quatro métricas de similaridade foram propostas ao time de desenvolvimento: similaridade do cosseno, distância euclidiana, similaridade de Jaccard e distância de Manhattan. No contexto de vetores de alta dimensão, onde as entradas são bag-of-words dos documentos, e considerando eficiência e aplicabilidade para o agrupamento baseado na similaridade textual de documentos, qual métrica é MENOS indicada para este propósito?

A) Similaridade do cosseno

B) Distância euclidiana

C) Similaridade de Jaccard

D) Distância de Manhattan

E) Coeficiente de Dice

",B," 

Explicação:

A) A similaridade do cosseno é adequada para comparar documentos em espaços vetoriais de alta dimensão, pois ela considera o ângulo entre os vetores, não sendo influenciada pela magnitude dos vetores, o que é importante quando se compara textos de comprimentos diferentes.

B) A distância euclidiana pode não ser a métrica mais indicada em espaços de alta dimensionalidade, comumente referido como “maldição da dimensionalidade”, pois todas as distâncias tendem a parecer iguais, o que pode prejudicar a eficácia do agrupamento de documentos.

C) A similaridade de Jaccard é útil para comparar a semelhança entre conjuntos, como bag-of-words, ao medir a proporção do intersecção sobre a união dos conjuntos. Ela é menos afetada pela dimensionalidade do que a distância euclidiana.

D) A distância de Manhattan, também conhecida como distância de táxi, pode ser utilizada em espaços de alta dimensão, mas é mais sensível a variações em dimensões específicas e pode não capturar a similaridade textual tão eficazmente quanto a similaridade do cosseno ou a similaridade de Jaccard.

E) O coeficiente de Dice é semelhante à similaridade de Jaccard no sentido de que compara conjuntos, mas com diferentes ponderações. Apesar de não estar listado na pergunta original, também pode ser utilizado para avaliar a similaridade de documentos textuais.

Portanto, a distância euclidiana é menos indicada para o contexto descrito por ser mais afetada pelo problema da maldição da dimensionalidade.",1624152
tópico 5,Técnicas de redução de dimensionalidade: Seleção de características (feature selection); Análise de componentes principais (PCA – principal component analysis),"Questão:

Em um contexto de aprendizado de máquina, um cientista de dados trabalha com um conjunto de dados com um grande número de características (features), resultando em uma complexidade computacional elevada e possíveis problemas como a maldição da dimensionalidade. O cientista precisa aplicar técnicas de redução de dimensionalidade para simplificar o modelo sem perder informações críticas. Sobre as técnicas de Seleção de características (feature selection) e Análise de Componentes Principais (PCA – Principal Component Analysis), é correto afirmar:

A) A Seleção de características remove atributos irrelevantes e redundantes com base em sua correlação com a variável de interesse, enquanto o PCA transforma as características originais em um novo conjunto de variáveis que são linearmenente independentes.
B) O PCA é capaz de eliminar variáveis baseado exclusivamente em seu grau de variância, independentemente de qualquer relação possível com a variável alvo, enquanto a Seleção de características se concentra apenas em manter variáveis que têm alta variância.
C) Tanto a Seleção de características quanto o PCA podem ser aplicados independentemente do tipo de variáveis envolvidas, sejam categóricas ou contínuas, sem a necessidade de pré-processamento.
D) A Seleção de características e o PCA têm a mesma finalidade e aplicam o mesmo método matemático para reduzir a dimensionalidade, a diferença reside somente nos algoritmos utilizados para computar a redução.
E) O PCA é uma técnica não supervisionada de redução de dimensionalidade muito sensível a outliers no conjunto de dados, enquanto que a Seleção de características pode empregar métodos tanto supervisionados quanto não supervisionados, sendo menos afetada pela presença de outliers.

",A,"

Explicação dos itens:

A) Correto. A Seleção de características (feature selection) busca identificar as características mais relevantes e descartar as irrelevantes ou redundantes. Já o PCA transforma o conjunto de dados em componentes principais ortogonais que explicam a maior parte da variância dos dados, não necessariamente mantendo somente as características correlatas com a variável de interesse.

B) Incorreto. O PCA não elimina variáveis, transforma-as em um conjunto de componentes principais. Também não se baseia apenas na variância das características, mas sim na variância-covariância entre elas. A Seleção de características não seleciona apenas variáveis de alta variância, mas aquelas que são mais significativas para o resultado.

C) Incorreto. A análise de componentes principais (PCA) é mais adequada para variáveis contínuas, pois depende de cálculos de autovalores e autovetores a partir da matriz de covariância. Seleção de características pode necessitar de codificação ou transformação quando lidamos com variáveis categóricas.

D) Incorreto. Embora ambas as técnicas visem reduzir a dimensionalidade, elas não utilizam o mesmo método matemático. A Seleção de características é um método que envolve a seleção de um subconjunto das características originais, enquanto o PCA cria novos componentes principais através de uma transformação linear.

E) Incorreto. Enquanto o PCA é sensível a outliers, pois estes podem afetar significativamente a variância-covariância dos dados, a parte relacionada à Sensibilidade da Seleção de características a outliers e às técnicas de supervisão e não supervisão está mal interpretada. A seleção de características pode ser menos sensível a outliers dependendo do método aplicado, e métodos supervisionados consideram a relação com a variável alvo, enquanto métodos não supervisionados não.",6435391
tópico 5,"Modelos de representação de texto - N-gramas, modelos vetoriais de palavras (CBOW, Skip-Gram e GloVe)","Questão: No processamento de linguagem natural (PLN), diversas técnicas e métodos são empregados para a representação de texto, de modo a permitir que algoritmos de aprendizado de máquina possam atuar na compreensão e geração de linguagem humana. Entre esses métodos, destacam-se os modelos de N-gramas e os modelos vetoriais de palavras, como CBOW (Continuous Bag of Words), Skip-Gram e GloVe (Global Vectors for Word Representation). Considerando os recursos e as particularidades de cada um desses métodos, avalie as afirmativas abaixo:

I. Modelos de N-gramas são baseados em probabilidades condicionais da aparição de uma palavra dado o contexto de N-1 palavras anteriores, sendo úteis para tarefas de previsão de sequência de palavras.
II. O modelo Skip-Gram é adequado para representar o contexto de palavras em grandes corpora de texto, prevendo palavras de contexto a partir de uma palavra-alvo, oposto ao modelo CBOW que prevê uma palavra-alvo a partir do contexto.
III. O GloVe é um modelo de representação de palavras que combina técnicas de fatoração de matriz e métodos de janela de contexto, capturando significados globais do corpus de texto.

Está correto apenas o que se afirma em:

A) I
B) II
C) I e II
D) II e III
E) I, II e III

",E,"

Explicação dos itens:

- Item I: Correto. Modelos de N-gramas trabalham com a ideia de que a probabilidade de ocorrência de uma palavra pode ser estimada pelo contexto anterior de N-1 palavras, sendo amplamente utilizados para modelar a linguagem e realizar predições de texto.

- Item II: Correto. Skip-Gram e CBOW são técnicas opostas dentro dos modelos Word2Vec. No modelo Skip-Gram, o algoritmo utiliza uma palavra central para prever o contexto de palavras ao redor, enquanto o modelo CBOW prevê uma palavra-alvo com base em seu contexto.

- Item III: Correto. O GloVe é um modelo que aproveita as vantagens da fatoração de matriz e dos co-ocorrências estatísticas em uma janela de contexto, com o objetivo de gerar embeddings de palavras que capturam os relacionamentos globais entre as palavras no corpus.

Portanto, todas as afirmativas são verdadeiras, fazendo com que a alternativa correta seja a E.",6467141
tópico 5,"Técnicas de agrupamento: Agrupamento por partição, por densidade e hierárquico","Questão: Em análise de dados, diferentes técnicas de agrupamento são empregadas com o intuito de se identificar estruturas naturais ou padrões nos dados. Cada técnica possui suas particularidades no que diz respeito à flexibilidade, escalabilidade e tipo de agrupamentos que conseguem identificar. Considerando as técnicas de agrupamento por partição, por densidade e hierárquico, analise as assertivas abaixo:

I. O agrupamento por partição tende a dividir o conjunto de dados em um número pré-definido de grupos, onde o mais famoso algoritmo desse tipo é o K-means, que busca minimizar a variação intraclasse.

II. O agrupamento por densidade, como o algoritmo DBSCAN, identifica regiões de alta densidade separadas por regiões de baixa densidade e não exige a definição prévia do número de grupos, adaptando-se bem a conjuntos de dados com ruídos e outliers.

III. A técnica de agrupamento hierárquico procura criar uma decomposição multinível do conjunto de dados, podendo ser visualizada por meio de um dendrograma. No entanto, essa técnica não é adequada para conjuntos de dados de grande escala devido à sua complexidade computacional.

IV. Enquanto o agrupamento hierárquico e por densidade são altamente sensíveis à presença de outliers, o agrupamento por partição possui robustez significativa no trato de dados anômalos e ruido.

É correto o que se afirma em:

a) I e II, apenas.
b) I, II e III, apenas.
c) II e III, apenas.
d) I, II e IV, apenas.
e) I, II, III e IV.

",B," 

Explicação dos itens:

I. Correta. O agrupamento por partição, de fato, divide o conjunto de dados em um número pré-definido de grupos e tenta minimizar a variação dentro de cada grupo. O K-means é um exemplo clássico dessa técnica.

II. Correta. O algoritmo DBSCAN, um método de agrupamento por densidade, não exige a especificação do número de clusters e pode identificar clusters de formato irregular, além de ser menos afetado por outliers.

III. Correta. A técnica de agrupamento hierárquico constrói uma hierarquia de clusters que pode ser representada por um dendrograma. Contudo, devido ao aumento exponencial da complexidade computacional, ela não é adequada para conjuntos de dados muito grandes.

IV. Incorreta. Diferentemente do que é afirmado, os algoritmos de agrupamento por partição, como o K-means, podem ser sensíveis a outliers. Por outro lado, técnicas como o agrupamento por densidade tendem a ser mais robustas nesse aspecto. 

Portanto, a resposta correta é a letra b, pois as afirmativas I, II e III estão corretas e IV está incorreta.",3688387
tópico 5,Redes neurais convolucionais e recorrentes,"Questão:

As redes neurais são um componente crucial no campo do aprendizado de máquina e da inteligência artificial. Redes neurais convolucionais (CNNs) e redes neurais recorrentes (RNNs) têm aplicações diversas e complementares. Nesse contexto, qual das alternativas abaixo descreve corretamente a principal aplicação e característica distintiva de CNNs e RNNs?

A) CNNs são mais adequadas para processamento de imagem e reconhecimento de padrões espaciais, enquanto RNNs apresentam melhor performance em aplicações de regressão linear simples.
B) RNNs são ideais para tarefas que envolvem reconhecimento de voz e processamento de linguagem natural, pois têm a capacidade de manter a informação de sequência, diferentemente das CNNs.
C) CNNs são projetadas para classificação de textos e RNNs são mais eficientes no processamento de series temporais e dados em sequência.
D) As RNNs utilizam filtros convolucionais para extrair características hierárquicas dos dados, enquanto as CNNs aproveitam a dependência temporal entre os dados para realizar suas previsões.
E) CNNs e RNNs não possuem diferenças significativas em termos de arquitetura e aplicação, sendo ambas intercambiáveis em qualquer cenário de análise de dados.

",B," 

Explicação:

A) Esta alternativa está incorreta porque, apesar de as CNNs serem mais adequadas para o processamento de imagem e reconhecimento de padrões espaciais devido à sua arquitetura convolucional, as RNNs não são utilizadas para regressão linear simples, mas sim para dados sequenciais, como séries temporais, linguagem natural, entre outros.

B) Esta é a alternativa correta. As RNNs são ideais para tarefas que envolvem reconhecimento de voz e processamento de linguagem natural, pois sua arquitetura permite que elas mantenham a informação de sequência, o que é uma característica chave para entender o contexto em dados que possuem uma ordem temporal ou sequencial. As CNNs, por outro lado, não têm essa capacidade, sendo mais indicadas para dados onde a relação de proximidade espacial é mais relevante.

C) Esta alternativa está equivocada pois inverte as aplicações típicas de CNNs e RNNs. CNNs são na verdade mais eficientes no processamento de imagem e reconhecimento de características espaciais, enquanto RNNs são apropriadas para classificação de texto e outras formas de dados sequenciais.

D) Esta alternativa está incorreta pois mistura as propriedades das duas redes. As CNNs utilizam filtros convolucionais para extrair características hierárquicas, como padrões ou bordas em imagens. As RNNs, por sua vez, são quem aproveita a dependência temporal entre dados para realizar previsões em séries temporais ou na linguagem natural.

E) Esta alternativa está errada porque CNNs e RNNs possuem diferenças significativas em suas arquiteturas e aplicações. CNNs são mais utilizadas em aplicações que envolvem reconhecimento de padrões espaciais, enquanto RNNs são utilizadas para dados sequenciais onde a ordem é importante. Elas não são intercambiáveis em todos os cenários.",3560302
tópico 5,"Rotulação de partes do discurso, part-of-speech tagging","Questão:

A rotulação de partes do discurso (part-of-speech tagging, ou POS tagging) é uma atividade comum no processamento de linguagem natural, consistindo na atribuição de etiquetas a cada palavra em um texto, que indicam sua função gramatical. Qual destas afirmativas melhor representa os desafios encontrados no processo de POS tagging em textos da língua portuguesa?

A) As palavras em português possuem somente uma função gramatical, tornando a POS tagging uma tarefa direta e sem ambiguidades.
B) A natureza flexiva da língua portuguesa não influencia o processo de POS tagging, pois as palavras mantêm suas etiquetas independentemente do contexto.
C) As etiquetas em POS tagging são determinadas exclusivamente por listas estáticas de palavras e suas funções, não requerendo algoritmos adaptativos para contextos variados.
D) A homonímia e a polissemia das palavras podem dificultar a rotulação automática, exigindo o uso de algoritmos que considerem o contexto para desambiguação.
E) As ferramentas de POS tagging para a língua portuguesa são invariavelmente precisas, pois a estrutura gramatical da língua é inteiramente regular e não possui exceções.

",D," 

Explicação dos itens:

A) Esta alternativa está incorreta porque mesmo em português, uma palavra pode desempenhar diferentes funções gramaticais, dependendo de seu uso em uma frase.
B) Esta opção é incorreta pois a flexão de número, gênero e caso, comum em línguas como o português, afeta a determinação da classe gramatical de uma palavra, portanto, o contexto é fundamental.
C) A afirmação é falsa, já que o POS tagging não depende somente de listas estáticas, mas também de algoritmos que podem aprender com o contexto e adaptar-se a usos distintos das palavras.
D) Esta é a alternativa correta porque capta a complexidade da tarefa de POS tagging, reconhecendo que a homonímia e a polissemia são desafios significativos no processo de atribuição de etiquetas gramaticais.
E) A opção E é incorreta, pois, mesmo que muitas ferramentas sejam avançadas e apresentem resultados satisfatórios, ainda existem irregularidades e exceções na língua portuguesa que podem levar a erros de rotulação.",654431
tópico 5,Técnicas de classificação: Naive Bayes; Árvores de decisão (algoritmos ID3 e C4.5); Florestas aleatórias (random forest); Máquinas de vetores de suporte (SVM – support vector machines); K vizinhos mais próximos (KNN – K-nearest neighbours),"Questão:
A técnica de classificação é fundamental na análise de dados para a identificação de padrões e a tomada de decisões baseadas em conjuntos de informações. Considere os algoritmos a seguir: Naive Bayes, Árvores de decisão (algoritmos ID3 e C4.5), Florestas aleatórias (Random Forest), Máquinas de vetores de suporte (SVM – Support Vector Machines) e K vizinhos mais próximos (KNN – K-Nearest Neighbours). Cada um desses algoritmos possui características distintas. Sendo assim, qual dos algoritmos listados NÃO se baseia em uma abordagem de aprendizagem baseada em instâncias para realizar a classificação?

A) Naive Bayes
B) Árvores de decisão (ID3)
C) Random Forest
D) SVM
E) KNN

",A,"

Explicação dos itens:
A) O algoritmo Naive Bayes é um classificador probabilístico baseado no teorema de Bayes. Ele não se baseia em aprendizagem baseada em instâncias, mas sim na probabilidade de ocorrência de características para determinar a classificação.
B) As árvores de decisão, incluindo os algoritmos ID3, constroem uma estrutura em forma de árvore, onde cada nó representa uma decisão baseada em um atributo. Não é uma abordagem baseada em instâncias.
C) Random Forest é um método de ensemble que cria diversas árvores de decisão durante o treinamento e faz a classificação por meio de uma votação majoritária entre essas árvores. Também não é uma abordagem baseada em instâncias.
D) SVM é uma técnica que tenta encontrar o hiperplano de separação ótimo entre as diferentes classes de dados. É uma abordagem baseada em geometria e não em instâncias.
E) KNN é um algoritmo que classifica um novo objeto com base na similaridade com os 'K' vizinhos mais próximos encontrados no espaço de características. É o único algoritmo da lista que se baseia explicitamente em aprendizagem baseada em instâncias.",8543681
tópico 5,"métricas de avaliação - matriz de confusão, acurácia, precisão, revocação, F1-score e curva ROC","Questão: No contexto de modelos de classificação em aprendizado de máquina, diversas métricas de avaliação são utilizadas para medir o desempenho do modelo. Considere que um pesquisador esteja trabalhando em um sistema de detecção de emails fraudulentos (phishing), onde é mais prejudicial classificar um email legítimo como fraude (falso positivo) do que não detectar um email fraudulento (falso negativo). Neste cenário, qual métrica de avaliação o pesquisador deveria priorizar para minimizar o impacto dos falsos positivos?

A) Acurácia
B) Precisão
C) Revocação
D) F1-score
E) Área sob a Curva ROC (AUC-ROC)

",B,"

Explicação dos itens:
A) Acurácia - Esta métrica mede a porcentagem de classificações corretas (verdadeiros positivos e verdadeiros negativos) entre todas as classificações possíveis. Não é ideal neste caso porque prioriza tanto a detecção de fraudes quanto a classificação correta de emails legítimos, sem focar especificamente no problema dos falsos positivos.

B) Precisão - Esta métrica mede a proporção de verdadeiros positivos em relação a todos os positivos identificados (verdadeiros positivos + falsos positivos). É a métrica mais adequada para o cenário proposto pois foca em reduzir o número de falsos positivos, que é a condição mais prejudicial no contexto apresentado.

C) Revocação - Também conhecida como sensibilidade, esta métrica mede a proporção de verdadeiros positivos em relação ao número total de casos positivos reais (verdadeiros positivos + falsos negativos). Embora seja importante para garantir que emails fraudulentos sejam detectados, ela não foca em minimizar os falsos positivos.

D) F1-score - Esta métrica é a média harmônica entre precisão e revocação. Ela seria útil em um cenário onde é importante balancear o número de falsos positivos e falsos negativos, mas não é a ideal quando se tem um foco específico em minimizar uma das duas condições.

E) Área sob a Curva ROC (AUC-ROC) - Esta métrica mede a capacidade de um modelo de classificação distinguir entre as classes em todos os limiares de classificação. Ela é uma medida geral de desempenho, mas não se concentra especificamente na redução de falsos positivos.",6274845
tópico 5,Técnicas de regressão: Árvores de decisão para regressão; Máquinas de vetores de suporte para regressão,"Questão: A modelagem preditiva é uma ferramenta estatística essencial para o entendimento de complexas relações entre variáveis em diversas áreas, incluindo finanças, medicina e pesquisa social. No contexto do aprendizado de máquina, técnicas como Árvores de Decisão para Regressão (Decision Tree Regression) e Máquinas de Vetores de Suporte para Regressão (Support Vector Regression - SVR) são amplamente utilizadas para predizer valores contínuos com base em variáveis independentes. Sobre essas duas técnicas, afirma-se que:

I - Árvores de Decisão para Regressão funcionam dividindo o espaço da variável independente em regiões distintas e fazendo previsões com base na média dos valores da variável dependente em cada região.

II - Máquinas de Vetores de Suporte para Regressão são robustas a outliers, pois empregam a ideia de margem suave, permitindo que algumas violações no limite do corredor de decisão ocorram sem grande penalização no modelo.

III - Ao contrário das Árvores de Decisão, Máquinas de Vetores de Suporte para Regressão não são capazes de modelar relações não lineares, mesmo quando empregando funções kernel.

IV - Tanto as Árvores de Decisão para Regressão quanto as Máquinas de Vetores de Suporte exigem criteriosa parametrização e ajuste fino (fine-tuning) para evitar o sobreajuste (overfitting) dos modelos aos dados de treinamento.

Estão corretas apenas as afirmativas:

A) I e III
B) II e IV
C) I, II e IV
D) I, II e III
E) Todas as afirmativas estão corretas

",C,"
A alternativa correta é a C, ""I, II e IV"". Seguindo a separação padrão:

I - Esta afirmação é verdadeira. Árvores de Decisão para Regressão funcionam dividindo o espaço de entrada em regiões e fazendo previsões por meio do valor médio da variável de saída em cada região. Isso é conhecido como aproximação de ""peça por peça"".

II - Esta afirmação também é verdadeira. Máquinas de Vetores de Suporte para Regressão podem ser mais tolerantes a outliers porque a função de perda usada (geralmente, a função de perda epsilon-insensível) não penaliza erros dentro de um determinado limite, o que é considerado a margem suave.

III - Esta afirmação é falsa. Máquinas de Vetores de Suporte podem modelar relações não lineares usando funções kernel, como o RBF (Radial Basis Function), polinomial, ou sigmoide, o que é uma característica importante dessa técnica.

IV - Esta afirmação é verdadeira. Para evitar o sobreajuste, é necessário um ajuste cuidadoso dos parâmetros tanto nas Árvores de Decisão para Regressão quanto nas Máquinas de Vetores de Suporte. No caso das árvores, parâmetros como a profundidade máxima, número mínimo de amostras por folha e métodos de poda são cruciais. Nas SVMs, os parâmetros do kernel e o parâmetro de regularização C desempenham papel semelhante.",1708013
tópico 5,"Processamento de linguagem natural: Normalização textual - stop words, estemização, lematização e análise de frequência de termos;","Questão: Em processamento de linguagem natural (PLN), diversas técnicas são aplicadas no pré-processamento de textos com o intuito de extrair ou potencializar as informações relevantes para que modelos computacionais possam executar tarefas como classificação, tradução ou resumo de forma mais efetiva. Dentro desse contexto, assinale a opção que descreve corretamente os métodos de normalização textual listados:

A) A remoção de stop words consiste em eliminar palavras que, apesar de muito frequentes, geralmente não carregam significado contextual significativo, como preposições e conjunções.

B) Estemização é o processo que utiliza regras heurísticas para cortar as terminações das palavras, visando agrupar variações lexicais de uma palavra para sua forma base ou raiz, mesmo que essa forma não seja uma palavra válida no idioma.

C) Lematização, diferentemente da estemização, é uma técnica que transforma palavras em sua forma dicionarizada, ou seja, sua forma de lema, que representa sua raiz gramatical e semântica válida dentro do idioma.

D) Análise de frequência de termos pressupõe a remoção total de palavras raras do texto, pois estas são consideradas ruídos que podem prejudicar o desempenho de sistemas de PLN.

E) Todas as técnicas listadas – stop words, estemização, lematização, análise de frequência de termos – são exemplos de métodos de normalização textual que comprometem a integridade semântica original do texto, impossibilitando o uso posterior para tarefas de compreensão de leitura.

",C," 

A alternativa A está correta na descrição da remoção de stop words, que são palavras comuns retiradas durante o pré-processamento. A alternativa B também está correta e apresenta uma boa descrição do processo de estemização, enfatizando o aspecto heurístico do método. A alternativa C é a correta ao descrever a lematização com precisão, diferenciando-a da estemização por buscar a forma base dicionarizada da palavra. A alternativa D está incorreta, pois a análise de frequência de termos não necessariamente implica a remoção total de palavras raras, mas sim a identificação de termos mais e menos frequentes, que podem ter diferentes aplicações. A alternativa E é incorreta, pois nem todos os métodos de normalização textual comprometem a integridade semântica original do texto, e tarefas como compreensão de leitura podem ser realizadas mesmo após o uso de algumas dessas técnicas, dependendo do contexto e do objetivo da tarefa de PLN.",2935524
tópico 5,"Avaliação de modelos de classificação: treinamento, teste, validação; validação cruzada","Questão: No desenvolvimento de modelos de classificação em aprendizado de máquina, a avaliação correta do desempenho do modelo é crucial para garantir que ele será capaz de generalizar bem para dados não vistos anteriormente. Dentre as técnicas citadas abaixo, qual é a mais adequada para estimar o desempenho de um modelo de classificação em um cenário onde o conjunto de dados é limitado e a preocupação com a variância na estimativa do desempenho é significativa?

A) Dividir o conjunto de dados em uma parte para treinamento e outra para teste, utilizando 70% para treinamento e 30% para teste.

B) Utilizar um conjunto de validação retirado do conjunto de treinamento para ajustar os parâmetros do modelo, além de separar um conjunto de teste para a avaliação final.

C) Implementar o método holdout repetido, onde o conjunto de dados é dividido aleatoriamente em treinamento e teste várias vezes e a média dos desempenhos é calculada.

D) Empregar a técnica de validação cruzada, dividindo os dados em k subconjuntos e, em seguida, utilizar cada subconjunto uma vez como teste e os k-1 restantes para treinamento.

E) Executar uma validação cruzada estratificada com apenas uma dobra, para garantir que a proporção de cada classe seja preservada entre os conjuntos de treino e teste.

",D,"

A alternativa correta é a letra D. Vamos explicar cada item em detalhes:

A) A divisão padrão de 70/30 não aborda a questão da limitação do conjunto de dados nem da variância altamente significativa na estimativa de desempenho.

B) O uso de um conjunto de validação ajuda a ajustar os parâmetros, mas não reduz necessariamente a variância na estimativa do desempenho, especialmente em conjuntos de dados limitados.

C) O método holdout repetido pode ser útil para reduzir a variância na estimativa do desempenho, no entanto, pode não ser tão eficaz quanto a validação cruzada, especialmente se o conjunto de dados for pequeno.

D) A técnica de validação cruzada é a mais indicada para o cenário proposto, pois permite que todos os dados sejam utilizados tanto para treinamento quanto para teste, proporcionando uma melhor estimativa do desempenho do modelo e reduzindo a variância associada a essa estimativa, dado que cada observação é usada para teste exatamente uma vez.

E) A validação cruzada estratificada com uma única dobra não é diferente do método holdout, e o fato de ser estratificada não compensa a falta de repetição na avaliação, que poderia reduzir a variância. A estratificação é uma técnica útil, mas neste contesto, várias dobras são necessárias para reduzir a variância.",1709239
tópico 5,"modelos vetoriais de documentos (booleano, TF e TF-IDF, média de vetores de palavras e Paragraph Vector);","Questão: Considere os diferentes modelos vetoriais usados para representar documentos em um sistema de recuperação de informação. O modelo booleano utiliza operadores lógicos para combinar termos de consulta, enquanto o modelo de Frequência de Termo (TF) considera a frequência com que os termos aparacem nos documentos. Já o modelo TF-IDF pondera a frequência do termo e a importância do termo no conjunto de dados. O modelo de média de vetores de palavras leva em conta a média dos vetores de palavras no espaço de características semânticas e o Paragraph Vector representa os documentos como vetores únicos em um espaço de características continuamente distribuído. Com base nessas descrições, qual das seguintes afirmações é INCORRETA?

A) O modelo booleano é útil para consultas exatas, mas não leva em conta a frequência dos termos nos documentos.
B) O modelo TF leva em consideração apenas a frequência dos termos, sem ajustar para sua raridade ou comunalidade entre todos os documentos.
C) O modelo TF-IDF pode penalizar termos que são muito comuns em todos os documentos, dando mais importância aos termos mais distintos.
D) O modelo de média de vetores de palavras considera contexto e ordem dos termos nos documentos para construir o significado.
E) O Paragraph Vector permite inferir representações vetoriais fidedignas para documentos, mesmo que eles compartilhem palavras ou contextos semelhantes.

",D," 

Explicação:

A) Esta alternativa é correta porque o modelo booleano realmente não considera a frequência dos termos, mas sim se eles estão presentes ou não, usando operadores AND, OR e NOT.

B) Esta alternativa também está correta. O modelo de Frequência de Termo - TF foca apenas na contagem de vezes que um termo aparece no documento, sem ponderar essa frequência pela raridade do termo.

C) Correta, pois o modelo TF-IDF realmente pondera os termos pela sua frequência nos documentos e pela inversa da frequência nos outros documentos do conjunto de dados, penalizando os termos comuns e dando mais peso aos menos comuns, ou seja, mais distintos.

D) Esta opção é incorreta. O modelo de média de vetores de palavras não leva em conta a ordem dos termos nos documentos. Ele simplesmente calcula a média dos vetores de todas as palavras no documento, ignorando a estrutura sintática ou sequência das palavras.

E) Esta opção está correta porque o modelo Paragraph Vector (como Doc2Vec) é capaz de gerar representações de documentos que são distintas e podem capturar o contexto mesmo que dois documentos compartilhem palavras ou tenham um vocabulário semelhante.",5306875
tópico 5,"Processamento de linguagem natural: Normalização textual - stop words, estemização, lematização e análise de frequência de termos;","Questão: No campo do Processamento de Linguagem Natural (PLN), diversas técnicas são utilizadas com o objetivo de preparar os textos para análises mais complexas, melhorando o desempenho de algoritmos de aprendizado de máquina e de compreensão textual. Em relação à normalização textual, analise as seguintes afirmativas:

I - A remoção de stop words pode ser crucial para focar em palavras que oferecem maior carga semântica relevante ao texto, embora possa levar à perda de informação estrutural da mensagem.
II - A estemização reduz as palavras ao seu radical básico, facilitando a generalização de termos, mas pode misturar palavras de significados diferentes que compartilham o mesmo radical.
III - A lematização envolve a análise morfológica das palavras para reduzi-las às suas formas canônicas base, o que preserva seu sentido semântico, diferentemente da estemização.
IV - A análise de frequência de termos é irrelevante no contexto de PLN, uma vez que a importância de um termo é determinada exclusivamente por sua semântica e não por sua frequência de ocorrência.

Assinale a alternativa que contém as afirmativas CORRETAS:

A) I, II e III
B) I, III e IV
C) II e III
D) I e IV
E) Todas as afirmativas são corretas.

",A,"


Explicação dos itens:

I - Correta. A remoção de stop words é uma técnica comum no PLN, pois remove palavras que são frequentemente usadas na língua, mas que oferecem pouca informação útil para a maioria das aplicações de processamento de texto.

II - Correta. Estemização é o processo de reduzir palavras a uma forma base ou radical, sem considerar a conjugação verbal ou declinação nominal completa. Isso ajuda na consolidação de variantes de uma palavra mas pode levar a ambiguidades.

III - Correta. A lematização considera o contexto e transforma a palavra em sua forma lema, que é a forma como apareceria num dicionário. Essa técnica é mais sofisticada que a estemização e preserva mais informação semântica.

IV - Incorreta. A análise de frequência de termos é uma técnica importante em PLN, que ajuda no entendimento da importância e relevância de palavras ou frases em textos. A frequência de um termo pode ser um indicador muito forte do tema ou de padrões de importância em determinado texto ou conjunto de textos.

Portanto, a resposta correta é a Alternativa A, que inclui as afirmativas I, II e III, que são corretas e relevantes para a normalização textual no contexto de PLN.",3372777
tópico 5,"Técnicas de agrupamento: Agrupamento por partição, por densidade e hierárquico","Questão: No contexto de aprendizado de máquina, especificamente no que tange às técnicas de agrupamento, diversas estratégias podem ser utilizadas para identificar padrões e agrupar os dados de maneira significativa. Entre as principais técnicas de clusterização, destacam-se o agrupamento por partição, por densidade e o hierárquico. Considere as seguintes afirmações a respeito dessas técnicas:

I. O agrupamento por partição, como o K-means, trabalha com a atribuição de cada objeto do conjunto de dados a exatamente um cluster, com o objetivo de minimizar a soma das distâncias quadráticas entre os pontos e o centróide do cluster ao qual pertencem.
  
II. O agrupamento por densidade, tal como o DBSCAN, identifica regiões de alta densidade de pontos que são separadas por regiões de baixa densidade e não exige a definição prévia do número de clusters, sendo particularmente útil para dados com formas irregulares ou ruído.
  
III. O agrupamento hierárquico cria uma decomposição hierárquica dos conjuntos de dados e pode ser do tipo aglomerativo, onde cada observação começa em seu próprio cluster e pares de clusters são fundidos à medida que se move para cima na hierarquia, ou do tipo divisivo, que opera em sentido inverso.

Assinale a opção que indica todas as afirmações corretas.

A) Apenas I e II estão corretas.
B) Apenas II e III estão corretas.
C) Apenas I está correta.
D) Apenas II está correta.
E) I, II e III estão corretas.

",E,"

Explicação dos itens:

I. A afirmação está correta. O agrupamento por partição, como o método K-means, é baseado na atribuição de cada ponto de dado a um cluster, com o objetivo de minimizar a variância intra-cluster, ou seja, a soma das distâncias quadráticas entre os pontos de dados e o centro (centróide) do cluster correspondente.

II. Esta afirmação também está correta. O agrupamento por densidade, como o DBSCAN (Density-Based Spatial Clustering of Applications with Noise), é baseado na identificação de áreas de alta densidade que são separadas umas das outras por áreas de baixa densidade. Esse método não requer a especificação prévia do número de clusters e pode descobrir clusters de formas arbitrárias, além de tratar pontos de ruído.

III. A afirmação III está correta. No agrupamento hierárquico, os clusters são formados de maneira sequencial, seja por um processo aglomerativo (também conhecido como bottom-up, onde clusters menores são fundidos em clusters maiores) ou divisivo (top-down, onde um cluster único é sucessivamente dividido em clusters menores).

Assim sendo, todas as afirmações I, II e III são verdadeiras em relação às características gerais das respectivas técnicas de agrupamento mencionadas.",8767396
tópico 5,"Rotulação de partes do discurso, part-of-speech tagging","Questão: A rotulação de partes do discurso (Part-of-Speech Tagging - PoS) é uma ferramenta fundamental no processamento de linguagem natural, utilizada para atribuir categorias gramaticais a cada palavra em um texto. Dada uma frase em português, uma aplicação de PoS irá classificar as palavras de acordo com sua função na sentença, como substantivos (N), verbos (V), adjetivos (ADJ), entre outras. Considere o seguinte exemplo de frase para a aplicação de etiquetas PoS:

""Felizes, os alunos saíram rapidamente da sala após o toque do sino.""

Quais seriam as classificações corretas para as palavras destacadas no contexto da frase acima?

A) Felizes - ADJ, alunos - N, saíram - V, rapidamente - ADV, toque - N, sino - N
B) Felizes - ADV, alunos - N, saíram - ADJ, rapidamente - V, toque - ADV, sino - ADJ
C) Felizes - V, alunos - ADJ, saíram - N, rapidamente - N, toque - V, sino - ADV
D) Felizes - N, alunos - ADV, saíram - ADJ, rapidamente - ADJ, toque - N, sino - V
E) Felizes - ADJ, alunos - V, saíram - N, rapidamente - ADV, toque - ADJ, sino - N

",A," 
Explicação dos itens:

A) ""Felizes"" é um adjetivo que qualifica o estado emocional dos alunos; ""alunos"" é um substantivo comum que refere-se aos indivíduos que estudam; ""saíram"" é um verbo no passado que indica a ação realizada pelos alunos; ""rapidamente"" é um advérbio que modifica o verbo, indicando a maneira como a ação foi executada; ""toque"" e ""sino"" são substantivos que indicam, respectivamente, a ação e o objeto que gerou o som que indica o final do período de aula.

B) Este item ficaria incorreto pois ""Felizes"" não é um advérbio; ""saíram"" não é um adjetivo; ""rapidamente"" não é um verbo; e ""toque"" não é um advérbio.

C) Este item também estaria errado, já que ""Felizes"" não funciona como um verbo; ""alunos"" não é um adjetivo, bem como ""saíram"" não é um substantivo; ""rapidamente"" não é um substantivo; ""toque"" não é um verbo; e ""sino"" também não é utilizado como um advérbio.

D) Mais uma vez, as classificações estão equivocadas: ""Felizes"" não é um substantivo; ""alunos"" não é um advérbio; ""saíram"" não é um adjetivo; ""rapidamente"" não é um adjetivo; e ""sino"" não é um verbo.

E) Este item contém classificações incorretas: ""Felizes"" não é utilizado como um verbo; ""alunos"" não funciona como um verbo na frase; ""saíram"" não é um substantivo; ""toque"" não é um adjetivo.",488080
tópico 5,Técnicas de classificação: Naive Bayes; Árvores de decisão (algoritmos ID3 e C4.5); Florestas aleatórias (random forest); Máquinas de vetores de suporte (SVM – support vector machines); K vizinhos mais próximos (KNN – K-nearest neighbours),"Questão:
Analise as seguintes afirmativas sobre algoritmos de classificação em Aprendizado de Máquina e indique se são verdadeiras ou falsas:

I) O algoritmo Naive Bayes baseia-se no teorema de Bayes e pressupõe a independência condicional entre as características do modelo dado a variável de classe.

II) As Árvores de Decisão utilizando os algoritmos ID3 e C4.5 diferem principalmente na forma como lidam com atributos contínuos e na utilização da poda para evitar o sobreajuste.

III) Florestas Aleatórias são um exemplo de algoritmos ensemble, onde múltiplas árvores de decisão são combinadas para produzir uma classificação final baseada na moda das classes.

IV) Máquinas de vetores de suporte (SVM) trabalham maximizando a margem entre as classes no espaço de características, sendo particularmente eficazes em casos de grande dimensão do espaço de características.

V) O algoritmo K vizinhos mais próximos (KNN) se baseia na premissa de que instâncias de dados semelhantes resolvem problemas semelhantes, por isso a classificação de uma nova instância é feita com base na classificação de suas k vizinhas mais distantes.

Está(ão) correta(s) apenas a(s) afirmativa(s):

a) I, II e IV
b) II, III e V
c) I, III e IV
d) I, II, III e IV
e) Todas as afirmativas estão corretas.

",C,"

Explicação dos itens:

I) Verdadeira. Naive Bayes utiliza o teorema de Bayes para a classificação e assume que todas as características são condicionalmente independentes, dada a classe.

II) Verdadeira. ID3 e C4.5 são algoritmos de árvores de decisão onde um dos diferenciais é a maneira como tratam atributos contínuos e realizam a poda das árvores para evitar o sobreajuste.

III) Verdadeira. As Florestas Aleatórias são um método ensemble que cria várias árvores de decisão e agrupa suas decisões para melhorar a classificação final.

IV) Verdadeira. SVM busca encontrar o hiperplano que otimiza a margem entre as diferentes classes. É bastante eficiente em espaços com muitas dimensões.

V) Falsa. O algoritmo KNN classifica uma nova instância com base na classificação de suas k vizinhas mais próximas, e não mais distantes como sugerido na afirmativa.",8281174
tópico 5,"Avaliação de modelos de classificação: treinamento, teste, validação; validação cruzada","Questão:

Em um projeto de aprendizado de máquina, a correta avaliação do desempenho de modelos de classificação é critica para assegurar a qualidade do processo de tomada de decisão. Sobre a avaliação de modelos de classificação, treinamento, teste, e validação, incluindo a técnica de validação cruzada, é INCORRETO afirmar que:

A) O conjunto de treinamento é utilizado para ajustar os parâmetros do modelo de classificação, permitindo que o modelo aprenda a partir dos dados disponíveis.
B) A validação cruzada é uma técnica que permite uma melhor estimativa do desempenho do modelo em dados não vistos, ao dividir o conjunto de dados em 'k' partes, alternando quais partes são usadas para treino e teste.
C) A taxa de erro obtida no conjunto de teste deve sempre ser usada como a única métrica para indicar a qualidade final do modelo, independentemente do contexto do problema.
D) É recomendável que, após a validação cruzada, se faça um teste final do modelo utilizando um conjunto de dados não utilizado nas etapas de treinamento ou validação.
E) No processo de validação, pode-se utilizar um conjunto separado dos dados, chamado de conjunto de validação, para ajustar hiperparâmetros do modelo e evitar o sobreajuste aos dados de treinamento.

",C,"

Explicação dos itens:

A) Correto. É uma descrição do propósito básico do conjunto de treinamento no processo de aprendizado de máquina.

B) Correto. A validação cruzada é uma técnica amplamente adotada para avaliar o desempenho dos modelos de maneira mais confiável, pois usa diferentes partes dos dados para treinamento e teste e repete esse processo 'k' vezes.

C) Incorreto. A afirmativa é incorreta porque não se deve confiar unicamente na taxa de erro do conjunto de teste. Dependendo do problema, outras métricas, como precisão, revocação (recall), F1 Score, área sob a curva ROC (AUC), entre outras, podem ser necessárias para uma avaliação mais completa do modelo.

D) Correto. Realizar um teste final com um conjunto de dados separado pode fornecer um indicador mais realista do desempenho do modelo em condições de novos dados não vistos.

E) Correto. O conjunto de validação é usado para selecionar e afinar hiperparâmetros do modelo, ajudando a escolher o melhor modelo sem utilizar o conjunto de testes, evitando assim potencial sobreajuste aos dados de treino e também ao de teste.",2893664
tópico 5,"modelos vetoriais de documentos (booleano, TF e TF-IDF, média de vetores de palavras e Paragraph Vector);","Questão:

A Recuperação de Informação (RI) tem como um de seus principais desafios identificar, dentro de um grande volume de dados, aqueles que são relevantes para uma determinada consulta. Vários modelos de representação de documentos têm sido propostos com o objetivo de melhorar a eficácia da RI. Sobre os modelos vetoriais de documentos, avalie as seguintes assertivas e assinale a opção correta:

I. O modelo Booleano é baseado na lógica booleana e usa operadores como AND, OR e NOT para combinar termos de busca, mas não leva em consideração a frequência dos termos nos documentos ou nas consultas.

II. O modelo TF (Term Frequency) melhora o modelo Booleano, considerando a frequência de ocorrência dos termos nos documentos, mas não leva em conta a relevância dos termos no corpus como um todo.

III. O modelo TF-IDF (Term Frequency-Inverse Document Frequency) é uma evolução do modelo TF, pois considera tanto a frequência do termo no documento quanto a sua importância inversa no conjunto de todos os documentos do corpus.

IV. A média de vetores de palavras é uma técnica que representa cada documento pela média dos vetores de todas as palavras nele contidas, não diferenciando a importância das palavras pelo seu contexto ou posição no documento.

V. O modelo Paragraph Vector tenta superar as limitações dos modelos baseados em palavras incorporando noções de ordem das palavras e contexto dentro de um documento, possibilitando a captura de padrões semânticos mais complexos.

Alternativas:

A) Apenas as assertivas I, II e III estão corretas.
B) Apenas as assertivas II, III e IV estão corretas.
C) Apenas as assertivas III, IV e V estão corretas.
D) Apenas as assertivas I, III e V estão corretas.
E) Todas as assertivas estão corretas.

",D,"

Explicação dos itens:

I. Correta. O modelo Booleano é baseado na lógica booleana, muito utilizado para operações de busca simples onde a presença ou ausência de termos é considerada, mas sem ponderações quanto à frequência dos termos.

II. Incorreta. O modelo TF considera sim a frequência dos termos nos documentos, porém, a assertiva contém uma imprecisão ao afirmar que o modelo TF não leva em conta a relevância dos termos no corpus – isso é considerado em modelos mais avançados como o TF-IDF.

III. Correta. O modelo TF-IDF leva em conta a frequência do termo no documento mas também pondera essa frequência inversamente à sua popularidade em todos os documentos, priorizando termos que são importantes no documento específico mas não são comuns em outros documentos.

IV. Incorreta. Embora a técnica descrita seja uma forma de representar documentos, ela não considera a relevância ou a importância dos termos, o que é uma simplificação excessiva. Existem técnicas como o Word Embeddings que levam em conta o contexto das palavras e podem gerar representações vetoriais mais ricas.

V. Correta. O modelo Paragraph Vector, também conhecido como Doc2Vec, é um avanço significativo sobre as representações puramente baseadas em palavras, pois leva em conta a ordem das palavras e o contexto em que aparecem, permitindo aprender representações que capturam nuances semânticas e sintáticas.",9315137
tópico 5,Redes neurais convolucionais e recorrentes,"Questão: Em aprendizado de máquina, as redes neurais convolucionais (CNNs) e as redes neurais recorrentes (RNNs) são amplamente utilizadas para diferentes tipos de tarefas devido à sua arquitetura especializada. Considerando as características e aplicações dessas redes, assinale a opção que descreve corretamente uma diferença fundamental entre CNNs e RNNs e uma aplicação típica para cada uma delas.

A) As CNNs são projetadas para processar dados em forma de grelha, como imagens, enquanto RNNs não conseguem processar dados sequenciais como um texto ou uma série temporal.

B) Os pesos nas CNNs são compartilhados ao longo de diferentes posições na entrada, o que as torna menos adequadas para dados sequenciais, enquanto RNNs são projetadas para lidar com dependências de longo prazo sem nenhuma dificuldade.

C) As RNNs aplicam a mesma função de transformação a cada entrada da sequência de dados, permitindo a captura de informações temporais, o que as torna ideais para tarefas como tradução automática de idiomas, em contraste com as CNNs, que são mais eficazes em reconhecimento de padrões visuais.

D) As RNNs utilizam filtros convolucionais para extrair recursos espaciais hierárquicos dos dados, o que as torna especialmente adequadas para análise de vídeos, enquanto as CNNs empregam unidades de memória para processar sequências de dados como séries temporais financeiras.

E) Ambas as CNNs e RNNs são igualmente eficazes em processar dados sequenciais e não sequenciais e não possuem diferenças significativas em sua arquitetura ou mecanismos de aprendizado.

",C," A diferença fundamental entre CNNs e RNNs citada na alternativa C é que as RNNs processam sequências de dados aplicando a mesma transformação a cada entrada e mantendo um estado interno que captura informações temporais. Isso as torna adequadas para tarefas como a tradução automática de idiomas, que envolve o entendimento de dependências complexas ao longo do tempo. Por outro lado, as CNNs são projetadas para reconhecer padrões visuais hierárquicos, como os encontrados em imagens, utilizando filtros convolucionais para extrair características espaciais, o que as torna ideais para aplicações como reconhecimento de imagens.

Explicação dos itens:

A) Esta alternativa está errada porque sugere que RNNs não podem processar dados sequenciais, quando na verdade são especializadas para tal.

B) A segunda parte da alternativa está incorreta, pois as RNNs têm dificuldades com dependências de longo prazo devido ao problema de desvanecimento do gradiente.

C) Esta alternativa está correta, descreve acertadamente as aplicações típicas e as operações das CNNs e das RNNs.

D) Esta alternativa está errada porque confunde os mecanismos funcionais das CNNs e das RNNs, atribuindo funções ao contrário.

E) Esta alternativa está errada porque sugere que não há diferença entre as arquiteturas de CNNs e RNNs, quando na realidade elas possuem características e aplicações distintas.",6724274
tópico 5,Técnicas de redução de dimensionalidade: Seleção de características (feature selection); Análise de componentes principais (PCA – principal component analysis),"Questão: Em análise de dados, diversas técnicas são utilizadas para redução de dimensionalidade, com o objetivo de simplificar modelos, reduzir custo computacional ou remover informações redundantes. Entre as técnicas de Seleção de características (feature selection) e Análise de componentes principais (PCA – principal component analysis), assinale a alternativa correta referente às suas aplicações e caracterizações.

A) Seleção de características é uma técnica que, ao contrário do PCA, mantém a interpretabilidade original das variáveis, pois seleciona um subconjunto de características originais relevantes para o modelo.

B) PCA transforma as variáveis originais em um novo conjunto de variáveis, chamadas de componentes principais, que não são escolhidas por sua relevância, mas por sua capacidade de reter a variabilidade dos dados originais.

C) Tanto a seleção de características como o PCA são métodos supervisionados, necessitando do conhecimento dos rótulos dos dados para determinar a redução de dimensionalidade mais apropriada.

D) O PCA é considerado uma técnica de feature selection, pois seleciona as variáveis mais importantes mantendo a variabilidade dos dados e ignorando as demais.

E) Seleção de características busca maximizar a variância dos dados enquanto minimiza a covariância entre as variáveis, processo inverso ao realizado pelo PCA.

",A," 
A seleção de características é um método de redução de dimensionalidade que busca escolher um subconjeto de características relevantes a partir das características originais, preservando a interpretabilidade das variáveis. Ela não transforma os dados, apenas seleciona dentre os existentes. Já o PCA, caracterizado na opção B, é um método não-supervisionado que transforma as variáveis originais em componentes principais, que são um novo conjunto de variáveis ortogonais que maximizam a variabilidade dos dados. A opção C está incorreta porque o PCA é um método não-supervisionado e não requer rótulos dos dados. A opção D não é correta porque PCA é uma técnica de transformação de características, e não seleção. Finalmente, a opção E confunde os objetivos das duas técnicas, já que a seleção de características não visa a maximização ou minimização de variância ou covariância.",6283704
tópico 5,"Modelos de representação de texto - N-gramas, modelos vetoriais de palavras (CBOW, Skip-Gram e GloVe)","Questão:

Modelos de representação de texto têm papel essencial em várias aplicações de processamento de linguagem natural, como tradução automática, análise de sentimentos e sistemas de recomendação. Entre os modelos de representação de texto baseados em aprendizado de máquina, os modelos vetoriais de palavras têm se destacado devido à sua eficácia em capturar semântica e sintaxe do idioma de maneira densa e contínua. Considere os seguintes modelos vetoriais de palavras:

I. CBOW (Continuous Bag of Words)
II. Skip-Gram
III. GloVe (Global Vectors for Word Representation)
IV. N-gramas

Estes modelos diferem em vários aspectos, incluindo a forma como levam em conta o contexto das palavras e a natureza do treinamento. Qual dos seguintes itens representa mais corretamente esses modelos?

A) CBOW e Skip-Gram são modelos que não utilizam contexto de palavras, enquanto N-gramas e GloVe baseiam-se fortemente no contexto para gerar a representação vetorial.
B) N-gramas considera sequências contínuas de palavras, enquanto que CBOW e Skip-Gram são baseados em redes neurais que aprendem representações vetoriais a partir do contexto de palavras isoladas.
C) GloVe combina as técnicas de matriz de co-ocorrência de palavras e de aprendizado de embeddings, enquanto que CBOW e Skip-Gram utilizam apenas a abordagem de embeddings aprendidos de forma isolada.
D) CBOW prevê uma palavra com base no contexto fornecido por outras palavras, Skip-Gram prevê o contexto baseado em uma palavra, e GloVe foca na co-ocorrência de pares de palavras em toda a base de dados.
E) Todos os modelos, CBOW, Skip-Gram, GloVe e N-gramas, utilizam redes neurais profundas e tem como princípio a maximização da probabilidade conjunta de ocorrência de palavras adjacentes.

",D,"

A alternativa A está incorreta porque tanto o CBOW quanto o Skip-Gram utilizam contexto de palavras para gerar representações vetoriais. A alternativa B está quase correta, exceto pelo fato de afirmar que o CBOW e o Skip-Gram aprendem representações vetoriais a partir do contexto de palavras ""isoladas"", o que não é verdade, pois os contextos são formados por múltiplas palavras. A alternativa C está incorreta, dado que o CBOW e o Skip-Gram são métodos baseados em contexto e não aprendem representações ""de forma isolada"". A alternativa E está incorreta porque os modelos N-gramas não utilizam redes neurais e não focam na probabilidade conjunta de ocorrência, mas sim em sequências contínuas de palavras (n-gramas). A alternativa D é a correta, pois descreve adequadamente que o CBOW usa múltiplas palavras para prever uma palavra central, o Skip-Gram usa uma palavra central para prever seu contexto, e o GloVe analisa a co-ocorrência global de pares de palavras para aprender representações vetoriais.",8070307
tópico 5,"métricas de avaliação - matriz de confusão, acurácia, precisão, revocação, F1-score e curva ROC","Questão:
Uma equipe de cientistas de dados está avaliando o desempenho de um modelo de classificação binária no contexto de diagnóstico médico, onde é fundamental não apenas identificar corretamente todos os casos positivos (doença presente), mas também reduzir ao máximo o número de falsos negativos, devido à gravidade de se deixar uma doença sem tratamento. Foram propostas diferentes métricas de avaliação para otimizar o modelo. Dentre as seguintes opções, qual métrica seria a mais adequada para se concentrar durante a fase de otimização do modelo, considerando o contexto apresentado?

A) Acurácia

B) Precisão

C) Revocação

D) F1-score

E) Área sob a curva ROC (AUC-ROC)

",C," 

Explicação dos itens:

A) Acurácia - Embora represente a proporção de predições corretas (verdadeiros positivos e verdadeiros negativos) sobre o total de casos, acurácia não é a métrica mais sensível quando se tenta minimizar os falsos negativos em contextos críticos.

B) Precisão - A precocidade é útil quando o custo dos falsos positivos é alto, mas neste caso, estamos mais preocupados com os falsos negativos.

C) Revocação - Também conhecida como sensibilidade, mede a proporção de verdadeiros positivos identificados corretamente dentre todos os casos positivos reais. É a métrica mais relevante quando o objetivo é minimizar os falsos negativos, como no diagnóstico de doenças.

D) F1-score - O F1-score é o balanço entre precisão e revocação, e embora seja uma métrica robusta, a ênfase em balanceá-las pode não ser ideal se a prioridade é realmente maximizar a identificação de todos os positivos (revocação).

E) Área sob a curva ROC (AUC-ROC) - A AUC-ROC é uma medida de desempenho que considera tanto a taxa de verdadeiros positivos quanto a taxa de falsos positivos. No entanto, em contextos médicos onde minimizar falsos negativos é crucial, a revocação pode ser uma métrica mais direcionada e, portanto, mais adequada do que a AUC-ROC.",2627860
tópico 5,Ajuste de modelos dentro e fora de amostra e overfitting,"Questão: Na análise de dados, o ajuste de modelos é uma prática fundamental para entender as relações entre variáveis. Entretanto, um problema comum ao ajustar modelos é o overfitting, que ocorre quando o modelo se ajusta muito bem ao conjunto de dados de treino, mas não generaliza adequadamente para novos dados. Com relação ao ajuste de modelos dentro e fora de amostra e o overfitting, avalie as seguintes afirmações:

I - Um modelo com overfitting tende a ter um desempenho excelente nos dados de treino, mas seu desempenho pode cair significativamente quando aplicado a um conjunto de dados de teste.
II - Ajustar um modelo com uma quantidade excessiva de parâmetros em relação à complexidade do fenômeno estudado aumenta o risco de overfitting.
III - Um sinal de overfitting é quando a métrica de desempenho melhora substancialmente na validação cruzada, mas se mantém consistente em conjuntos de validação não vistos.
IV - A redução da complexidade do modelo, a incorporação de técnicas de regularização e a utilização de um maior conjunto de dados de treino são estratégias eficazes para mitigar o risco de overfitting.

É correto o que se afirma em:

A) I e II, apenas.
B) III e IV, apenas.
C) I, II e IV, apenas.
D) II, III e IV, apenas.
E) I, II, III e IV.

",C,"

Explicação dos itens:

I - Esta afirmativa está correta. O overfitting faz com que o modelo seja altamente ajustado aos dados de treino, capturando até mesmo os ruídos ou flutuações aleatórias. Isso pode prejudicar sua capacidade de generalização para novos conjuntos de dados, o que é um comportamento indesejado.

II - Também correta. Modelos com muitos parâmetros são mais flexíveis e, por isso, são capazes de se ajustar muito bem aos dados de treino, o que pode levar ao overfitting.

III - Esta afirmativa está incorreta. Um sinal de overfitting não é a melhoria substancial na validação cruzada; ao contrário, é quando a métrica de desempenho é excelente nos dados de treino, mas cai significativamente em um conjunto de dados de validação ou teste, que não foram utilizados para o ajuste do modelo.

IV - Esta afirmação é verdadeira. Reduzir a complexidade do modelo, aplicar regularização (como o Ridge ou Lasso) ou aumentar o tamanho do conjunto de dados de treino são métodos comuns para reduzir o risco de overfitting. A regularização penaliza os grandes coeficientes e pode levar a um modelo mais generalista, prevenindo o ajuste excessivo aos dados de treino.",5585883
tópico 5,"Métricas de similaridade textual - similaridade do cosseno, distância euclidiana, similaridade de Jaccard, distância de Manhattan e coeficiente de Dice","Questão:

A análise de similaridade textual é fundamental no processamento de linguagem natural para diversas aplicações, tais como sistemas de recomendação, detecção de plágio e agrupamento de documentos. Várias métricas podem ser utilizadas para quantificar a similaridade entre documentos de texto, cada uma com suas características e aplicações específicas. Considerando as métricas de similaridade do cosseno, distância euclidiana, similaridade de Jaccard, distância de Manhattan e coeficiente de Dice, avalie as seguintes afirmativas e selecione a opção correta:

I. A similaridade do cosseno e o coeficiente de Dice são baseados na ideia de ângulo entre dois vetores no espaço multidimensional, sendo mais adequados para medir a similaridade em textos com base na frequência dos termos.

II. A distância euclidiana e a distância de Manhattan são exemplos de métricas de distância e podem ser influenciadas pela magnitude dos vetores de características, sendo mais sensíveis ao tamanho dos documentos.

III. A similaridade de Jaccard é particularmente útil para medir a similaridade entre conjuntos, sendo menos sensível a diferenças no tamanho dos documentos, uma vez que leva em conta apenas a presença ou ausência de termos.

IV. A distância de Manhattan, também conhecida como distância L1, mede a similaridade entre textos somando a diferença absoluta entre as coordenadas de seus vetores de características e é menos sensível a outliers do que a distância euclidiana.

Qual é a alternativa correta?

A) Apenas as afirmativas I e II estão corretas.
B) Apenas as afirmativas II e III estão corretas.
C) Apenas as afirmativas I, II e III estão corretas.
D) Apenas as afirmativas I, III e IV estão corretas.
E) Todas as afirmativas estão corretas.

",C,"

Explicação dos itens:

I. Correta. Tanto a similaridade do cosseno quanto o coeficiente de Dice comparam a orientação de dois vetores de termos no espaço de características, ignorando a magnitude e focando na direção dos vetores. Isso os torna adequados para medir a similaridade baseada na frequência dos termos, ignorando o comprimento dos documentos.

II. Correta. A distância euclidiana e a distância de Manhattan são métricas de distância que calculam a diferença entre dois vetores pontualmente. Eles podem ser afetados pelo comprimento dos vetores (documentos), o que pode ser um problema se os documentos tiverem tamanhos muito diferentes.

III. Correta. A similaridade de Jaccard mede a proporção de termos comuns dividida pela união dos termos de dois conjuntos (documentos), focando apenas na presença ou ausência de termos e não na sua frequência, tornando-a menos sensível ao tamanho do documento.

IV. Incorreta. A distância de Manhattan é, de fato, conhecida como distância L1, e mede a soma das diferenças absolutas entre as coordenadas de vetores em um espaço multidimensional. No entanto, não é menos sensível a outliers do que a distância euclidiana, essa característica é mais associada à distância euclidiana, que é calculada como a raiz quadrada da soma dos quadrados das diferenças (distância L2) e, em função disso, é mais afetada por pontos de dados extremos (outliers).",9997756
tópico 5,Técnicas de regressão: Árvores de decisão para regressão; Máquinas de vetores de suporte para regressão,"Questão:
Dentre as técnicas avançadas de regressão utilizadas em problemas de modelagem preditiva, Árvores de Decisão e Máquinas de Vetores de Suporte (SVMs) são amplamente aplicadas. Considerando o contexto de modelos de regressão com essas técnicas, analise as seguintes afirmações:

I. Árvores de decisão para regressão realizam a segmentação do espaço de entrada em regiões homogêneas, utilizando como critério a redução máxima possível do erro quadrático.
II. Máquinas de vetores de suporte para regressão (SVR) buscam encontrar o hiperplano que melhor ajusta os dados, com uma margem que pode ser controlada pelo parâmetro de regularização C, o qual determina o trade-off entre a complexidade do modelo e a tolerância a erros.
III. O processo de poda em árvores de decisão serve para aumentar a complexidade do modelo com o objetivo de melhor capturar as relações não lineares nos dados.
IV. Em SVR, o uso de kernels permite lidar com dados que não são linearmente separáveis, mapeando-os para um espaço de maior dimensão onde se espera que sejam linearmente separáveis.

Assinale a opção que contém apenas as afirmações corretas.

A) I e II
B) II e III
C) I e IV
D) II e IV
E) III e IV

",A,"

Explicação dos itens:
A) A afirmação I está correta porque é assim que árvores de decisão para regressão funcionam, dividindo o espaço de entrada em regiões com base na minimização do erro quadrático. A afirmação II também está correta, pois as SVMs de fato usam o parâmetro de regularização C para balancear a complexidade do modelo e o quanto de erro é aceitável.
B) A afirmação III está incorreta, pois a poda em árvores de decisão é utilizada para evitar overfitting, reduzindo a complexidade do modelo, e não para aumentá-la.
C) A afirmação III, como mencionado anteriormente, é incorreta.
D) A afirmação IV está correta pois o uso de kernels em SVR de fato permite lidar com dados não linearmente separáveis, mas a afirmação III ainda é incorreta.
E) A afirmação III é incorreta e a afirmação IV está correta pelo mesmo motivo discutido anteriormente.",2798896
tópico 5,"Avaliação de modelos de classificação: treinamento, teste, validação; validação cruzada","Questão: Na avaliação de modelos de classificação, é crucial dividir o conjunto de dados adequadamente para garantir que o modelo seja genérico e robusto para dados não vistos anteriormente. Dadas as metodologias de treinamento, teste e validação, e validação cruzada, escolha a alternativa que melhor descreve a função da técnica de validação cruzada no processo de avaliação de modelos de classificação.

A) A validação cruzada consiste em dividir o conjunto de dados em duas partes: uma para treinamento e outra para teste, permitindo a avaliação direta do desempenho do modelo.
B) A validação cruzada é um método que utiliza todo o conjunto de dados para treinar o modelo, seguida por uma etapa de teste que confirma seu desempenho.
C) A validação cruzada é usada apenas na fase inicial de treinamento do modelo, para garantir uma rápida avaliação dos parâmetros antes de passar para a etapa de teste.
D) A validação cruzada é uma técnica que permite repetir e calcular a média dos resultados da avaliação do modelo em diferentes subconjuntos do dado de treino, oferecendo uma medida mais robusta de performance.
E) A validação cruzada é uma etapa opcional que é utilizada apenas em modelos de classificação complexos, para determinar a influência das variáveis independentes no resultado.

",D," 

Explicação dos itens:
A) A alternativa A descreve incorretamente a validação cruzada, pois ela não se limita a uma simples divisão do conjunto de dados em treino e teste. A valiação cruzada envolve múltiplas rodadas de divisão e avaliação.
B) A alternativa B falha em descrever a validação cruzada adequadamente porque sugere que todo o conjunto de dados é usado para treinamento em uma etapa única, o que não representa o processo iterativo e de múltiplas avaliações da validação cruzada.
C) A alternativa C está incorreta porque a validação cruzada não é usada apenas na fase inicial de treinamento, mas em todo o processo de avaliação do modelo.
D) A alternativa D é a correta, pois descreve com precisão que a validação cruzada envolve particionar o conjunto de dados de treinamento em subconjuntos, treinar e testar o modelo em iterações, usando diferentes subconjuntos a cada rodada para uma avaliação mais abrangente da performance.
E) A alternativa E está incorreta porque a validação cruzada não é opcional nem limitada a modelos complexos; ela serve para fornecer uma avaliação mais confiável do modelo, independentemente da complexidade ou das variáveis envolvidas.",5171368
tópico 5,"Métricas de similaridade textual - similaridade do cosseno, distância euclidiana, similaridade de Jaccard, distância de Manhattan e coeficiente de Dice","Questão: Em análise de dados textuais, diversas métricas são empregadas para quantificar a similaridade ou distância entre textos. Dentre as afirmações abaixo sobre métricas de similaridade textual, identifique a correta quanto à aplicação e característica de cada uma das seguintes métricas: similaridade do cosseno, distância euclidiana, similaridade de Jaccard, distância de Manhattan e coeficiente de Dice.

A) A similaridade do cosseno é uma métrica que leva em conta a magnitude dos vetores representando os textos e é ideal para textos de comprimentos bastante distintos.

B) A distância euclidiana é uma métrica apropriada para espaços de alta dimensão, como os espaços vetoriais de textos, uma vez que ela não sofre com a maldição da dimensionalidade.

C) A similaridade de Jaccard é uma métrica baseada na interseção sobre a união de termos entre dois conjuntos, sendo particularmente útil quando os pesos dos termos não são relevantes.

D) A distância de Manhattan é calculada somando-se as diferenças absolutas entre as coordenadas correspondentes dos vetores de texto, sendo extremamente sensível às diferenças em dimensões raras nos dados.

E) O coeficiente de Dice é uma medida que compara a similaridade entre amostras considerando apenas as interseções, ignorando a união dos elementos nos conjuntos, o que a torna ideal para dados esparsos.

",C,"

Explicação dos itens:

A) Incorrecto. A similaridade do cosseno mede o ângulo entre dois vetores e é de fato apropriada para comparar textos de diferentes comprimentos, mas porque ela é independente da magnitude (e não porque leva a magnitude em conta).

B) Incorrecto. A distância euclidiana pode não ser a ideal para espaços de alta dimensão devido à maldição da dimensionalidade, onde o aumento das dimensões pode levar a uma perda de significado na medida de distância.

C) Correto. A similaridade de Jaccard é útil para comparar a semelhança entre conjuntos, baseando-se na proporção de interseção para a união dos conjuntos e é eficaz quando os pesos dos elementos não são considerados.

D) Incorrecto. A distância de Manhattan soma as diferenças absolutas das dimensões dos vetores, mas não é extremamente sensível a diferenças em dimensões raras. Ela é sensível à variação em qualquer dimensão e é usada em casos onde as distâncias devem ser tratadas de forma linear.

E) Incorrecto. O coeficiente de Dice considera tanto a interseção quanto o dobro da interseção no denominador, dividido pela soma dos tamanhos dos dois conjuntos. Não ignora a união dos conjuntos e é usado quando a duplicação da interseção é importante para a análise.",4173727
tópico 5,Ajuste de modelos dentro e fora de amostra e overfitting,"Questão: Em estatística e aprendizado de máquina, o ajuste de modelos é um processo que envolve a correspondência de um modelo matemático aos dados observados, buscando minimizar a diferença entre os valores observados e os preditos pelo modelo. Um modelo bem ajustado deve apresentar boa performance tanto dentro da amostra (in-sample) quanto fora da amostra (out-of-sample). No entanto, um problema conhecido é o overfitting, que ocorre quando um modelo é excessivamente complexo e se ajusta muito bem aos dados da amostra, mas não generaliza bem para novos dados. Considerando esses conceitos, qual das seguintes afirmações é CORRETA sobre ajuste de modelos e overfitting?

A) Um modelo com overfitting geralmente apresenta desempenho superior em dados fora da amostra em relação a dados dentro da amostra.
B) A simplificação excessiva de um modelo, visando evitar overfitting, nunca irá resultar em underfitting, pois a generalização para dados novos será sempre melhorada.
C) Técnicas de regularização, como o método Ridge (L2) ou Lasso (L1), podem ajudar a prevenir overfitting ao penalizar a complexidade do modelo.
D) A validação cruzada é uma técnica que deve ser evitada, pois tende a aumentar o risco de overfitting, em vez de ajudar a avaliar a capacidade de generalização de um modelo.
E) Overfitting é preferível a underfitting, uma vez que modelos mais complexos são sempre mais precisos, independentemente de sua aplicação a dados novos.

",C,"

A alternativa A é incorreta porque um modelo com overfitting geralmente terá um desempenho pior em dados fora da amostra, uma vez que está sobreajustado aos dados de treinamento. A alternativa B é incorreta porque a simplificação excessiva de um modelo pode, de fato, resultar em underfitting, que é quando o modelo é muito simples para capturar a complexidade dos dados. A alternativa C é correta, pois as técnicas de regularização, como Ridge e Lasso, são usadas exatamente para prevenir o overfitting ao adicionar um termo de penalidade que restringe os coeficientes do modelo. Isso faz com que o modelo seja menos propenso a se ajustar ao ruído nos dados de treinamento e, consequentemente, a ter melhor generalização. A alternativa D é incorreta porque a validação cruzada é uma técnica amplamente usada para avaliar a capacidade de generalização de um modelo, reduzindo a probabilidade de overfitting ao garantir que o modelo seja testado em diferentes subconjuntos dos dados. A alternativa E é incorreta porque overfitting não é desejável; um modelo que é excessivamente complexo pode não ser aplicável a novos dados, enquanto underfitting e overfitting são condições que devem ser equilibradas para alcançar a melhor performance de generalização.",7949714
tópico 5,"modelos vetoriais de documentos (booleano, TF e TF-IDF, média de vetores de palavras e Paragraph Vector);","Questão: No contexto de Recuperação de Informação, diversos modelos vetoriais são aplicados para representar documentos e consultas a fim de lidar com a organização e recuperação de grandes coleções de texto. Com relação aos modelos vetoriais de documentos, assinale a opção que descreve corretamente uma das características ou usos dos modelos Booleano, TF (Term Frequency), TF-IDF (Term Frequency-Inverse Document Frequency), média de vetores de palavras e Paragraph Vector.

A) O modelo booleano utiliza expressões algébricas para avaliar a presença ou ausência de termos nos documentos, possibilitando buscas complexas, mas não considera a frequência de termos como fator relevante.

B) O modelo TF é baseado unicamente na frequência dos termos dentro do documento, sem levar em conta a sua importância relativa em relação à coleção de documentos como um todo.

C) O modelo TF-IDF pondera a frequência dos termos no documento pela frequência inversa desses mesmos termos na coleção de documentos, de modo que termos comuns recebem pesos menores, enquanto termos mais raros são mais valorizados.

D) A média de vetores de palavras consiste em representar um documento pelo produto da matriz de termos pelo vetor de termos únicos, considerando assim todas as inter-relações possíveis entre os termos.

E) O Paragraph Vector, também conhecido como Doc2Vec, é um modelo que ignora a semântica e relações sintáticas entre as palavras, focando apenas na frequência e dispersão dos termos no corpus.

",C,"

Explicação dos itens:

A) Correto para o modelo booleano – Não leva em conta a frequência de termos, mas apenas a sua presença ou ausência e é capaz de realizar buscas complexas com operadores lógicos.
    
B) Correto para o modelo TF – Foca na frequência de termos no documento, mas não leva em conta a sua importância em relação à coleção inteira, o que pode ser uma limitação.
    
C) Correto para o modelo TF-IDF – Considera não só a frequência de um termo no documento, mas também a sua raridade em toda a coleção de documentos, tendo como base a frequência inversa do termo nos documentos.
    
D) Incorreto – A média de vetores de palavras representa um documento através da média dos vetores de suas palavras, comumente gerados a partir de modelos de linguagem como Word2Vec, mas não envolve uma matriz de termos pelo vetor de termos únicos.
    
E) Incorreto – O Paragraph Vector leva em conta a ordem das palavras e aprende representações de vetores fixas para os parágrafos/documentos, capturando a semântica e as informações contextuais.",3538764
tópico 5,"Processamento de linguagem natural: Normalização textual - stop words, estemização, lematização e análise de frequência de termos;","Questão: Em um projeto de Processamento de Linguagem Natural (PLN), a normalização textual é uma etapa fundamental para preparar os dados para tarefas downstream, como classificação de texto, análise de sentimentos, ou extracção de informações. Sobre os processos de normalização de textos, avalie as seguintes afirmações:

I. A remoção de stop words é uma técnica que envolve a eliminação de palavras comuns, que são consideradas irrelevantes para o entendimento do significado principal do texto, como preposições, artigos e conjunções.
II. A estemização refere-se ao processo de reduzir uma palavra à sua raiz ou forma base, frequentemente resultando em um fragmento de palavra que pode não ter um significado lexical independente.
III. A lematização é um processo mais sofisticado que a estemização e busca reduzir a palavra à sua forma base ou dicionário, o que exige uma compreensão do contexto e da função morfológica da palavra no texto.
IV. Análise de frequência de termos nunca deve ser utilizada em conjunto com a remoção de stop words, pois a remoção dessas palavras comuns pode distorcer a importância de outros termos dentro de um corpus.

A alternativa que contém todas as afirmações corretas é:

A) I, II e III
B) I e IV
C) II, III e IV
D) I, II, III e IV
E) Apenas III e IV

",A," 
I. Correta. A remoção de stop words é uma técnica padrão em PLN utilizada para reduzir o ruído no texto e focar em palavras que carregam mais significado.

II. Correta. A estemização é um processo comum de normalização textual que reduz as palavras às suas raízes ou formas de base, embora essas formas estemizadas possam nem sempre ser palavras válidas na língua.

III. Correta. A lematização é um processo mais avançado que a estemização, levando em consideração o contexto e aspectos morfológicos para retornar a forma canônica ou de dicionário de uma palavra.

IV. Incorreta. A análise de frequência de termos é geralmente utilizada em conjunto com a remoção de stop words para identificar e destacar termos importantes em um texto. A remoção das stop words é feita justamente para evitar que a frequência dessas palavras comuns interfira na análise dos termos relevantes.",9690077
tópico 5,Técnicas de classificação: Naive Bayes; Árvores de decisão (algoritmos ID3 e C4.5); Florestas aleatórias (random forest); Máquinas de vetores de suporte (SVM – support vector machines); K vizinhos mais próximos (KNN – K-nearest neighbours),"Questão: Na área de aprendizado de máquina, diversas técnicas de classificação são empregadas para resolver problemas de reconhecimento de padrões. Dentre as técnicas a seguir, qual é considerada um método de ensemble que constrói múltiplas árvores de decisão durante o treinamento e prediz a classe de uma nova amostra pela classe mais frequente (moda) prevista pelas árvores individuais?

A) Naive Bayes
B) Árvores de Decisão ID3
C) Árvores de Decisão C4.5
D) Florestas Aleatórias (Random Forest)
E) K vizinhos mais próximos (KNN – K-nearest neighbours)

",D," 

Explicação dos itens:

A) Naive Bayes - É um classificador probabilístico fundamentado no teorema de Bayes, e assume que os atributos são independentes entre si. Não é um método de ensemble.

B) Árvores de Decisão ID3 - Algoritmo que cria uma árvore de decisão baseada no ganho de informação para atributos categóricos. Não é um método de ensemble, pois constrói uma única árvore de decisão.

C) Árvores de Decisão C4.5 - Uma evolução do ID3 que lida também com atributos contínuos e valores faltantes. Assim como o ID3, não é um método de ensemble.

D) Florestas Aleatórias (Random Forest) - Um método de ensemble que cria uma coleção de árvores de decisão durante o treinamento e para a classificação de novas amostras utiliza a votação da maioria das árvores (moda).

E) K vizinhos mais próximos (KNN – K-nearest neighbours) - Um método de classificação baseado em instância que atribui a uma nova amostra a classe mais comum entre os k vizinhos mais próximos. Não é um método de ensemble e não cria modelos durante o treinamento.",9388369
tópico 5,Técnicas de regressão: Árvores de decisão para regressão; Máquinas de vetores de suporte para regressão,"Questão: Em análise preditiva, técnicas de regressão são utilizadas para modelar e analisar a relação entre uma variável dependente e uma ou mais variáveis independentes. Duas técnicas comumente empregadas são as Árvores de Decisão para Regressão e as Máquinas de Vetores de Suporte para Regressão (SVR). Em relação a essas técnicas, é correto afirmar que:

A) Árvores de Decisão para Regressão não podem lidar com dados não-lineares, enquanto Máquinas de Vetores de Suporte para Regressão são específicas para relações lineares entre as variáveis.

B) Árvores de Decisão para Regressão dividem o espaço dos dados em regiões homogêneas, enquanto Máquinas de Vetores de Suporte para Regressão procuram o hiperplano que melhor divide as categorias de dados em um espaço de maior dimensão.

C) Máquinas de Vetores de Suporte para Regressão utilizam árvores para representar o modelo de decisão, e Árvores de Decisão para Regressão se baseiam em vetores de suporte para estabelecer os limites de decisão.

D) Árvores de Decisão para Regressão são robustas a outliers, e Máquinas de Vetores de Suporte para Regressão têm alta sensibilidade a outliers no conjunto de dados.

E) Tanto Árvores de Decisão para Regressão quanto Máquinas de Vetores de Suporte para Regressão são capazes de modelar relações não-lineares, no entanto, as SVRs utilizam uma técnica conhecida como kernel trick para efetivamente transformar o espaço dos dados e capturar a não-linearidade.

",E,"
Explicação dos itens:

A) Esta alternativa é incorreta porque Árvores de Decisão para Regressão podem lidar com dados não-lineares e Máquinas de Vetores de Suporte para Regressão podem ser usadas tanto para relações lineares quanto não-lineares, dependendo da escolha do kernel.

B) A primeira parte da alternativa é correta no sentido de que Árvores de Decisão para Regressão dividem o espaço dos dados em regiões homogêneas. No entanto, a segunda parte é incorreta, pois implica que as Máquinas de Vetores de Suporte para Regressão só se aplicam à classificação e não à regressão, o que não é verdade.

C) Essa alternativa é incorreta porque inverte a aplicação das técnicas. As Árvores de Decisão para Regressão são as que utilizam árvores e as Máquinas de Vetores de Suporte para Regressão são as que se baseiam em vetores de suporte.

D) Esta alternativa é incorreta. Árvores de Decisão para Regressão podem ser afetadas por outliers, pois eles podem influenciar na formação das divisões da árvore. As SVR, por outro lado, podem ser robustas a outliers, dependendo da configuração da margem de erro ε e do kernel usado.

E) Esta é a alternativa correta. Ambas as técnicas são capazes de modelar relações não-lineares. As Árvores de Decisão para Regressão o fazem por meio de divisões sucessivas do espaço dos dados, enquanto as Máquinas de Vetores de Suporte para Regressão utilizam o kernel trick para transformar o espaço dos dados e encontrar uma função que aproxime da melhor maneira possível a relação entre as variáveis.",917227
tópico 5,"métricas de avaliação - matriz de confusão, acurácia, precisão, revocação, F1-score e curva ROC","Questão: Em um contexto de classificação binária, onde é essencial minimizar os Falsos Positivos sem negligenciar a capacidade do modelo de identificar corretamente os Positivos Verdadeiros, um cientista de dados precisa escolher a métrica de avaliação mais adequada para otimizar o desempenho do modelo. Qual das seguintes métricas é considerada mais apropriada para atender a essa necessidade específica?

A) Acurácia
B) Precisão
C) Revocação
D) F1-score
E) Área sob a curva ROC (AUC-ROC)

",B,"

Explicações dos itens:

A) Acurácia é uma métrica que mede a proporção de previsões corretas (tanto Positivos Verdadeiros como Negativos Verdadeiros) em relação ao total de casos. No entanto, não é a mais apropriada quando é fundamental diferenciar os Positivos Verdadeiros dos Falsos Positivos.

B) Precisão é a métrica que avalia a proporção de Positivos Verdadeiros em relação a todos os casos classificados como positivos (soma de Positivos Verdadeiros e Falsos Positivos). Nesse cenário, a precisão é crucial, pois o objetivo é minimizar os Falsos Positivos.

C) Revocação, ou sensibilidade, mede a proporção de Positivos Verdadeiros em relação ao total de casos que são realmente positivos (soma de Positivos Verdadeiros e Falsos Negativos). Embora seja importante, a revocação não se concentra especificamente em minimizar os Falsos Positivos.

D) F1-score é a média harmônica entre precisão e revocação. Serve como uma métrica equilibrada que é útil quando ambas as métricas são importantes, mas não é a mais adequada para o foco exclusivo em minimizar Falsos Positivos.

E) Área sob a curva ROC (AUC-ROC) é uma medida de desempenho que compara a taxa de verdadeiros positivos com a taxa de falsos positivos a diferentes limiares. Embora forneça uma avaliação geral do desempenho do modelo em todos os limiares de classificação, não fornece foco específico na minimização de Falsos Positivos em comparação com outras métricas.",9408461
tópico 5,Redes neurais convolucionais e recorrentes,"Questão:
Em aprendizado de máquina, as redes neurais convolucionais (CNNs) e as redes neurais recorrentes (RNNs) desempenham papéis fundamentais no processamento de diferentes tipos de dados e aprendizado de padrões complexos. Considere as seguintes afirmações sobre o funcionamento e as aplicações de CNNs e RNNs:

I - As redes neurais convolucionais são adequadas para dados onde a localização e topologia das informações são importantes, como no processamento de imagens, onde a vizinhança dos pixels tem um papel relevante na identificação de características.

II - Redes neurais recorrentes são projetadas para lidar com sequências de dados, onde o contexto ou a ordem dos elementos são cruciais, sendo amplamente usadas em tarefas de processamento de linguagem natural e séries temporais.

III - Um dos desafios das RNNs é o problema do desvanecimento do gradiente, onde contribuições de informações de etapas anteriores se tornam insignificantes para etapas longas da sequência, dificultando a aprendizagem de dependências de longo prazo.

IV - As CNNs normalmente requerem que a sequência temporal dos dados seja transformada em uma representação que preserve aspectos temporais antes de serem processadas, ao contrário das RNNs que processam sequências temporais diretamente.

Está(ão) correta(s) a(s) afirmação(ões):

A) I e II, apenas.
B) I, II e III, apenas.
C) I, III e IV, apenas.
D) II, III e IV, apenas.
E) I, II, III e IV.

",B,"

Explicação:

I - Correta. As CNNs são especialmente projetadas para processar dados estruturados em grade como imagens, onde a localização espacial dos pixels é essencial para identificar padrões como bordas, cantos, e texturas.

II - Correta. As RNNs são apropriadas para lidar com sequências de dados tais como texto ou séries temporais, onde a informação anterior influencia o entendimento do estado atual ou futuro dos dados.

III - Correta. Um problema bem conhecido das RNNs tradicionais é o desvanecimento do gradiente, que pode dificultar a aprendizagem de dependências de longo prazo em sequências de dados. Modelos como LSTM ou GRU foram desenvolvidos para mitigar esse problema.

IV - Incorreta. As CNNs são usadas para extrair recursos de imagens e dados espaciais; não são projetadas para tratar sequências temporais sem transformação. Redes recorrentes, como as RNNs, são capazes de processar sequências temporais e capturar a dependência temporal diretamente, o que as torna mais adequadas para essa tarefa do que as CNNs.

Portanto, a alternativa correta é a B, que inclui as afirmações I, II e III.",8930802
tópico 5,"Modelos de representação de texto - N-gramas, modelos vetoriais de palavras (CBOW, Skip-Gram e GloVe)","Questão:
A área de Processamento de Linguagem Natural tem evoluído significativamente com a adoção de modelos matemáticos e estatísticos que permitem representar palavras e sentenças de maneira que máquinas possam interpretar e trabalhar com textos de maneira eficaz. Dentre esses modelos, os N-gramas e os modelos vetoriais como CBOW, Skip-Gram e GloVe desempenham papéis importantes. Com base no entendimento desses modelos, analise as afirmações a seguir:

I. N-gramas são sequências contíguas de N itens de uma dada sequência de texto, utilizados principalmente para modelar a linguagem e prever a próxima palavra em uma frase com base na probabilidade condicional.

II. O modelo CBOW (Continuous Bag of Words) prediz uma palavra-alvo com base no contexto em que aparece, enquanto o Skip-Gram, de forma contrária, usa uma palavra-alvo para prever o contexto circundante.

III. O GloVe (Global Vectors for Word Representation) é um modelo que combina técnicas de fatorização de matrizes e modelos de janela de contexto, com o objetivo de assimilar coocorrências globais de palavras para gerar representações vetoriais densas.

É correto o que se afirma em:

A) I, apenas.
B) II, apenas.
C) III, apenas.
D) I e II, apenas.
E) I, II e III.

",E,"

Explicação dos itens:

I. Correto. A afirmação descreve adequadamente a natureza dos N-gramas e seu uso comum em modelagem de linguagem e previsão de próximas palavras.

II. Correto. A descrição aqui traça os objetivos principais dos modelos CBOW e Skip-Gram de maneira acurada. CBOW utiliza o contexto para prever uma palavra-alvo, enquanto que o Skip-Gram faz o oposto, usando uma palavra-alvo para prever as palavras de contexto.

III. Correto. O GloVe é um modelo de representação vetorial de palavras que agrega informações globais de coocorrência e trabalha com a fatorização de grandes matrizes de coocorrência para produzir vetores densos para representação de palavras.

Logo, todas as afirmações são corretas, tornando a alternativa E como a resposta adequada.",7727679
tópico 5,Técnicas de redução de dimensionalidade: Seleção de características (feature selection); Análise de componentes principais (PCA – principal component analysis),"Questão: Em análises de grandes conjuntos de dados, técnicas de redução de dimensionalidade são empregadas para simplificar modelos de aprendizado de máquina, remover ruídos e facilitar a visualização de dados. Entre as técnicas de redução de dimensionalidade, a Seleção de Características (Feature Selection) e a Análise de Componentes Principais (PCA – Principal Component Analysis) são frequentemente utilizadas. Considerando estas técnicas, analise as seguintes afirmações:

I. A Seleção de Características é um processo que escolhe um subconjunto de características relevantes no conjunto de dados com a finalidade de melhorar o desempenho do modelo e reduzir a complexidade computacional.

II. A Análise de Componentes Principais é um método não supervisionado que transforma as variáveis originais do conjunto de dados em um novo conjunto de variáveis correlacionadas chamadas componentes principais.

III. Uma diferença fundamental entre a Seleção de Características e PCA é que a primeira mantém o significado original das variáveis escolhidas, enquanto a segunda transforma as variáveis originais em um novo conjunto de variáveis, o que pode dificultar a interpretação dos resultados.

Quais afirmações estão corretas?

A) Apenas I e II.
B) Apenas I e III.
C) Apenas II e III.
D) I, II e III.

",B,"

Explicação dos itens:

A) A afirmação II está incorreta porque PCA não cria um novo conjunto de variáveis correlacionadas, mas sim variáveis não correlacionadas (componentes principais), que são combinações lineares das variáveis originais.

B) Esta é a alternativa correta. A afirmação I está correta, pois a Seleção de Características de fato envolve escolher um subconjunto de características significativas. A afirmação III também está correta, destacando a principal diferença entre a Seleção de Características, que preserva o significado original das variáveis, e PCA, que gera um novo conjunto de variáveis por meio de combinações lineares das originais sendo estes de difícil interpretação.

C) Erroneamente sugere que a afirmação II está correta quando na verdade PCA gera componentes principais que são não correlacionados, não correlacionadas como mencionado.

D) É incorreta pois inclui a afirmação II como verdadeira. A PCA produz um conjunto de componentes principais não correlacionados que são combinações lineares das variáveis originais.",1777130
tópico 5,"Rotulação de partes do discurso, part-of-speech tagging","Questão: No processamento de linguagem natural, a rotulação de partes do discurso (Part-of-Speech Tagging - POS) é uma etapa fundamental para a compreensão e análise sintática de textos. O algoritmo de Viterbi é frequentemente utilizado em modelos baseados em cadeias de Markov ocultas (Hidden Markov Models - HMMs) para realizar essa tarefa. Considerando os princípios deste algoritmo, assinale a alternativa que descreve adequadamente o funcionamento do algoritmo de Viterbi no contexto de rotulação de partes do discurso.

A) O algoritmo de Viterbi considera apenas a probabilidade de transição entre tags, desconsiderando o vocabulário do texto.

B) O algoritmo de Viterbi determina o caminho mais provável através de um espaço de estados de tags considerando tanto as probabilidades de transição entre tags quanto as probabilidades de emissão das palavras.

C) O algoritmo de Viterbi é um método não probabilístico que se baseia em regras gramaticais pré-definidas para a atribuição de tags.

D) O algoritmo de Viterbi depende de um treinamento prévio supervisionado, utilizando-se de redes neurais para estabelecer as associações entre palavras e tags.

E) O algoritmo de Viterbi utiliza um dicionário fixo de palavras-tipo, não sendo capaz de lidar com vocabulário desconhecido ou fora do vocabulário de treinamento.

",B,"

A) Item incorrecto, pois o algoritmo de Viterbi leva em consideração tanto a probabilidade de transição entre tags quanto a probabilidade de emissão (a probabilidade de uma tag gerar uma determinada palavra).

B) Item correto, o algoritmo de Viterbi encontra o caminho mais provável (sequência de tags) para uma dada sequência de palavras, utilizando tanto a probabilidade de transição entre as tags (modelo de Markov) quanto a probabilidade de emissão (probabilidade de uma tag gerar uma determinada palavra).

C) Item incorrecto, pois o algoritmo de Viterbi é baseado em probabilidades e não somente em regras gramaticais.

D) Item incorrecto, embora o treinamento supervisionado seja necessário para estabelecer as probabilidades do modelo, o algoritmo de Viterbi não depende de redes neurais, e sim de cadeias de Markov ocultas.

E) Item incorrecto, o HMM pode lidar com palavras desconhecidas através de técnicas como smoothing, que ajustam as probabilidades para palavras fora do vocabulário de treinamento.",6216088
tópico 5,"Técnicas de agrupamento: Agrupamento por partição, por densidade e hierárquico","Questão: Em análise de dados, as técnicas de agrupamento (clustering) são usadas para dividir o conjunto de dados em grupos segundo o grau de semelhança entre os membros de cada grupo. Cada técnica de agrupamento tem suas próprias características e aplicações. Considerando as técnicas de agrupamento por partição, por densidade e hierárquico, associe as características listadas a seguir à técnica correspondente.

I - Esta técnica visa identificar regiões densas de objetos no espaço de dados que são separadas por regiões de baixa densidade. É particularmente útil para identificar clusters de forma arbitrária.
II - Baseia-se na criação de uma hierarquia de clusters que podem ser representados por um dendrograma. A abordagem pode ser aglomerativa ou divisiva.
III - Melhor exemplificada pelo algoritmo k-means, esta técnica cria uma divisão do conjunto de dados em um número pré-definido de clusters, onde cada objeto pertence ao cluster com o centroide mais próximo.

Assinale a alternativa que associa corretamente as técnicas de agrupamento às suas características.

A) I - Agrupamento por partição; II - Agrupamento por densidade; III - Agrupamento hierárquico.
B) I - Agrupamento por densidade; II - Agrupamento hierárquico; III - Agrupamento por partição.
C) I - Agrupamento hierárquico; II - Agrupamento por partição; III - Agrupamento por densidade.
D) I - Agrupamento por partição; II - Agrupamento hierárquico; III - Agrupamento por densidade.
E) I - Agrupamento hierárquico; II - Agrupamento por densidade; III - Agrupamento por partição.

",B,"

Explicação dos itens:

I - Agrupamento por densidade: Técnica descrita corresponde ao agrupamento baseado em densidade, como o DBSCAN (Density-Based Spatial Clustering of Applications with Noise), que consegue identificar formas complexas de clusters e lida bem com outliers.

II - Agrupamento Hierárquico: A descrição da criação de uma hierarquia e representação por um dendrograma é característica do agrupamento hierárquico, que pode ser aglomerativo (bottom-up) ou divisivo (top-down).

III - Agrupamento por partição: A técnica descrita é a exemplificação clássica do algoritmo k-means, que é uma forma de agrupamento por partição, na qual se define a priori o número de clusters, e cada objeto é atribuído ao cluster mais próximo com base na distância para o centroid do cluster.",2356372
tópico 5,Técnicas de classificação: Naive Bayes; Árvores de decisão (algoritmos ID3 e C4.5); Florestas aleatórias (random forest); Máquinas de vetores de suporte (SVM – support vector machines); K vizinhos mais próximos (KNN – K-nearest neighbours),"Questão: Em um cenário de aprendizado de máquina, um cientista de dados está desenvolvendo um modelo de classificação para prever se mensagens de e-mail são spam ou não. O conjunto de dados é composto por características como frequência de palavras específicas, presença de links e formatações suspeitas. Considerando os algoritmos de Naive Bayes, Árvores de Decisão (ID3 e C4.5), Florestas Aleatórias, Máquinas de Vetores de Suporte (SVM) e K Vizinhos Mais Próximos (KNN), qual dos seguintes métodos seria mais adequado inicialmente devido à sua capacidade de lidar bem com a hipótese de independência condicional entre os atributos e sua eficiência em termos de tempo de computação para este tipo de tarefa?

A) Árvores de decisão (ID3).
B) Florestas Aleatórias.
C) Máquinas de Vetores de Suporte (SVM).
D) K Vizinhos Mais Próximos (KNN).
E) Naive Bayes.

",E," 

A alternativa correta é E) Naive Bayes. Naive Bayes é amplamente utilizado para a classificação de spam de e-mail devido à sua suposição de independência condicional entre as características, que simplifica o cálculo, facilitando a modelagem em cenários onde as características podem ser consideradas independentes dadas as classes. Além disso, Naive Bayes é eficiente em termos de tempo de computação, o que o torna ideal para a tarefa descrita. Vamos analisar brevemente as outras opções:

A) Árvores de Decisão (ID3) não presumem independência entre atributos e podem levar a árvores excessivamente complexas se houver muitos atributos, resultando em overfitting.

B) Florestas Aleatórias são um conjunto de árvores de decisão e poderiam potencialmente ser usadas, mas não são tão eficientes quanto Naive Bayes em relação ao tempo de treinamento e complexidade do modelo, especialmente quando a independência entre atributos é uma suposição razoável.

C) Máquinas de Vetores de Suporte (SVM) são poderosas para encontrar o hiperplano ótimo de separação entre classes, mas podem ser menos eficientes em termos de tempo de computação em comparação com Naive Bayes, especialmente em conjuntos grandes de dados.

D) K Vizinhos Mais Próximos (KNN) não faz suposições sobre a distribuição dos dados, mas é computacionalmente intensivo, pois requer o cálculo da distância a todos os pontos de treinamento durante o teste, o que pode ser impraticável em conjuntos de dados grandes.",1650100
tópico 5,Técnicas de regressão: Árvores de decisão para regressão; Máquinas de vetores de suporte para regressão,"Questão:

Uma empresa de energia deseja construir um modelo preditivo para antecipar a demanda de energia elétrica com base em diversos fatores climáticos e econômicos. O analista de dados da empresa está comparando os métodos de Árvores de Decisão para regressão e Máquinas de Vetores de Suporte para regressão (Support Vector Regression - SVR). Considerando as características dos dados e dos métodos, qual é a afirmação correta sobre a aplicação destas técnicas?

A) As Árvores de Decisão são menos propensas ao sobreajuste (overfitting) do que as SVR, devido à sua estrutura simplificada.
B) As Árvores de Decisão, por serem modelos baseados em divisões hierárquicas, não são capazes de capturar relacionamentos lineares nos dados.
C) As Máquinas de Vetores de Suporte para regressão podem ser mais eficazes em encontrar uma margem de separação quando há muitas variáveis de entrada e o relacionamento entre elas e a resposta é complexo.
D) As Árvores de Decisão têm a vantagem de serem invariáveis à escala das variáveis de entrada, o que não ocorre com as SVR, que requerem normalização de dados.
E) As Máquinas de Vetores de Suporte para regressão não conseguem tratar eficientemente dados com grande quantidade de características, sendo recomendadas apenas para conjuntos de dados pequenos e de baixa dimensionalidade.

",C,"

A alternativa A está incorreta, pois as Árvores de Decisão são propensas ao sobreajuste, especialmente quando têm muitos níveis de profundidade ou quando não são devidamente podadas. A alternativa B está incorreta porque as Árvores de Decisão são capazes de capturar tanto relações não lineares quanto lineares; sua capacidade não se limita à estrutura dos dados. A alternativa C é a correta, pois as SVR são projetadas para lidar com problemas de alta dimensionalidade e podem ser muito eficazes quando existe um relacionamento complexo entre as variáveis de entrada. A alternativa D está incorreta; apesar das Árvores de Decisão não exigirem normalização de entrada, isso não é uma vantagem absoluta sobre as SVR, já que a normalização dos dados é uma prática comum na preparação dos dados para diversos métodos estatísticos e de aprendizagem de máquina. A alternativa E está incorreta, já que as SVR são também aplicáveis a grandes conjuntos de dados e podem lidar eficientemente com alta dimensionalidade, especialmente quando são utilizados truques do kernel.",2980955
tópico 5,"Avaliação de modelos de classificação: treinamento, teste, validação; validação cruzada","Questão:

Dentro do contexto de aprendizado de máquina e análise de dados, a avaliação de modelos de classificação é um procedimento crítico para garantir que o modelo apresente um desempenho generalizado, evitando problemas como overfitting ou underfitting. Acerca das técnicas e estratégias de avaliação, julgue os itens a seguir e assinale a opção correta:

I. A divisão do dataset em conjuntos de treinamento e teste é uma prática comum que visa separar dados para que o modelo seja treinado e validado de forma independente.
II. A validação cruzada consiste em dividir o dataset completo em apenas dois subconjuntos: um para treinamento e outro para teste, repetindo esse processo uma única vez.
III. No procedimento de validação cruzada k-fold, o dataset é dividido em k subconjuntos mutuamente exclusivos, e o processo de treinamento e validação é repetido k vezes, com cada subconjunto servindo uma vez como teste e as k-1 vezes restantes como treinamento.

É correto apenas o que se afirma em:

A) I
B) II
C) III
D) I e II
E) I e III

",E," 

Explicação dos itens:

I. Correto. Essa é uma prática padrão para avaliar modelos de uma maneira que seja representativa do desempenho em dados não observados durante o treinamento do modelo.

II. Incorreto. A descrição apresentada corresponde à divisão simples de treinamento/teste e não à validação cruzada. A validação cruzada envolve múltiplas divisões e iterações para treinar e validar o modelo.

III. Correto. Essa é a descrição exata da validação cruzada k-fold, uma técnica robusta para avaliação de modelos que visa reduzir a variabilidade dos resultados devido à divisão de treino-teste.

Sendo assim, a alternativa correta é a E, que afirma que I e III estão corretos e II está incorreto.",9344577
tópico 5,Técnicas de redução de dimensionalidade: Seleção de características (feature selection); Análise de componentes principais (PCA – principal component analysis),"Questão: Em processamento de dados e machine learning, a redução de dimensionalidade é frequentemente utilizada para melhorar o desempenho de modelos preditivos e facilitar a visualização de dados. Duas técnicas populares para redução de dimensionalidade são a Seleção de Características (Feature Selection) e a Análise de Componentes Principais (PCA). Sobre essas técnicas, considere as seguintes afirmações:

I. A Seleção de Características é um processo que consiste em selecionar um subconjunto de características relevantes para uso na construção de modelos, descartando as demais por serem redundantes ou pouco informativas.

II. A PCA é uma técnica que transforma os dados originais em um novo conjunto de variáveis, as componentes principais, que são combinações lineares das variáveis originais e mantêm a maior parte da variabilidade dos dados.

III. A PCA pode ser considerada uma técnica de seleção de características, pois seleciona as componentes mais significativas e descarta as outras.

IV. Ao contrário da Seleção de Características, a PCA sempre preserva a interpretabilidade original das variáveis, uma vez que as componentes principais são diretamente relacionadas com as variáveis iniciais.

A alternativa que contém apenas as afirmações corretas é:

A) I e II
B) I e III
C) II e IV
D) I, II e III
E) Todas estão corretas.

",A,"

Explicação dos itens:

I. Correto. A Seleção de Características foca em identificar as características mais relevantes para o modelo e eliminar as que são desnecessárias, o que pode melhorar o desempenho do modelo e reduzir o risco de overfitting.

II. Correto. A Análise de Componentes Principais (PCA) é uma técnica de transformação que procura novas direções de máxima variância nos dados, resumindo-os em componentes principais que são menos numerosos que as variáveis originais.

III. Incorreto. A PCA é uma técnica de transformação de características e não de seleção. Apesar de reduzir a dimensionalidade, ela cria novas variáveis (componentes principais) que são combinações lineares das originais, ao invés de selecionar variáveis já existentes.

IV. Incorreto. As componentes principais da PCA são combinações lineares das variáveis originais e podem ser difíceis de interpretar em termos das variáveis originais. Portanto, a interpretabilidade das variáveis pode ser perdida após a aplicação da PCA.",604529
tópico 5,"Rotulação de partes do discurso, part-of-speech tagging","Questão:
Considere que um pesquisador em processamento de linguagem natural está trabalhando com rotulação de partes do discurso (Part-of-Speech Tagging - POS Tagging) em um corpus do idioma português. O cientista deseja aplicar um modelo de aprendizado de máquina supervisionado para automatizar o processo de anotação, visando a atribuir etiquetas gramaticais às palavras. Qual das opções abaixo revela uma etapa essencial antes da implementação do algoritmo de POS Tagging, que poderá influenciar diretamente a eficiência do modelo?

A) Conversão de todas as palavras para letras minúsculas para evitar a categorização de palavras iguais como diferentes.
B) Utilização exclusiva de textos literários no treinamento para assegurar uma rica variedade de estruturas gramaticais.
C) Inclusão de etiquetas de emoção no conjunto de dados, ampliando a compreensão semântica das sentenças pelo modelo.
D) Remoção completa de pontuação do texto para simplificar o vocabulário a ser analisado pelo algoritmo.
E) Preparação de um conjunto de dados anotados manualmente que servirá como referência para o treino do algoritmo.

",E,"

Explicação dos itens:

A) A conversão de palavras para letras minúsculas é uma técnica comum em tarefas de processamento de texto, mas pode não ser apropriada para POS Tagging, onde a capitalização pode oferecer pistas importantes para identificação das classes gramaticais, como substantivos próprios. Logo, essa etapa não é essencial e pode ser contraproducente.

B) Embora textos literários possam oferecer diversidade linguística, limitar o treinamento a esse tipo de texto pode reduzir a generalização do modelo, visto que outros gêneros textuais têm estruturas e usos linguísticos que podem não estar presentes na literatura. Portanto, tal exclusividade não é recomendada.

C) As etiquetas de emoção não são diretamente relevantes para o processo de POS Tagging, que lida primariamente com classes gramaticais e não com a análise de sentimentos ou conteúdo emocional das sentenças.

D) A pontuação pode ser crucial na delimitação de sentenças e no entendimento do papel gramatical das palavras adjacentes. Remover totalmente a pontuação pode prejudicar a identificação correta das partes do discurso.

E) A preparação de um conjunto de dados anotados manualmente é essencial para treinar um modelo de aprendizado de máquina supervisionado. O modelo depende desse conjunto para aprender a reconhecer padrões e aplicar as etiquetas corretas, influenciando significativamente a eficiência do modelo no POS Tagging. É a etapa mais relevante antes da implementação do algoritmo.
",1614640
tópico 5,"Processamento de linguagem natural: Normalização textual - stop words, estemização, lematização e análise de frequência de termos;","Questão:
No campo da Processamento de Linguagem Natural (PLN), a normalização textual é um passo fundamental para preparar dados de texto para análises subsequentes. Dentre as técnicas de normalização, tem-se a remoção de stop words, estemização, lematização e análise de frequência de termos. Considerando estas técnicas, avalie as afirmativas a seguir e assinale a alternativa correta:

I - A remoção de stop words consiste em eliminar palavras que são comuns e com pouco valor informativo para os modelos, tais como preposições, conjunções e pronomes.

II - A estemização reduz uma palavra à sua forma radical, eliminando afixos flexionais, o que pode levar à geração de ""stems"" que não correspondem a uma palavra válida na língua.

III - A lematização, ao contrário da estemização, busca reduzir a palavra à sua forma canônica ou de dicionário, o que geralmente resulta em uma forma válida na língua e pode ser dependente do contexto.

IV - A análise de frequência de termos ignora a relevância semântica dos termos e considera apenas sua contagem absoluta dentro de um corpus de texto, podendo levar a distorções na interpretação de importância dos termos.

A) Apenas I e II estão corretas.
B) Apenas I, II e III estão corretas.
C) Apenas II e IV estão corretas.
D) Apenas IV está correta.
E) Todas estão corretas.

",B," 

Explicação dos itens:

I - Correto. A remoção de stop words é uma prática comum em PLN para eliminar palavras que são frequentemente consideradas irrelevantes para o significado de um texto e que podem ser encontradas com alta frequência nos dados.

II - Correto. A estemização reduz palavras à sua raiz ou stem, que muitas vezes não é uma palavra com significado independente. Por exemplo, ""running"" pode ser reduzido a ""run"", mas ""better"" pode ser reduzido a um stem como ""bett"", que não é uma palavra em inglês.

III - Correto. A lematização é um processo mais sofisticado que leva em consideração o contexto e converte a palavra para a sua forma lema, que é uma palavra válida. Por exemplo, ""saw"" seria lematizado para ""see"" (verbo) ou ""saw"" (substantivo) dependendo do contexto.

IV - Incorreto. A análise de frequência de termos não necessariamente ignora a relevância semântica dos termos. Os termos podem ser pesados por sua frequência de ocorrência, mas também podem ser ajustados pelo uso de técnicas complementares como o TF-IDF (term frequency-inverse document frequency), que ajuda a ajustar a contagem de termos pela sua relevância em um corpus de documentos.",9452779
tópico 5,Redes neurais convolucionais e recorrentes,"Questão: Em reconhecimento de padrões usando aprendizado de máquina, as redes neurais se destacam pelo seu desempenho e flexibilidade. Redes neurais convolucionais (CNNs) e redes neurais recorrentes (RNNs) são especialmente notáveis por suas aplicações em processamento de imagens e sequências temporais, respectivamente. Considere as seguintes afirmações sobre CNNs e RNNs:

I. As CNNs utilizam filtros convolucionais para capturar hierarquias de características em dados espaciais, como imagens, onde a localização relativa de características é fundamental para a tarefa de reconhecimento.

II. RNNs são especialmente adequadas para dados sequenciais, como séries temporais ou linguagem natural, devido à sua capacidade de manter um estado interno que representa informações de entradas anteriores.

III. Uma desvantagem conhecida das RNNs é o problema de vanishing gradient, que torna difícil para a rede aprender dependências de longo prazo.

IV. CNNs são preferencialmente utilizadas em comparação às RNNs para tarefas que envolvem sequências temporais ou padrões onde a ordem é crucial, pois são mais eficientes na captura de relações temporais.

Assinale a alternativa que contém todas as afirmações corretas:

A) I e II apenas.
B) I, II e III apenas.
C) II, III e IV apenas.
D) I, III e IV apenas.
E) Todas as afirmações estão corretas.

",B," 

Explicação dos itens:

I. Correta. CNNs são projetadas para processar dados em grade, como imagens, e utilizam a convolução para identificar padrões espaciais hierárquicos, o que é fundamental para reconhecimento de imagens.

II. Correta. RNNs têm a capacidade única de conectar informações anteriores à tarefa atual, usando loops internos para processar sequências de dados, tornando-as ideais para dados sequenciais.

III. Correta. O problema de vanishing gradient ocorre quando os gradientes se tornam muito pequenos durante o treinamento, o que dificulta a atualização dos pesos das camadas iniciais da rede e, por consequência, o aprendizado de dependências de longo prazo.

IV. Incorreta. CNNs não são otimizadas para dados sequenciais onde a ordem das entradas é importante; RNNs e suas variantes, como LSTM e GRU, são as mais apropriadas para esses cenários. CNNs são mais adaptadas para processar dados espaciais, como imagens. 

Portanto, as afirmações I, II e III estão corretas, e a afirmação IV está incorreta.",284812
tópico 5,"Técnicas de agrupamento: Agrupamento por partição, por densidade e hierárquico","Questão: Em relação às técnicas de agrupamento (clustering) utilizadas em análise de dados, diferentes abordagens podem ser aplicadas dependendo do tipo de dados e do resultado desejado. Considere as seguintes afirmações a respeito de três técnicas de agrupamento: por partição, por densidade e hierárquico.

I. O agrupamento por partição divide o conjunto de dados em conjuntos distintos, onde cada conjunto representa um cluster. O K-Means é um exemplo de algoritmo baseado nesta técnica que busca minimizar a soma dos quadrados dentro de cada cluster.

II. A técnica de agrupamento por densidade permite a identificação de regiões de alta densidade de pontos, separadas por regiões de baixa densidade. O DBSCAN é um algoritmo que exemplifica essa abordagem, identificando clusters com formas irregulares e capaz de lidar com ruídos.

III. No agrupamento hierárquico, os dados são analisados para construir uma hierarquia de grupos. Este método pode ser 'aglomerativo', onde cada ponto de dados começa como um cluster individual e pares de clusters são fundidos à medida que se sobe na hierarquia, ou 'divisivo', que funciona na direção oposta, começando com um cluster único que é dividido sucessivamente em clusters menores.

Está correto o que se afirma em:

A) I, apenas.
B) II, apenas.
C) III, apenas.
D) I e II, apenas.
E) I, II e III.

",E,"

Explicação dos itens:

A) Este item é incorreto porque afirma que apenas a afirmação I está correta. No entanto, as afirmações II e III também estão corretas.

B) Este item é incorreto porque afirma que apenas a afirmação II está correta. As afirmações I e III também são verdadeiras e relevantes para o entendimento das técnicas de clustering.

C) Este item é incorreto porque afirma que apenas a afirmação III está correta. As afirmações I e II também são verdadeiras e oferecem informações valiosas sobre os métodos de agrupamento por partição e por densidade.

D) Este item é incorreto porque exclui a afirmação III, que é uma descrição correta do método de agrupamento hierárquico, incluindo suas variações aglomerativa e divisiva.

E) Este item é o correto, uma vez que as afirmações I, II e III estão todas corretas. A afirmação I descreve corretamente o agrupamento por partição e o algoritmo K-Means, a afirmação II descreve adequadamente o agrupamento por densidade e o algoritmo DBSCAN, e a afirmação III acuradamente descreve o agrupamento hierárquico e suas abordagens aglomerativa e divisiva.",8672090
tópico 5,Ajuste de modelos dentro e fora de amostra e overfitting,"Questão:

A fundamentação teórica de modelos estatísticos e a capacidade de fazer previsões confiáveis são objetivos principais na análise de dados. Considerando a avaliação de desempenho de modelos e a problemática de ajuste de modelos dentro e fora de amostra, assinale a alternativa correta sobre overfitting.

A) Overfitting é desejável, pois indica que o modelo se ajustou perfeitamente aos dados de treinamento e, portanto, terá um excelente desempenho em dados novos.

B) Overfitting ocorre quando um modelo é excessivamente complexo e captura o ruído específico dos dados de treinamento, afetando negativamente a capacidade de generalização do modelo.

C) Um modelo com overfitting normalmente apresenta um erro de treinamento alto e um erro de teste baixo, indicando que o modelo não aprendeu os padrões nos dados de treinamento.

D) Para evitar o overfitting, recomenda-se sempre utilizar o maior volume possível de variáveis preditoras para capturar toda a variabilidade dos dados de treinamento.

E) A validação cruzada é uma técnica que tende a aumentar o risco de overfitting, pois utiliza múltiplas partições dos dados para avaliar o desempenho do modelo.

",B,"

Explicação dos itens:

A) Esta alternativa é falsa porque o overfitting caracteriza-se justamente pela falta de capacidade de generalização. Apesar de um ajuste perfeito nos dados de treinamento, o modelo falha ao ser aplicado a novos conjuntos de dados.

B) Esta é a alternativa correta. Overfitting acontece quando o modelo é tão complexo que aprende não apenas os padrões subjacentes, mas também o ruído específico aos dados de treinamento, piorando assim o desempenho em dados não vistos.

C) Esta afirmação está errada porque um modelo com overfitting apresentaria um erro de treinamento muito baixo (já que se ajustou perfeitamente aos dados de treino) e um erro de teste alto (indicando que o modelo não generaliza bem para novos dados).

D) Na verdade, adicionar mais variáveis preditoras sem critério pode piorar o overfitting, pois o modelo se torna cada vez mais complexo e sensível ao ruído nos dados de treinamento.

E) A validação cruzada é uma técnica usada para avaliar a capacidade de generalização de um modelo e, portanto, ajuda a mitigar o risco de overfitting, já que o modelo é testado em vários subconjuntos de dados diferentes.",5931438
tópico 5,"métricas de avaliação - matriz de confusão, acurácia, precisão, revocação, F1-score e curva ROC","Questão:
A avaliação de modelos de classificação é uma etapa fundamental em projetos de Machine Learning. A utilização correta de métricas permite entender a qualidade da classificação efetuada pelo modelo. Dentre as métricas para avaliação de classificadores, a matriz de confusão é uma ferramenta poderosa que fornece a base para o cálculo de diversas outras métricas importantes. Supondo que um determinado modelo de classificação binária produziu a seguinte matriz de confusão para um conjunto de teste:

|                     | Previsão Positiva | Previsão Negativa |
|---------------------|-------------------|-------------------|
| Classe Verdadeira Positiva  |        80         |        20         |
| Classe Verdadeira Negativa  |        30         |        70         |

Com base nesses dados, analise as afirmativas abaixo e marque a opção que apresenta a métrica corretamente calculada.

A) Acurácia é dada por (80+20)/(80+20+30+70) = 0,50.
B) Precisão é calculada como 80/(80+30) ≈ 0,727.
C) Revocação (sensibilidade) é dada por 80/(80+70) ≈ 0,533.
D) O F1-score é calculado por 2*(Precisão * Revocação)/(Precisão + Revocação) com Precisão ≈ 0,727 e Revocação ≈ 0,533, resultando em um F1-score aproximado de 0,617.
E) A área sob a curva ROC (AUC-ROC) pode ser interpretada diretamente a partir da matriz de confusão apresentada, resultando em um valor de 0,75.

",B,"

Explicação dos itens:
A) A acurácia é calculada como a soma dos verdadeiros positivos e verdadeiros negativos dividida pelas todas as predições, ou seja, (80+70)/(80+20+30+70) = 150/200 = 0,75. A alternativa (A) está incorreta.
B) Precisão (também conhecida como valor preditivo positivo) é a proporção de verdadeiros positivos entre todas as previsões positivas do classificador, ou seja, 80/(80+30) = 80/110 ≈ 0,727. A alternativa (B) está correta.
C) Revocação (sensibilidade ou taxa de verdadeiro positivo) é a proporção de verdadeiros positivos dividida pela soma de verdadeiros positivos e falsos negativos, ou seja, 80/(80+20) = 80/100 = 0,80. A alternativa (C) está incorreta.
D) O F1-score é a média harmônica entre precisão e revocação. Portanto, ele deveria ser calculado usando a precisão de 0,727 e a revocação de 0,80. O cálculo fornecido na alternativa (D) estaria correto se a revocação não estivesse errada. Assim, alternativa (D) está incorreta.
E) A área sob a curva ROC (AUC-ROC) é uma métrica que resume a performance do classificador independente do limiar de decisão e não pode ser diretamente interpretada a partir da matriz de confusão sem cálculos adicionais ou informações sobre as taxas de falso positivo e verdadeiro positivo em diferentes limiares. Portanto, a alternativa (E) está incorreta.",5596431
tópico 5,"Métricas de similaridade textual - similaridade do cosseno, distância euclidiana, similaridade de Jaccard, distância de Manhattan e coeficiente de Dice","Questão: No processamento de linguagem natural, as métricas de similaridade textual são fundamentais para diversas aplicações, como sistemas de recomendação, detecção de plágio e categorização de texto. Cada métrica tem suas particularidades e é mais adequada para diferentes tipos de dados ou objetivos. Considerando as métricas de similaridade do cosseno, distância euclidiana, similaridade de Jaccard, distância de Manhattan e coeficiente de Dice, qual delas NÃO é baseada diretamente no cálculo de uma distância geométrica entre pontos representantes dos textos em um espaço vetorial?

A) Similaridade do cosseno
B) Distância euclidiana
C) Similaridade de Jaccard
D) Distância de Manhattan
E) Coeficiente de Dice

",C,"

Explicação dos itens:

A) Similaridade do cosseno - Esta métrica baseia-se no cálculo do cosseno do ângulo entre dois vetores no espaço vetorial, portanto considera o ângulo e não a distância geométrica direta entre os pontos.

B) Distância euclidiana - Representa a distância convencional entre dois pontos em um espaço vetorial, calculada a partir da raiz quadrada da soma dos quadrados das diferenças entre as coordenadas correspondentes dos dois pontos.

C) Similaridade de Jaccard - A similaridade de Jaccard não é baseada no espaço vetorial, mas sim na interseção sobre a união de dois conjuntos. Portanto, não é uma distância geométrica, mas uma medida de sobreposição entre conjuntos.

D) Distância de Manhattan - Também conhecida como distância do táxi, é uma métrica de distância que soma as diferenças absolutas das coordenadas dos pontos, representando um percurso retilíneo como se estivesse percorrendo um grid urbano.

E) Coeficiente de Dice - Assim como a similaridade de Jaccard, o coeficiente de Dice calcula a relação entre a interseção duas vezes o tamanho da interseção e a soma dos tamanhos dos dois conjuntos, sendo usado para medir a sobreposição entre eles.

A alternativa correta é a letra C, pois a similaridade de Jaccard é uma métrica que calcula a similaridade entre conjuntos baseando-se na proporção dos seus elementos em comum, contrapondo-se ao cálculo de distâncias geométricas no espaço vetorial como os outros itens mencionados.",8412058
tópico 5,"Modelos de representação de texto - N-gramas, modelos vetoriais de palavras (CBOW, Skip-Gram e GloVe)","Questão: Em relação à modelagem computacional de linguagem e representação de textos para processamento por algoritmos de aprendizado de máquina, diversas técnicas podem ser aplicadas para codificar informações contextuais e semânticas. Sobre os modelos baseados em N-gramas, Continuous Bag of Words (CBOW), Skip-Gram e GloVe (Global Vectors for Word Representation), avalie as afirmações abaixo e escolha a opção correta.

I. O modelo de N-gramas captura a sequência de palavras em uma janela fixa de N itens, ajudando a preservar a informação da ordem em que as palavras aparecem no texto.

II. CBOW e Skip-Gram são técnicas derivadas do modelo Word2Vec, em que CBOW prevê uma palavra-alvo com base no contexto circundante, e Skip-Gram prevê o contexto a partir de uma palavra-alvo.

III. GloVe é uma abordagem que combina os elementos da factorização da matriz de coocorrência de palavras com métodos de aprendizagem local contextuais, como CBOW e Skip-Gram.

IV. Enquanto o modelo de N-gramas escala linearmente com o tamanho do corpus e o valor de N, os modelos Word2Vec e GloVe podem ser menos escaláveis devido à complexidade dos algoritmos de otimização envolvidos.

É correto o que se afirma em:

A) I e II, apenas.
B) II e III, apenas.
C) I, II e III, apenas.
D) II, III e IV, apenas.
E) I, II, III e IV.

",C,"

Explicação dos itens:

A) Incorreta. A afirmação IV é falsa. Embora o modelo de N-gramas possa ter uma escalabilidade afetada pelo tamanho do corpus e pelo valor de N, os modelos Word2Vec e GloVe, a despeito da complexidade dos algoritmos, são projetados para serem eficientes e escaláveis com grandes conjuntos de dados.

B) Incorreta. A afirmação I também é verdadeira, pois os modelos de N-gramas de fato capturam a sequência de palavras.

C) Correta. As afirmações I, II e III são verdadeiras. A afirmação I está correta ao descrever que os N-gramas mantêm a ordem das palavras. A afirmação II também está correta ao diferenciar CBOW e Skip-Gram como métodos de predição de palavras com base no contexto ou previsão do contexto a partir de uma palavra. A afirmação III descreve corretamente a abordagem do GloVe, que combina técnicas de coocorrência de palavras com aprendizado contextual.

D) Incorreta. Como a afirmação IV é falsa, esta alternativa está incorreta.

E) Incorreta. Esta alternativa é incorreta porque inclui a afirmação IV que é falsa.",3343553
tópico 5,"Modelos de representação de texto - N-gramas, modelos vetoriais de palavras (CBOW, Skip-Gram e GloVe)","Questão: Em Processamento de Linguagem Natural, diferentes modelos de representação de texto são adotados para possibilitar que máquinas compreendam e processem linguagem humana. Dentre as técnicas empregadas, estão os modelos baseados em N-gramas e modelos vetoriais de palavras, como CBOW (Continuous Bag of Words), Skip-Gram e GloVe (Global Vectors for Word Representation). Considere as seguintes afirmações sobre esses modelos:

I. N-gramas são sequências contínuas de N itens a partir de um dado conjunto de texto ou fala. No contexto de PLN, esses itens são usualmente palavras, e o modelo de bigramas (2-gramas) é particularmente utilizado para capturar a probabilidade da ocorrência de uma palavra condicionada à palavra precedente.

II. No modelo CBOW, o algoritmo prevê a palavra atual com base no contexto de palavras vizinhas. Por outro lado, o modelo Skip-Gram trabalha na direção oposta, usando a palavra atual para prever o contexto.

III. O modelo GloVe é treinado a partir de matrizes de co-ocorrência de palavras dentro de um corpus. Computa as representações vetoriais de palavras de maneira que preserva mais a semântica e as relações sintáticas do que modelos baseados apenas no contexto local de uma palavra.

Assinale a opção que indica todas as afirmações verdadeiras.

A) I e II apenas.
B) I e III apenas.
C) II e III apenas.
D) I, II e III.
E) Nenhuma das afirmações é verdadeira.

",D," 

Explicação dos itens:

I. A afirmação está correta. N-gramas são sequências de N itens de texto. No caso de um bigrama, ele olha para pares de palavras consecutivas, o que pode ser útil para modelar a probabilidade de palavras, baseando-se na palavra anterior.

II. Correto também. CBOW e Skip-Gram são modelos relacionados, onde CBOW usa o contexto para prever uma palavra alvo e Skip-Gram usa uma palavra alvo para prever o contexto. CBOW tende a ser mais rápido e melhor para palavras frequentes, enquanto Skip-Gram pode capturar melhor representações para palavras mais raras.

III. Também verdadeira. O modelo GloVe é baseado em co-ocorrências globais no corpus inteiro para aprender as representações vetoriais, ao contrário de métodos como o Word2Vec (que inclui CBOW e Skip-Gram), que consideram apenas o contexto local. Esta abordagem permite que o GloVe capture relações interessantes entre palavras, como analogias ou semelhanças mais sutis.",5995781
tópico 5,Ajuste de modelos dentro e fora de amostra e overfitting,"Questão: 

Dados econômico-financeiros são frequentemente modelados para realizar previsões ou inferências sobre comportamentos futuros de determinadas variáveis de interesse. Contudo, um dos problemas recorrentes nesse tipo de modelagem é o overfitting, que ocorre quando um modelo é excessivamente complexo e se ajusta muito bem aos dados dentro da amostra, mas não tem um bom desempenho na previsão de novos dados, ou seja, fora da amostra. Para mitigar o risco de overfitting, qual das seguintes alternativas apresenta uma técnica válida?

A) Incrementar a complexidade do modelo adicionando mais variáveis explicativas com significância estatística marginal.

B) Separar o conjunto de dados em uma amostra de treino para ajustar o modelo e uma amostra de teste para validar sua capacidade preditiva.

C) Focar exclusivamente nos resultados de ajuste dentro da amostra para garantir a precisão nas previsões futuras.

D) Aumentar o número de parâmetros até que o modelo se ajuste perfeitamente aos ruídos e flutuações específicos dos dados históricos.

E) Ignorar a possibilidade de variação estrutural nos dados ao longo do tempo, considerando que o modelo ajustado será sempre estável.

",B," 

Explicação:

A) Esta alternativa é incorreta porque adicionar mais variáveis pode levar à complexidade desnecessária e aumentar o risco de overfitting, especialmente se as variáveis adicionadas tiverem significância marginal.

B) Esta alternativa é correta porque dividir os dados em uma amostra de treino e uma de teste ajuda a avaliar a capacidade do modelo de generalizar para novos dados. É uma técnica comum para verificar a robustez do modelo contra overfitting.

C) Focar somente nos ajustes dentro da amostra é um erro, já que modelos podem apresentar bom desempenho nessas condições, mas falhar ao prever dados não observados, o que denota overfitting.

D) Aumentar o número de parâmetros para se ajustar aos ruídos específicos dos dados é exatamente o que caracteriza o overfitting e deve ser evitado, pois compromete a capacidade preditiva do modelo.

E) Ignorar mudanças estruturais no decorrer do tempo é perigoso porque modelos econômico-financeiros devem ser adaptáveis a mudanças no ambiente e nas relações entre variáveis, o que poderia levar ao erro de modelagem e perda de previsibilidade.",622628
tópico 5,"Técnicas de agrupamento: Agrupamento por partição, por densidade e hierárquico","Questão: Em análise de dados, técnicas de agrupamento, também conhecidas como clustering, são utilizadas para identificar grupos com características similares dentro de um conjunto de dados. Diferentes abordagens de agrupamento possuem características próprias e são aplicadas conforme as peculiaridades dos dados e os objetivos da análise. Considere as seguintes afirmativas sobre as principais técnicas de agrupamento:

I. O agrupamento por partição, como o algoritmo K-Means, organiza os dados em um número pré-definido de grupos, buscando minimizar a variância intracluster e maximizar a variância intercluster.

II. O agrupamento por densidade, tal como o DBSCAN, identifica regiões de alta densidade que são separadas por regiões de baixa densidade e não requer a definição prévia do número de grupos.

III. O agrupamento hierárquico cria uma estrutura em árvore que representa as relações de proximidade entre os dados, podendo ser construída de forma aglomerativa ou divisiva, e geralmente requer o corte em um nível específico para determinar os clusters finais.

É correto o que se afirma em:

A) I, II e III.
B) Apenas I e II.
C) Apenas II e III.
D) Apenas III.
E) Apenas I.

",A," 

Explicação: 

Item I: Correto. O agrupamento por partição, como o mencionado K-Means, trabalha distribuindo os dados em um determinado número de clusters e otimiza a alocação dos pontos de forma a minimizar a dispersão dentro dos clusters e maximizar a separação entre clusters diferentes.

Item II: Correto. O DBSCAN é um exemplo bem conhecido de algoritmo de agrupamento por densidade que identifica regiões de alta densidade separadas por regiões de baixa densidade. Este método não exige que o usuário especifique o número de clusters de antemão, pois identifica os clusters baseando-se na densidade da região.

Item III: Correto. O agrupamento hierárquico cria um dendrograma, ou seja, uma estrutura em forma de árvore que mostra as relações de proximidade entre os dados. Pode ser realizado por métodos aglomerativos, que começam com cada ponto como um cluster individual e os combinam progressivamente, ou divisivos, que começam com um cluster único e o dividem sucessivamente. Para definir a quantidade de clusters, geralmente é feito um corte horizontal no dendrograma em uma altura que considera adequada para a análise.",9989514
tópico 5,Redes neurais convolucionais e recorrentes,"Questão: Em relação às Redes Neurais Convolucionais (CNNs) e Redes Neurais Recorrentes (RNNs), é correto afirmar que:

A) Ambas as arquiteturas são adequadas para o processamento sequencial de dados, como no caso de séries temporais ou linguagem natural.
B) CNNs têm como característica a capacidade de manter o estado ao longo do tempo, o que as torna ideais para a análise de vídeos.
C) RNNs são mais adequadas para tarefas que envolvem reconhecimento de padrões espaciais em imagens, devido à aplicação de filtros convolucionais.
D) As CNNs são especialmente eficazes no reconhecimento de padrões visuais devido à sua arquitetura que imita a conexão de neurônios no córtex visual humano.
E) RNNs superam CNNs em todas as tarefas de aprendizado de máquina, já que são capazes de processar uma quantidade ilimitada de dados sequenciais sem perder informação.

",D,"

Explicação dos itens: 
A) Esta afirmativa é parcialmente verdadeira, pois enquanto RNNs são projetadas especificamente para dados sequenciais, CNNs são melhor aplicadas em dados com padrões espaciais, como imagens.
B) Esta afirmativa está incorreta. São as RNNs (e suas variações, como LSTM e GRU) que mantêm um estado ao longo do tempo, tornando-as mais adequadas para análise de vídeos, quando comparadas a CNNs comuns.
C) A afirmativa inverte os papéis das redes. CNNs são as que utilizam filtros convolucionais para reconhecimento de padrões espaciais em imagens, enquanto as RNNs são adequadas para sequências de dados, como texto e séries temporais.
D) Esta é a afirmativa correta. CNNs são eficazes no reconhecimento de padrões visuais, como partes de faces em tarefas de reconhecimento facial, devido à sua arquitetura inspirada no córtex visual dos seres humanos.
E) Esta afirmação é incorreta. RNNs têm limitações, como a dificuldade em lidar com sequências muito longas devido ao problema do desaparecimento ou explosão de gradientes, e não são superiores em todas as tarefas comparadas com CNNs.",7007705
tópico 5,"Processamento de linguagem natural: Normalização textual - stop words, estemização, lematização e análise de frequência de termos;","Questão: Em processamento de linguagem natural (PLN), diversas técnicas de pré-processamento são empregadas para preparar os dados textuais para análise e modelagem. Cada uma dessas técnicas tem um objetivo específico e pode ter um impacto significativo no desempenho de algoritmos de PLN. Considere as afirmações abaixo sobre normalização textual:

I. A remoção de stop words é uma técnica que consiste em eliminar palavras que aparecem frequentemente no texto, mas que são consideradas de pouca relevância para a compreensão do conteúdo, como artigos, preposições e conjunções.

II. A estemização é o processo de redução de palavras flexionadas (ou às vezes derivadas) ao seu tronco, base ou raiz, geralmente na forma escrita do radical.

III. A lematização é um processo mais sofisticado que a estemização, pois leva em consideração o contexto e converte a palavra em sua forma base ou dicionário, conhecida como lema.

IV. A análise de frequência de termos é utilizada para identificar e remover as palavras mais raras do texto, a fim de focar apenas naquelas que aparecem com alta frequência.

Qual das afirmações acima NÃO descreve corretamente a técnica de pré-processamento mencionada?

A) I
B) II
C) III
D) IV

",D," 

Explicação dos itens:

A) I - Esta afirmação é correta. A remoção de stop words é uma técnica comum em PLN que visa reduzir o tamanho do dataset eliminando palavras que são consideradas de pouca valia para a análise que está sendo feita.

B) II - Esta afirmação também está correta, descreve adequadamente a estemização, que foca em reduzir palavras à sua forma de raiz, não levando em conta o contexto do uso da palavra.

C) III - Esta afirmação é verdadeira e contrapõe corretamente a lematização à estemização, destacando o uso do contexto e a conversão para a forma básica de dicionário, fornecendo um significado mais preciso.

D) IV - A análise de frequência de termos não é utilizada para identificar e remover palavras raras. Pelo contrário, essa técnica é utilizada para identificar as palavras que são mais importantes em um texto, o que geralmente inclui termos que aparecem com maior frequência. Portanto, essa afirmação está incorreta, e é a resposta à questão.",8674360
tópico 5,Técnicas de classificação: Naive Bayes; Árvores de decisão (algoritmos ID3 e C4.5); Florestas aleatórias (random forest); Máquinas de vetores de suporte (SVM – support vector machines); K vizinhos mais próximos (KNN – K-nearest neighbours),"Questão: 
Uma empresa de tecnologia está desenvolvendo um sistema de recomendação de produtos e deseja implementar um modelo de classificação que seja robusto a ruídos e capaz de lidar com grandes volumes de dados. A equipe de ciência de dados avaliou diferentes algoritmos e precisa selecionar aquele que melhor atenda aos requisitos do sistema. Considerando as características dos algoritmos de classificação Naive Bayes, ID3 e C4.5 para árvores de decisão, Random Forest, SVM e KNN, qual seria a escolha mais adequada?

A) Naive Bayes, pela sua eficiência em grandes conjuntos de dados e capacidade de lidar com dados não normalizados.
B) ID3, pois cria árvores de decisão com excelente generalização para dados não vistos anteriormente.
C) Random Forest, por ser menos suscetível a ruídos e capaz de processar grandes volumes de dados mantendo a acurácia.
D) SVM, devido à sua habilidade em encontrar a margem máxima de separação entre as classes.
E) KNN, pelo seu baixo custo computacional e facilidade de implementação em sistemas de recomendação.

",C," 

Explicação dos itens:

A) O Naive Bayes é conhecido por ser simples e rápido no treinamento, sendo eficiente em grandes conjuntos de dados. No entanto, ele assume a independência entre os atributos, o que pode não ser ideal para sistemas de recomendação onde a relação entre os produtos pode ser relevante.

B) O algoritmo ID3 pode criar modelos que se ajustam demais aos dados de treino (overfitting), tendo dificuldades com dados não vistos anteriormente, e também é sensível a ruídos nos dados.

C) Random Forest é um algoritmo de ensemble que cria múltiplas árvores de decisão (floresta) e usa votação para melhorar a robustez e a generalização. É menos suscetível ao overfitting e lida bem com dados ruídos e grandes volumes de dados, sendo uma escolha eficaz para a situação descrita.

D) SVM é poderoso em encontrar um hiperplano ótimo para separação das classes em problemas de classificação binária. Embora seja eficiente, ele pode ser computacionalmente intensivo em grandes conjuntos de dados, o que pode não ser ideal para a empresa nesse cenário.

E) KNN tem um custo computacional elevado durante a fase de teste, pois requer a comparação de cada instância de teste com todas as instâncias de treino, e não lida bem com conjuntos de dados muito grandes. Portanto, não é uma escolha adequada para o requisito da empresa.",4714539
tópico 5,"Métricas de similaridade textual - similaridade do cosseno, distância euclidiana, similaridade de Jaccard, distância de Manhattan e coeficiente de Dice","Questão:

Na área de Processamento de Linguagem Natural (PLN), métricas de similaridade textual são fundamentais para diversas aplicações, como recuperação da informação, detecção de plágio, e sistemas de recomendação. Considere um cenário onde dois documentos de texto precisam ser comparados quanto ao seu conteúdo semântico. 

Assinale a opção que descreve corretamente uma métrica de similaridade textual inadequada para mensurar a similaridade semântica entre dois documentos baseando-se apenas na frequência dos termos.

A) Similaridade do Cosseno - mede o cosseno do ângulo entre dois vetores multidimensionais, sendo cada vetor uma representação do documento no espaço de termos.

B) Distância Euclidiana - calcula a raiz quadrada da soma dos quadrados das diferenças entre as coordenadas dos vetores que representam os documentos.

C) Similaridade de Jaccard - computa a similaridade entre dois conjuntos, sendo a divisão do tamanho da interseção pelo tamanho da união dos conjuntos.

D) Distância de Manhattan - soma o valor absoluto das diferenças entre as coordenadas de dois pontos em um espaço n-dimensional.

E) Coeficiente de Dice - considera duas vezes o número de termos comuns entre os documentos dividido pela soma dos termos nos dois documentos.

",B,"

A - Similaridade do Cosseno é uma métrica apropriada para medir a similaridade semântica, pois leva em conta a orientação, mas não a magnitude dos vetores que representam os documentos.

B - A Distância Euclidiana não é a mais indicada para medir similaridade semântica nesse contexto porque ela é mais sensível à magnitude dos vetores do que à sua direção, o que pode não refletir adequadamente a similaridade de conteúdo quando a frequência dos termos é considerada.

C - Similaridade de Jaccard é adequada para textos convertidos em conjuntos de termos, focando na presença ou ausência de termos e não na sua frequência.

D - Distância de Manhattan é uma métrica que pode ser usada na comparação de documentos, pois foca nas diferenças absolutas entre os termos, mas pode não ser tão precisa quanto a similaridade do cosseno para refletir a similaridade semântica.

E - Coeficiente de Dice é outra métrica de similaridade que foca na proporção de termos compartilhados, sendo útil em contextos semânticos.",8631878
tópico 5,Técnicas de regressão: Árvores de decisão para regressão; Máquinas de vetores de suporte para regressão,"Questão:
Em um contexto de modelagem preditiva, diversas técnicas podem ser empregadas para prever variáveis contínuas a partir de conjuntos de dados. Entre essas técnicas, as Árvores de Decisão para Regressão (Regression Trees) e as Máquinas de Vetores de Suporte para Regressão (Support Vector Regression - SVR) se destacam por suas características peculiares. Nesse sentido, ao utilizarmos essas técnicas, é importante compreender suas propriedades e aplicações. Qual das seguintes afirmações melhor descreve as características e diferenças entre Árvores de Decisão para Regressão e Máquinas de Vetores de Suporte para Regressão?

A) Árvores de Decisão para Regressão não podem modelar relações não-lineares, enquanto as Máquinas de Vetores de Suporte para Regressão são capazes de modelar tanto relações lineares quanto não-lineares.
B) Máquinas de Vetores de Suporte para Regressão são especialmente eficazes em espaços de alta dimensionalidade, ao contrário das Árvores de Decisão, que tendem a sofrer com o aumento da dimensionalidade.
C) Árvores de Decisão para Regressão são mais suscetíveis a sobreajuste (overfitting) comparadas com as Máquinas de Vetores de Suporte para Regressão, que possuem regularização intrínseca.
D) Máquinas de Vetores de Suporte para Regressão sempre necessitam de um volume maior de dados para treinamento em comparação às Árvores de Decisão para Regressão.
E) Árvores de Decisão para Regressão são baseadas em técnicas de kernel, enquanto que as Máquinas de Vetores de Suporte para Regressão utilizam particionamento de espaço.

",C," 
Explicação dos itens:

A) A afirmação é incorreta porque as Árvores de Decisão para Regressão podem modelar relações não-lineares através de sua estrutura de ramificação; portanto, elas não estão limitadas a relações lineares.

B) Esta afirmação é verdadeira, pois as Máquinas de Vetores de Suporte para Regressão são eficientes em lidar com espaços de alta dimensionalidade devido ao uso de funções de kernel. No entanto, não é uma diferença pura em relação às Árvores de Decisão, pois elas também podem ser eficazes em várias dimensões com as pré-poda e pós-poda adequadas.

C) Esta é a afirmação correta. Árvores de Decisão para Regressão são propensas ao sobreajuste, principalmente quando são profundas e não são adequadamente podadas. As Máquinas de Vetores de Suporte para Regressão têm mecanismos de regularização embutidos, como a margem de otimização e os termos de penalidade, que ajudam a prevenir o sobreajuste.

D) Esta afirmação é falsa porque, dependendo da complexidade do problema, ambas as técnicas podem requerer mais ou menos dados. Não é correto afirmar que as SVRs sempre necessitam de mais dados.

E) A afirmação é falsa porque são as Máquinas de Vetores de Suporte para Regressão que utilizam funções de kernel para transformar o espaço de entrada em um espaço de características onde é mais fácil realizar a regressão, enquanto Árvores de Decisão para Regressão particionam o espaço de entrada baseadas em valores de características.",6570145
tópico 5,Técnicas de redução de dimensionalidade: Seleção de características (feature selection); Análise de componentes principais (PCA – principal component analysis),"Questão:

Considere um conjunto de dados de alta dimensionalidade que precisa ser preparado para análise preditiva em um contexto de aprendizado de máquina. O cientista de dados responsável pelo projeto está considerando usar técnicas de redução de dimensionalidade para melhorar a eficácia dos algoritmos de classificação subconjacentes. Entre as técnicas disponíveis, a Seleção de Características (Feature Selection) e a Análise de Componentes Principais (PCA – Principal Component Analysis) são duas abordagens comuns. Nestes termos, avalie as afirmativas a seguir:

I. A Seleção de Características é um método que busca preservar a interpretabilidade dos dados, retirando atributos redundantes ou irrelevantes, mas mantendo apenas as variáveis originais mais significativas.

II. PCA é uma técnica que transforma as variáveis originais em um novo conjunto de variáveis lineares não correlacionadas chamadas componentes principais, que são combinados em ordem de variância explicada.

III. PCA pode ser mais apropriado que Seleção de Características em contextos onde a interpretabilidade das variáveis não é crítica e a compressão de dados é desejada para reduzir o tempo de treinamento dos modelos.

IV. A Seleção de Características é preferida em cenários onde a redução da dimensionalidade deve ser realizada sem qualquer alteração nas variáveis originais, diferentemente do PCA, que cria uma representação transformada das variáveis.

Está correto apenas o que se afirma em:

A) I e II.
B) I, II e III.
C) II e IV.
D) I, III e IV.
E) Todas estão corretas.

",B,"

Explicação dos itens:

I. Correta. A seleção de características visa identificar as mais significativas para o modelo e descartar aquelas que são redundantes ou pouco informativas, mantendo a natureza das variáveis originais. 

II. Correta. PCA transforma o conjunto de dados original em componentes principais que são não correlacionados e ordenados de forma que os primeiros componentes retenham a maior parte da variância dos dados.

III. Correta. PCA é útil em situações em que se deseja reduzir a dimensionalidade dos dados para diminuir a complexidade dos cálculos e o tempo de treinamento, e a interpretabilidade das componentes principais não é uma exigência.

IV. Incorreta. Esta afirmativa é, na verdade, uma descrição da Seleção de Características e não uma vantagem em comparação com PCA. A afirmativa parece sugerir uma contradição entre as opções de redução de dimensionalidade, mas a Seleção de Características é justamente o método que não altera as variáveis originais, enquanto o PCA as transforma.

A opção correta é a letra B, pois as afirmativas I, II e III estão corretas e representam adequadamente os conceitos e diferenças entre as técnicas de redução de dimensionalidade mencionadas.",3587923
tópico 5,"modelos vetoriais de documentos (booleano, TF e TF-IDF, média de vetores de palavras e Paragraph Vector);","Questão: Em sistemas de recuperação de informações, diversos modelos vetoriais são empregados para representar documentos e consultas, possibilitando a comparação e o cálculo de relevância de documentos em relação a uma busca. Entre os modelos vetoriais de documentos temos o Booleano, TF (Term Frequency), TF-IDF (Term Frequency-Inverse Document Frequency), média de vetores de palavras e Paragraph Vector. Considerando esses modelos, avalie as seguintes afirmações:

I. O modelo Booleano utiliza operadores lógicos como AND, OR e NOT para combinar termos de busca, porém não provê um ranqueamento dos documentos baseando-se na frequência dos termos.
II. O modelo de TF considera apenas a frequência absoluta dos termos nos documentos, ao passo que o TF-IDF ajusta essa frequência com base na importância do termo em toda a coleção de documentos.
III. A média de vetores de palavras é uma técnica que ignora a ordem das palavras, mas captura a semântica ao somar e depois dividir a soma pelo número de palavras para formar um único vetor representativo.
IV. Paragraph Vector é uma extensão da média de vetores de palavras que considera a ordem das palavras e a posição dos parágrafos no documento para gerar representações mais expressiva de textos de tamanho variável.

Está(ão) correta(s) a(s) afirmação(ões):

A) I e II apenas.
B) II e III apenas.
C) I, II e III apenas.
D) II, III e IV apenas.
E) I, II, III e IV.

",E,"

Explicação dos itens:

I. Correta. O modelo Booleano de fato utiliza operadores lógicos e não considera a frequência dos termos nos documentos para ranqueamento, o que pode ser uma desvantagem quando comparado a modelos que permitem uma avaliação graduada de relevância.

II. Correta. O modelo TF contabiliza quantas vezes um termo aparece em um documento, mas não considera a importância relativa do termo na coleção como um todo. Já o TF-IDF compensa isso ao reduzir o peso de termos que aparecem em muitos documentos da coleção (comuns), aumentando a relevância de termos mais raros.

III. Correta. O método de média de vetores de palavras gera um único vetor que representa o documento pela média dos vetores de todas as palavras no documento. Isso não leva em conta a ordem ou a sintaxe, mas proporciona uma noção geral da semântica.

IV. Correta. O modelo Paragraph Vector, também conhecido como Doc2Vec, vai além ao considerar a ordem das palavras e a posição do texto para criar uma representação vetorial que captura mais nuances da estrutura e contexto do que a simples média de vetores de palavras.",1885788
tópico 5,"Rotulação de partes do discurso, part-of-speech tagging","Questão:

Na área de Processamento de Linguagem Natural (PLN), a rotulação de partes do discurso, ou part-of-speech tagging (POS tagging), é um passo fundamental para compreensão estrutural e semântica das sentenças. Em relação às técnicas de POS tagging, avalie as seguintes afirmações:

I) Taggers baseados em regras utilizam padrões linguísticos manuais para atribuir as etiquetas adequadas às palavras de acordo com o seu contexto e morfologia.

II) Taggers estocásticos fazem uso de algoritmos que dependem de análises probabilísticas, geralmente baseados em modelos como Hidden Markov Models (HMM) e Conditional Random Fields (CRF).

III) Taggers de aprendizado profundo, como as redes neurais recorrentes (RNNs) e as redes de atenção, como o Transformer, dispensam completamente o uso de dados anotados manualmente, pois aprendem a estrutura gramatical de forma inteiramente autônoma.

IV) Independentemente da abordagem utilizada, é consenso que a análise morfológica das palavras é desnecessária para uma rotulação de partes do discurso eficiente.

Está(ão) correta(s) apenas a(s) afirmação(ões):

A) I e II
B) I, II e III
C) II e III
D) III e IV
E) I, II e IV

",A,"

Explicação dos itens:

A) A afirmação I está correta porque os taggers baseados em regras utilizam conhecimento linguístico pré-definido para rotular as palavras. A afirmação II também está correta, pois taggers estocásticos aplicam modelos probabilísticos para realizar a rotulação baseando-se em dados anotados previamente.

B) A afirmação III está incorreta porque, embora os taggers de aprendizado profundo, como redes neurais, sejam altamente eficazes, eles frequentemente exigem grandes volumes de dados anotados para treinamento. Não aprendem autonomamente sem qualquer supervisão ou dados prévios.

C) As afirmações sobre taggers estocásticos e de aprendizado profundo são avaliadas individualmente em II e III. A afirmação III foi considerada incorrecta na explicação da alternativa B.

D) A afirmação IV é incorrecta. A análise morfológica das palavras é muitas vezes crítica para uma rotulação precisa de partes do discurso, uma vez que a forma das palavras pode fornecer informações valiosas sobre sua função gramatical.

E) A afirmação IV é incorrecta, como explicado na alternativa D. Portanto, as afirmações I e II são as únicas corretas.",5861067
tópico 5,"métricas de avaliação - matriz de confusão, acurácia, precisão, revocação, F1-score e curva ROC","Questão: Um cientista de dados está desenvolvendo um modelo de classificação binária para prever a ocorrência de uma doença rara em um grande conjunto de pacientes. Uma vez que a prevalência da doença é muito baixa, o modelo precisa ser muito cuidadoso para não gerar um número excessivo de falsos positivos, o que poderia sujeitar pacientes saudáveis a procedimentos desnecessários e onerosos. O cientista formula diferentes métricas de desempenho relacionadas à matriz de confusão para escolher o melhor modelo. Considerando as características do problema apresentado, a métrica que DEVE ser priorizada para a avaliação do modelo é:

A) Acurácia, pois representa a proporção de casos corretamente classificados sobre o total de casos avaliados.
B) Precisão, pois indica a proporção de casos positivos corretos dentre todos os classificados como positivos.
C) Revocação, pois mede a proporção de casos positivos corretos dentre todos os casos positivos reais.
D) F1-score, pois combina precisão e revocação em uma média harmônica, equilibrando ambas as métricas.
E) Curva ROC, pois proporciona uma análise visual do desempenho do modelo em diversos limiares de classificação.

",B,"

Explicação dos itens:

A) Acurácia poderia não ser a melhor métrica neste contexto, uma vez que a doença é rara e um modelo que prediz negativo para todos os casos poderia apresentar uma acurácia enganosamente alta devido ao desequilíbrio de classes.

B) A precisão é a métrica mais importante neste cenário porque se preocupa com a proporção de predições positivas que são de fato positivas. Isso ajuda a minimizar o número de falsos positivos, que é especialmente crucial neste contexto médico.

C) A revocação é importante, mas em um contexto onde os falsos positivos têm consequências significativas, ela pode ser menos crítica que a precisão. Um modelo com alta revocação mas baixa precisão pode levar a muitos falsos alarmes.

D) O F1-score é útil quando se deseja um equilíbrio entre precisão e revocação, mas neste caso queremos priorizar a minimização de falsos positivos em detrimento de potencialmente perder alguns positivos verdadeiros.

E) A curva ROC é uma ferramenta útil para avaliar o desempenho do modelo em vários limiares, mas ela não enfoca diretamente na minimização de falsos positivos como a precisão faz.",7283822
tópico 5,"Avaliação de modelos de classificação: treinamento, teste, validação; validação cruzada","Questão: Em um projeto de classificação de textos utilizando aprendizado de máquina, um cientista de dados está enfrentando o desafio de avaliar a performance do modelo de forma robusta. O conjunto de dados é relativamente pequeno e o risco de overfitting é uma preocupação. Para tanto, ele considera diferentes estratégias de avaliação do modelo. Com base nesse cenário, qual das seguintes estratégias é a mais adequada para avaliar o modelo de classificação de texto, fornecendo uma melhor estimativa da performance em dados não vistos?

A) Dividir o conjunto de dados em um conjunto de treinamento (80%) e um conjunto de teste (20%), utilizando o conjunto de teste uma única vez para avaliação final do modelo.

B) Utilizar o método holdout, dividindo os dados igualmente em três partes: treinamento, teste e validação, usando a validação para ajustar os hiperparâmetros e o teste para avaliação final.

C) Empregar validação cruzada com 5-folds no conjunto de treinamento para ajuste de hiperparâmetros e depois testar com um conjunto de teste separado para a avaliação final.

D) Adotar um método de bootstrapping para repetidamente amostrar o conjunto de dados com reposição e avaliar a performance em cada amostra para aprimorar o treinamento do modelo.

E) Utilizar apenas um conjunto de treinamento para ambos treinar e testar o modelo, confiando em medidas internas do algoritmo, como a entropia, para avaliar sua performance.

",C,"

Explicação dos itens:

A) Essa abordagem básica é simples, mas pode não ser adequada para conjuntos de dados pequenos, pois a divisão pode resultar em uma avaliação que não representa bem a capacidade do modelo de generalizar.

B) O método holdout é uma estratégia válida, mas pode não ser a mais eficiente em termos de uso de dados, especialmente quando o conjunto de dados é pequeno.

C) A validação cruzada com k-folds é uma técnica poderosa para conjuntos de dados menores, pois permite o uso de todos os dados para treinamento e teste, e fornece uma estimativa mais estável do desempenho do modelo. O uso de um conjunto de teste separado garante que a avaliação final seja feita em dados não vistos.

D) Bootstrapping é útil para estimar a incerteza de uma estimativa estatística, mas pode não ser a melhor escolha para avaliar a performance do modelo de classificação, especialmente se o conjunto de dados for pequeno e o risco de overfitting for alto.

E) Utilizar o mesmo conjunto de dados para treino e teste pode levar a uma falsa percepção de precisão do modelo, conhecida como overfitting, onde o modelo se ajusta perfeitamente aos dados de treino, mas falha em generalizar para novos dados.",3890771
tópico 5,Redes neurais convolucionais e recorrentes,"Questão: No contexto do aprendizado profundo (Deep Learning), distintas arquiteturas de redes neurais são empregadas para o tratamento de diferentes tipos de dados e problemas. As Redes Neurais Convolucionais (CNNs) e as Redes Neurais Recorrentes (RNNs) são duas arquiteturas fundamentais para compreensão do estado da arte em aprendizado de máquina. Assinale a alternativa correta a respeito das características e aplicações mais adequadas destas redes:

A) CNNs são mais adequadas para a análise de dados temporais sequenciais, como na previsão de séries temporais, devido à sua estrutura ciclíca capaz de capturar dependências temporais.

B) RNNs são especialmente eficientes no processamento de imagens e visão computacional, pois suas camadas convolucionais são capazes de extrair características hierárquicas do espaço visual.

C) CNNs são apropriadas para tarefas relacionadas à visão computacional e ao processamento de imagens, em que a detecção de características locais e a invariância à translação são fundamentais.

D) As RNNs, pela natureza de suas conexões recorrentes, não são capazes de processar entradas de comprimentos variáveis, o que as torna inadequadas para problemas de processamento de linguagem natural.

E) A aplicação de RNNs e CNNs é restrita à classificação de dados estruturados, como tabelas e bases de dados em formatos tabulares, pois essas arquiteturas não se adaptam bem a dados não estruturados.

",C,"

Explicação dos itens:

A) Este item está incorreto porque as CNNs não são idealmente estruturadas para dados temporais sequenciais. Quem desempenha este papel são as RNNs devido à sua estrutura que pode capturar dependências ao longo do tempo.

B) Este item está incorreto porque as RNNs, diferentemente das CNNs, não possuem camadas convolucionais e não são especializadas no processamento de imagens, mas sim em dados sequenciais como texto e áudio.

C) Este item está correto. As CNNs são projetadas para processar imagens, aproveitando a localidade espacial dos dados visuais. A invariância à translação é uma característica-chave das CNNs, permitindo que elas detectem padrões visuais, independentemente de onde apareçam na imagem.

D) Este item está incorreto. As RNNs podem, de fato, processar entradas de comprimentos variáveis, o que é uma de suas principais vantagens. Isso as torna adequadas para o processamento de linguagem natural e outras tarefas sequenciais.

E) Este item está incorreto. RNNs e CNNs são amplamente utilizadas para dados não estruturados, como texto, áudio e imagens. Ambas redes têm sido fundamentais para avanços em várias áreas, incluindo visão computacional, reconhecimento de fala e processamento de linguagem natural.",2915449
tópico 5,"métricas de avaliação - matriz de confusão, acurácia, precisão, revocação, F1-score e curva ROC","Questão: Em um projeto de classificação para detectar fraudes em transações bancárias, um cientista de dados aplicou um classificador e obteve os seguintes resultados a partir da matriz de confusão:

- Verdadeiros positivos (TP): 80
- Verdadeiros negativos (TN): 920
- Falsos positivos (FP): 30
- Falsos negativos (FN): 70

O cientista deseja avaliar o desempenho do classificador utilizando diversas métricas. Com base nos dados fornecidos, qual é o valor da revocação (recall) para a classe de interesse (fraudes)?

A) 0,53
B) 0,97
C) 0,78
D) 0,53
E) 1,00

",B,"

Explicação dos itens:

A) 0,53 - Este valor está incorreto. A revocação é calculada como TP / (TP + FN), que não resulta em 0,53.

B) 0,97 - Este valor está correto. A revocação (recall) é dada pela fórmula TP / (TP + FN), que no caso é 80 / (80 + 70) = 0,5333. O valor mais próximo é 0,53, e por isso a letra B está correta pela arredondamento.

C) 0,78 - Esta resposta está incorreta. Não é o resultado do cálculo de revocação com os valores fornecidos.

D) 0,53 - Embora o número 0,53 apareça novamente, a resposta correta é a letra B, onde o valor da revocação foi arredondado para duas casas decimais.

E) 1,00 - Este valor está incorreto. Uma revocação de 1,00 indicaria que todos os exemplos positivos verdadeiros foram identificados corretamente, o que não é o caso aqui.

Nota: A revocação é uma métrica particularmente importante em contextos como o da detecção de fraudes, pois indica a capacidade do modelo de identificar todos os casos positivos relevantes, mesmo que isso signifique aceitar mais falsos positivos.",5296769
tópico 5,"Técnicas de agrupamento: Agrupamento por partição, por densidade e hierárquico","Questão: Em análise de dados, as técnicas de agrupamento são amplamente utilizadas para identificar estruturas e padrões subjacentes nos dados. Dentre as principais técnicas de agrupamento, temos o agrupamento por partição, por densidade e hierárquico. Considerando essas técnicas, analise as seguintes afirmações:

I. O agrupamento por partição divide o conjunto de dados em vários grupos onde cada dado deve pertencer a um grupo exclusivamente, sendo o k-means um exemplo clássico deste tipo de agrupamento.

II. Agrupamento por densidade foca na identificação de regiões de densidade variada, considerando como grupos as regiões de alta densidade separadas por regiões de baixa densidade, com o DBSCAN sendo um representante desse método.

III. O agrupamento hierárquico cria uma divisão dos dados que pode ser visualizada como um dendrograma, que não requer a pré-especificação do número de grupos, com o método BIRCH sendo um exemplo desse tipo de técnica.

Assinale a opção que contém todas as afirmações corretas:

A) I e II apenas.
B) II e III apenas.
C) I e III apenas.
D) I, II e III.
E) Nenhuma das afirmações é correta.

",C,"

Explicação dos itens:

A) I e II apenas. - Esta opção é incorreta porque a afirmação III é verdadeira em parte, exceto pelo exemplo dado (BIRCH), que na verdade é um método de agrupamento por partição e não hierárquico.

B) II e III apenas. - Esta opção é incorreta pelo mesmo motivo que a anterior: a afirmação III cita incorretamente o método BIRCH.

C) I e III apenas. - Esta é a alternativa correta, pois a afirmação I está correta, descrevendo corretamente o agrupamento por partição e o k-means. A afirmação II também está correta, pois DBSCAN é de fato um algoritmo de agrupamento baseado em densidade. No entanto, a afirmação III é apenas parcialmente verdadeira, pois enquanto a descrição do agrupamento hierárquico é precisa, o método BIRCH não é um exemplo desse tipo de técnica – é na verdade um método de agrupamento por partição.

D) I, II e III. - Esta opção não está correta porque a afirmação III está incorreta quanto ao exemplo fornecido, como mencionado.

E) Nenhuma das afirmações é correta. - Esta opção está incorreta porque as afirmações I e II estão corretas em sua descrição dos tipos de agrupamento.",4853844
tópico 5,Ajuste de modelos dentro e fora de amostra e overfitting,"Questão:
A análise de ajuste de modelos estatísticos é fundamental em diversos campos, como na econometria, na ciência de dados e no aprendizado de máquina. Um dos principais problemas enfrentados por analistas na construção de modelos preditivos é o overfitting, ou seja, quando um modelo se ajusta muito bem aos dados de treino, mas não generaliza o aprendizado para dados novos, não vistos anteriormente. Sobre o ajuste de modelos dentro e fora da amostra, e o fenômeno do overfitting, considere as seguintes afirmações:

I. Um modelo com overfitting apresenta uma alta variância, adaptando-se às irregularidades do conjunto de treino ao ponto de capturar ruído, em vez de representar a relação subjacente entre as variáveis de maneira fiel.

II. Um indicativo de overfitting é quando o modelo apresenta um desempenho consideravelmente melhor no conjunto de treino em comparação ao conjunto de teste.

III. A validação cruzada é uma técnica que nunca pode ser usada para detectar overfitting, pois ela otimiza o desempenho do modelo somente no conjunto de treino.

IV. Métodos de regularização, como a penalidade LASSO ou Ridge, são comumente utilizados para tentar mitigar o problema de overfitting por adicionar um termo de penalidade à complexidade do modelo.

Está(ão) correta(s) apenas a(s) afirmativa(s):

A) I e IV
B) I, II e III
C) II e IV
D) I, II e IV
E) Todas estão corretas

",A,"

Explicação:

Item I: Correto. Overfitting é caracterizado por uma alta variância, onde o modelo é sensível demais aos dados de treino, incluindo o ruído, o que prejudica sua capacidade de generalização para novos dados.

Item II: Correto. Um sinal clássico de overfitting é, de fato, um modelo que tem um desempenho muito bom nos dados de treino mas falha ao prever novos dados, ou seja, tem um desempenho pobre no conjunto de teste.

Item III: Incorreto. A questão utiliza uma palavra tendenciosa, ""nunca"", mas, na realidade, a validação cruzada é uma técnica poderosa para detectar o overfitting, pois permite avaliar como o modelo performa em diferentes subconjuntos dos dados, dando uma ideia de como ele pode generalizar.

Item IV: Correto. Técnicas de regularização realmente são usadas para prevenir overfitting, adicionando uma penalidade que desencoraja o modelo de se tornar excessivamente complexo.",8322120
tópico 5,"Avaliação de modelos de classificação: treinamento, teste, validação; validação cruzada","Questão: Para construir um modelo de classificação robusto e generalizável, um cientista de dados deve preocupar-se com o processo de avaliação e validação do modelo. Sobre as práticas de treinamento, teste, validação e validação cruzada, analise as afirmativas a seguir e assinale a opção correta.

I. O conjunto de dados de treinamento é empregado para ajustar os parâmetros do modelo, enquanto o conjunto de teste é utilizado para avaliar a performance do modelo e estimar como ele se comportará em dados novos e não vistos.

II. A validação cruzada é um método que consiste em dividir os dados em um único conjunto de treinamento e teste, e assim é adequada para avaliar modelos em conjuntos de dados de pequeno porte.

III. Durante a validação cruzada k-fold, o conjunto de dados é dividido aleatoriamente em k subconjuntos (ou 'folds') de aproximadamente igual tamanho. Cada fold é usado uma vez como um conjunto de teste, enquanto os k-1 folds remanescentes compõem o conjunto de treinamento.

IV. Para garantir que as métricas de avaliação do modelo sejam confiáveis, é recomendável utilizar o conjunto de teste múltiplas vezes durante a fase de ajuste e seleção de hiperparâmetros do modelo.

A) Apenas as afirmativas I, II e III estão corretas.
B) Apenas as afirmativas I e III estão corretas.
C) Apenas a afirmativa II está correta.
D) Apenas as afirmativas I e IV estão corretas.
E) Todas as afirmativas estão corretas.

",B,"

Explicação dos itens:

I. Esta afirmativa é correta. O conjunto de treinamento é utilizado para ajustar ou 'treinar' o modelo, e o conjunto de teste é utilizado após o treinamento para avaliar a performance do modelo em dados não vistos, simulando como o modelo pode se comportar em uma aplicação real.

II. Esta afirmativa é incorreta. A validação cruzada é justamente um método onde os dados são divididos em vários conjuntos de treinamento e teste (os 'folds'), não apenas um único conjunto, com o objetivo de garantir que o modelo seja avaliado de maneira mais abrangente e confiável.

III. Esta afirmativa é correta. A validação cruzada k-fold é uma técnica amplamente utilizada que busca reduzir a variância do modelo, garantindo que cada exemplo dos dados tenha a chance de aparecer no conjunto de teste e no de treinamento.

IV. Esta afirmativa é incorreta. Utilizar o conjunto de teste múltiplas vezes durante o ajuste de hiperparâmetros pode levar ao vazamento de informações do teste para o treinamento. O ideal é usar um conjunto de validação separado para a seleção de hiperparâmetros e deixar o conjunto de teste para a avaliação final, de forma a evitar overfitting e garantir uma estimativa imparcial da performance do modelo.",7910256
tópico 5,"modelos vetoriais de documentos (booleano, TF e TF-IDF, média de vetores de palavras e Paragraph Vector);","Questão: Na área de Recuperação de Informações, os modelos vetoriais de documentos são amplamente utilizados para representar e comparar documentos em um espaço de características, facilitando a tarefa de encontrar documentos relevantes para uma determinada consulta. Considere as seguintes afirmações sobre os modelos vetoriais de documentos:

I. O modelo booleano utiliza operadores lógicos AND, OR e NOT para combinar termos de consulta, retornando documentos que satisfazem exatamente a expressão lógica, sem considerar a frequência dos termos nos documentos.

II. O modelo TF (Term Frequency) avalia a relevância de um documento para uma consulta baseado na frequência com que os termos da consulta aparecem no documento, mas não leva em conta a importância do termo no conjunto de dados em geral.

III. O modelo TF-IDF (Term Frequency-Inverse Document Frequency) é uma extensão do modelo TF, que pondera a frequência dos termos pela sua raridade nos documentos do corpus, de forma a ressaltar termos que são comuns em um documento, mas raros no corpus como um todo.

IV. O modelo de média de vetores de palavras gera representações de documentos calculando a média de todos os vetores de palavras contidos no documento, podendo perder nuances importantes de contexto ou ordem das palavras.

V. O modelo Paragraph Vector, também conhecido como Doc2Vec, ao contrário do modelo de média de vetores de palavras, leva em consideração a ordem das palavras e a estrutura do documento para gerar uma representação vetorial única para cada documento.

Assinale a opção que corresponde às afirmativas corretas sobre os modelos vetoriais de documentos.

A) Todas as afirmativas estão corretas.
B) Apenas as afirmativas I, III e V estão corretas.
C) Apenas as afirmativas II, IV e V estão corretas.
D) Apenas as afirmativas I, II, III e IV estão corretas.
E) Apenas as afirmativas II, III e IV estão corretas.

",B,"

Explicação dos itens:

A) Incorreta, pois afirmativa II está errada, visto que o modelo TF, apesar de considerar a frequência dos termos, não pondera a sua importância em relação à raridade dos termos no conjunto de dados, o que é feito pelo modelo TF-IDF.

B) Correta, pois afirmativa I acerta ao dizer que o modelo booleano retorna documentos de acordo com expressões lógicas exatas, afirmativa III está correta ao descrever o modelo TF-IDF, que leva em consideração tanto a frequência dos termos quanto a sua importância no corpus, e afirmativa V está correta ao descrever o modelo Paragraph Vector que incorpora contexto e ordem das palavras.

C) Incorreta por incluir a afirmativa II, que tem uma inexactidão ao sugerir que o modelo TF não considera a importância geral dos termos no dataset, o qual é na realidade abordado pelo modelo TF-IDF.

D) Incorreta por incluir a afirmativa II, que está errada pelo mesmo motivo explicado acima.

E) Incorreta também pela inclusão da afirmativa II e por excluir a afirmativa V, que está correta ao descrever as capacidades do modelo Paragraph Vector.",4615026
tópico 5,"Processamento de linguagem natural: Normalização textual - stop words, estemização, lematização e análise de frequência de termos;","Questão: No campo do Processamento de Linguagem Natural (PLN), a normalização textual é uma etapa crucial para preparar um texto para análises mais complexas. Considerando as técnicas de pré-processamento de textos, analise as seguintes afirmações:

I. A remoção de stop words é realizada para eliminar palavras que são frequentemente usadas em uma língua, mas que portam pouca informação relevante para a análise semântica, como preposições e conjunções.

II. A estemização é um processo que reduz as palavras ao seu radical, frequentemente resultando em um formato que não corresponde à forma de base lexical, mas mantém uma consistência suficiente para análise comparativa de termos.

III. A lematização é semelhante à estemização, no entanto, procura trazer a palavra para sua forma canônica ou de dicionário, chamada de lema, que representa a base morfológica correta de acordo com a flexão de número, gênero ou tempo.

IV. A análise de frequência de termos ignora completamente o contexto e a ordem das palavras, focando unicamente na quantidade de vezes que cada palavra aparece no texto, não contribuindo para o entendimento da estrutura sintática do mesmo.

Está(ão) correta(s) a(s) afirmação(ões):

A) I, II e III apenas.
B) I e IV apenas.
C) II e III apenas.
D) II, III e IV apenas.
E) Todas as afirmações estão corretas.

",A,"

Explicação dos itens:

I. Correto. Esta afirmação descreve com precisão a função das stop words na normalização textual. Stop words são comumente filtradas fora do texto para se concentrar em palavras mais significativas.

II. Correto. A estemização é uma técnica de normalização que reduz as palavras ao seu radical, que pode ou não ser idêntico ao lema ou forma dicionárica da palavra, mas é suficiente para análise de padrões de uso de palavras.

III. Correto. A lematização é um processo mais sofisticado que a estemização e busca reduzir a palavra à sua forma base ou de dicionário, também conhecida como lema, considerando análise morfológica completa.

IV. Incorreto. A análise de frequência de termos de fato concentra-se na contagem da ocorrência das palavras, mas não é verdade que ela ""não contribua"" para o entendimento da estrutura sintática do texto. A posição relativa dos termos e as coocorrências podem fornecer insights sobre a estrutura e o uso linguístico dentro do texto, ainda que mais técnicas possam ser necessárias para uma análise sintática mais aprofundada.",2576682
tópico 5,Técnicas de classificação: Naive Bayes; Árvores de decisão (algoritmos ID3 e C4.5); Florestas aleatórias (random forest); Máquinas de vetores de suporte (SVM – support vector machines); K vizinhos mais próximos (KNN – K-nearest neighbours),"Questão: A precisão é uma métrica importante na avaliação de modelos de classificação e a escolha do algoritmo adequado pode ser crucial para a efetividade da classificação em um grande conjunto de dados com múltiplas classes e atributos contínuos. Considerando os algoritmos Naive Bayes, Árvores de decisão ID3 e C4.5, Florestas aleatórias, Máquinas de vetores de suporte (SVM) e KNN, qual deles é considerado mais apropriado para lidar com categorias múltiplas e atributos contínuos, oferecendo resultados estáveis mesmo diante de um volume significativo de dados?

A) Naive Bayes
B) Árvores de decisão ID3
C) Florestas aleatórias
D) Máquinas de vetores de suporte (SVM)
E) K vizinhos mais próximos (KNN)

",C,"

A) O Naive Bayes pode não ser o mais adequado para conjuntos de dados com múltiplos atributos contínuos devido à sua suposição de independência entre os atributos, o que nem sempre é o caso na prática.

B) Árvores de decisão ID3 são baseadas em entropia e ganho de informação e podem sofrer de ""overfitting"" em conjuntos de dados muito grandes, além de não lidar intrinsecamente bem com atributos contínuos sem pré-processamento.

C) Florestas aleatórias, que são um conjunto de árvores de decisão, geralmente lidam melhor com sobreajuste e fornecem uma classificação robusta em situações com múltiplas classes e atributos contínuos. Elas trabalham bem em larga escala devido a sua capacidade de treinar múltiplas árvores em subconjuntos do conjunto de dados, tornando-as apropriadas para o cenário descrito na questão.

D) SVM (Máquinas de vetores de suporte) podem ser eficazes com atributos contínuos e grandes volumes de dados, porém, tradicionalmente, eles têm desempenho inferior comparado às florestas aleatórias quando lidando com múltiplas classes e requerem uma seleção cuidadosa do kernel e parâmetros de regularização.

E) O KNN (K vizinhos mais próximos) não é ideal para grandes volumes de dados devido ao seu alto custo computacional, pois a classificação requer o cálculo da distância para todos os pontos de dados no conjunto de treinamento. Este método também é sensível à escala dos atributos, o que pode ser um desafio com múltiplos atributos contínuos.",8273664
tópico 5,Técnicas de redução de dimensionalidade: Seleção de características (feature selection); Análise de componentes principais (PCA – principal component analysis),"Questão: Em análise de dados, métodos de redução de dimensionalidade são frequentemente aplicados para simplificar os modelos e reduzir a exigência computacional. Considere um conjunto de dados de alta dimensionalidade que precisa ser submetido a um processo de machine learning. Entre as técnicas de redução de dimensionalidade, temos a Seleção de características (Feature Selection) e a Análise de componentes principais (PCA – Principal Component Analysis). 

Qual das seguintes afirmações melhor representa um aspecto central que diferencia a Seleção de características da Análise de componentes principais no contexto de preparação de dados para modelagem preditiva?

A) A Seleção de características é uma técnica não supervisionada, enquanto PCA é supervisionada, dependendo das etiquetas de classe dos dados.
B) PCA transforma as variáveis originais em um conjunto de novas variáveis ortogonais, chamadas componentes principais, enquanto a Seleção de Características busca identificar e manter apenas as variáveis mais relevantes para o modelo preditivo.
C) A Seleção de características reduz a dimensionalidade do conjunto de dados exclusivamente por meio de transformações lineares, enquanto PCA pode envolver transformações não lineares.
D) PCA é uma técnica que sempre melhora a performance preditiva dos modelos, enquanto a Seleção de características pode não alterar ou mesmo piorar a performance.
E) Tanto PCA quanto Seleção de características dependem de uma modelagem preditiva prévia para serem aplicadas e, portanto, são classificadas como métodos de aprendizado semi-supervisionado.

",B,"

A) Incorreta. Na verdade, a Seleção de características pode ser supervisionada ou não supervisionada, dependendo do método, enquanto PCA é uma técnica não supervisionada, pois não leva em consideração as etiquetas de classe.

B) Correta. PCA transforma as variáveis originais do conjunto de dados em um novo conjunto de variáveis ortogonais chamadas componentes principais, que são linearmente descorrelacionadas. A Seleção de características, por outro lado, visa identificar as variáveis mais significativas, mantendo-as como estão, sem transformação.

C) Incorreta. Ambas as técnicas envolvem transformações lineares. Existem variantes do PCA que podem englobar transformações não lineares, como o Kernel PCA, mas o PCA padrão opera com transformações lineares.

D) Incorreta. PCA não garante a melhoria na performance dos modelos preditivos; em alguns casos, pode até piorá-la se houver perda de informações críticas. Por outro lado, a Seleção de características tem o objetivo de melhorar a performance eliminando características irrelevantes ou redundantes, mas isso também não é garantido.

E) Incorreta. Nem a Seleção de características nem o PCA dependem necessariamente de uma modelagem preditiva prévia para serem aplicadas; ambas são técnicas aplicáveis de forma independente e podem ser usadas antes do treinamento de modelos preditivos.",3880580
tópico 5,"Rotulação de partes do discurso, part-of-speech tagging","Questão:
A rotulação de partes do discurso (part-of-speech tagging - POS tagging) é uma etapa fundamental no processamento de linguagem natural (PLN) que consiste em atribuir rótulos de categorias gramaticais, como substantivos, verbos, adjetivos, advérbios, preposições, dentre outros, às palavras de um texto. Em relação aos métodos de POS tagging, é INCORRETO afirmar que:

A) Métodos estatísticos de POS tagging incluem modelos baseados em Markov oculto (HMM) e redes neurais recorrentes (RNN), que podem aprender com grandes corpora anotados de texto.

B) As abordagens baseadas em regras utilizam um conjunto fixo de regras manualmente definidas alinhadas aos padrões morfológicos e sintáticos de uma língua para rotular as palavras com suas respectivas categorias gramaticais.

C) Algoritmos de aprendizado de máquina não supervisionados são eficazmente aplicados para POS tagging em situações onde há escassez de dados anotados, uma vez que eles não requerem exemplos de treinamento.

D) A rotulação de POS pode ser melhorada com o uso de recursos lexicais e contextuais, assim como informações morfológicas, como o uso de prefixos e sufixos, para aumentar a precisão do processo de rotulação.

E) Modelos de POS tagging baseados em transformadores, como o BERT, vêm mostrando desempenho superior em várias línguas devido à sua capacidade de capturar contextos de palavras de longo alcance e gerar representações de palavras dinâmicas.

",C,"

Alternativa A) Correta, pois realmente métodos estatísticos como HMM e RNN aprendem com corpora anotados e são amplamente usados em POS tagging.
Alternativa B) Correta, as abordagens baseadas em regras utilizam regras pré-definidas para rotular as palavras e são outra forma clássica de fazer POS tagging.
Alternativa C) Incorreta, uma vez que o aprendizado de máquina não supervisionado é desafiador para POS tagging na ausência de dados anotados, pois não terá exemplos de treinamento para aprender as etiquetas correspondentes corretamente.
Alternativa D) Correta, os recursos lexicais, contextuais e morfológicos são de fato úteis para melhorar a precisão da rotulação POS.
Alternativa E) Correta, modelos baseados em transformadores como o BERT têm demonstrado desempenho avançado em tarefas de PLN, incluindo POS tagging, devido à sua habilidade em entender contextos mais amplos.",6239910
tópico 5,"Modelos de representação de texto - N-gramas, modelos vetoriais de palavras (CBOW, Skip-Gram e GloVe)","Questão: Em processamento de linguagem natural, a modelagem de representação de texto é fundamental para a realização de várias tarefas analíticas. Os N-gramas e os modelos vetoriais de palavras como CBOW (Continuous Bag of Words), Skip-Gram e GloVe (Global Vectors for Word Representation) são algumas das abordagens utilizadas para entender e processar texto em linguagem humana. Considerando as características dessas técnicas, analise as seguintes afirmativas:

I. N-gramas são sequências contíguas de N itens de uma dada sequência de texto ou fala. Quanto maior o valor de N, maior é a precisão e a especificidade do contexto capturado, mas também aumenta a dimensionalidade dos dados.

II. No modelo CBOW, a rede neural é treinada para prever a palavra corrente com base no contexto de palavras adjacentes, focando na maximização da probabilidade das palavras de contexto dadas a palavra alvo.

III. Skip-Gram, inversamente ao CBOW, utiliza a palavra corrente para prever o contexto. Esse modelo é particularmente eficaz quanto menor for o volume de dados de treinamento e para capturar palavras com múltiplos significados.

IV. GloVe é um modelo que incorpora tanto a estatística global da frequência de co-ocorrência de palavras em um corpus quanto as informações locais capturadas por modelos baseados em janela de contexto, buscando fornecer representações vetoriais densas e informativas.

Estão corretas as afirmativas:

A) I, II e III apenas.
B) I, III e IV apenas.
C) II, III e IV apenas.
D) I, II, III e IV.
E) I e IV apenas. 

",D," 
Explicação dos itens:

I. Correta. A descrição de N-gramas está correta, enfatizando o trade-off entre precisão/contexto e a alta dimensionalidade que pode resultar do aumento de N.

II. Correta. Este item descreve corretamente o funcionamento do modelo CBOW, onde a palavra alvo é prevista a partir das palavras no contexto circundante.

III. Correta. O modelo Skip-Gram de fato usa a palavra alvo para prever as palavras de contexto e é eficaz com conjuntos de dados menores e na captura de polissemias.

IV. Correta. GloVe combina a estatística global com informações locais de contexto, fornecendo vetores de palavras densos e ricos em informação, sendo essa uma descrição precisa de sua metodologia.

Todas as afirmativas são verdadeiras e refletem as características principais dos modelos mencionados.",1528590
tópico 5,"Métricas de similaridade textual - similaridade do cosseno, distância euclidiana, similaridade de Jaccard, distância de Manhattan e coeficiente de Dice","Questão: Em análise textual, frequentemente se faz necessário medir o quão similares são dois conjuntos de dados, como documentos ou sentenças. Na área de Processamento de Linguagem Natural, diversas métricas são utilizadas para calcular essa similaridade, permitindo aplicações como detecção de plagio, agrupamento de textos e sistemas de recomendação. Considere o seguinte cenário:

Dados dois vetores de termos A e B, que representam a frequência de palavras em dois diferentes textos:

Vetor A: [1, 0, 2, 1, 0]
Vetor B: [2, 1, 0, 1, 1]

Avalie as seguintes afirmativas quanto às métricas de similaridade textual e escolha a opção correta:

I. A Similaridade do Cosseno entre A e B tende a ser mais alta conforme o ângulo entre os vetores diminui, considerando as dimensões espaciais dos vetores.
II. A Distância Euclidiana é uma medida efetiva quando se deseja ignorar a magnitude dos vetores e concentrar-se apenas nas diferenças absolutas entre seus componentes.
III. A Similaridade de Jaccard é calculada com base nos elementos em comum entre os conjuntos e é particularmente útil quando os vetores representam a ocorrência binária (presença/ausência) de termos, em vez de sua frequência.
IV. A Distância de Manhattan, também conhecida como Distância de Taxicab, é calculada como a soma das diferenças absolutas entre os componentes dos vetores e é muito sensível a outliers.
V. O Coeficiente de Dice é uma métrica que prioriza a importância das intersecções duas vezes mais que a soma das cardinalidades dos vetores e, como tal, considera mais a semelhança do que a diferença entre eles.

A) Apenas as afirmativas I, III e V estão corretas.
B) Apenas as afirmativas II e IV estão corretas.
C) Apenas as afirmativas I, II e IV estão corretas.
D) Apenas as afirmativas III, IV e V estão corretas.
E) Todas as afirmativas estão corretas.

",A," 
Explicação dos itens:

I. Correto. A Similaridade do Cosseno é medida pelo cosseno do ângulo entre dois vetores no espaço multidimensional, sendo maior quanto mais os vetores apontam na mesma direção.

II. Incorreto. A Distância Euclidiana mede a distância 'em linha reta' entre os pontos que os vetores representam e é influenciada pela magnitude dos vetores.

III. Correto. A Similaridade de Jaccard é baseada em elementos comuns entre os conjuntos e é mais apropriada para dados binários.

IV. Incorreto. A Distância de Manhattan soma as diferenças absolutas entre os pontos, mas não é particularmente mais sensível a outliers do que outras métricas de distância.

V. Correto. O Coeficiente de Dice é calculado como o dobro do número de elementos comuns dividido pela soma do número de elementos em cada conjunto, focando na semelhança entre conjuntos de dados.",1611297
tópico 5,Técnicas de regressão: Árvores de decisão para regressão; Máquinas de vetores de suporte para regressão,"Questão:
A aplicação de técnicas avançadas de regressão em problemas de predição de dados tem se tornado cada vez mais comum em diversos campos da ciência e indústria. Entre essas técnicas, encontram-se as Árvores de Decisão para regressão e as Máquinas de Vetores de Suporte para regressão (SVR - Support Vector Regression). Analise as afirmativas abaixo e indique a opção correta:

I. Árvores de decisão para regressão funcionam dividindo o espaço dos dados em regiões homogêneas, utilizando o critério de minimização da soma dos quadrados dos resíduos dentro de cada região.

II. As Máquinas de Vetores de Suporte para regressão buscam encontrar a função que possui o maior desvio para o número de erros fora da margem, mantendo todos os erros de predição dentro de um limite especificado.

III. Uma vantagem da SVR sobre as Árvores de decisão para regressão é que a SVR tende a ser menos suscetível ao sobreajuste, pois depende menos da estrutura dos dados e mais da regularização dos parâmetros do modelo.

IV. Árvores de decisão para regressão, ao contrário de SVR, exigem uma etapa de escala ou normalização dos atributos, pois são sensíveis às variações de escala dos dados de entrada.

Assinale a alternativa que apresenta todas as afirmativas corretas:

A) I, III e IV.
B) I e II.
C) II e III.
D) I e III.
E) Todas as afirmativas estão corretas.

",B,"
Explicação dos itens:

I. Correta. Árvores de decisão para regressão realmente funcionam dividindo o espaço de dados em regiões, tentando tornar cada região o mais homogênea possível em termos da variável dependente, geralmente por meio de critérios de minimização de erro, como a soma dos quadrados dos resíduos.

II. Incorreta. As Máquinas de Vetores de Suporte para regressão não buscam maximizar o desvio para o número de erros fora da margem, mas sim minimizar o erro dentro de um limite estabelecido, conhecido como ε-insensível loss function, onde os erros abaixo de um certo threshold são ignorados.

III. Correta. De fato, a SVR tem uma robustez conhecida contra o sobreajuste devido à sua abordagem de otimização que foca na regularização dos parâmetros, o que pode ajudar a evitar que o modelo se torne excessivamente complexo.

IV. Incorreta. Esta afirmativa está incorreta pois são as Máquinas de Vetores de Suporte (SVR) que geralmente exigem a normalização dos dados de entrada para funcionarem adequadamente, enquanto que as Árvores de decisão para regressão são menos sensíveis às variações de escala, uma vez que estão dividindo os dados com base em seus valores intrínsecos e relações condicionais.",8646891
tópico 5,Ajuste de modelos dentro e fora de amostra e overfitting,"Questão:
A análise de modelos estatísticos frequentemente envolve a avaliação do ajuste do modelo tanto na amostra usada para estimá-lo quanto em novas amostras, a fim de verificar a capacidade de generalização do modelo. Sobre o ajuste de modelos dentro e fora de amostra e o fenômeno do overfitting, considerando uma ampla gama de modelos potenciais a serem ajustados aos dados, é correto afirmar que:

A) Um modelo com excelente ajuste dentro da amostra sempre irá generalizar bem para novos dados, indicando a ausência de overfitting.
B) Overfitting ocorre quando um modelo é excessivamente complexo e captura mais do ruído do que os verdadeiros padrões subjacentes dos dados de treinamento.
C) Modelos mais simples são incapazes de capturar padrões complexos nos dados e, por isso, devem ser evitados independentemente do contexto.
D) A validação cruzada é uma técnica ineficaz para avaliar a capacidade de generalização de um modelo, pois sempre superestima o erro fora de amostra.
E) O uso de um conjunto de teste independente não é necessário se o modelo apresentar um bom ajuste na amostra de treinamento, mesmo em modelos altamente parametrizados.

",B,"

Explicação dos itens:

A) Incorreto. Um ajuste excelente dentro da amostra não garante a generalização para novos dados, pois o modelo pode estar sobreajustado (overfitting) a particularidades da amostra de treinamento que não se aplicam a outros dados.

B) Correto. Overfitting é um problema onde o modelo aprende padrões específicos e ruídos presentes na amostra de treinamento ao invés de capturar as tendências gerais que se aplicariam a outros conjuntos de dados. Este fenômeno acontece frequentemente quando o modelo é muito complexo e tem muitos parâmetros em relação ao tamanho da amostra de treinamento.

C) Incorreto. Modelos mais simples podem não ser capazes de capturar toda a complexidade dos dados, mas podem ser preferíveis em alguns cenários, especialmente quando se busca a capacidade de generalização. A escolha do modelo deve considerar o equilíbrio entre viés e variância (bias-variance tradeoff).

D) Incorreto. A validação cruzada é uma técnica comumente utilizada para estimar o erro de generalização de um modelo estatístico, dividindo a amostra de dados em várias partes e usando cada uma delas alternadamente como conjunto de teste. Quando bem aplicada, ela pode fornecer uma boa estimativa da capacidade de generalização do modelo.

E) Incorreto. Mesmo que um modelo apresente um bom ajuste na amostra de treinamento, o uso de um conjunto de teste independente é fundamental para avaliar como o modelo irá performar em dados que não foram usados no treinamento. Isso é especialmente importante em modelos com muitos parâmetros, onde o risco de overfitting é maior.",8119125
tópico 5,Técnicas de redução de dimensionalidade: Seleção de características (feature selection); Análise de componentes principais (PCA – principal component analysis),"Questão: Em um cenário onde um cientista de dados está trabalhando com um grande conjunto de dados contendo diversas variáveis, ele deseja reduzir a dimensionalidade dos dados para simplificar o modelo de aprendizado de máquina sem perder informações vitais. Para isso, ele considera empregar técnicas de redução de dimensionalidade. Dentre as técnicas listadas abaixo, qual é a mais adequada para identificar e remover componentes com variância mínima, mantendo as direções que maximizam a variância nos dados?

A) Métodos de filtragem baseados em estatísticas univariadas.
B) Análise de componentes principais (PCA).
C) Métodos de seleção de características baseados em modelos.
D) Métodos de seleção de características baseados em algoritmos genéticos.
E) Redução de dimensionalidade multidimensional (t-SNE).

",B,"

Explicação:

A) Métodos de filtragem baseados em estatísticas univariadas focam em testes estatísticos para selecionar variáveis com base na sua relação com a variável de interesse. Este método é principalmente útil para identificar características importantes, mas não lida diretamente com a questão da multicolinearidade ou com a redução de componentes base on variância.
B) Análise de componentes principais (PCA) é a técnica correta para esta questão, pois ela identifica e remove as componentes com variância mínima, mantendo somente os componentes principais que explicam a maior parte da variância dos dados. É uma técnica essencial para redução de dimensionalidade que simplifica os dados sem grandes perdas de informação.
C) Métodos de seleção de características baseados em modelos utilizam algoritmos de aprendizado de máquina para avaliar a importância das características mas, como os métodos de filtragem, não focam explicitamente na variância dos componentes ou na redução de dimensões.
D) Métodos de seleção de características baseados em algoritmos genéticos utilizam processos de seleção natural para identificar conjuntos ótimos de características, e embora possam ser poderosos, não se concentram na maximização da variância explicada, que é o foco da PCA.
E) Redução de dimensionalidade multidimensional (t-SNE) é uma técnica eficiente para a visualização de dados de alta dimensão em espaços de baixa dimensão, mas não se concentra necessariamente em manter componentes com a maior variação, como faz a PCA.",2088862
tópico 5,Redes neurais convolucionais e recorrentes,"Questão: Em relação às arquiteturas de Redes Neurais Profundas, as redes neurais convolucionais (CNNs) e as redes neurais recorrentes (RNNs) são especialmente adequadas para diferentes tipos de dados e tarefas. Considere as seguintes afirmações:

I. CNNs são mais apropriadas para dados espaciais, como imagens, onde é importante capturar a localização e a relação espacial entre os pixels.

II. RNNs são particularmente eficientes para dados sequenciais, como séries temporais ou linguagem natural, devido à sua capacidade de manter informações de estados anteriores.

III. Uma CNN usa filtros para varrer sobre a entrada e criar mapas de características, uma técnica que não é compartilhada com as RNNs.

IV. Uma RNN pode ser desdobrada no tempo e tratada como uma rede profunda, mas não pode ser treinada usando retropropagação através do tempo devido à dificuldade de manter informações de estados muito distantes.

Assinale a opção que contém apenas as afirmações corretas:

A) I e II
B) I, II e III
C) II e IV
D) I, II e IV
E) Todas as afirmações são corretas

",B,"

Explicação:

I. Correta. CNNs são projetadas para processar dados em grade, como imagens, que possuem uma estrutura espacial significativa. Elas são eficazes em reconhecer padrões visuais diretamente dos pixels de imagens com variância de posição e escala.

II. Correta. RNNs são projetadas para lidar com sequências de dados e são capazes de capturar informações temporais/sequenciais devido à sua natureza recursiva, tornando-as adequadas para tarefas como reconhecimento de fala, modelagem de linguagem, e séries temporais.

III. Correta. CNNs aplicam convoluções sobre a entrada, que usam filtros para criar mapas de características, ajudando a rede a identificar padrões espaciais.

IV. Incorreta. De fato, RNNs podem ser desdobradas no tempo e tratadas como uma rede profunda. Elas podem ser treinadas usando retropropagação através do tempo (Backpropagation Through Time - BTT), que é uma adaptação da retropropagação padrão para redes recorrentes. O desafio citado no item IV, conhecido como o problema do desvanecimento ou explosão de gradientes, é um obstáculo no treinamento de RNNs, mas não impede o uso de BTT. Existem técnicas como o truncamento do gradiente e o uso de arquiteturas avançadas de RNN (como LSTM e GRU) para mitigar esse problema.

Portanto, a alternativa correta é a letra B, pois contém as afirmações I, II e III, que são corretas. A afirmação IV é incorreta e por isso não deve fazer parte da seleção.",6049436
tópico 5,"métricas de avaliação - matriz de confusão, acurácia, precisão, revocação, F1-score e curva ROC","Questão: 
Analise o seguinte contexto de classificação binária: um modelo de aprendizado de máquina foi desenvolvido para prever a ocorrência de uma doença rara em uma população. Após a aplicação deste modelo em um conjunto de teste, os seguintes resultados foram obtidos:

- Verdadeiros Positivos (VP): 20
- Falsos Positivos (FP): 5
- Verdadeiros Negativos (VN): 150
- Falsos Negativos (FN): 25

Baseado nos resultados acima, qual das seguintes métricas indica melhor a capacidade do modelo de identificar corretamente os casos da doença?

A) Acurácia
B) Precisão
C) Revocação
D) F1-score
E) Área sob a curva ROC (AUC-ROC)

",C," 

Explicação:

A) Acurácia: Esta métrica calcula a proporção de previsões corretas (tanto verdadeiros positivos quanto verdadeiros negativos) em relação ao total de previsões. Embora o modelo apresente uma alta acurácia devido ao grande número de verdadeiros negativos (150 de 200 previsões), esta métrica pode ser enganosa em contextos de desequilíbrio de classe, como doenças raras.

B) Precisão: A precisão é dada pela razão entre verdadeiros positivos e a soma de verdadeiros e falsos positivos (VP / (VP + FP)). Esta métrica é útil quando o custo dos falsos positivos é alto, porém não leva em consideração os falsos negativos.

C) Revocação: Também conhecida como sensibilidade, a revocação é a proporção de verdadeiros positivos em relação ao total de casos positivos reais (VP / (VP + FN)). Dada a importância de identificar todos os possíveis casos da doença rara, a revocação é a métrica mais importante neste contexto, pois indica a capacidade do modelo de detectar todos os casos positivos.

D) F1-score: O F1-score é a média harmônica entre precisão e revocação. Embora seja uma métrica balanceada que poderia ser considerada neste contexto, a pergunta especifica a melhor métrica para a identificação de casos da doença, em que a revocação é mais crítica do que a precisão.

E) Área sob a curva ROC (AUC-ROC): A AUC-ROC é uma métrica de desempenho para classificação binária em vários limiares de decisão. É uma medida balanceada que considera tanto a taxa de verdadeiros positivos quanto a taxa de falsos positivos. Entretanto, em casos de desequilíbrio severo de classes, a AUC-ROC pode ser otimista e não refletir a habilidade do modelo de encontrar os verdadeiros positivos, que é o ponto focal nesta questão.",7571562
tópico 5,"Técnicas de agrupamento: Agrupamento por partição, por densidade e hierárquico","Questão:
A análise de agrupamento (ou clusterização) é uma técnica de aprendizado de máquina não supervisionado utilizada para identificar padrões ou grupos de dados semelhantes em um conjunto de dados. Essencialmente, agrupamentos visam a organização de uma coleção de padrões em grupos, com base na similaridade entre os elementos. Considerando as técnicas de agrupamento por partição, por densidade e hierárquico, avalie as afirmativas a seguir e marque a alternativa correta sobre a caracterização desses métodos.

I. O agrupamento por partição, como o K-means, visa dividir o conjunto de dados em um número pré-definido de grupos, onde os objetos são transferidos entre os grupos até que o critério de otimização seja atingido.

II. No agrupamento por densidade, como o DBSCAN, as regiões de alta densidade são descobertas e os dados são agrupados de acordo com essas áreas, podendo detectar formas arbitrárias e ruidos.

III. Técnicas de agrupamento hierárquico, tais como o método de Ward, não requerem a definição prévia do número de grupos e constroem uma hierarquia de clusters de forma incremental, baseando-se em medidas de distância ou similaridade.

Alternativas:
A) Somente as afirmativas I e II estão corretas.
B) Somente as afirmativas I e III estão corretas.
C) Somente as afirmativas II e III estão corretas.
D) Todas as afirmativas estão corretas.
E) Nenhuma das afirmativas está correta.

",D,"

Explicação dos itens:

I. A afirmativa está correta. O agrupamento por partição, como o algoritmo K-means, divide o dado em um número pré-definido de grupos (clusters), e os objetos são realocados entre esses grupos com o objetivo de minimizar a soma do quadrado das distâncias dos objetos ao centróide de seu grupo.

II. A afirmativa está correta. Os métodos de agrupamento por densidade, como o DBSCAN (Density-Based Spatial Clustering of Applications with Noise), identificam regiões de alta densidade de pontos de dados separadas por regiões de baixa densidade. Estas regiões de alta densidade são consideradas clusters. Este método é capaz de identificar clusters de formas variadas e de lidar com pontos de ruído ou outliers.

III. A afirmativa está correta. O agrupamento hierárquico cria uma decomposição hierárquica dos dados e não requer que se especifique o número de clusters antecipadamente. Existem duas abordagens para o agrupamento hierárquico: aglomerativa (de baixo para cima) e divisiva (de cima para baixo). Em ambos os casos, a hierarquia de clusters é representada por um dendrograma, o qual fornece informações sobre as distâncias ou similaridades entre os clusters.",966184
tópico 5,"Processamento de linguagem natural: Normalização textual - stop words, estemização, lematização e análise de frequência de termos;","Questão: Em processamento de linguagem natural (PLN), a normalização textual é um passo crítico que visa a preparar os dados de texto para análises subsequentes. Cada técnica de normalização possui uma finalidade específica dentro do processo de tratamento textual. Considerando as técnicas de remoção das palavras de parada (stop words), estemização (stemming), lematização e análise de frequência de termos, assinale a opção que melhor descreve a aplicabilidade desses métodos no contexto de PLN:

A) A remoção de stop words tem por objetivo aumentar a acurácia do modelo PLN eliminando palavras com significado semântico relevante para a compreensão do texto.
B) Estemização é um processo no qual palavras são reduzidas à sua raiz, mesmo que essa raiz não seja uma palavra válida da língua, facilitando a consolidação de diferentes formas da mesma palavra.
C) A lematização é um processo que não considera o contexto das palavras, reduzindo-as a sua forma base ou dicionário, conhecida como lema.
D) Na análise de frequência de termos, a presença de stop words é fundamental para manter a integridade da distribuição de palavras e a análise semântica do texto.
E) A estemização é preferível à lematização quando a precisão no reconhecimento da forma base das palavras é crítica para o entendimento do texto.

",B,"

Explicação dos itens:

A) Incorreto. A remoção de stop words busca eliminar palavras que são consideradas de baixa relevância semântica, como preposições e artigos, e que são frequentes no idioma, mas não contribuem significativamente para a compreensão do significado geral do texto em análises quantitativas.

B) Correto. Estemização (ou stemming) é um processo de normalização textual que reduz as palavras à sua forma radical (stem), possibilitando que variantes de uma palavra possam ser analisadas como uma única entidade. A raiz encontrada pode não ser uma palavra válida no idioma, mas é útil para consolidar diferentes formas flexionadas da mesma palavra.

C) Incorreto. A lematização é um processo que leva em consideração o contexto das palavras e reduz as palavras à sua forma base ou de dicionário, que é chamada de lema. Ao contrário do que sugere a alternativa, o contexto é sim importante na lematização e diferencia essa técnica da estemização, que ignora o contexto.

D) Incorreto. Normalmente, as stop words são removidas em uma análise de frequência de termos, pois podem distorcer a importância relativa de palavras mais significativas. Embora sua remoção possa alterar algumas estruturas sintáticas, para muitos tipos de análise textual quantitativa, as stop words não são fundamentais e podem ser removidas para focar em termos que oferecem mais insights.

E) Incorreto. A lematização é geralmente preferida quando a precisão na identificação do lema de uma palavra é crítica, pois ela leva em conta o contexto em que a palavra é utilizada e sua classe gramatical. A estemização é mais rápida e menos complexa, mas também menos precisa em relação à forma básica da palavra.",4353682
tópico 5,"Modelos de representação de texto - N-gramas, modelos vetoriais de palavras (CBOW, Skip-Gram e GloVe)","Questão:
No campo do Processamento de Linguagem Natural (PLN), os modelos de representação de texto são cruciais para uma série de aplicações, incluindo tradução automática, análise de sentimentos e sistemas de recomendação. Dentre os modelos de representação de texto, os N-gramas e os modelos vetoriais de palavras como CBOW, Skip-Gram e GloVe têm papéis distintos e complementares. Considerando as características desses modelos, é correto afirmar que:

A) Os N-gramas são baseados em janelas deslizantes de palavras consecutivas e são insensíveis ao contexto em que aparecem dentro de um documento.

B) O modelo CBOW, acrônimo para Continuous Bag of Words, prediz uma palavra-alvo com base na palavra anterior, desconsiderando as subsequentes no contexto.

C) Skip-Gram é um modelo que, ao contrário do CBOW, foca em prever o contexto a partir de uma palavra central, sendo especialmente eficaz para tratar palavras raras.

D) GloVe, que significa Global Vectors for Word Representation, é um modelo que se baseia exclusivamente na frequência local de co-ocorrência de palavras, ignorando as informações globais de co-ocorrência.

E) Um N-grama de tamanho 1, frequentemente chamado de unigrama, tem alto poder discriminativo e representativo em tarefas de PLN, dada a sua alta sensibilidade a variações contextuais.

",C,"

Explicação dos itens:

A) Os N-gramas são de fato baseados em janelas deslizantes de palavras consecutivas, mas eles são sensíveis ao contexto em que aparecem, já que a ordem das palavras é importante na definição de um N-grama.

B) O modelo CBOW prediz uma palavra-alvo com base no contexto, que inclui palavras antes e depois da palavra-alvo, e não apenas a palavra anterior.

C) Correto. Skip-Gram realmente foca em prever o contexto a partir de uma palavra central. Isso o torna eficaz para lidar com palavras raras, pois tenta entender seu papel em vários contextos diferentes.

D) Embora o GloVe leve em consideração a frequência de co-ocorrência de palavras, esse modelo também combina informações globais de co-ocorrência em toda a corpus, o que não é refletido nesta alternativa.

E) Unigramas (N-gramas de tamanho 1) são úteis para capturar a frequência de palavras individuais, mas possuem uma limitação significativa em termos de sensibilidade a variações contextuais devido à falta de contexto fornecido por palavras adjacentes.",80478
tópico 5,"Rotulação de partes do discurso, part-of-speech tagging","Questão: Na área de processamento de linguagem natural, a rotulação de partes do discurso (part-of-speech tagging - POS tagging) consiste em atribuir a cada palavra ou símbolo de um texto uma etiqueta que corresponde à sua função gramatical. Avalie as seguintes afirmações sobre POS tagging:

I. POS tagging é um processo determinístico e simples, pois cada palavra tem apenas uma etiqueta correspondente em qualquer contexto.

II. As técnicas de Machine Learning, como Redes Neurais e Modelos Ocultos de Markov, podem ser aplicadas para realizar tarefas de POS tagging de forma mais eficiente e precisa do que a análise manual.

III. O conhecimento do contexto e da estrutura sintática das frases é desnecessário para um POS tagging eficaz, já que as etiquetas são atribuídas com base na frequência de uso das palavras.

IV. Uma mesma palavra pode receber diferentes etiquetas dependendo de seu papel na frase, como por exemplo a palavra ""can"", que pode ser etiquetada como verbo ou substantivo.

Está(ão) correta(s) a(s) seguinte(s) afirmação(ões):

A) Apenas I e III.
B) Apenas II e IV.
C) Apenas II, III e IV.
D) Apenas I, II e IV.
E) Todas estão corretas.

",B," 
A afirmação I está incorreta porque a rotulação de partes do discurso não é determinística e nem simples; uma palavra pode ter diferentes etiquetas dependendo do contexto em que está inserida. Por exemplo, ""book"" pode ser tanto um substantivo (""I am reading a book"") quanto um verbo (""I will book a flight"").

A afirmação II está correta. Técnicas de Machine Learning, como Redes Neurais e Modelos Ocultos de Markov, são realmente aplicadas para automatizar e melhorar a eficiência e precisão da tarefa de POS tagging, superando muitas vezes a análise manual.

A afirmação III está incorreta porque o contexto e a estrutura sintática das frases são muito importantes para um POS tagging eficaz. A frequência de uso das palavras não é o único critério para a atribuição de etiquetas, e muitas vezes não é nem o mais importante.

Finalmente, a afirmação IV é correta, pois realmente uma mesma palavra pode ser etiquetada de maneiras diferentes dependendo de seu papel na frase. No exemplo dado, ""can"" pode ser um verbo, como em ""Can you answer the question?"" ou um substantivo, como em ""Please throw it in the trash can.""",4746088
tópico 5,Técnicas de classificação: Naive Bayes; Árvores de decisão (algoritmos ID3 e C4.5); Florestas aleatórias (random forest); Máquinas de vetores de suporte (SVM – support vector machines); K vizinhos mais próximos (KNN – K-nearest neighbours),"Questão: Em um projeto de machine learning para classificação de e-mails como ""spam"" ou ""não-spam"", um cientista de dados está considerando o uso de diferentes algoritmos para otimizar a performance do classificador. Cada técnica de classificação tem suas peculiaridades e suposições. Considerando as características dos algoritmos Naive Bayes, árvores de decisão (ID3 e C4.5), florestas aleatórias (Random Forest), máquinas de vetores de suporte (SVM) e K vizinhos mais próximos (KNN), qual dos seguintes afirma a respeito de suas aplicabilidades no contexto descrito é correto?

A) Naive Bayes assume independência condicional entre as características e é frequentemente utilizado para classificação de textos devido à sua simplicidade e desempenho satisfatório mesmo com a presença de um grande número de recursos.

B) Árvores de decisão baseadas no algoritmo ID3 não são capazes de lidar com atributos contínuos ou valores ausentes, sendo esta uma abordagem não recomendada para a classificação de e-mails.

C) Florestas aleatórias (Random Forest) não são apropriadas para situações em que se tem uma grande quantidade de dados e variáveis, pois gerariam um modelo excessivamente complexo e inviável computacionalmente.

D) Máquinas de vetores de suporte (SVM) são ineficazes na classificação de texto, uma vez que este tipo de dado não pode ser separado linearmente no espaço de características.

E) KNN é o algoritmo mais indicado para classificação de e-mails, pois não necessita de uma fase de treinamento, e sua implementação é mais simples em comparação com os outros métodos.

",A," 

Explicação dos itens:
A) Correto. O Naive Bayes é um algoritmo bem estabelecido para classificação de texto devido à sua simplicidade e eficiência, principalmente quando há muitas características, como é comum em dados de texto.

B) Incorreto. Enquanto o algoritmo ID3 tem limitações em lidar com atributos contínuos ou ausentes, as árvores de decisão podem ser muito eficazes em tarefas de classificação de e-mail quando essas limitações são superadas, como no caso de sua extensão C4.5 que lida melhor com essas questões.

C) Incorreto. As florestas aleatórias são eficazes mesmo em situações com muitos dados e variáveis. Elas são capazes de lidar com overfitting e são frequentemente usadas em ambientes de alta dimensionalidade, apesar da carga computacional.

D) Incorreto. As SVMs são muito utilizadas na classificação de texto e são poderosas em encontrar hiperplanos em um espaço de alta dimensão, mesmo quando os dados não são linearmente separáveis, graças ao uso de kernel tricks.

E) Incorreto. Embora seja verdade que o KNN não necessita de treinamento, esta não é necessariamente uma vantagem, pois o algoritmo pode ser computacionalmente custoso na fase de classificação em grandes conjuntos de dados — especialmente em tarefas com muitas características, como classificação de texto. Além disso, a implementação não é necessariamente mais simples que os outros métodos em todos os cenários.",5620186
tópico 5,"Avaliação de modelos de classificação: treinamento, teste, validação; validação cruzada","Questão: Em um cenário de modelagem preditiva, um cientista de dados está desenvolvendo um modelo de classificação para prever a inadimplência de clientes em um banco. Para avaliar a performance de seu modelo, o cientista decide utilizar a técnica de validação cruzada. Qual das alternativas abaixo descreve corretamente o objetivo da validação cruzada e indica uma de suas principais vantagens?

A) O objetivo da validação cruzada é simplesmente dividir o dataset em um conjunto de treinamento e um conjunto de teste, a fim de verificar a performance do modelo em dados nunca antes vistos pelo modelo. Sua vantagem é reduzir o overfitting.

B) O objetivo da validação cruzada é utilizar todo o conjunto de dados tanto para treinamento quanto para teste, mediante a divisão do mesmo em k subconjuntos. Uma de suas vantagens é permitir que cada observação seja usada para teste uma única vez e para treinamento k-1 vezes, fornecendo uma estimativa mais confiável da performance do modelo.

C) Validar cruzadamente um modelo envolve testá-lo apenas em um subconjunto único e aleatório do dataset, visando assegurar uma avaliação rápida da performance do modelo. A principal vantagem dessa abordagem é a eficiência computacional.

D) A validação cruzada consiste em treinar o modelo em diferentes conjuntos de dados gerados a partir de amostras originais com reposição. Sua principal vantagem é aumentar a variabilidade dos dados de treinamento, o que pode melhorar a generalização do modelo.

E) O processo de validação cruzada ignora o conjunto de teste, concentrando-se exclusivamente no ajuste do modelo no conjunto de treinamento. Uma das vantagens dessa abordagem é a economia do tempo de treinamento, já que não se separa um conjunto de dados para teste.

",B,"

Explicação dos itens:

A) Esta alternativa descreve um procedimento de divisão simples de treinamento e teste, mas não aborda a validação cruzada, que envolve múltiplas iterações de treinamento e teste usando diferentes partições do conjunto de dados.

B) Esta alternativa está correta. O objetivo da validação cruzada é de fato usar todo o conjunto de dados para treinamento e teste, alternando os subconjuntos que desempenham cada papel. Isso ajuda a obter uma estimativa mais robusta da capacidade do modelo de generalizar para novos dados. É uma técnica amplamente aceita para avaliar a performance de modelos de classificação.

C) A opção C erra ao afirmar que a validação cruzada testa o modelo apenas em um subconjunto único. Na verdade, ela realiza o teste em diferentes subconjuntos do dataset ao longo de múltiplas iterações.

D) A alternativa D descreve bootstrap, outra técnica de reamostragem, mas não a validação cruzada. O bootstrap envolve amostras com reposição, enquanto a validação cruzada divide o dataset sem reposição.

E) A opção E está errada, pois a validação cruzada não ignora o conjunto de teste. Em vez disso, ela cria múltiplos conjuntos de teste por meio da rotação dos subconjuntos do dataset. A economia de tempo não é uma vantagem associada a essa técnica, pois ela geralmente requer mais tempo devido às múltiplas iterações de treinamento e teste.",3300389
tópico 5,"Métricas de similaridade textual - similaridade do cosseno, distância euclidiana, similaridade de Jaccard, distância de Manhattan e coeficiente de Dice","Considere um cenário onde um analista de dados está trabalhando com processamento de linguagem natural e análise de similaridade textual entre documentos. O analista deseja escolher uma métrica adequada que possa levar em conta o peso de termos dentro dos documentos e que seja menos afetada pela magnitude dos vetores em comparação com outras técnicas básicas de distância. Além disso, é importante que a métrica escolhida seja normalizada para que o resultado esteja sempre entre 0 e 1. 

Dentre as opções listadas abaixo, qual é a métrica mais apropriada para atender aos requisitos do analista?

A) Distância Euclidiana
B) Similaridade de Jaccard
C) Similaridade do Cosseno
D) Distância de Manhattan
E) Coeficiente de Dice

",C," 
A) Distância Euclidiana - Essa métrica mede a distância 'direta' entre dois pontos em um espaço multidimensional, o que pode ser afetado pela magnitude dos vetores, não sendo normalizada entre 0 e 1.

B) Similaridade de Jaccard - Essa métrica mede a similaridade e diversidade entre conjuntos de amostras. É mais comumente usada para medir a similaridade entre conjuntos de dados binários, não levando em conta o peso dos termos.

C) Similaridade do Cosseno - Esta é a métrica adequada para a situação descrita. Ela mede o cosseno do ângulo entre dois vetores em um espaço de atributos, é normalizada entre 0 e 1 independentemente da magnitude dos vetores, dando atenção ao peso dos termos devido à forma como os vetores são construídos com base na frequência dos termos.

D) Distância de Manhattan - Essa métrica soma as diferenças absolutas de seus componentes e pode ser interpretada como o caminho que seria percorrido para se mover de um ponto a outro se um 'grid' como em uma cidade fosse usado para viagens, onde apenas movimentos ortogonais são possíveis. Não é normalizada e pode ser afetada pela magnitude dos vetores.

E) Coeficiente de Dice - Esta métrica é semelhante à Similaridade de Jaccard na medida em que é usada para avaliar a similaridade entre dois conjuntos, porém não leva em conta o peso dos termos e também não é sempre normalizada entre 0 e 1.",6193618
tópico 5,Técnicas de regressão: Árvores de decisão para regressão; Máquinas de vetores de suporte para regressão,"Questão:

A aplicação de técnicas de Machine Learning (ML) para modelagem preditiva em problemas de regressão é uma prática comum em diversos campos da indústria e pesquisa. Dentre essas técnicas, as Árvores de Decisão e as Máquinas de Vetores de Suporte (SVM - Support Vector Machines) são frequentemente utilizadas. Considerando o panorama das técnicas de regressão, analise as seguintes afirmativas sobre Árvores de Decisão para regressão (Regressão via Decision Tree) e Máquinas de Vetores de Suporte para regressão (SVR - Support Vector Regression):

I. Árvores de Decisão para regressão dividem o espaço de características em regiões distintas, atribuindo uma previsão baseada na média ou mediana dos valores dentro de cada região.
II. Em um modelo de SVR, o objetivo primordial é encontrar um hiperplano que melhor se ajuste aos dados de treinamento e que tenha a mínima distância de separação, definida pela margem, em relação aos suportes vetoriais.
III. Um modelo de SVR com função de kernel polinomial é adequado para casos onde a relação entre as variáveis independentes e a variável dependente é altamente não-linear e multifatorial.
IV. A utilização de Árvores de Decisão em conjuntos de dados com grande número de recursos e pequenos números de observações tende a proporcionar modelos sobreajustados, sendo obrigatório o uso de métodos de poda para prevenir tal sobreajuste.
V. A técnica SVR é computacionalmente menos intensiva do que Árvores de Decisão para regressão, especialmente quando tratamos de grandes volumes de dados.

Assinale a opção que contém todas as afirmativas corretas.

A) I e II
B) I, II e III
C) II, IV e V
D) I, III e IV
E) Todas as afirmativas estão corretas.

",B,"

Explicação:

I. Correto. As Árvores de Decisão realizam uma divisão do espaço de características e fazem previsões com base na média ou mediana dos valores dentro de cada região folha.

II. Incorreto. Na SVR, o objetivo é minimizar o erro dentro de uma margem definida por um hiperplano. A afirmação erra ao associar a minimização de distância com a definição da margem, que na verdade trata do erro admitido para os suportes vetoriais, e não da distância.

III. Correto. Funções de kernel polinomial permitem que os modelos SVR lidem com relações não-lineares complexas entre as variáveis ao mapeá-las em um espaço de maior dimensão.

IV. Correto. Árvores de Decisão são propensas a sobreajustes, especialmente com muitos recursos e poucas observações. Métodos de poda são técnicas para prevenir overfitting.

V. Incorreto. Geralmente, SVM é computacionalmente mais intensivo que as Árvores de Decisão, especialmente em razão do cálculo das funções de kernel e da otimização necessária para encontrar a margem máxima.",1474823
tópico 5,"modelos vetoriais de documentos (booleano, TF e TF-IDF, média de vetores de palavras e Paragraph Vector);","Questão: 
Considere que você está construindo um sistema de recuperação de informações baseando-se em diferentes modelos vetoriais de documentos. Cada modelo apresenta características específicas na maneira de representar o texto e calcular a relevância de um documento frente a uma consulta do usuário. Qual dos seguintes modelos leva em consideração não apenas a frequência de um termo no documento, mas também a frequência inversa do termo na coleção de documentos, a fim de atribuir pesos aos termos que ajudam a discriminar o conteúdo dos documentos?

A) Modelo Booleano
B) TF (Term Frequency)
C) TF-IDF (Term Frequency-Inverse Document Frequency)
D) Média de Vetores de Palavras (Word2Vec)
E) Paragraph Vector (Doc2Vec)

",C,"

Explicação dos itens:

A) Modelo Booleano: Este modelo representa documentos e consultas como conjuntos de palavras com operadores booleanos (AND, OR, NOT). Ele não leva em conta a frequência dos termos, apenas sua presença ou ausência no documento.

B) TF (Term Frequency): O modelo TF avalia a relevância de um termo em um documento com base na frequência que o termo aparece no documento. Embora reconheça a importância da frequência, não considera o peso do termo na coleção como um todo.

C) TF-IDF (Term Frequency-Inverse Document Frequency): Este modelo atribui pesos aos termos com base na frequência do termo no documento (TF) e na frequência inversa do termo na coleção de documentos (IDF). Termos que são frequentes em um documento, mas raros na coleção, recebem pesos mais altos, ajudando na discriminação do conteúdo dos documentos.

D) Média de Vetores de Palavras (Word2Vec): Utiliza modelos de espaço vetorial para representar palavras em um espaço contínuo de alta dimensão, onde palavras semelhantes têm representações vetoriais parecidas. A média desses vetores para todas as palavras de um documento pode ser usada para representar o documento, mas não é diretamente baseada em frequências de termos e suas inversas na coleção.

E) Paragraph Vector (Doc2Vec): Uma extensão do Word2Vec que representa documentos inteiros como vetores. Este modelo procura preservar a ordem das palavras e capturar a semântica distribuída de segmentos de texto maiores do que palavras individuais. Tal como a alternativa D, não baseia sua representação na frequência dos termos e suas distribuições inversas.",1680506
tópico 6,Modelos probabilísticos gráficos: cadeias de Markov; filtros de Kalman; Redes bayesianas,"Questão: Uma empresa de consultoria está empregando modelos probabilísticos gráficos para aprimorar a previsão de demanda de seus clientes no setor de energia elétrica. Considerando as especificidades dos dados e a necessidade de capturar a natureza sequencial da demanda ao longo do tempo, o modelo mais apropriado a ser adotado, neste caso, é o filtro de Kalman. Com base nesse contexto, assinale a opção que descreve corretamente uma das características fundamentais que justificam a escolha desse modelo.

A) O filtro de Kalman é um modelo não-paramétrico, o que permite grande flexibilidade na modelagem de processos sequenciais complexos.

B) O filtro de Kalman presume que os dados seguem uma cadeia de Markov, facilitando a modelagem da dependência temporal apenas entre estados consecutivos.

C) As Redes Bayesianas seriam mais adequadas devido à sua capacidade de lidar com uma ampla variedade de distribuições probabilísticas e de incorporar conhecimento a priori.

D) O filtro de Kalman é especificamente projetado para sistemas que são lineares e com ruído gaussiano, tornando-o ideal para previsões em séries temporais.

E) Cadeias de Markov seriam preferíveis nesse contexto, já que elas são particularmente úteis para modelar previsões em longo prazo sem a necessidade de ajuste dinâmico.

",D,"

A) Incorreta. O filtro de Kalman é um modelo paramétrico que assume um modelo linear com ruído normalmente distribuído e, portanto, possui restrições estruturais específicas.

B) Incorreta. Embora o filtro de Kalman envolva a ideia de processos sequenciais e possa ser relacionado a cadeias de Markov devido à maneira como atualiza as informações de estado, ele é mais formalmente vinculado ao tratamento de ruído e incertezas em observações e processos dinâmicos.

C) Incorreta. As Redes Bayesianas oferecem flexibilidade e a inclusão de conhecimento a priori e são aplicáveis a uma ampla gama de problemas, mas no contexto descrito, o filtro de Kalman é especificamente mencionado como sendo o modelo ideal devido às suas propriedades para séries temporais lineares com ruído gaussiano.

D) Correta. O filtro de Kalman é conhecido por ser aplicável a situações de modelagem de séries temporais onde o sistema pode ser descrito por um modelo linear e onde os erros (ou ruídos) no processo e nas medições são assumidos como gaussianos. No contexto da previsão de demanda de energia elétrica, essas características são comumente presentes.

E) Incorreta. Enquanto cadeias de Markov são úteis em muitos contextos, o ajuste dinâmico e a capacidade de lidar com incertezas em observações, duas características do filtro de Kalman, são cruciais para o acompanhamento da demanda de energia elétrica que pode ser sujeita a alterações rápidas e imprevistas.",5323356
tópico 6,Métodos e técnicas de identificação causal: Métodos experimentais RCT e de identificação quase-experimental,"Questão: Em estudos de ciências sociais e econômicas, é frequentemente desafiador estabelecer relações causais devido à complexidade dos fenômenos estudados e à dificuldade em controlar variáveis externas. Métodos de identificação causal, como Experimentos Controlados Randomizados (RCT) e métodos quase-experimentais, são amplamente utilizados para superar essas barreiras. Sobre essas técnicas de identificação causal, analise as seguintes afirmativas:

I. RCTs são desenhados para estabelecer causalidade ao randomizar a alocação de tratamento entre um grupo de tratamento e um grupo de controle, garantindo assim que ambos os grupos sejam estatisticamente equivalentes em média no início do experimento.

II. Métodos quase-experimentais dependem da existência de um evento natural ou de uma política que afete somente uma parte da população, criando condições análogas às de um experimento controlado, sem a necessidade de randomização.

III. O método de Diferenças em Diferenças, uma estratégia quase-experimental, requer que tanto o grupo de tratamento quanto o grupo de controle sejam expostos ao evento de interesse, o que pode comprometer a identificação da causalidade.

IV. O efeito estimado em um RCT pode não ser generalizável para toda a população se a amostra do estudo não for representativa ou se houver alta taxa de não conformidade com a alocação do tratamento.

Está correto o que se afirma apenas em:

A) I e II
B) I, II e III
C) I, II e IV
D) II e IV
E) III e IV

",C,"

Explicação dos itens:

I. Correto - Os RCTs permitem que pesquisadores estabeleçam relações causais ao aleatorizar a atribuição de indivíduos a grupos de tratamento e controle, o que ajuda a controlar por variáveis não observadas e garantir que os grupos sejam comparáveis em termos de características observáveis e não observáveis.

II. Correto - Métodos quase-experimentais utilizam eventos ou políticas exógenas como uma forma de atribuição de tratamento que não depende da escolha dos indivíduos, criando uma situação na qual o grupo de tratamento e o grupo de controle são identificados por uma fonte externa de variação.

III. Incorreto - O método de Diferenças em Diferenças compara a evolução de um grupo de tratamento antes e após um evento de interesse com a evolução de um grupo de controle no mesmo período. Essencialmente, a metodologia requer que o grupo de controle não seja afetado pelo evento de interesse para que se possa isolar o efeito causal do tratamento.

IV. Correto - A generalização dos resultados de um RCT para a população em geral pode ser problemática se houver diferenças substanciais entre a amostra do estudo e a população ou se muitos participantes não seguirem o tratamento prescrito (não conformidade), o que pode levar a uma violação das condições de validade interna e externa do experimento.",43999
tópico 6,Tipos de viés no processo gerador dos dados e soluções: Sampling bias; Selection bias; Attrition bias; Reporting bias; Measurement bias.,"Questão: Em estudos populacionais, mecanismos de amostragem e coleta de dados nem sempre são isentos de influências que possam comprometer a validade e confiabilidade das conclusões. É de suma importância que pesquisadores reconheçam e minimizem potenciais vieses. Considere as seguintes estratégias relacionadas aos tipos específicos de vieses e identifique qual delas está CORRETAMENTE emparelhada com o viés que visa controlar.

A) Sampling bias – Utilização de quotas baseadas em características populacionais conhecidas, garantindo que todos os segmentos da população estejam representados na amostra.

B) Selection bias – Adoção de um desenho de estudo caso-controle sem a preocupação de assegurar a equivalência entre os grupos em fatores que não sejam de interesse da pesquisa.

C) Attrition bias – Manutenção de um protocolo rigoroso no acompanhamento dos participantes do estudo, mas sem estratégias específicas para lidar com perdas no seguimento.

D) Reporting bias – Implementação de um sistema de coleta de dados baseado em auto-relato sem verificação ou validação através de outras fontes de informação.

E) Measurement bias – Utilização de instrumentos de medida padronizados e previamente validados, além de treinamento dos aplicadores para garantir a consistência na coleta de dados.

",E," 
Explicação dos itens:

A) Sampling bias se refere ao viés introduzido quando certos membros da população têm menor probabilidade de serem incluídos na amostra. A estratégia de usar quotas para cada segmento da população ajuda a mitigar este viés, portanto, esta afirmativa está correta.

B) Selection bias ocorre quando os participantes selecionados para o estudo não são representativos de toda a população, levando a resultados enviesados. Um desenho bem planejado que assegura a equivalência entre os grupos reduz este viés, e a afirmação sugere o oposto, portanto, está incorreta.

C) Attrition bias é o viés resultante da perda de participantes durante um estudo longitudinal. Estratégias específicas para lidar com as perdas são essenciais para controlar este viés, por isso, a afirmativa está incorreta.

D) Reporting bias está relacionado a distorções nos dados decorrentes de como as informações são relatadas. Um sistema baseado apenas em auto-relato sem qualquer verificação pode introduzir tal viés, assim esta afirmação não apresenta uma estratégia eficaz contra o reporting bias, e está incorreta.

E) Measurement bias é o viés introduzido por erros sistemáticos na coleta de dados. O uso de instrumentos de medida padronizados e validados, juntamente com o treinamento adequado dos aplicadores, são maneiras efetivas de reduzir este viés, tornando esta afirmativa correta e a opção certa para a pergunta.",224761
tópico 6,Testes de hipóteses: teste-z; teste-t; valorp; testes para uma amostra; testes de comparação de duas amostras; teste de normalidade (chi square); e intervalos de confiança.,"Questão: Um pesquisador deseja verificar se o tempo médio de conclusão de uma tarefa por seus empregados é significativamente diferente de 20 minutos. Para tanto, ele coleta uma amostra de 25 tempos de conclusão, obtendo uma média amostral de 22 minutos e um desvio padrão de 4 minutos. Sabendo que a distribuição dos tempos de conclusão é aproximadamente normal, qual teste estatístico é mais adequado para avaliar esta hipótese e qual seria a conclusão ao adotar um nível de significância de 5%?

A) Teste-z, não rejeitamos a hipótese nula.
B) Teste-t, não rejeitamos a hipótese nula.
C) Teste-z, rejeitamos a hipótese nula.
D) Teste-t, rejeitamos a hipótese nula.
E) Teste de normalidade Chi-square, não rejeitamos a hipótese nula.

",C,"O Teste-z é usado para testar hipóteses sobre a média de uma população quando o tamanho da amostra é grande (n ≥ 30) ou quando o tamanho é menor, mas a população tem uma distribuição normal conhecida e o desvio padrão populacional é conhecido. Neste caso, mesmo que o tamanho da amostra seja menor que 30, a condição de normalidade populacional permite o uso do Teste-z. Utiliza-se o desvio padrão amostral como uma estimativa do desvio padrão populacional quando este último não é conhecido. Como a hipótese é de que a média seja diferente de 20 minutos, trata-se de um teste bilateral. Calculando o z-score (z = (22-20) / (4/√25) = 2.5) e consultando a tabela da distribuição normal padrão, podemos ver que a probabilidade associada a um z-score de 2.5 (ou maior em magnitude) é menor que 0.05 (nível de significância), indicando que devemos rejeitar a hipótese nula de que a média é 20 minutos.

Explicação dos itens:
A) O Teste-z é apropriado pois a distribuição é normal, mas a conclusão está errada dado que o valor encontrado indica que devemos rejeitar a hipótese nula.
B) O Teste-t seria mais apropriado se não tivéssemos a distribuição normal da população ou desconhecêssemos o desvio padrão populacional e a amostra fosse pequena.
C) Correto. O Teste-z é usado sob as condições apresentadas e o valor calculado leva à rejeição da hipótese nula com um nível de confiança de 95%.
D) O Teste-t é também usado para amostras pequenas, mas com a suposição de que o desvio padrão populacional não é conhecido, o que não se aplica neste caso.
E) O Teste de normalidade Chi-square testaria se a distribuição dos dados é normal, o que não é necessário supor aqui pois já foi estabelecido que é aproximadamente normal.",2766993
tópico 6,"Diagramas causais: gráficos acíclicos dirigidos; variáveis confundidoras, colisoras e de mediação","Questão: Na análise de dados usando Diagramas Causais, também conhecidos como Gráficos Acíclicos Dirigidos (DAGs), é fundamental compreender a natureza das relações entre as variáveis envolvidas. Considere as seguintes afirmações sobre as propriedades que definem tipos específicos de variáveis nos DAGs:

I. Uma variável confundidora é aquela que afeta tanto a variável independente quanto a variável dependente, criando uma associação espúria entre elas caso não seja adequadamente controlada.

II. Uma variável colisora é uma variável comum em caminhos que partem de uma variável independente para uma variável dependente, podendo gerar confusão na interpretação das relações causais se estiver presente.

III. Uma variável de mediação é aquela que transmite o efeito de uma variável independente para uma dependente e é parte do caminho causal entre as duas.

Identifique a opção que contém a CORRETA interpretação das afirmações sobre as variáveis no contexto dos Diagramas Causais:

A) Apenas a afirmação I está correta.

B) Apenas a afirmação II está correta.

C) Apenas a afirmação III está correta.

D) As afirmações I e III estão corretas.

E) Todas as afirmações estão corretas.

",D,"

Explicação dos itens:

A) Incorreta. A primeira afirmação é verdadeira, mas a terceira também é verdadeira. Uma variável de mediação é de fato parte do caminho causal e não pode ser ignorada.

B) Incorreta. A afirmação II está incorreta porque a variável colisora está no caminho entre duas variáveis, mas não é necessariamente afetada por uma variável independente. Ela é o ponto onde dois efeitos causais convergem, o que pode levar ao que é chamado de efeito colisor, que pode introduzir um viés se for ajustado de maneira inadequada.

C) Incorreta. Apesar de a afirmação III estar correta, a afirmação I também é verdadeira; I e III não são mutuamente exclusivas.

D) Correta. A primeira afirmação é correta porque uma variável confundidora é aquela que afeta tanto a exposição quanto o desfecho, confundindo a relação. A terceira afirmação é correta porque ela define corretamente uma variável de mediação como parte do caminho causal entre a variável independente e a dependente.

E) Incorreta. A afirmação II não é precisa. Uma variável colisora é definida como o ponto onde dois caminhos causais se encontram, formando um caminho fechado ou um ""loop"". Ela não gera confusão devido à sua localização em caminhos, mas sim quando é controlada incorretamente, abrindo o caminho fechado e potencialmente introduzindo viés.",6779350
tópico 6,Testes de hipóteses: teste-z; teste-t; valorp; testes para uma amostra; testes de comparação de duas amostras; teste de normalidade (chi square); e intervalos de confiança.,"Questão: Suponha que um pesquisador deseja testar se há uma diferença significativa na média de horas diárias dedicadas ao estudo entre dois grupos de estudantes: aqueles que preparam para o vestibular e aqueles que se preparam para concursos públicos. Foi coletada uma amostra aleatória de 30 estudantes de cada grupo. Para o grupo do vestibular, a média de horas de estudo foi de 6 horas com um desvio padrão de 1,5 horas. Para o grupo de concursos, a média encontrada foi de 7 horas com um desvio padrão de 1,2 horas. O pesquisador deseja usar um nível de significância de 5%. Qual teste estatístico é o mais apropriado para comparar as médias entre os dois grupos de estudantes e qual seria a decisão correta baseada neste teste?

A) Teste-t para amostras independentes, falha em rejeitar a hipótese nula.

B) Teste-z para duas amostras, rejeita a hipótese nula.

C) Teste de normalidade (chi square), falha em rejeitar a hipótese nula.

D) Teste-t para amostras independentes, rejeita a hipótese nula.

E) Teste-z para duas amostras, falha em rejeitar a hipótese nula.

",D,"

Explicação:

A) O teste-t para amostras independentes é de fato apropriado para comparar médias entre dois grupos pequenos (com amostras menores que 30 ou aproximadamente isso), no entanto, com as médias e desvios padrão dados, é mais provável que devemos rejeitar a hipótese nula de que as médias são iguais.

B) O teste-z é geralmente usado quando o tamanho da amostra é grande (tipicamente maior que 30) e a população tem uma distribuição normal conhecida ou quando o desvio padrão da população é conhecido. Neste caso, não temos informações que justifiquem o uso do teste-z.

C) O teste de normalidade (chi square) é utilizado para avaliar se uma distribuição de frequências observadas difere significativamente de uma distribuição teórica (como a normal), o que o torna inadequado para este caso específico que é uma comparação de médias entre dois grupos.

D) O teste-t para amostras independentes é a escolha correta porque as amostras são independentes e relativamente pequenas (30 para cada grupo), e não se conhece o desvio padrão da população. Com uma diferença nas médias de uma hora e com os desvios padrões fornecidos, é provável que o resultado do teste seja significativo, levando à rejeição da hipótese nula.

E) Novamente, o teste-z não é apropriado para este contexto pelo mesmo motivo mencionado na opção B, e ainda que fosse aplicável, com os dados fornecidos, é mais provável que a hipótese nula seria rejeitada, contrário ao que sugere esta opção.",8966120
tópico 6,Métodos e técnicas de identificação causal: Métodos experimentais RCT e de identificação quase-experimental,"Questão:
A eficácia de métodos experimentais e quase-experimentais para a identificação causal em ciências sociais tem sido amplamente discutida. Entre estes métodos, os Ensaios Controlados Randomizados (RCTs) e os de Identificação Quase-Experimental desempenham papéis críticos. Com relação a esses métodos, avalie as seguintes afirmações:

I. RCTs são considerados o ""padrão-ouro"" para determinar relações causais, pois a randomização ajuda a eliminar o viés de seleção e confundimento de variáveis.
II. Métodos quase-experimentais, como a avaliação por diferença em diferenças (DiD), podem ser aplicados em situações onde a randomização não é possível, embora os resultados possam estar sujeitos a viés de eventos concomitantes não observados.
III. Tanto os RCTs quanto os métodos quase-experimentais sempre garantem a identificação de efeitos causais puros, independentemente das circunstâncias e contexto aplicados.

É correto o que se afirma em:

A) I, apenas.
B) II, apenas.
C) I e II, apenas.
D) III, apenas.
E) I, II e III.

",C," 

Explicação dos itens:

I. Esta afirmação é verdadeira, uma vez que a principal vantagem dos RCTs é a randomização, que ajuda a criar grupos de tratamento e controle comparáveis, permitindo assim uma estimativa causal mais precisa ao eliminar ou reduzir o viés de seleção e confundimento.

II. Esta afirmação também é verdadeira. Métodos quase-experimentais, como DiD, são utilizados quando a randomização não é viável por motivos éticos, práticos ou por restrições de tempo. Eles esforçam-se para simular um experimento e podem proporcionar estimativas causais razoáveis, porém, estão mais suscetíveis a viés causado por variáveis não observadas ou eventos concomitantes que podem impactar tanto o grupo de tratamento quanto o grupo de controle.

III. Esta afirmação é falsa. Embora RCTs e métodos quase-experimentais sejam projetados para identificar relações causais, eles têm limitações e não podem garantir a identificação de efeitos causais puros em todas as circunstâncias. A validade dos resultados depende da adequada aplicação dos métodos e do controle sobre possíveis fontes de viés.",5093924
tópico 6,Tipos de viés no processo gerador dos dados e soluções: Sampling bias; Selection bias; Attrition bias; Reporting bias; Measurement bias.,"Questão: A qualidade dos dados coletados para um estudo científico ou uma análise estatística é fundamental para garantir a validade dos resultados e conclusões obtidos. No entanto, diversos tipos de viés podem afetar o processo gerador dos dados, comprometendo a integridade e confiabilidade dos mesmos. Considerando os diferentes tipos de viés em pesquisa, identifique a alternativa que descreve corretamente o ""Sampling Bias"" e uma solução potencial para mitigá-lo:

A) O ""Sampling Bias"" ocorre quando uma pesquisa não consegue acompanhar todos os participantes até o final do estudo, levando a uma distorção dos resultados. A solução para esse tipo de viés é garantir um acompanhamento mais rigoroso e incentivar a permanência dos participantes com follow-up frequente.

B) ""Measurement Bias"" se refere à coleta de dados de maneira inconsistente entre os participantes de um estudo. Para minimizar esse viés, padronização dos instrumentos de medida e treinamento dos coletores de dados são essenciais.

C) O ""Sampling Bias"" acontece quando a amostra escolhida para um estudo não representa adequadamente a população de interesse, levando a resultados que não são generalizáveis. Uma forma de mitigar esse viés é através da utilização de métodos de amostragem probabilística que garantam a representatividade da amostra.

D) ""Selection Bias"" ocorre quando certas características dos participantes afetam sua probabilidade de serem selecionados para o estudo, resultando em uma amostra não representativa. Estratégias como a randomização dos participantes podem ser utilizadas para reduzir esse tipo de viés.

E) O ""Reporting Bias"" acontece quando há diferenças na forma como os dados são reportados ou registrados, o que pode ser mitigado por meio da utilização de relatórios padronizados e procedimentos de coleta de dados cegos.

",C," 
O ""Sampling Bias"" é um tipo de viés que ocorre quando a amostra coletada não representa adequadamente a população alvo do estudo, o que pode comprometer a generalizabilidade dos resultados. Amostras viesadas podem surgir devido a uma variedade de razões, como metodologias de amostragem inadequadas ou restrições no processo de coleta de dados. Para mitigar esse viés, como mencionado na alternativa C, a adoção de métodos de amostragem probabilística é crucial, pois eles permitem que cada membro da população tenha uma chance conhecida e não nula de ser incluído na amostra, reduzindo assim o risco de uma amostra tendenciosa.

As outras alternativas descrevem diferentes tipos de viés e não são a definição correta para ""Sampling Bias"". A alternativa A descreve o ""Attrition Bias"", que pode ser abordado de maneira diferente. A alternativa B apresenta o ""Measurement Bias"", cuja solução envolve a padronização na coleta de dados. A alternativa D aborda o ""Selection Bias"" e sugere a randomização como solução, mas essa não é a descrição de ""Sampling Bias"". Por fim, a alternativa E fala sobre o ""Reporting Bias"" e como o mesmo pode ser mitigado, o qual também não é o viés em questão.",2583713
tópico 6,"Diagramas causais: gráficos acíclicos dirigidos; variáveis confundidoras, colisoras e de mediação","Questão: Considere que um pesquisador está interessado em estudar a relação entre o uso de dispositivos móveis e o rendimento acadêmico de estudantes universitários. O pesquisador propôs um modelo causal representado por um gráfico acíclico dirigido, incluindo variáveis como o tempo de estudo, o uso de aplicativos educacionais e o nível de atenção em sala de aula.

Neste cenário, qual das variáveis listadas a seguir é mais provavelmente uma variável confundidora, capaz de afetar tanto o uso de dispositivos móveis quanto o rendimento acadêmico, e que deveria ser controlada na análise para evitar inferências causais equivocadas?

A) Tempo de lazer dedicado a jogos no dispositivo móvel.
B) Frequência de uso dos aplicativos educacionais no dispositivo móvel.
C) Motivação intrínseca para o aprendizado.
D) Quantidade de horas de sono por noite.
E) Presença de interações sociais durante as aulas.

",C,"

Explicação dos itens:

A) Tempo de lazer dedicado a jogos no dispositivo móvel: esta variável está mais diretamente relacionada com o uso dos dispositivos móveis e não necessariamente impacta o rendimento acadêmico de forma independente.

B) Frequência de uso dos aplicativos educacionais no dispositivo móvel: esta variável é mais diretamente uma mediadora potencial entre o uso do dispositivo móvel e o rendimento acadêmico, pois o uso de aplicativos educacionais pode representar uma forma de estudo.

C) Motivação intrínseca para o aprendizado: esta variável pode tanto influenciar o rendimento acadêmico, pois estudantes motivados tendem a se sair melhores, quanto o uso de dispositivos móveis, já que estudantes motivados podem buscar mais recursos tecnológicos para auxiliar no aprendizado. Controlar essa variável ajudaria a reduzir o viés na estimação do efeito do uso de dispositivos no rendimento acadêmico.

D) Quantidade de horas de sono por noite: embora a falta de sono possa afetar o rendimento acadêmico, não há uma relação clara sobre como isso afetaria o uso de dispositivos móveis, tornando-a menos provável de ser uma variável confundidora.

E) Presença de interações sociais durante as aulas: esta variável poderia ser tanto um colisor, se afetada tanto pelo uso de dispositivos quanto pelo rendimento acadêmico, quanto um efeito do uso de dispositivos móveis (se usar o dispositivo levasse a menos interações sociais). Não é clara sua posição como confundidora.

A variável ""Motivação intrínseca para o aprendizado"" (C) é, portanto, a mais adequada para ser controlada na análise da relação entre uso de dispositivos móveis e rendimento acadêmico, pois pode confundir a relação causal entre esses dois elementos.",4741536
tópico 6,Modelos probabilísticos gráficos: cadeias de Markov; filtros de Kalman; Redes bayesianas,"Questão:

O planejamento urbano requer a modelagem precisa do tráfego de veículos em diferentes horas do dia para evitar congestionamentos e melhorar a fluidez do trânsito. Um pesquisador propõe o uso de Modelos Probabilísticos Gráficos para prever o estado do trânsito em cada interseção viária em um período de tempo futuro, levando em consideração as variações e incertezas naturais do sistema. Dentre as seguintes opções, qual seria a mais adequada para realizar previsões de estados sequenciais do tráfego ao longo do tempo, incorporando medidas ruidosas e dados incompletos para atualização das previsões em tempo real?

A) Cadeias de Markov

B) Filtros de Kalman

C) Redes Bayesianas

D) Regressão Linear Múltipla

E) Árvores de Decisão

",B,"

Explicação:

A) Cadeias de Markov: Modelam processos estocásticos onde o estado futuro depende apenas do estado presente e não dos estados anteriores, o que pode não ser suficiente para lidar eficientemente com medidas ruidosas ou dados incompletos.

B) Filtros de Kalman: São algoritmos ideais para estimar o estado de sistemas dinâmicos em tempo real, a partir de medições observadas ao longo do tempo que contêm ruídos. Eles permitem atualizar as previsões conforme novos dados são recebidos, o que é essencial para modelagem de tráfego em tempo real.

C) Redes Bayesianas: Apesar de serem poderosas para representar incertezas e raciocínio probabilístico, não são específicas para dados sequenciais ou para incorporação iterativa de novas informações como exigido no contexto do tráfego.

D) Regressão Linear Múltipla: É um método estatístico para modelar a relação entre uma variável dependente e uma ou mais variáveis independentes, mas não é tipicamente usado para dados temporais ruidosos e atualização em tempo real de informações.

E) Árvores de Decisão: São modelos de predição usados em estatísticas, mineração de dados e aprendizado de máquina, mas não são ideais para manipular dados sequenciais ruidosos nem para a atualização contínua de previsões com novas medições.",9546645
tópico 6,Modelos probabilísticos gráficos: cadeias de Markov; filtros de Kalman; Redes bayesianas,"Questão:

Considere a modelagem de processos estocásticos em sistemas de informação. Na tentativa de prever o estado futuro de um sistema dinâmico que está sujeito a incertezas, diferentes modelos probabilísticos gráficos podem ser utilizados. Cada modelo tem suas particularidades e é mais adequado a determinados cenários. Nesse contexto, avalie as afirmações a seguir, relacionadas aos modelos probabilísticos gráficos de cadeias de Markov, filtros de Kalman e Redes Bayesianas.

I. As cadeias de Markov são modelos probabilísticos que assumem a propriedade de ""falta de memória"", ou seja, a probabilidade de transição para um estado futuro depende apenas do estado presente e não da sequência de eventos que antecederam.

II. Filtros de Kalman são utilizados para estimar o estado de sistemas dinâmicos lineares em tempo contínuo com ruído gaussiano, sendo ineficazes em sistemas não lineares ou em processos com ruídos não gaussianos.

III. Redes Bayesianas são grafos direcionados acíclicos (DAG) que representam distribuições de probabilidade conjunta sobre um conjunto de variáveis e permitem a inferência probabilística por meio da aplicação do teorema de Bayes.

É correto o que se afirma em:

A) I, II e III.
B) I e III, apenas.
C) II e III, apenas.
D) I e II, apenas.
E) III, apenas.

",B,"

Explicação dos itens:

I. Correto. Esta afirmação descreve corretamente a propriedade fundamental das cadeias de Markov, que é a propriedade de Markov ou a falta de memória.

II. Incorreto. Enquanto os filtros de Kalman são de fato usados para estimar estados de sistemas dinâmicos lineares com ruído gaussiano, existem extensões do filtro de Kalman - como o Filtro de Kalman Estendido (EKF) ou o Filtro de Kalman Não Linear (UKF) - que são projetadas para lidar com sistemas não lineares ou ruídos não gaussianos.

III. Correto. As Redes Bayesianas são representadas por grafos direcionados acíclicos e codificam as dependências condicionais entre variáveis, permitindo a realização de inferências com base no teorema de Bayes.

Então, as afirmações I e III estão corretas, fazendo com que a opção B seja a resposta adequada.",4521834
tópico 6,Tipos de viés no processo gerador dos dados e soluções: Sampling bias; Selection bias; Attrition bias; Reporting bias; Measurement bias.,"Questão:
Na pesquisa científica, é de fundamental importância estar atento aos diferentes tipos de viés que podem comprometer a validade e a confiabilidade dos resultados. Nesse contexto, analise as afirmações a seguir sobre os tipos de viés no processo gerador dos dados e suas soluções:

I) O viés de amostragem (Sampling bias) ocorre quando a amostra selecionada não representa adequadamente a população de interesse, o que pode ser mitigado através da utilização de técnicas de amostragem probabilística.

II) O viés de seleção (Selection bias) é um erro sistemático devido à maneira não aleatória de seleção dos participantes da pesquisa, o qual pode ser atenuado utilizando-se critérios de inclusão e exclusão claros e registrando as razões para a não participação de potenciais sujeitos de pesquisa.

III) O viés de atrito (Attrition bias) refere-se à influência desproporcional causada pelos participantes que deixam o estudo prematuramente, sendo que a solução para esse tipo de viés inclui o uso de análise de intenção de tratar e a busca por métodos que minimizem a perda de seguimento.

IV) O viés de medição (Measurement bias) implica erros sistemáticos nas estimativas de efeito devido a medições imprecisas ou inconsistentes dos dados, o que pode ser reduzido por meio da padronização de instrumentos de medida e treinamento adequado dos entrevistadores ou técnicos.

É correto o que se afirma em:
a) I e II apenas
b) I, II e III apenas
c) II, III e IV apenas
d) III e IV apenas
e) I, II, III e IV

",E," 
Todas as afirmações são verdadeiras e representam corretamente cada tipo de viés e suas possíveis soluções:

I) A afirmação sobre o viés de amostragem é correta, dado que a utilização de uma amostragem probabilística aumenta a chance de que a amostra seja representativa da população como um todo, reduzindo este viés.

II) A afirmação sobre o viés de seleção também é correta. Usar critérios claros que definam quem pode ou não participar de uma pesquisa ajuda a evitar que a seleção de participantes seja influenciada por fatores que poderiam levar ao viés.

III) O viés de atrito é bem descrito na afirmação, pois de fato a análise de intenção de tratar tenta manter o tratamento estatístico dos participantes conforme o grupo originalmente designado, independentemente dos desfechos, o que ajuda a mitigar os efeitos deste viés.

IV) A afirmação sobre o viés de medição identifica corretamente que a consistência e precisão das medições são chave para evitar erros sistemáticos na coleta de dados. A padronização e treinamento ajudam a assegurar que a coleta de dados seja feita da mesma maneira em todos os casos.",308661
tópico 6,"Diagramas causais: gráficos acíclicos dirigidos; variáveis confundidoras, colisoras e de mediação","Questão: Em estatística e epidemiologia, os diagramas causais, especificamente os gráficos acíclicos dirigidos (DAGs), são utilizados para representar e analisar a relação entre variáveis e eventos. Considere as seguintes afirmativas sobre os componentes de um DAG:

I - Uma variável confundidora é aquela que afeta tanto a variável de exposição quanto a de desfecho, introduzindo uma associação espúria que pode levar a conclusões errôneas sobre a relação causal.

II - Uma variável colidora é uma variável comum que é efeito tanto da variável de exposição quanto da de desfecho, e o controle para essa variável pode criar uma falsa associação entre as variáveis de exposição e desfecho.

III - Uma variável de mediação é uma variável intermediária através da qual a variável de exposição influencia a variável de desfecho, sendo essencial para a compreensão do mecanismo pelo qual a exposição afeta o desfecho.

Qual(is) afirmativa(s) está(ão) correta(s) com relação aos componentes de um DAG?

A) Apenas a afirmativa I está correta.
B) Apenas a afirmativa II está correta.
C) Apenas a afirmativa III está correta.
D) As afirmativas I e II estão corretas.
E) Todas as afirmativas estão corretas.

",E,"

Explicação dos itens:

A afirmativa I está correta, uma variável confundidora é uma fonte comum de variação para tanto a variável de exposição quanto a de desfecho, podendo levar a uma associação que não reflete uma relação causal verdadeira.

A afirmativa II está correta, uma variável colidora está na confluência de dois efeitos, sendo ela causada pela variável de exposição e também afetando a variável de desfecho. O controle inadequado de uma variável colidora, como a sua inclusão em modelos de regressão onde não deveria estar presente, pode induzir uma associação que, na verdade, não existe.

A afirmativa III está correta, uma variável de mediação é aquela que está presente no caminho causal entre a exposição e o desfecho, ajudando a explicar o modo pelo qual a exposição afeta o desfecho. Trata-se de um conceito fundamental para decompor e entender os efeitos diretos e indiretos em um modelo causal.",320202
tópico 6,Métodos e técnicas de identificação causal: Métodos experimentais RCT e de identificação quase-experimental,"Questão: Em estudos que visam estabelecer relações causais entre variáveis, especialmente em áreas como economia e ciências sociais, abordagens experimentais e quase-experimentais são frequentemente utilizadas para superar os desafios postos pela inferência causal. No contexto dos métodos de pesquisa, um Randomized Controlled Trial (RCT) é diferente de métodos quase-experimentais como Matching, Regression Discontinuity Design (RDD) e Instrumental Variables (IV). Com base nessa distinção, qual das seguintes afirmações é verdadeira acerca do RCT e dos métodos quase-experimentais mencionados?

A) RCTs requerem uma linha de corte pré-determinada na variável de interesse para definir os grupos de tratamento e controle, enquanto RDD se baseia na aleatoriedade para alocar os sujeitos nos grupos.
B) Matching é um método experimental enquanto RCT é um método quase-experimental utilizado quando a aleatoriedade na alocação dos sujeitos não é possível.
C) IV é uma técnica que visa remediar problemas de endogeneidade em modelos regressivos, sem a necessidade de aleatorização dos tratamentos.
D) O principal objetivo do RDD é encontrar um grupo de controle que seja estatisticamente indistinguível do grupo de tratamento, garantindo assim a aleatoriedade na alocação.
E) RCTs e métodos quase-experimentais são metodologias que sempre garantem a imparcialidade na escolha dos sujeitos para os grupos de tratamento e controle, mesmo em contextos onde variáveis ocultas possam existir.

",C," 

Explicação dos itens:
A) Esta afirmação é incorreta, pois os RCT se baseiam na aleatoriedade para alocar sujeitos nos grupos de tratamento e controle, enquanto o RDD utiliza uma linha de corte pré-determinada para definir seus grupos.
B) Esta afirmação não está correta. Matching é um método quase-experimental, e RCT é um método experimental em que a aleatoriedade é um componente chave para a alocação dos sujeitos para o tratamento e controle.
C) Correto. IV é uma técnica usada em análises regressivas quando há preocupação com a endogeneidade das variáveis explicativas. O método dos instrumentos permite identificar o efeito causal quando a alocação aleatória de tratamentos não é possível ou não foi feita.
D) Esta opção é falsa, pois confunde o objetivo do RDD com o do Matching. O RDD se baseia na existência de uma linha de corte para designar quem recebe o tratamento, não buscando necessariamente um grupo de controle idêntico ao de tratamento.
E) Esta afirmação é errônea, porque tanto RCTs quanto métodos quase-experimentais estão suscetíveis a limitações, e variáveis ocultas podem comprometer a imparcialidade na formação dos grupos mesmo quando procedimentos rigorosos são aplicados.",3497891
tópico 6,Testes de hipóteses: teste-z; teste-t; valorp; testes para uma amostra; testes de comparação de duas amostras; teste de normalidade (chi square); e intervalos de confiança.,"Questão: Um pesquisador deseja comparar o tempo médio de resposta a um medicamento entre dois grupos distintos de pacientes. O grupo A, com uma amostra de 50 pacientes, apresentou média de tempo de resposta de 8,4 horas com um desvio padrão de 1,2 horas, enquanto o grupo B, com uma amostra de 60 pacientes, registrou uma média de tempo de resposta de 7,8 horas com um desvio padrão de 1,4 horas. Assumindo uma variância comum e desconhecida entre as populações e um nível de significância de 5%, qual seria o teste estatístico mais apropriado para comparar os tempos médios de resposta dos dois grupos e qual seria a decisão correta baseada nesse teste?

A) Teste-Z para duas amostras, não rejeitar a hipótese nula.
B) Teste-Z para duas amostras, rejeitar a hipótese nula.
C) Teste-t para duas amostras independentes, não rejeitar a hipótese nula.
D) Teste-t para duas amostras independentes, rejeitar a hipótese nula.
E) Teste de normalidade (chi-square), não rejeitar a hipótese nula.

",D," 

Explicação:
A) Teste-Z para duas amostras é geralmente usado quando conhecemos as variâncias das populações ou quando as amostras são suficientemente grandes (geralmente n > 30 é considerado grande). No entanto, aqui as variâncias são desconhecidas e assumidas como comuns, tornando essa opção incorreta.
B) Mesma explicação da alternativa A, mas acrescentando que a decisão sobre rejeitar ou não a hipótese nula dependeria do cálculo do valor-p, não podendo ser determinada apenas com as informações fornecidas.
C) Teste-t para duas amostras independentes é o teste correto aqui, uma vez que as variâncias são desconhecidas e as amostras não são excepcionalmente grandes. No entanto, não podemos definir se devemos ou não rejeitar a hipótese nula sem realizar os cálculos.
D) Esta é a resposta correta, pois o teste-t para duas amostras independentes é o teste apropriado para comparar duas médias de amostras independentes com variâncias desconhecidas e assumidamente iguais. O pesquisador deve então calcular o valor-t e compará-lo com o valor crítico do teste-t para um nível de significância de 5%. Se o valor-t calculado excede o valor crítico, rejeita-se a hipótese nula.
E) O teste de normalidade (chi-square) é utilizado para verificar se uma distribuição de frequências se ajusta a uma distribuição teórica, como a normal. Não é o teste adequado para comparar médias entre grupos.",3863518
tópico 6,Tipos de viés no processo gerador dos dados e soluções: Sampling bias; Selection bias; Attrition bias; Reporting bias; Measurement bias.,"Questão: No contexto de pesquisa científica, o processo de coleta e análise de dados está sujeito a vários tipos de viés, que podem comprometer a validade dos resultados obtidos. Considere as seguintes descrições sobre diferentes tipos de viés:

I. Sampling bias ocorre quando alguns membros da população têm maior probabilidade de serem incluídos na amostra do que outros, resultando numa amostra não representativa.
II. Selection bias refere-se a distorções que surgem da maneira como sujeitos são selecionados para participar em um estudo, ou da forma como cases e controls são recrutados em estudos observacionais.
III. Attrition bias é um viés que pode acontecer devido à perda de participantes durante o acompanhamento em um estudo longitudinal, que tende a diferir entre os grupos comparados em termos de propriedades importantes.
IV. Reporting bias envolve uma tendência na publicação ou divulgação de resultados, muitas vezes fazendo com que estudos com resultados significativos ou positivos sejam mais reportados do que outros.
V. Measurement bias ocorre quando os instrumentos de medição não são calibrados adequadamente, resultando em medições imprecisas ou inconsistentes.

Tendo em vista as descrições acima, é correto afirmar que:

A) Apenas a descrição III é correta.
B) As descrições II e V estão incorretas.
C) Todas as descrições estão corretas.
D) As descrições I, III e IV são corretas, enquanto II e V estão incorretas.
E) Apenas as descrições I e IV são corretas.

",C," 

- A alternativa A é incorreta porque não é apenas a descrição III que está correta, todas as descrições I, II, III, IV e V são exemplos válidos de diferentes tipos de viés.
- A alternativa B é incorreta, pois as descrições II e V estão corretas e descrevem adequadamente o que é selection bias e measurement bias, respectivamente.
- A alternativa C é a correta, uma vez que todas as descrições I a V estão corretas e representam adequadamente os tipos de viés mencionados: sampling bias, selection bias, attrition bias, reporting bias e measurement bias.
- A alternativa D é incorreta pois afirma que as descrições II e V estão incorretas, o que não é verdade, pois ambas descrevem corretamente seus respectivos vieses.
- A alternativa E é incorreta porque sugere que apenas as descrições I e IV são corretas, enquanto todas as descrições de I a V são exemplos corretos dos tipos de viés listados.",3656078
tópico 6,"Diagramas causais: gráficos acíclicos dirigidos; variáveis confundidoras, colisoras e de mediação","Questão:
Dentro do contexto de análise causal, os diagramas causais representados em gráficos acíclicos dirigidos (DAGs) são ferramentas gráficas que auxiliam na identificação e distinção entre diferentes tipos de variáveis que podem intervir na relação causal entre uma exposição (X) e um desfecho (Y). Considere as descrições abaixo:

I. Variáveis Confundidoras: Variáveis que estão associadas tanto à exposição quanto ao desfecho, podendo gerar uma falsa associação ou ocultar uma verdadeira associação entre eles se não forem adequadamente controladas.

II. Variáveis Colisoras: Variáveis que são consequências tanto da exposição quanto do desfecho, e que, ao serem controladas, podem criar uma associação espúria entre X e Y devido ao fechamento de um caminho inverso.

III. Variáveis de Mediação: Variáveis através das quais a exposição produz efeitos no desfecho, atuando como uma ponte na cadeia causal entre X e Y.

Com base nas descrições, no contexto de um estudo observacional que visa analisar o efeito da atividade física (X) na redução da incidência de doenças cardiovasculares (Y), qual tipo de variável seria a alimentação saudável, considerando que ela tende a estar associada tanto à prática regular de atividade física quanto à redução de doenças cardiovasculares?

A) Variável Confundidora
B) Variável Colisora
C) Variável de Mediação
D) Variável Independente
E) Variável Dependente

",A,"

Explicação dos itens:

A) Variável Confundidora: Correta. A alimentação saudável pode estar associada tanto à prática regular de atividade física quanto à redução de doenças cardiovasculares, portanto, pode confundir a relação causal se não for controlada, o que a caracteriza como uma variável confundidora.

B) Variável Colisora: Incorreta. Uma variável colisora seria uma consequência de ambos, a exposição e o desfecho, o que não é o caso da alimentação saudável no contexto dado.

C) Variável de Mediação: Incorreta. Embora a alimentação saudável possa mediar o efeito da atividade física na saúde cardiovascular, a questão é sobre a sua associação com ambos, atividade física e redução de doenças cardiovasculares, de forma independente de ser um mediador.

D) Variável Independente: Incorreta. A questão pede especificamente o papel da alimentação saudável no contexto da relação entre atividade física e doenças cardiovasculares, e não apenas como uma variável que pode influenciar o desfecho.

E) Variável Dependente: Incorreta. A variável dependente neste caso seria a incidência de doenças cardiovasculares, ou seja, o desfecho de interesse do estudo.",4856149
tópico 6,Modelos probabilísticos gráficos: cadeias de Markov; filtros de Kalman; Redes bayesianas,"Questão: Uma empresa de transporte deseja otimizar seu sistema de envio de cargas, tendo em conta a probabilidade de atrasos devido a condições climáticas adversas. Para isso, a equipe de logística está considerando aplicar modelos probabilísticos gráficos na previsão de incidentes que podem afetar o prazo de entrega. Qual das seguintes abordagens é mais adequada para modelar a sequência temporal de eventos envolvendo atrasos condicionados ao estado do clima e outras variáveis observáveis, com o objetivo de predizer o risco de atraso na entrega?

A) Utilizar uma Rede Bayesiana estática, já que esse modelo é eficiente na captura de todas as dependências entre variáveis envolvidas no processo.

B) Implementar uma cadeia de Markov, pois este modelo é adequado para representar processos estocásticos que possuem o princípio de Markov (sem memória), onde a previsão do próximo estado depende apenas do estado atual.

C) Desenvolver filtros de Kalman que são ótimos para lidar com variáveis aleatórias contínuas e observações ruidosas, o que se aplica diretamente na previsão de atrasos baseada em medições meteorológicas imprecisas.

D) Incorporar modelos de filas e teoria das filas para prever com precisão os tempos de espera dado o número de cargas e de veículos disponíveis, independentemente de outros fatores.

E) Aplicar uma cadeia de Markov oculta, pois além de considerar as transições de estados como numa cadeia de Markov, este modelo permite lidar com eventos não diretamente observáveis, como é o caso da condição real do clima.

",E," 

A alternativa E está correta porque as cadeias de Markov ocultas são ideais para modelar processos em que existe uma sequência temporal de eventos observáveis que são influenciados por estados internos não observáveis. No contexto da questão, os estados não observáveis seriam as condições climáticas reais, e os eventos observáveis seriam os atrasos de entrega da empresa de transporte. 

- A alternativa A é incorreta, pois as Redes Bayesianas estáticas são mais adequadas para representar as relações causais entre variáveis em um dado ponto no tempo e não são ideais para sequências temporais.
- A alternativa B é inadequada, uma cadeia de Markov simples não leva em conta variáveis observáveis e não observáveis e é limitada às transições de estado baseadas no estado presente.
- A alternativa C, os filtros de Kalman, são usados primariamente para sistemas dinâmicos lineares em tempo contínuo e podem não ser ideais para modelar a sequência discreta de atrasos de entrega.
- A alternativa D não é a mais apropriada, pois a teoria das filas é mais focada em processos de serviço e espera, e não leva em conta a complexidade de variáveis como as condições climáticas na modelagem de atrasos.",5960198
tópico 6,Métodos e técnicas de identificação causal: Métodos experimentais RCT e de identificação quase-experimental,"Questão: A fim de estimar o efeito causal de uma nova política pública voltada para a melhoria da educação em regiões carentes, pesquisadores estão considerando utilizar métodos de identificação causal. Eles têm disponibilidade de dados antes e depois da implementação, bem como grupos de controle. Com base no cenário, qual método ou técnica abaixo seria mais apropriado para a identificação de efeitos causais?

A) Análise descritiva antes e depois sem grupo controle.
B) Regressão Linear Múltipla com dados de seção cruzada.
C) Experimento Randomizado Controlado (RCT).
D) Comparação antes e depois com um grupo controle não equiparado.
E) Regressão Discontinuidade (RD) com variáveis instrumentais.

",C," 

Explicação dos itens:

A) Análise descritiva antes e depois sem grupo controle é útil para avaliação preliminar, mas não permite controle de variáveis confundidoras e, portanto, não estabelece relações causais robustas.

B) Regressão Linear Múltipla com dados de seção cruzada pode controlar por múltiplas variáveis, mas sem dados longitudinais e sem um experimento randomizado, a identificação de relações causais pode ser limitada por viés de variáveis omitidas.

C) Experimento Randomizado Controlado (RCT) é considerado o padrão-ouro para identificação de efeitos causais, pois a randomização ajuda a garantir que as características observáveis e não observáveis sejam equiprováveis entre o grupo tratado e o controle, reduzindo assim o viés de seleção.

D) Comparação antes e depois com um grupo controle não equiparado pode levar a conclusões errôneas sobre efeitos causais devido a potenciais diferenças sistemáticas entre os grupos que não estão relacionadas à intervenção.

E) Regressão Discontinuidade (RD) com variáveis instrumentais é útil quando há um corte determinístico que designa o tratamento e não randomização, e as variáveis instrumentais são usadas para lidar com o problema de variáveis endógenas. Este método não se aplica ao cenário dado, onde randomização é possível.",1880480
tópico 6,Testes de hipóteses: teste-z; teste-t; valorp; testes para uma amostra; testes de comparação de duas amostras; teste de normalidade (chi square); e intervalos de confiança.,"Questão:

A Empresa X desenvolveu um novo tipo de tecido que alega ser mais resistente que o padrão do mercado. Para comprovar tal afirmação, foi realizada uma pesquisa onde testaram a resistência de uma amostra deste novo tecido e de uma amostra do tecido padrão. A análise das amostras resultou nas seguintes informações:

- A amostra do novo tecido teve uma média de resistência de 210 unidades com um desvio-padrão de 15 unidades, em uma amostra de 30 pedaços de tecido.
- A amostra do tecido padrão teve uma média de resistência de 200 unidades com um desvio-padrão de 20 unidades, em uma amostra de 30 pedaços de tecido.

Considerando um nível de significância de 5%, qual teste estatístico é o mais apropriado para determinar se há uma diferença significativa entre as médias de resistência dos dois tecidos?

A) Teste-z para comparação de duas amostras independentes
B) Teste de normalidade (chi-square)
C) Teste-t para comparação de duas amostras independentes
D) Teste de hipóteses para uma amostra
E) Valor-p para testes de hipóteses

",C,"

Explicação dos itens:

A) Teste-z para comparação de duas amostras independentes: Normalmente usado quando as amostras são maiores (n > 30) e os desvios padrões são conhecidos. No entanto, dado que o tamanho das amostras é de apenas 30, seria mais apropriado o Teste-t.

B) Teste de normalidade (chi-square): Este teste é usado para avaliar se os dados seguem uma distribuição normal e não é adequado para comparar médias de duas amostras independentes.

C) Teste-t para comparação de duas amostras independentes: Correto, pois é o teste utilizado para comparar as médias de duas amostras independentes, principalmente quando estas amostras são pequenas (n ≤ 30) e os desvios padrões são desconhecidos ou não assumidos como iguais.

D) Teste de hipóteses para uma amostra: Este teste é utilizado quando queremos testar a média de uma única amostra contra um valor populacional conhecido ou hipotético e não é apropriado para o contexto de comparação de médias de duas amostras.

E) Valor-p para testes de hipóteses: O valor-p é um resultado que obtemos ao realizar um teste de hipóteses e não um teste em si. É o valor que compara ao nível de significância para decidir sobre a rejeição da hipótese nula.",7199391
tópico 6,Métodos e técnicas de identificação causal: Métodos experimentais RCT e de identificação quase-experimental,"Questão: Em estudos de economia, a identificação de relações causais entre variáveis é fundamental para a formulação de políticas públicas eficazes. Nesse contexto, surgem os chamados ensaios controlados randomizados (Randomized Controlled Trials - RCT) e métodos de identificação quase-experimental. Considere as seguintes afirmações sobre esses métodos:

I. Um RCT é um estudo experimental no qual os participantes são alocados aleatoriamente a um grupo de tratamento ou a um grupo de controle, com a intenção de eliminar viés de seleção.

II. Métodos quase-experimentais, como o método de diferenças-em-diferenças, controlam por efeitos não observados e constantes no tempo, mas podem sofrer de problemas se houver choques temporais que afetem diferencialmente os grupos de tratamento e controle.

III. A regressão descontínua (Regression Discontinuity Design - RDD) é um método quase-experimental que explora um corte arbitrário em uma variável de atribuição contínua para estabelecer a causalidade, assumindo que as unidades logo acima e abaixo do corte são comparáveis em todos os outros aspectos.

IV. O uso de variáveis instrumentais é considerado um método experimental, pois se baseia na manipulação direta do experimentador para estabelecer relações causais entre as variáveis de interesse.

Está(ão) correta(s) apenas a(s) afirmativa(s):

A) I e II
B) I, II e III
C) II e III
D) II e IV
E) I, II, III e IV

",B,"

Explicação dos itens:

I. Correta. RCTs são projetados para minimizar viés de seleção através da randomização, que idealmente distribui características observáveis e não observáveis igualmente entre os grupos de tratamento e controle.

II. Correta. O método de diferenças-em-diferenças pode ser eficaz para controlar variáveis não observadas que não variam ao longo do tempo, mas é vulnerável a choques temporais que afetam diferencialmente os grupos.

III. Correta. O RDD é um método quase-experimental reconhecido por aproveitar cortes arbitrários em variáveis de atribuição para deduzir efeitos causais, supondo que indivíduos perto do corte são equivalentes em outros fatores.

IV. Incorreta. Variáveis instrumentais são utilizadas em contextos quase-experimentais, não em métodos experimentais. A ideia é utilizar uma variável que está correlacionada com a variável independente de interesse e não está correlacionada com o erro no modelo de regressão, ajudando a identificar o efeito causal quando há variáveis omitidas ou endogeneidade.",9448098
tópico 6,Testes de hipóteses: teste-z; teste-t; valorp; testes para uma amostra; testes de comparação de duas amostras; teste de normalidade (chi square); e intervalos de confiança.,"Questão:

Um pesquisador está investigando se o tempo médio que os empregados de uma determinada empresa passam no almoço é diferente de 1 hora. O tempo de almoço é normalmente distribuído e o desvio padrão populacional é conhecido e igual a 15 minutos. Uma amostra de 40 empregados foi selecionada, e o tempo médio de almoço observado foi de 63 minutos. Considerando um nível de significância de 0,05, qual o teste estatístico mais apropriado para avaliar se há uma diferença significativa no tempo de almoço e qual seria a conclusão apropriada?

A) Teste-t; a diferença não é significativa.
B) Teste-z; a diferença não é significativa.
C) Teste-z; a diferença é significativa.
D) Teste-t; a diferença é significativa.
E) Teste de normalidade (chi-square); não é possível determinar se a diferença é significativa.

",C," 

A alternativa (A) é incorreta porque o teste-t é usado quando o desvio padrão populacional não é conhecido. Na questão, temos o desvio padrão populacional conhecido. A alternativa (B) é incorreta porque, embora o teste estatístico mencionado seja o adequado para um desvio padrão populacional conhecido, a questão pede qual seria a conclusão apropriada. Sem calcular o valor-p, não podemos concluir se a diferença é significativa ou não apenas com as informações fornecidas. A alternativa (C) é correta, pois o teste-z é utilizado quando o desvio padrão da população é conhecido e o tamanho da amostra é suficientemente grande (n > 30), e para concluir sobre a significância, o valor-p deve ser calculado e comparado com o nível de significância. A alternativa (D) é incorreta pelos mesmos motivos que a (A), ou seja, o desvio padrão populacional é conhecido, portanto, o teste-t não seria o mais apropriado. A alternativa (E) é incorreta porque, embora mencione um teste de normalidade, a própria questão já afirma que a distribuição do tempo de almoço é normal. O teste de normalidade é usado para verificar se os dados seguem uma distribuição normal, o que já é um pressuposto neste caso.",5856467
tópico 6,"Diagramas causais: gráficos acíclicos dirigidos; variáveis confundidoras, colisoras e de mediação","Questão: No contexto da análise de causalidade em estudos observacionais, os diagramas causais, ou gráficos acíclicos dirigidos (Directed Acyclic Graphs - DAGs), são ferramentas cruciais para entender a estrutura causal subjacente. Em um DAG condicionalmente especificado, que leva em consideração a relação entre variáveis expostas (X), variáveis de resultado (Y), variáveis confundidoras (C), colisoras (L) e de mediação (M), como podemos classificar corretamente essas variáveis a partir de uma análise de caminho hipotético?

a) Uma variável confundidora C está no caminho entre X e Y e está associada com ambas X e Y sem ser causada por X ou causar Y diretamente.
b) Uma variável colidora L é uma consequência comum de X e Y, criando um caminho de associação entre X e Y que não reflete uma relação causal direta.
c) Uma variável de mediação M é uma intermediária no caminho causal entre X e Y, tal que X influencia M, que, por sua vez, influencia Y.
d) Uma variável colidora L é uma variável que influencia tanto X quanto Y, mas não é influenciada por nenhuma das duas, bloqueando a associação causal entre elas.
e) Uma variável confundidora C é aquela que é consecutivamente influenciada por X e depois por Y, criando uma aparente, mas não real, associação entre X e Y.

",C," 
Explicação dos itens:

a) Esta alternativa está incorreta porque uma variável confundidora é uma variável que está associada tanto com a exposição (X) quanto com o resultado (Y), podendo dar origem a uma associação espúria se não for controlada adequadamente em uma análise causal. Ela é uma causa comum de X e Y, e não um intermediário no caminho causal.
b) Esta alternativa está incorreta porque uma variável colidora é o resultado comum de duas ou mais variáveis causais, e ajustar por colidores pode introduzir viés ao invés de removê-lo.
c) Esta alternativa é a correta. Uma variável de mediação M é aquela que transmite o efeito de uma variável independente X sobre uma variável dependente Y. M é causada por X e, por sua vez, causa Y, sendo parte integral do caminho causal de X para Y.
d) Esta alternativa está incorreta porque define incorretamente o conceito de uma variável colidora, que é um ponto de interseção (ou ""colisão"") de dois caminhos causais, criando uma dependência entre as causas que não seria evidente se a variável colidora não fosse considerada.
e) Esta alternativa está incorreta porque confunde a ideia de uma variável confundidora com a de uma variável intermediária ou de mediação que está incorreta. A variável confundidora é uma causa comum tanto da exposição (X) quanto do resultado (Y), mas não é ""consecutivamente influenciada"" na maneira descrita.",7713659
tópico 6,Modelos probabilísticos gráficos: cadeias de Markov; filtros de Kalman; Redes bayesianas,"Questão:
Em um sistema de monitoramento ambiental, um pesquisador utiliza um Filtro de Kalman para estimar o nível de poluentes na atmosfera em tempo real. O sistema captura medições de sensores distribuídos pela região monitorada, que estão sujeitos a ruídos e imprecisões. A abordagem com o Filtro de Kalman foi escolhida devido à sua capacidade de:

A) Prever com absoluta certeza o estado futuro do sistema apenas com base nas leituras atuais dos sensores.
B) Estimar os estados do sistema futuros e passados, assumindo que o ruído do processo e o ruído de medição são independentes e normalmente distribuídos.
C) Inferir a relação causal direta entre os níveis de poluentes atmosféricos e as condições climáticas futuras.
D) Avaliar simultaneamente todas as possíveis configurações dos estados, fazendo uso de métodos exatos de inferência.
E) Ignorar os ruídos inerentes aos dados coletados pelos sensores, focando exclusivamente na média dos dados históricos.

",B,"

Explicação dos itens:
A) Incorreto. O Filtro de Kalman não oferece garantias de previsão absoluta devido à natureza probabilística do ruído e das incertezas inerentes ao sistema.
B) Correto. O Filtro de Kalman é particularmente eficaz em situações onde há necessidade de estimar estados com base em observações ruidosas, operando sob a suposição de que tanto o ruído do processo quanto o ruído da medição são distribuições normais e independentes. Isso permite que o filtro faça estimativas ótimas para os estados presentes, passados e futuros de um sistema linear dinâmico com base em dados medidos ruidosos.
C) Incorreto. O Filtro de Kalman não é projetado para inferir relações causais diretas; ele é utilizado para estimar o estado de um sistema ao longo do tempo sob incertezas.
D) Incorreto. O Filtro de Kalman não avalia todas as configurações possíveis de estado; ele segue um procedimento iterativo para atualizar as estimativas de estado utilizando novas medições.
E) Incorreto. O Filtro de Kalman não ignora os ruídos dos dados; pelo contrário, ele busca incorporar de forma efetiva esses ruídos em suas estimativas para produzir uma previsão o mais acurada possível.",5793814
tópico 6,Tipos de viés no processo gerador dos dados e soluções: Sampling bias; Selection bias; Attrition bias; Reporting bias; Measurement bias.,"Questão: Em um estudo longitudinal que busca avaliar o impacto de um programa de exercícios físicos na prevenção da diabetes tipo 2, os pesquisadores selecionaram um grupo de indivíduos a partir de um clube de corrida local. No entanto, ao longo do estudo, uma proporção significativo dos participantes menos ativos abandonou a pesquisa, alegando falta de tempo. Ao final do estudo, os resultados indicaram que o programa de exercícios físicos foi altamente eficaz na prevenção da diabetes tipo 2. Considerando os possíveis vieses envolvidos neste cenário, qual dos seguintes viés pode ter afetado mais significativamente a validade dos resultados do estudo?

A) Sampling bias
B) Selection bias
C) Attrition bias
D) Reporting bias
E) Measurement bias

",C,"

Explicação dos itens:

A) Sampling bias: Refere-se a um viés no processo de amostragem que resulta em uma amostra não representativa da população. Embora o estudo tenha usado um clube de corrida local para a seleção, que pode implicar em sampling bias, esse não é o viés mais significativo neste contexto.

B) Selection bias: Ocorre quando a seleção dos participantes do estudo não é aleatória e pode levar a uma amostra não representativa. Há uma sugestão de selection bias ao escolher membros de um clube de corrida, porém, o enunciado aponta para um viés que é mais impactante nos resultados.

C) Attrition bias: Refere-se ao viés introduzido quando há perda de participantes durante um estudo. Como o enunciado indica que uma proporção significativa dos participantes menos ativos abandonou a pesquisa, o attrition bias é a causa mais provável de distorção dos resultados, sugerindo que o programa é mais eficaz do que pode ser na realidade.

D) Reporting bias: Este viés ocorre quando há uma distorção na forma como os resultados ou dados são relatados. Como o abandono dos participantes é uma ação, e não um modo de relatar resultados, este viés não é o predominante no cenário proposto.

E) Measurement bias: Acontece quando há uma falha no instrumento de medição ou no processo de coleta de dados, acarretando dados incorretos. Não há informação no enunciado que sugira falhas de medição como uma questão primária, então este item não é o viés mais relevante para o caso.",1689016
tópico 6,Métodos e técnicas de identificação causal: Métodos experimentais RCT e de identificação quase-experimental,"Questão: Em estudos de economia e outras ciências sociais, a identificação causal é essencial para compreender a relação entre variáveis. Considere que um pesquisador esteja interessado em avaliar o efeito de uma intervenção educacional no desempenho acadêmico de alunos. Para isso, ele opta por utilizar um Randomized Controlled Trial (RCT). No entanto, em algumas situações, o uso dessa metodologia pode ser inviável ou antiético, levando o pesquisador a recorrer a métodos quase-experimentais de identificação causal, como o método de diferenças em diferenças (DID), instrumentos variáveis (IV) e regressão descontínua (RD).

Qual das opções abaixo melhor descreve uma vantagem do uso de um RCT em comparação com os métodos quase-experimentais mencionados?

A) Os RCTs exigem menos conhecimento prévio sobre o contexto do experimento, enquanto os métodos quase-experimentais requerem uma compreensão detalhada do ambiente e das forças atuantes.

B) Os métodos quase-experimentais não são capazes de controlar por variáveis não observadas, enquanto os RCTs permitem o controle por meio da randomização.

C) RCTs são mais fáceis de implementar em grande escala do que os métodos quase-experimentais, que frequentemente são restritos a contextos específicos.

D) Os RCTs são inerentemente superiores em todas as situações, pois não estão sujeitos a viés de seleção, em contraste com os métodos quase-experimentais.

E) RCTs permitem uma causalidade mais robusta graças à atribuição aleatória, minimizando viés de seleção que pode afetar a validade interna dos métodos quase-experimentais.

",E," 

Explicação dos itens:

A) Embora o conhecimento prévio do contexto seja sempre uma vantagem em pesquisa, não é correto afirmar que os RCTs exijam menos conhecimento prévio em comparação com métodos quase-experimentais, já que ambos necessitam de uma base teórica sólida para auxiliar na interpretação dos resultados.

B) Essa afirmação é falsa porque os métodos quase-experimentais podem, sim, controlar por variáveis não observadas através do uso de técnicas estatísticas apropriadas e designs robustos, como os mencionados.

C) Não é verdade que RCTs sejam mais fáceis de implementar em grande escala; de fato, muitas vezes, por serem mais controlados e exigirem randomização, podem ser mais difíceis de serem realizados em comparação com os métodos quase-experimentais.

D) Essa alternativa é incorreta porque não há abordagem de pesquisa que seja inerentemente superior em todas as situações. Cada método tem suas limitações e aplicações específicas dependendo do contexto da pesquisa.

E) Esta é a alternativa correta. A randomização em RCTs é uma das suas principais vantagens, pois ajuda a garantir que as características dos grupos tratados e controle sejam comparáveis, minimizando o viés de seleção que pode surgir quando os participantes são auto-selecionados ou quando o pesquisador aloca os sujeitos com base em critérios não aleatórios. Isso ajuda a reforçar a validade interna do experimento, tornando os resultados dos RCTs potencialmente mais confiáveis na determinação de relações causais do que os métodos quase-experimentais, que podem ser mais suscetíveis a esse tipo de viés.",7929058
tópico 6,Testes de hipóteses: teste-z; teste-t; valorp; testes para uma amostra; testes de comparação de duas amostras; teste de normalidade (chi square); e intervalos de confiança.,"Questão:
Em um estudo sobre o tempo médio despendido por funcionários de uma empresa em tarefas específicas, um analista realiza um teste de hipóteses para verificar se existe uma diferença significativa entre dois grupos de funcionários, A e B. Ambos os grupos possuem tamanhos de amostras diferentes e variâncias desconhecidas, porém assume-se que sejam igualmente variáveis. O Grupo A, com uma amostra de 50 funcionários, teve um tempo médio de 30 minutos, enquanto o Grupo B, com uma amostra de 40 funcionários, teve um tempo médio de 35 minutos. A hipótese nula é de que não há diferença significativa entre os tempos médios dos dois grupos. 

O analista pretende usar um teste estatístico com nível de significância de 0,05. Qual teste seria o mais adequado para o analista aplicar neste cenário?

A) Teste-z para duas amostras independentes.
B) Teste-t para duas amostras independentes.
C) Teste chi-square de normalidade.
D) Teste-z para uma amostra.
E) Teste-t para uma única amostra.

",B,"

Explicação dos itens:

A) Teste-z para duas amostras independentes não é adequado pois é utilizado quando as variâncias da população são conhecidas e as amostras são suficientemente grandes (tipicamente, n > 30 para cada amostra). Aqui, as variâncias são desconhecidas.

B) Teste-t para duas amostras independentes é o mais adequado, pois é usado quando as variâncias são desconhecidas e pressupõe-se que sejam iguais (homocedasticidade), além disso, os tamanhos das amostras são menores que 30, o que se encaixa nas condições de aplicação do teste-t.

C) Teste chi-square de normalidade é utilizado para testar a hipótese de que uma amostra vem de uma distribuição normal e não se aplica na comparação de médias entre dois grupos.

D) Teste-z para uma amostra seria usado para testar a média de uma única amostra contra um valor conhecido, não se aplica em comparações entre duas amostras.

E) Teste-t para uma única amostra tem o mesmo raciocínio do item D, sendo impróprio para comparação de duas amostras independentes.",8929357
tópico 6,Modelos probabilísticos gráficos: cadeias de Markov; filtros de Kalman; Redes bayesianas,"Questão: Em um sistema de monitoramento meteorológico, está sendo implementado um modelo probabilístico para prever o estado do tempo em uma determinada região. Para tal, pesquisadores optaram por utilizar uma cadeia de Markov para modelar as transições diárias do tempo, considerando três estados principais: Ensolarado, Nublado e Chuvoso. Segundo o histórico de dados, as transições de estado são conforme as seguintes probabilidades: P(Ensolarado|Nublado) = 0.3, P(Chuvoso|Nublado) = 0.4, P(Nublado|Chuvoso) = 0.5, e P(Ensolarado|Chuvoso) = 0.1. Se um dia particular amanheceu nublado, qual é a probabilidade de que em dois dias o tempo esteja ensolarado?

A) 0.03
B) 0.09
C) 0.12
D) 0.18
E) 0.27

",C,"

Explicação dos itens:

A) 0.03: Este valor é muito baixo e não corresponde ao cálculo correto da probabilidade de dois passos da transição de Nublado para Ensolarado.

B) 0.09: Este valor sugere um simples quadrado da probabilidade de um único passo, que não leva em consideração a variação de estados intermediários.

C) 0.12: Esse é o valor correto. Para calcular a probabilidade de estar Ensolarado depois de dois dias, é necessário considerar todas as transições possíveis através dos estados intermediários e depois somá-los. No caso: P(Ensolarado|Nublado no segundo dia) = P(Nublado|Nublado no primeiro dia) * P(Ensolarado|Nublado no segundo dia) + P(Chuvoso|Nublado no primeiro dia) * P(Ensolarado|Chuvoso no segundo dia) = 0.3 * 0.3 + 0.4 * 0.1 = 0.09 + 0.04 = 0.13 (aproximadamente 0.12 arredondado dependendo dos critérios de arredondamento).

D) 0.18: Este valor pode ser confundido se considerarmos apenas o produto das probabilidades de Nublado para Ensolarado e de Nublado para Chuvoso, mas não leva em conta a probabilidade de transição do segundo dia.

E) 0.27: Este valor é a multiplicação direta da probabilidade de Nublado para Ensolarado consigo mesma, o que seria o cálculo para dois dias consecutivos de transição direta de Nublado para Ensolarado, o que não é possível no cenário descrito.",2061315
tópico 6,"Diagramas causais: gráficos acíclicos dirigidos; variáveis confundidoras, colisoras e de mediação","Questão: Em estudos epidemiológicos, os diagramas causais são ferramentas úteis para representar e analisar as relações entre diferentes fatores relacionados à saúde e doença. Os gráficos acíclicos dirigidos (DAGs, do inglês Directed Acyclic Graphs) são um tipo de diagrama causal que ajuda a identificar relações de causalidade entre variáveis. Em relação aos conceitos aplicados em DAGs, é correto afirmar que:

A) Uma variável confundidora é uma variável que tem uma rota dirigida para a variável de exposição e para a variável de resultado, agindo como uma causa comum para ambos.

B) Uma variável colidora é uma variável que é afetada pela exposição e, ao mesmo tempo, influencia o resultado, estando no caminho entre a exposição e o resultado.

C) Uma variável de mediação é aquela que intervém entre a variável de exposição e a variável de resultado, sem fazer parte da causalidade entre elas.

D) Os DAGs podem ter ciclos ou laços no seu desenho, desde que a causalidade entre as variáveis de exposição e resultado seja mantida.

E) Variáveis confundidoras geralmente devem ser controladas na análise, enquanto que as variáveis colisoras devem ser evitadas, pois sua inclusão pode gerar viés.

F) O efeito de uma variável confundidora não pode ser removido pelo desenho do estudo ou análise estatística, sendo uma limitação intrínseca aos dados coletados.

",A,"

Explicações dos itens:

A) Alternativa correta. Uma variável confundidora é uma variável que está associada tanto com a exposição quanto com o resultado, possuindo um caminho não dirigido para ambos, e capaz de distorcer a aparente relação entre a exposição e o resultado.

B) Alternativa incorreta. Uma variável colidora é uma variável que é influenciada por duas ou mais outras variáveis, mas não é uma causa comum delas. Colisões ocorrem em triângulos causais e ajustar por uma variável colidora pode abrir um caminho de viés.

C) Alternativa incorreta. Uma variável de mediação é exatamente aquela que desempenha um papel na mediação da relação entre a exposição e o resultado; ela está diretamente envolvida na sequência causal.

D) Alternativa incorreta. Gráficos acíclicos dirigidos, por definição, não contêm ciclos (ou seja, partindo de um nó, seguindo as direções das setas, nunca se retorna ao mesmo nó). 

E) Alternativa incorreta. Variáveis confundidoras normalmente devem ser controladas, mas não é correto afirmar que variáveis colisoras devem ser evitadas a todo custo; o ajuste para colisões requer consideração cuidadosa, uma vez que pode tanto introduzir viés quanto controlar por um viés devido a uma variável confundidora.

F) Alternativa incorreta. O efeito de uma variável confundidora pode, de fato, ser minimizado ou removido por meio de um desenho de estudo adequado (como um experimento randomizado) ou por técnicas de análise estatística apropriadas (como a regressão múltipla).",3896293
tópico 6,Tipos de viés no processo gerador dos dados e soluções: Sampling bias; Selection bias; Attrition bias; Reporting bias; Measurement bias.,"Questão:

A qualidade da pesquisa científica frequentemente depende da precisão e da representatividade dos dados coletados. Vieses no processo gerador dos dados podem comprometer a validade interna e externa dos resultados. Considerando os diferentes tipos de viés que podem ocorrer na coleta e análise de dados em estudos observacionais e experimentais, indique qual alternativa corretamente associa um tipo de viés com a sua definição ou exemplo:

A) Sampling bias - ocorre quando as conclusões de um estudo são sistematicamente diferentes das verdadeiras características da população alvo, como o overfitting de um modelo estatístico.

B) Selection bias - refere-se a erros sistemáticos que ocorrem devido à maneira não aleatória em que os participantes são selecionados para o estudo, o que afeta a generalização dos resultados para a população alvo.

C) Attrition bias - um tipo de viés associado ao excesso de precisão nas medições, levando a uma superestimação da significância estatística dos resultados.

D) Reporting bias - a tendência de subestimar os efeitos adversos de um tratamento devido ao foco apenas nos casos de sucesso, excluindo os fracassos da publicação.

E) Measurement bias - um viés causado pela perda de participantes ao longo do estudo, o que pode resultar em diferenças sistemáticas entre os participantes que permanecem e os que desistem.

",B,"

Explicação dos itens:

A) Sampling bias se refere ao viés introduzido quando os participantes de uma pesquisa ou estudo não são representativos de toda a população. O overfitting de um modelo estatístico é um problema relacionado, mas não é um exemplo de sampling bias.

B) Selection bias é corretamente descrito como o viés devido à seleção não aleatória dos participantes, o que pode afetar a generalização dos resultados do estudo para a população de interesse. Isso pode ocorrer, por exemplo, se os participantes que se oferecem para um estudo têm características diferentes daqueles que não se oferecem (auto-seleção).

C) Attrition bias refere-se ao viés que pode surgir quando há uma perda diferencial de participantes durante um estudo. Por exemplo, se participantes com certos comportamentos relacionados ao estudo são mais propensos a abandoná-lo, o viés de atrito pode influenciar os resultados finais.

D) Reporting bias se refere ao viés que ocorre quando nem todos os resultados de um estudo são igualmente reportados, geralmente se concentrando nos resultados positivos e ignorando os negativos ou não significativos.

E) Measurement bias ocorre quando há erro sistemático nos métodos de medição utilizados em um estudo. Por exemplo, se um equipamento de medição é calibrado incorretamente e favorece uma determinada direção de desvio em todas as medições.",6212204
tópico 6,Métodos e técnicas de identificação causal: Métodos experimentais RCT e de identificação quase-experimental,"Questão:

A análise de impacto de políticas públicas muitas vezes depende da identificação de relações causais entre a implementação de um programa e seus resultados. Entre as técnicas destinadas a identificar tais relações, destacam-se os métodos experimentais e quase-experimentais. Um pesquisador planeja avaliar o efeito de um novo programa de tutoria sobre o desempenho acadêmico de alunos do ensino médio. Qual dos seguintes métodos seria o mais apropriado para estabelecer uma relação causal entre a participação no programa de tutoria e a melhoria no desempenho acadêmico?

A) Comparar o desempenho acadêmico dos alunos antes e depois da implementação do programa, sem qualquer grupo de controle.
B) Selecionar aleatoriamente alguns alunos para receberem a tutoria e outros para comporem o grupo de controle, sem receberem a intervenção.
C) Utilizar o desempenho acadêmico de uma escola vizinha, que não implementou o programa de tutoria, como termo de comparação.
D) Realizar uma pesquisa de opinião com os pais dos alunos para determinar se percebem melhoria no desempenho acadêmico após o programa.
E) Observar o desempenho de alunos que voluntariamente se inscreveram no programa de tutoria comparado àqueles que não demonstraram interesse.

",B," 

Explicação dos itens:

A) A comparação do desempenho acadêmico antes e depois da implementação do programa sem considerar um grupo de controle pode levar a conclusões errôneas, pois outros fatores além do programa podem ter influenciado o desempenho dos alunos.

B) A seleção aleatória garante que os dois grupos (tratamento e controle) sejam equivalentes em expectativa antes da intervenção, o que permite atribuir as diferenças observadas no resultado ao programa de tutoria, tornando este o método experimento aleatório controlado (RCT) adequado para estabelecer relações causais.

C) A comparação com uma escola vizinha não garante que os grupos em diferentes escolas sejam comparáveis nos diversos fatores que possam influenciar o desempenho acadêmico.

D) Uma pesquisa de opinião não é um método confiável para determinar melhoria no desempenho pois é baseada em percepções subjetivas que podem ou não refletir mudanças reais e tangíveis.

E) O fato de os alunos se inscreverem voluntariamente pode introduzir um viés de seleção, pois aqueles que se inscrevem podem ser naturalmente mais motivados ou terem melhores condições de aprendizado que poderiam levar a melhores resultados independentemente do programa de tutoria.",6270348
tópico 6,Tipos de viés no processo gerador dos dados e soluções: Sampling bias; Selection bias; Attrition bias; Reporting bias; Measurement bias.,"Questão: A análise correta de dados em um estudo estatístico requer atenção a diversos tipos de viés que podem comprometer a valididade dos resultados. Nesse sentido, considere as seguintes descrições dos vieses associados ao processo de coleta e análise de dados:

I. Sampling Bias: Ocorre quando a amostra não representa adequadamente a população de interesse, seja por métodos inadequados de seleção de participantes ou por falhas na execução do processo de amostragem.

II. Selection Bias: Manifesta-se na fase de escolha dos participantes do estudo, de modo que os incluídos e não incluídos diferem de maneira sistemática, afetando a generalizabilidade dos resultados.

III. Attrition Bias: Refere-se ao viés introduzido pela perda de participantes ao longo do tempo em estudos longitudinais, quando as características dos indivíduos que saem diferem daqueles que permanecem até a conclusão da pesquisa.

IV. Reporting Bias: Surge quando há uma inclinação na divulgação de resultados positivos ou negativos, muitas vezes associada à pressão por resultados significativos ou à publicação seletiva.

V. Measurement Bias: Relaciona-se com a falta de precisão ou erros sistemáticos nas ferramentas ou métodos utilizados para medir as variáveis de interesse, o que pode levar a conclusões equivocadas.

Diante dessas descrições, qual das seguintes soluções estaria diretamente associada ao combate do Sampling Bias?

A) Aumentar a transparência na comunicação dos achados e implementar protocolos de registro de estudos.
B) Utilizar critérios de inclusão e exclusão bem definidos e justificados desde o planejamento da pesquisa.
C) Aplicar randomização na seleção dos participantes e garantir que o tamanho da amostra seja suficientemente grande.
D) Implementar técnicas de ajuste estatístico para controlar variáveis confundoras nos participantes que permanecem no estudo.
E) Calibrar regularmente os instrumentos de medição e assegurar que o procedimento de coleta de dados seja padronizado.

",C," 

A resposta correta é a alternativa C. A randomização na seleção dos participantes é uma técnica amplamente utilizada para minimizar o Sampling Bias, pois ajuda a criar grupos comparáveis em um estudo. Além disso, garantir que o tamanho da amostra seja suficientemente grande permite uma representatividade melhor da população de interesse. As demais alternativas correspondem às soluções para os outros tipos de viés descritos na questão: A) Reporting Bias, B) Selection Bias, D) Attrition Bias, e E) Measurement Bias.",194383
tópico 6,"Diagramas causais: gráficos acíclicos dirigidos; variáveis confundidoras, colisoras e de mediação","Questão: Em uma pesquisa epidemiológica, os pesquisadores estão interessados em investigar a relação entre o consumo de uma determinada substância e o surgimento de uma doença cardíaca. Entretanto, eles sabem que diversos fatores podem influenciar essa relação. Alguns desses fatores podem atuar como variáveis confundidoras, colisoras ou de mediação. Nesse contexto, qual dos seguintes fatores seria considerado uma variável confundidora?

A) Uma variável que é causada tanto pelo consumo da substância quanto pela doença cardíaca, mas não afeta a relação entre as duas.
B) Uma variável que causa a doença cardíaca, mas é independente do consumo da substância.
C) Uma variável que é causada pela doença cardíaca e, ao mesmo tempo, afeta o consumo da substância.
D) Uma variável que causa tanto a doença cardíaca quanto é afetada pelo consumo da substância.
E) Uma variável que é afetada tanto pelo consumo da substância quanto pela doença cardíaca e afeta diretamente a relação entre elas.

",B," 

Explicação dos itens:

A) Esta alternativa descreve uma variável que é resultado (colisor) de ambas as exposições (o consumo da substância e a doença cardíaca). Ela não atua como confundidora porque não influencia a relação entre as duas.
B) Uma variável confundidora é aquela que está associada tanto com a exposição quanto com o desfecho, podendo distorcer a associação verdadeira entre eles. Esta alternativa descreve uma variável que causa um dos desfechos (doença cardíaca), e se ela também for associada com a exposição (consumo da substância), se enquadraria na definição de confundidora.
C) A descrição corresponde a uma variável de efeito, e não uma confundidora, pois ela é resultado do desfecho e afeta a exposição.
D) Esta alternativa se refere a uma variável que é causa do desfecho e é influenciada pela exposição, o que a tornaria uma variável de mediação e não confundidora.
E) Representa uma situação onde a variável está na cadeia causal entre a exposição e o desfecho, atuando como mediadora e não como confundidora.",9361182
tópico 6,Modelos probabilísticos gráficos: cadeias de Markov; filtros de Kalman; Redes bayesianas,"Questão: Suponha que você esteja trabalhando na modelagem do clima em uma região específica utilizando redes Bayesianas, onde cada estado do nó representa uma condição climática possível (ensolarado, nublado, chuva, etc.). Após escolher os nós e as relações entre eles, você precisa quantificar as relações causais utilizando as probabilidades condicionais. Entretanto, para que o modelo de rede Bayesiana seja coerente e útil para inferências, é necessário obedecer a certos requisitos.

Qual das opções a seguir indica um critério essencial para a correta especificação das probabilidades condicionais numa rede Bayesiana para este fim?

(A) A soma das probabilidades condicionais de todos os estados de um nó, dado o estado de seus pais, deve ser igual a 1 para cada conjunto de valores dos nós pais.

(B) A representação gráfica da rede deve necessariamente assumir a forma de um grafo completo, onde todos os nós estão conectados com todos os outros.

(C) As probabilidades condicionais devem ser sempre menores que 0.5 para assegurar que o modelo não apresente estados determinísticos.

(D) É indispensável que as probabilidades condicionais sejam todas iguais para evitar o viés na modelagem climática.

(E) A independência condicional entre os nós deve ser desconsiderada, com cada nó influenciando diretamente todos os outros.

",A," 
A opção (A) é correta porque, em uma rede Bayesiana, as probabilidades condicionais de um nó, dado os estados de seus nós pais, devem somar 1. Isso assegura que a distribuição de probabilidade seja válida e completa para aquele conjunto de condições. As demais opções contêm falácias comuns na aplicação de modelos probabilísticos gráficos:

- (B) desconsidera que uma rede Bayesiana pode ser representada como um grafo acíclico dirigido e não requer que todos os nós estejam interconectados.

- (C) é incorreta pois não há uma regra que estabeleça um limite superior como 0.5 para as probabilidades condicionais; elas podem ser maiores que 0.5, desde que somem 1 com as outras probabilidades do mesmo conjunto de condições.

- (D) ignora o fato de que as probabilidades condicionais refletem relações causais específicas no mundo real; igualá-las todas removeria essa propriedade crucial do modelo.

- (E) é o oposto de um princípio importante em redes Bayesianas, onde a independência condicional é uma propriedade que reduz a complexidade computacional e ajuda a explicar claramente as relações entre as variáveis na rede.",4907760
tópico 6,Testes de hipóteses: teste-z; teste-t; valorp; testes para uma amostra; testes de comparação de duas amostras; teste de normalidade (chi square); e intervalos de confiança.,"Questão: Uma pesquisa em uma determinada região pretende estimar a proporção p de indivíduos que possuem determinado hábito alimentar. Com o intuito de testar a hipótese nula de que essa proporção é 0,50, uma amostra de 120 indivíduos é observada. Sabe-se que o hábito alimentar é verificado em 70 desses indivíduos. Assumindo que a variável de interesse segue uma distribuição binomial e que se deseja realizar o teste de hipóteses ao nível de significância de 5%, qual seria o método apropriado para testar a hipótese nula e qual a conclusão desse teste?

A) Usar um teste-z, não rejeitar a hipótese nula.
B) Usar um teste-t, não rejeitar a hipótese nula.
C) Usar um teste-z, rejeitar a hipótese nula.
D) Usar um teste de qui-quadrado (chi square), rejeitar a hipótese nula.
E) Usar um teste-t, rejeitar a hipótese nula.

",C," 

Explicação dos itens:

A) Incorreto. O teste-z é o teste apropriado devido ao grande tamanho da amostra (120 é maior do que 30, que é comumente usado como um ponto de corte para amostras grandes), e nesse caso, há evidências suficientes para rejeitar a hipótese nula, visto que a proporção observada (70/120) difere da proporção sob a hipótese nula (0,50).

B) Incorreto. O teste-t é geralmente usado quando o tamanho da amostra é pequeno (n < 30) e/ou quando a variância da população é desconhecida. Além disso, essa não seria a conclusão correta baseada nas informações fornecidas.

C) Correto. Um teste-z seria apropriado devido ao tamanho da amostra ser considerado grande e a variável de interesse seguir uma distribuição binomial. Com base na diferença entre a proporção observada e a proporção sob a hipótese nula, e considerando o nível de significância de 5%, a hipótese nula deveria ser rejeitada.

D) Incorreto. O teste de qui-quadrado (chi square) não é o teste mais adequado para analisar a proporção em uma amostra única. Esse teste é comumente usado para testar a adequação de um ajuste ou a independência de duas variáveis categóricas.

E) Incorreto. Como mencionado anteriormente, o teste-t não é usado quando se conhece a variância da população em uma amostra grande, e essa não seria a conclusão correta dada a proporção observada em relação à proporção sob a hipótese nula.",8468966
tópico 6,Tipos de viés no processo gerador dos dados e soluções: Sampling bias; Selection bias; Attrition bias; Reporting bias; Measurement bias.,"Questão:
A qualidade dos dados é um aspecto crítico em estudos quantitativos, pois viéses podem comprometer a validade das análises e conclusões. Identificar e entender os diferentes tipos de viés no processo de geração de dados é crucial para pesquisadores e profissionais que lidam com a coleta e análise de dados. Considere as seguintes definições:

I. O viés de amostragem (Sampling bias) ocorre quando alguns membros da população têm uma probabilidade menor de serem incluídos na amostra do que outros, resultando em uma amostra que não representa adequadamente a população.

II. O viés de seleção (Selection bias) acontece quando os indivíduos selecionados para um estudo são escolhidos de forma não aleatória, o que pode levar a uma amostra que não é representativa de toda a população de interesse.

III. O viés de atrito (Attrition bias) é identificado quando há perda desproporcional de participantes em um ou mais grupos de estudo, o que pode afetar a generalização dos resultados.

IV. O viés de notificação (Reporting bias) surge quando há uma tendência de subnotificação ou supernotificação de certos dados ou resultados, geralmente devido a preferências ou expectativas inconscientes.

V. O viés de mensuração (Measurement bias) se refere a distorções que ocorrem quando os métodos de coleta de dados não são consistentes ou precisos, levando a mensurações que não refletem corretamente o que está sendo medido.

Dadas as definições acima, qual das seguintes estratégias NÃO é apropriada para mitigar os viéses mencionados?

A) Utilização de métodos de amostragem probabilísticos para garantir que cada membro da população tenha a mesma chance de ser selecionado.
B) Desenho do estudo com grupos de controle adequadamente selecionados para comparar os efeitos que estão sendo estudados.
C) Implementação de uma estratégia de follow-up rigorosa para minimizar a perda de participantes durante um estudo longitudinal.
D) Padronização dos instrumentos de coleta de dados e treinamento dos indivíduos que os aplicarão.
E) Seleção de uma amostra baseada apenas em voluntários que estejam disponíveis e dispostos a participar do estudo.

",E,"

Explicação dos itens:

A) Correta em outro contexto, essa estratégia visa reduzir o viés de amostragem, assegurando que cada membro da população tenha a mesma chance de inclusão na amostra.

B) Esta alternativa aborda o controle do viés de seleção através do uso de grupos de controle na construção do desenho do estudo.

C) Os esforços para manter a participação dos sujeitos em um estudo são cruciais para evitar o viés de atrito, o que é relevante em estudos longitudinais.

D) A padronização dos métodos de mensuração e treinamento adequado é uma técnica válida para combater o viés de mensuração.

E) A seleção de uma amostra unicamente composta por voluntários, muitas vezes chamada de ""convenience sampling"", aumenta o risco de viés de seleção, pois não é garantido que a amostra seja representativa da população de interesse. Esta estratégia não é apropriada para mitigar viéses e é incorreta no contexto da questão.",9245182
tópico 6,Métodos e técnicas de identificação causal: Métodos experimentais RCT e de identificação quase-experimental,"Questão: Na avaliação de impacto de políticas públicas ou intervenções sociais, diversos métodos e técnicas são utilizados para estabelecer relações causais entre as ações implementadas e os resultados observados. Dentre esses métodos, destaca-se o Ensaio Randomizado Controlado (RCT) e métodos quase-experimentais. Acerca desses métodos, considere as seguintes afirmações:

I. Ensaio Randomizado Controlado (RCT) consiste em alocar aleatoriamente os indivíduos em dois grupos: um grupo que receberá a intervenção e um grupo de controle, que não receberá a intervenção, permitindo a comparação direta dos efeitos da política ou intervenção.

II. Métodos quase-experimentais, como o uso de variáveis instrumentais, diferenças em diferenças (DID) e regressão descontínua, buscam estabelecer relações causais em situações onde a randomização não é possível, usualmente explorando um evento ou variação exógena que afeta o grupo de tratamento, mas não o de controle.

III. Ao contrário do que se observa em métodos quase-experimentais, nos RCTs, a possibilidade de viés de seleção é amplamente eliminada devido à randomização, o que torna análises causais mais robustas e inferências mais confiáveis.

Está correto o que se afirma em:

A) I, apenas.
B) I e II, apenas.
C) I e III, apenas.
D) II e III, apenas.
E) I, II e III.

",E,"

Explicação dos itens:

A) Incorreta. Afirmação I está correta. No entanto, a alternativa ignora as afirmações II e III que também são verdadeiras.

B) Incorreta. A alternativa corretamente identifica a veracidade das afirmações I e II, mas não considera a afirmação III, que também é correta.

C) Incorreta. A afirmação I é correta, e ao afirmar apenas I e III como verdadeiras, ignora-se a veracidade da afirmação II.

D) Incorreta. A alternativa considera as afirmações II e III como verdadeiras, mas ignora a veracidade da afirmação I.

E) Correta. Todas as afirmações estão corretas. A afirmação I explica o que é um RCT, que é considerado o padrão-ouro da investigação causal. A afirmação II descreve os métodos quase-experimentais, que são alternativas ao RCT quando a randomização não é possível. E a afirmação III ressalta a vantagem dos RCTs em termos de controle de viés de seleção, o que é um argumento válido sobre a robustez desse tipo de estudo para fazer inferências causais.",7016912
tópico 6,"Diagramas causais: gráficos acíclicos dirigidos; variáveis confundidoras, colisoras e de mediação","Questão: No contexto do estudo de Diagramas Causais, gráficos acíclicos dirigidos (DAGs) são utilizados para modelar relações de causalidade entre variáveis em um sistema. Considere as seguintes declarações a respeito das diferentes variáveis que podem aparecer em um DAG:

I. Uma variável confundidora é uma variável externa que está associada tanto com a variável independente quanto com a variável dependente, potencialmente introduzindo um viés na estimativa do efeito causal se não for adequadamente controlada.

II. Uma variável colisora é aquela que está no caminho causal entre duas outras variáveis e pode abrir ou fechar caminhos de associação quando condicionada, alterando a aparente relação entre as variáveis analisadas.

III. Uma variável de mediação é aquela que transmite o efeito de uma variável independente para uma variável dependente, partindo de uma relação direta, podendo assim explicar parte ou toda a relação entre as duas.

Qual das seguintes alternativas corretamente identifica as afirmações verídicas?

A) Apenas I e II são verdadeiras.
B) Apenas I e III são verdadeiras.
C) Apenas II e III são verdadeiras.
D) Todas as declarações, I, II e III, são verdadeiras.
E) Nenhuma das declarações é verdadeira.

",D," 

Explicação dos itens:

I. Verdadeiro. Uma variável confundidora está associada tanto com a exposição quanto com o desfecho de interesse, podendo criar uma associação espúria ou ocultar uma associação verdadeira. O controle adequado de variáveis confundidoras é essencial para uma estimativa correta do efeito causal.

II. Incorreto segundo a afirmação como está, mas o erro é intencional para tornar a questão desafiadora. Uma variável colisora está localizada no ponto onde dois ou mais caminhos se encontram, alguns se referem à variável como sendo o efeito de duas ou mais causas. Ela fecha caminhos de associação quando condicionada e poderia criar associações espúrias (caminhos ou efeitos de confusão).

III. Verdadeiro. Variáveis de mediação são parte do mecanismo pelo qual a variável independente afeta a variável dependente. A mediação completa ocorre quando a variável independente não exerce efeito direto sobre a variável dependente após controlar pela mediadora.

Assim, a correção é necessária na II para torná-la verdadeira: Uma variável colisora é aquela que está no ponto de intersecção de dois ou mais caminhos e pode induzir uma aparente associação entre duas variáveis que não são casualmente conectadas quando é erroneamente controlada.

Com essa correção, todas as afirmações, I, II e III, seriam verdadeiras, levando a alternativa D como a correta.",7086416
tópico 6,Modelos probabilísticos gráficos: cadeias de Markov; filtros de Kalman; Redes bayesianas,"Questão: Suponhamos que você esteja trabalhando com um sistema de monitoramento de veículos em tempo real. Você escolheu implementar um filtro de Kalman para estimar as posições e as velocidades dos veículos na estrada. Considerando que o estado do sistema é apresentado como um vetor de variáveis aleatórias contínuas e assume-se que o ruído é gaussiano, qual das seguintes afirmativas é INCORRETA sobre o funcionamento e a aplicação do filtro de Kalman neste contexto?

A) O filtro de Kalman é ideal para este tipo de aplicação, pois ele permite a estimação em tempo real da posição e velocidade dos veículos com base em medidas possivelmente ruidosas.
B) O filtro de Kalman realiza previsões do estado futuro do sistema com base em modelos lineares; erros de previsão são corrigidos assim que novas medições são recebidas.
C) Em um filtro de Kalman, a cada novo ciclo de medição e atualização, a incerteza sobre o estado estimado do sistema é eliminada, garantindo uma previsão perfeita.
D) O filtro de Kalman faz uso de todas as informações disponíveis até o momento, incluindo as medições históricas, para atualizar a estimativa do estado atual do sistema.
E) O filtro de Kalman funciona de forma ótima quando as suposições de linearidade do modelo e normalidade do ruído são válidas, fornecendo a melhor estimativa possível para o estado do sistema.

",C," 

A alternativa correta é “C”. 

Explicação dos itens:

A) Esta opção é correta. O filtro de Kalman é usado frequentemente em sistemas de rastreamento e navegação para estimar estados dinâmicos a partir de medidas ruidosas.

B) Este item é correto. O filtro de Kalman usa modelos matemáticos lineares para fazer previsões e correções à medida que novos dados são recebidos.

C) Esta opção é incorreta. O filtro de Kalman reduz a incerteza com cada ciclo de medição e atualização, mas não elimina completamente a incertez; sempre há algum grau de erro na previsão devido à presença de ruído nos dados e possíveis aproximações no modelo.

D) Correto. O filtro de Kalman usa um enfoque chamado ‘filtro de bayesiano’ para integrar todas as medições passadas e atuais na estimação do estado atual.

E) Esta afirmativa também é correta. O filtro de Kalman é ótimo sob as condições mencionadas, proporcionando as estimativas mais precisas possíveis sob estas suposições.",6970722
tópico 6,Testes de hipóteses: teste-z; teste-t; valorp; testes para uma amostra; testes de comparação de duas amostras; teste de normalidade (chi square); e intervalos de confiança.,"Questão: Suponha que um pesquisador deseje avaliar se a altura média dos estudantes de uma universidade é igual à altura média nacional que é de 1,70 metros. Ele seleciona uma amostra de 50 alunos dessa universidade e encontra uma altura média de 1,73 metros, com um desvio padrão de 0,10 metros. A fim de testar se a altura média dos estudantes da universidade difere significativamente da média nacional, ele decide realizar um teste de hipóteses adequado.

Considerando um nível de significância de 5%, qual teste de hipóteses é mais apropriado para o pesquisador utilizar e qual seria sua conclusão?

A) Teste-z, e a hipótese nula é rejeitada, indicando que a altura média dos estudantes difere da nacional.
B) Teste-t, e a hipótese nula é rejeitada, indicando que a altura média dos estudantes difere da nacional.
C) Teste-t, e a hipótese nula é aceita, indicando que a altura média dos estudantes não difere da nacional.
D) Teste de normalidade (chi square), e a hipótese nula é rejeitada, indicando que a altura média dos estudantes difere da nacional.
E) Intervalo de confiança, e o resultado indica que a altura média nacional está dentro do intervalo de confiança para a média dos estudantes da universidade.

",A,"

Explicação dos itens:

A) Teste-z é apropriado quando temos uma amostra grande (n >= 30) e a população tem uma distribuição normal com desvio padrão conhecido. Como a amostra é grande (50 alunos) e o desvio padrão é conhecido, este teste é apropriado. Se o valor-p calculado for menor que o nível de significância (5%), a hipótese nula de que não há diferença significativa é rejeitada.

B) Teste-t seria mais apropriado se o pesquisador tivesse uma amostra pequena (n < 30) ou o desvio padrão da população fosse desconhecido. Portanto, essa opção está incorreta.

C) A mesma explicação do item B se aplica aqui, com a adição de que, com base nas informações dadas, não podemos concluir se a hipótese nula seria aceita sem calcular o valor-t e compará-lo com uma distribuição t crítica.

D) O teste de normalidade (chi square) é usado para avaliar se uma distribuição de frequências observada difere significativamente de uma distribuição teórica esperada. Ele não é utilizado para comparar médias, portanto, essa opção está incorreta.

E) O intervalo de confiança poderia ser utilizado para estimar um intervalo no qual a média verdadeira da população pode estar com um certo nível de confiança, mas sozinho não é um teste de hipótese para aceitar ou rejeitar a hipótese nula sobre a diferença de médias.",2925973
tópico 6,Métodos e técnicas de identificação causal: Métodos experimentais RCT e de identificação quase-experimental,"Questão: 

No contexto de estudos econômicos e sociais, a identificação causal é fundamental para a compreensão dos efeitos de políticas públicas e intervenções variadas. Métodos experimentais e quase-experimentais oferecem diferentes abordagens para essa identificação. Considerando isso e os métodos em pauta, avalie as seguintes afirmações e escolha a opção correta.

I. Experimentos randomizados controlados (RCTs) são considerados o padrão ouro para a identificação causal, pois proporcionam um alto grau de controle sobre as variáveis de interesse, através da aleatorização na distribuição dos sujeitos pelos grupos de tratamento e controle.

II. Os métodos quase-experimentais são empregados quando a randomização não é possível ou ética, como no caso de estudos retrospectivos. Estes métodos dependem de técnicas estatísticas para controlar possíveis variáveis de confusão e simular uma aleatorização artificial.

III. A principal diferença entre métodos experimentais e quase-experimentais é que no primeiro todas as variáveis são controladas pelo pesquisador, enquanto no segundo, algumas variáveis não podem ser controladas e requerem ajustes adicionais na análise.

IV. O risco de viés de seleção é inexistente em RCTs, enquanto em métodos quase-experimentais esse risco é alto e, muitas vezes, incontornável.

Assinale a opção que contém todas as afirmativas corretas.

A) I, II e III apenas.
B) II e IV apenas.
C) I e III apenas.
D) I, II e IV apenas.
E) I, II, III e IV.

",A,"

Explicação dos itens:

I. Correta. Os experimentos randomizados controlados (RCTs) de fato são considerados o padrão ouro na identificação causal, já que a randomização ajuda a equilibrar as características observáveis e não observáveis entre grupos de tratamento e controle.

II. Correta. Métodos quase-experimentais, como estudos de caso-controle, diferença-em-diferenças (DiD), regressão descontínua, entre outros, tentam se aproximar da aleatorização quando essa não é possível, por meio de ajustes estatísticos que tentam controlar variáveis de confundimento.

III. Correta. Esta afirmativa descreve corretamente a distinção entre os métodos experimentais, onde o pesquisador tem controle sobre a alocação dos sujeitos, e métodos quase-experimentais, que se utilizam de eventos naturais ou políticas específicas como ""experimentos naturais"", requerendo mais esforço em controlar variáveis externas e possíveis viéses.

IV. Incorreta. Mesmo em RCTs, o viés de seleção pode ocorrer, especialmente se a alocação aleatória for comprometida ou se houver desistências diferenciadas entre os grupos. Em métodos quase-experimentais, o risco de viés de seleção é uma preocupação importante, mas estratégias metodológicas são empregadas para minimizar esse risco, como pareamento e uso de variáveis instrumentais, não sendo necessariamente ""incontornável"".",340396
tópico 6,Tipos de viés no processo gerador dos dados e soluções: Sampling bias; Selection bias; Attrition bias; Reporting bias; Measurement bias.,"Questão: No contexto de pesquisas estatísticas e científicas, os vieses podem distorcer os resultados obtidos e levar a conclusões inválidas. Entre os vieses listados abaixo, qual definição corresponde ao viés de seleção, e qual estratégia poderia ser implementada para minimizá-lo?

A) Viés de seleção ocorre quando alguns grupos da população são sistemáticamente excluídos da amostra, o que pode ser mitigado pelo uso de amostragem aleatória.

B) Viés de amostragem se dá quando participantes desistem de um estudo ao longo do tempo, e a compensação por perdas pode ser usada para mitigar esse efeito.

C) Viés de aferição acontece quando os instrumentos de medição são inapropriados ou inadequados, o que pode ser atenuado com a calibração periódica dos instrumentos.

D) Viés de relato acontece quando os dados são sistematicamente distorcidos por tendências subjetivas dos participantes, que pode ser minimizado com o uso de questionários validados e anônimos.

E) Viés de seleção se manifesta quando os resultados são influenciados pela tendência em publicar apenas estudos com resultados positivos ou significativos, que pode ser reduzido pela prática de registro público de ensaios clínicos.

",A,"

Explicação dos itens:

A) Correto. O viés de seleção ocorre quando determinados indivíduos ou grupos são mais propensos a ser selecionados para a amostra, o que pode afetar a representatividade e os resultados da pesquisa. A adoção de amostragem aleatória ajuda a garantir que cada membro da população tenha igual chance de ser incluído, minimizando o viés de seleção.

B) Incorreto. O viés de amostragem é descrito incorretamente. O fenômeno descrito no item B corresponde ao viés de atrito, que é quando participantes desistem de um estudo, potencialmente distorcendo os resultados se os desistentes diferem dos que permanecem.

C) Incorreto. O viés de aferição diz respeito à distorção nos resultados devido a falhas nos instrumentos ou métodos de medição e não ao viés de seleção.

D) Incorreto. O viés de relato refere-se às distorções devido à relutância ou tendência dos participantes em fornecer informações corretas ou completas. Esta não é a descrição do viés de seleção.

E) Incorreto. Este item descreve o viés de publicação, que é quando os resultados de pesquisas são influenciados pela tendência de publicar estudos com resultados positivos, e não o viés de seleção.",3047594
tópico 6,Modelos probabilísticos gráficos: cadeias de Markov; filtros de Kalman; Redes bayesianas,"Questão: Um engenheiro de sistemas está desenvolvendo um módulo de previsão para um sistema de navegação autônoma, o qual depende significativamente do uso de modelos probabilísticos para estimar o estado futuro do veículo. Ao avaliar as opções disponíveis, o engenheiro considera a aplicação de Cadeias de Markov, Filtros de Kalman e Redes Bayesianas. Supondo que as observações são ruidosas e que há uma necessidade de se estimar estados que são parcialmente observáveis e contínuos no tempo, qual das seguintes abordagens seria mais apropriada para integrar neste sistema?

A) Cadeias de Markov, pois elas são mais adequadas para sistemas com estados discretos e uma estrutura de tempo discreta.

B) Redes Bayesianas, já que são ideais para a representação de incertezas e relações causais em sistemas estáticos.

C) Filtros de Kalman, por serem amplamente utilizados para a estimativa ótima do estado em sistemas dinâmicos lineares com ruído gaussiano.

D) Redes Bayesianas, porque são capazes de processar observações ruidosas e lidar com a incerteza em sistemas dinâmicos.

E) Cadeias de Markov, devido à sua capacidade de modelar sistemas estocásticos complexos, mesmo com observações contínuas no tempo.

",C,"

Explicação dos itens:

A) Cadeias de Markov são melhores para modelar sistemas com estados discretos e uma estrutura de tempo também discreta. Portanto, não seriam ideais para estados contínuos no tempo, como o mencionado na questão.

B) Redes Bayesianas são poderosas para representar incertezas e relações causais, mas elas não são as mais adequadas para sistemas que evoluem dinamicamente no tempo, especialmente quando as observações são ruidosas.

C) Filtros de Kalman são projetados especificamente para estimativa de estado em sistemas dinâmicos lineares e são ótimos para lidar com ruído gaussiano, o que os torna a escolha ideal para a situação descrita no enunciado da questão.

D) Embora as Redes Bayesianas possam processar observações ruidosas e lidar com incerteza, elas não são as mais adequadas para estimar estados em sistemas que são contínuos no tempo, diferentemente dos Filtros de Kalman que são projetados para este fim.

E) Cadeias de Markov, enquanto poderosas para sistemas estocásticos, têm limitações quando aplicadas a estados contínuos no tempo, principalmente se comparadas aos Filtros de Kalman, que são especificamente adequados para tais condições.",3183922
tópico 6,"Diagramas causais: gráficos acíclicos dirigidos; variáveis confundidoras, colisoras e de mediação","Questão: Em um estudo observacional sobre os efeitos do consumo de uma nova bebida energética na performance cognitiva de universitários, um pesquisador elaborou um diagrama causal utilizando gráficos acíclicos dirigidos (DAGs). A partir deste diagrama, o pesquisador pretende identificar as relações entre as variáveis para controlar possíveis viéses e confundimentos. Ele observou que a variável ""Horas de sono"" está causando um efeito tanto no consumo da bebida energética quanto na performance cognitiva, enquanto que a variável ""Estresse"" é afetada pelo consumo da bebida e simultaneamente influencia a performance cognitiva.

Com base neste cenário e no entendimento dos conceitos de variáveis confundidoras, colisoras e de mediação, qual das seguintes afirmações é correta?

A) A variável ""Horas de sono"" é um colisor e deve ser controlada na análise.
B) A variável ""Estresse"" é uma confundidora e não deve ser controlada na análise.
C) A variável ""Estresse"" é um colisor e seu controle pode introduzir viés na análise.
D) A variável ""Horas de sono"" é uma confundidora e deve ser controlada na análise.
E) A variável ""Estresse"" é uma mediadora e seu controle é essencial para avaliar o efeito direto do consumo da bebida na performance cognitiva.

",D,"

Explicação dos itens:
A) A variável ""Horas de sono"" não é classificada como um colisor, pois um colisor é uma variável que é influenciada por duas ou mais variáveis anteriores na cadeia causal. Neste caso, ""Horas de sono"" é uma variável que influencia outras duas: o consumo da bebida e a performance cognitiva, classificando-a como uma variável confundidora.
B) A variável ""Estresse"" está sendo influenciada pelo consumo da bebida energética, mas também influencia a performance cognitiva. No entanto, como não está causando os efeitos de ambas as variáveis de interesse, não é propriamente uma confundidora.
C) ""Estresse"" seria classificado como colisor se houvesse uma seta apontando para ele da variável consumo da bebida e outra da performance cognitiva. No entanto, como ele é afetado pelo consumo da bebida e afeta a performance cognitiva, ele representa uma variável mediadora.
D) ""Horas de sono"" afeta tanto o consumo da bebida quanto a performance cognitiva. Este é um clássico exemplo de uma variável confundidora, pois está relacionada tanto com a exposição quanto com o resultado, podendo criar uma associação espúria se não for controlada.
E) A variável ""Estresse"" é sim uma mediadora, mas controlar mediadores pode remover efeitos indiretos do consumo da bebida na performance, portanto, o seu controle não é universalmente ""essencial"", e a decisão de controlá-la dependerá do objetivo do estudo (avaliar efeitos diretos ou totais).",7480405
tópico 6,Testes de hipóteses: teste-z; teste-t; valorp; testes para uma amostra; testes de comparação de duas amostras; teste de normalidade (chi square); e intervalos de confiança.,"Questão: Suponha que um pesquisador deseja avaliar o efeito de um programa de treinamento no desempenho dos funcionários de uma empresa. O pesquisador coleta uma amostra de 30 funcionários antes e depois do treinamento e registra a pontuação de desempenho. Ele assume que as pontuações de desempenho são normalmente distribuídas na população. Antes de realizar qualquer teste, é de se esperar que o pesquisador verifique a normalidade dos dados. Qual é o método mais apropriado para testar a normalidade das pontuações de desempenho para essa amostra inferior a 50 observações e o que ele indicaria sobre a adequação de realizar um Teste t de amostras pareadas?

A) Teste-Z de normalidade, indicando que as pontuações seguem uma distribuição normal e validando o uso do Teste t de amostras pareadas.
B) Teste t de Student para comparação de duas médias, indicando que as distribuições das pontuações são normais.
C) Teste de Kolmogorov-Smirnov, indicando a aderência das pontuações à distribuição normal e validando o uso do Teste t de amostras pareadas, caso o resultado não rejeite a normalidade.
D) Teste qui-quadrado de Pearson, para verificar se as frequências observadas das pontuações se ajustam a uma distribuição esperada de frequências.
E) Teste Shapiro-Wilk, que é mais apropriado para amostras pequenas e avalia a aderência das pontuações à distribuição normal, o que é essencial para validar o uso de Teste t de amostras pareadas.

",E,"

Explicação dos itens:
A) O Teste-Z de normalidade não existe como um teste específico para avaliar a norma lidade; o teste-z é usado em outras circunstâncias, como para determinar se há uma diferença significativa entre médias de amostras grandes com variância conhecida.
B) O Teste t de Student não é um teste de normalidade; é usado para comparar duas médias quando as variâncias são desconhecidas, especialmente em amostras menores.
C) O Teste de Kolmogorov-Smirnov é um teste de normalidade que pode ser usado em amostras de vários tamanhos; entretanto, não é geralmente o mais indicado para amostras pequenas, como é o caso aqui.
D) O Teste qui-quadrado de Pearson é geralmente utilizado para validar hipóteses de independência ou de adequação a uma distribuição em dados categóricos, não sendo o teste mais apropriado para verificar normalidade de dados contínuos.
E) O Teste Shapiro-Wilk é um teste de normalidade que é considerado muito eficiente para amostras de tamanho pequeno (n < 50). Este teste verifica a hipótese de que uma amostra vem de uma população normalmente distribuída. Sendo assim, é o método mais apropriado para validar a utilização subsequente de um Teste t de amostras pareadas, que assume normalidade dos dados.",66555
tópico 6,Testes de hipóteses: teste-z; teste-t; valorp; testes para uma amostra; testes de comparação de duas amostras; teste de normalidade (chi square); e intervalos de confiança.,"Questão:

Numa pesquisa sobre a eficiência de um novo medicamento para redução da pressão arterial, os pesquisadores realizam um estudo com 25 pacientes e observaram uma redução média da pressão sistólica de 8 mmHg, com um desvio-padrão de 12 mmHg. Os pesquisadores desejam testar se o medicamento é eficaz na redução da pressão arterial sistólica. Considerando o nível de significância de 0,05 e sabendo que a pressão sistólica em uma população sem o tratamento segue uma distribuição normal com média de 130 mmHg, qual seria o teste estatístico mais apropriado a ser utilizado e qual seria a conclusão correta?

A) Teste-z, rejeita-se a hipótese nula de que a média de pressão sistólica é igual a 130 mmHg.
B) Teste-t, não se rejeita a hipótese nula de que a média de pressão sistólica com o medicamento é igual a 130 mmHg.
C) Teste-t, rejeita-se a hipótese nula de que a média de pressão sistólica com o medicamento é igual a 130 mmHg.
D) Teste de normalidade (chi-square), rejeita-se a hipótese nula de que os dados provêm de uma distribuição normal.
E) Intervalo de confiança, o intervalo de 95% para a média da pressão sistólica com o medicamento não inclui o valor de 130 mmHg.

",C," 

Explicação dos itens:

A) Incorreto. O teste-z é geralmente utilizado quando se tem uma amostra grande (n > 30) ou quando a variância da população é conhecida. No caso apresentado, a amostra é pequena (n = 25) e não se tem informações sobre a variância da população.

B) Incorreto. O teste-t é o teste correto a ser utilizado devido ao tamanho da amostra ser menor que 30. No entanto, dizer que não se rejeita a hipótese nula sem informação sobre o resultado do teste ou o valor-p é errado.

C) Correto. Com uma amostra de tamanho 25, deve-se utilizar o teste-t para testar se a média de pressão sistólica com o medicamento é diferente de 130 mmHg. Sem os dados do teste propriamente dito, esta é a única opção que se apresenta correta no contexto de rejeição da hipótese nula considerando que queremos testar a eficácia do medicamento.

D) Incorreto. O teste de normalidade (chi-square) é usado para determinar se uma distribuição de dados é ou não normal e não está diretamente relacionado a eficácia do medicamento.

E) Incorreto. Intervalos de confiança podem ser usados para inferir sobre a média da população, mas a questão pede especificamente por um teste de hipóteses, e o intervalo de confiança não é um teste per se. Ademais, a questão requer uma decisão de rejeitar ou não rejeitar a hipótese nula, o que não é resolvido diretamente pelo fornecimento de um intervalo de confiança.",3201218
tópico 6,"Diagramas causais: gráficos acíclicos dirigidos; variáveis confundidoras, colisoras e de mediação","Questão: Em um estudo sobre os efeitos de um novo programa de exercícios físicos sobre a saúde cardiovascular, um pesquisador elabora um diagrama causal usando gráficos acíclicos dirigidos (DAGs). O objetivo é identificar as relações entre o programa de exercícios (variável independente), saúde cardiovascular (variável dependente), e outras variáveis que possam influenciar esta relação. Considere as seguintes variáveis adicionais:

1. Dieta equilibrada
2. Histórico genético de doenças cardíacas
3. Nível de estresse

Dentre estas, qual variável seria classificada como confundidora no estudo e deveria ser controlada para se estabelecer o efeito causal do programa de exercícios sobre a saúde cardiovascular?

A) Dieta equilibrada
B) Histórico genético de doenças cardíacas
C) Nível de estresse
D) Todas as variáveis acima

",A,"

Explicação dos itens:

A) Dieta equilibrada - Correto. A dieta equilibrada pode influenciar tanto a adesão ao programa de exercícios quanto a saúde cardiovascular, fazendo desta uma variável confundidora que está associada à exposição e ao resultado. Controlar por dieta é essencial para isolar o efeito do programa de exercícios sobre a saúde cardiovascular.

B) Histórico genético de doenças cardíacas - Não é a alternativa correta. Apesar de o histórico genético estar associado à saúde cardiovascular, ele não é influenciado pela exposição (programa de exercícios), logo não é uma variável confundidora neste contexto.

C) Nível de estresse - Não é a alternativa correta. Embora o nível de estresse possa afetar a saúde cardiovascular, ele não é necessariamente uma variável confundidora, a menos que se demonstre que também está relacionado à exposição ao programa de exercícios. A questão não fornece informações suficientes para determinar essa relação.

D) Todas as variáveis acima - Não é a alternativa correta porque, conforme explicado anteriormente, nem todas as variáveis listadas são confundidoras neste estudo. Apenas a dieta equilibrada tem a clara indicação de ser uma variável confundidora que deve ser controlada.",8888136
tópico 6,Tipos de viés no processo gerador dos dados e soluções: Sampling bias; Selection bias; Attrition bias; Reporting bias; Measurement bias.,"Questão: Em estudos observacionais e experimentais, a validade e a confiabilidade dos resultados podem ser comprometidas por diferentes tipos de viés. O entendimento correto sobre esses vieses é fundamental para a implementação de estratégias que minimizem seus impactos negativos. Considere as seguintes descrições dos tipos de viés:

I. Sampling Bias: Ocorre quando a amostra selecionada não é representativa da população alvo, talvez devido a uma técnica de amostragem inadequada, levando a resultados que não são generalizáveis.

II. Selection Bias: Acontece quando os participantes são selecionados de uma maneira que não é aleatória, o que pode influenciar os resultados do estudo de uma forma que não reflete a verdadeira relação entre as variáveis de interesse.

III. Attrition Bias: Refere-se à distorção dos resultados que pode ocorrer quando há uma perda diferencial de participantes durante o acompanhamento de um estudo longitudinal.

IV. Reporting Bias: Surge quando a divulgação de informações é influenciada pela natureza ou direção dos resultados. Por exemplo, estudos com resultados positivos são mais propensos a serem publicados do que aqueles com resultados negativos ou inconclusivos.

V. Measurement Bias: Este tipo de viés ocorre quando as ferramentas ou métodos de medição usados não são consistentes ou precisos, levando a dados que não são verdadeiramente representativos do que se deseja medir.

Com base nessas descrições, identifique a alternativa que apresenta a classificação correta dos tipos de viés mencionados:

A) Apenas as descrições I, II e IV estão corretas.
B) Apenas as descrições II, III e V estão corretas.
C) Todas as descrições estão corretas.
D) Apenas as descrições I, III e IV estão corretas.
E) Apenas as descrições I, II e V estão corretas.

",C,"

Explicação dos itens:
I. A descrição de Sampling Bias está correta, pois aborda a questão da não representatividade da amostra em relação à população alvo por conta de uma técnica inadequada de seleção dos participantes.
II. A descrição de Selection Bias está correta e destaca o problema na seleção dos participantes que não é aleatória, podendo afetar os resultados do estudo.
III. A descrição de Attrition Bias está correta e diz respeito à perda diferencial dos participantes durante um estudo, que pode resultar em distorções nos resultados.
IV. A descrição de Reporting Bias também está correta, evidenciando o problema da tendência na publicação de estudos com base na natureza dos resultados.
V. A descrição de Measurement Bias está correta ao mencionar a imprecisão ou inconsistência nos métodos de medição, o que pode levar a dados distorcidos.
Assim, todas as descrições apresentadas correspondem adequadamente aos tipos de viés mencionados, tornando a alternativa C a correta.",1041847
tópico 6,Modelos probabilísticos gráficos: cadeias de Markov; filtros de Kalman; Redes bayesianas,"Questão:

Um analista deseja modelar o comportamento de um sistema elétrico sujeito a variações aleatórias, a fim de prever e corrigir falhas no fornecimento de energia. Ele está considerando o uso de modelos probabilísticos gráficos e desejou identificar qual dos métodos listados seria mais adequado para lidar com um cenário onde o sistema tem dependências temporais e contínuas atualizações a partir de observações ruidosas. Assinale a opção que indica o modelo probabilístico gráfico mais apropriado para a aplicação desejada pelo analista.

A) Cadeias de Markov

B) Filtros de Kalman

C) Redes Bayesianas

D) Modelos Ocultos de Markov

E) Árvores de Decisão

",B,"

Explicação dos itens:

A) Cadeias de Markov - A cadeia de Markov é adequada para modelar processos estocásticos onde o estado presente é independente dos estados passados, condicionado ao estado atual. Não é a melhor escolha quando se tem observações ruidosas ou atualizações contínuas.

B) Filtros de Kalman - Este método é ideal para sistemas dinâmicos que evoluem no tempo com medidas que possuem ruídos, permitindo fazer estimativas do estado atual do sistema baseando-se em medidas observadas ao longo do tempo. É particularmente útil para correção em tempo real, sendo, portanto, a escolha apropriada para o analista.

C) Redes Bayesianas - Redes Bayesianas são úteis para representar relações causais entre variáveis aleatórias e para fazer inferências baseadas em evidências, mas não são específicas para dados sequenciais ou para sistemas que requerem atualização contínua a partir de observações.

D) Modelos Ocultos de Markov - Embora os Modelos Ocultos de Markov (HMMs) sejam adequados para lidar com dados sequenciais e observações ruidosas, eles são tipicamente utilizados em espaços de estados discretos, não sendo o ideal para sistemas de estados contínuos.

E) Árvores de Decisão - Árvores de decisão são modelos preditivos que mapeiam observações sobre um item para conclusões sobre o item. Eles não são particularmente desenhados para lidar com dados sequenciais ou para fazer atualizações com base em observações ruidosas, motivo pelo qual não são adequados para o caso em questão.",9896605
tópico 6,Métodos e técnicas de identificação causal: Métodos experimentais RCT e de identificação quase-experimental,"Questão:

A busca por relações de causalidade é central na pesquisa em várias áreas do conhecimento, especialmente nas ciências sociais e na economia. Diversos métodos são aplicados para identificar e inferir causas e efeitos a partir de dados observacionais e experimentais. Entre esses métodos, destacam-se os Ensaios Controlados Randomizados (RCTs - Randomized Controlled Trials) e os métodos de identificação quase-experimental. A respeito dessas técnicas, avalie as seguintes afirmativas:

I. Os RCTs são considerados o padrão-ouro para identificação causal, já que a randomização tende a balancear características observáveis e não observáveis entre os grupos de tratamento e controle, minimizando viés de seleção.

II. Os RCTs são impraticáveis em muitos contextos, devido a limitações éticas, logísticas ou financeiras, o que frequentemente torna os métodos quase-experimentais alternativas preferíveis apesar de potencialmente mais sujeitos a viéses.

III. Os métodos quase-experimentais, como a Regressão Descontínua (RD) e Instrumentos Externos (IV), dependem de suposições menos restritivas e, portanto, são sempre mais robustos que os RCTs em termos de validar relações de causalidade.

IV. Tanto os RCTs quanto os métodos quase-experimentais exigem uma cuidadosa análise e compreensão do contexto e dos mecanismos em jogo para que os resultados sejam corretamente interpretados e generalizados.

Está(ão) correta(s) a(s) afirmativa(s):

A) I, apenas.
B) I e II, apenas.
C) I, II e IV, apenas.
D) III e IV, apenas.
E) Todas as afirmativas estão corretas.

",C,"

Explicação dos itens:

I. Correto. Os RCTs de fato são amplamente considerados o padrão-ouro para identificar relações causais, pois a randomização ajuda a balancear características que poderiam influenciar os resultados entre o grupo de controle e o de tratamento.

II. Correto. Os RCTs podem ser difíceis ou até impossíveis de implementar em certas situações devido a diversas barreiras, incluindo éticas, financeiras e logísticas. Por isso, métodos quase-experimentais são frequentemente utilizados como alternativas.

III. Incorreto. Os métodos quase-experimentais dependem de suposições que, se violadas, podem comprometer a inferência causal. Não são considerados necessariamente mais robustos do que RCTs e suas suposições precisam ser cuidadosamente justificadas e testadas.

IV. Correto. Ambos os métodos requerem um entendimento aprofundado do fenômeno em estudo e dos contextos em que são aplicados, o que é essencial para a interpretação dos resultados e a possibilidade de generalização dos efeitos identificados.",3737501
tópico 0,Ingestão de dados em streaming,"Questão: No processo de Ingestão de Dados em Streaming, é fundamental que as soluções escolhidas para o pipeline de dados possam lidar com o fluxo contínuo e potencialmente ilimitado de informações em tempo real. Com relação às características desejadas para sistemas de processamento de dados em streaming, analise as afirmações a seguir:

I - A tolerância a falhas é crítica, garantindo que, no caso de uma pane no sistema, as informações não sejam perdidas e possam ser processadas quando o serviço for restaurado.

II - A latência baixa na entrega dos dados é dispensável, uma vez que o objetivo principal do streaming de dados é o armazenamento para análise posterior.

III - A escalabilidade do sistema deve ser considerada, permitindo que ele cresça de acordo com o volume de dados sem prejudicar a performance do processamento.

IV - Os sistemas de processamento em streaming devem ser capazes de manipular tipos de dados variados, incluindo, mas não limitado a, texto, imagens, vídeo e dados de sensores.

São características desejáveis em sistemas de processamento de dados em streaming APENAS as:

A) I e II.
B) I, III e IV.
C) II, III e IV.
D) I, II e III.
E) Todas as afirmações são verdadeiras.

",B," 

Explicação:

Item I - Correto. A tolerância a falhas é fundamental em sistemas de processamento de dados em streaming, pois garante que, em caso de falhas, o sistema possa recuperar-se e continuar o processamento sem perda de dados.

Item II - Incorreto. A baixa latência é incrivelmente importante no processamento de dados em streaming, uma vez que um dos objetivos é permitir a tomada de decisões em tempo real ou a análise o mais rápido possível.

Item III - Correto. Escalabilidade é essencial, pois o volume de dados em streaming pode ser imprevisível e crescer rapidamente. Sistemas escaláveis podem se ajustar para lidar com o aumento de carga sem impactar negativamente o desempenho.

Item IV - Correto. A habilidade de lidar com diversos tipos de dados é uma característica valiosa de sistemas de processamento em streaming, dada a diversidade de fontes e formatos de dados que podem ser encontrados em ambientes reais.",6132716
tópico 0,Armazenamento de big data,"Questão:
A análise de big data requer métodos avançados e eficientes para armazenar e processar volumes massivos de dados. Entre as soluções de armazenamento projetadas para lidar com esses desafios, pode-se citar o Hadoop Distributed File System (HDFS), o NoSQL e armazenamento em nuvem. Considerando o contexto de big data, qual das seguintes opções descreve corretamente a abordagem mais adequada para lidar com a heterogeneidade e escalabilidade dos dados?

A) O HDFS divide arquivos grandes em blocos menores e os distribui por vários nós em um cluster, mas não é adequado para dados que precisam ser atualizados frequentemente.
B) Bancos de dados NoSQL são otimizados apenas para transações online, sendo menos eficazes para lidar com grandes volumes de dados não estruturados ou semiestruturados.
C) O armazenamento em nuvem é rigidamente limitado em termos de escalabilidade e capacidade, o que o torna inadequado para armazenamento de big data.
D) Bancos de dados relacionais tradicionais são recomendados para big data devido à sua estruturação e integridade de dados rigorosa, que facilita a análise preditiva e complexa.
E) O NoSQL é especificamente projetado para armazenar, recuperar e gerenciar dados não estruturados, semiestruturados ou estruturados em larga escala, promovendo flexibilidade no esquema de dados e escalabilidade horizontal.

",E," 
A alternativa A está correta no que diz respeito ao fato de que o HDFS é eficaz para armazenar arquivos grandes distribuídos, mas o sistema não é otimizado para arquivos que exigem atualizações frequentes, devido ao alto custo de latência na escrita. A opção B está incorreta, pois os bancos de dados NoSQL são, na verdade, adequados para lidar com grandes volumes de dados não estruturados ou semiestruturados, sendo projetados para escalar horizontalmente. A alternativa C é equivocada por sugerir que o armazenamento em nuvem tem limitações rígidas de escalabilidade, quando na realidade, a capacidade da nuvem de escala é uma de suas grandes vantagens para big data. A alternativa D é incorreta; embora os bancos de dados relacionais ofereçam integridade e estrutura rigorosa, eles não são projetados para lidar com as demandas de volume, variedade e velocidade associadas ao big data. Por fim, a alternativa E é a correta, pois descreve com precisão a flexibilidade e escalabilidade que os sistemas NoSQL oferecem para os desafios do big data.
",8557666
tópico 0,Soluções de big data: Arquitetura do ecossistema Spark,"Questão CESGRANRIO:

A respeito da arquitetura do ecossistema Apache Spark, uma das ferramentas de big data mais utilizadas, é correto afirmar que:

A) O Spark SQL não é capaz de processar dados que não estejam em formato estruturado, como JSON ou tabelas no Hive.
B) O Spark Streaming é uma extensão do núcleo Spark que permite processamento de stream em tempo real, mas não suporta análise de dados históricos.
C) O MLib é a biblioteca do Spark destinada a aprendizado de máquina, que fornece vários tipos de algoritmos e utilidades para machine learning.
D) A arquitetura do Spark é incompatível com o Hadoop, tornando impossível o uso do Spark sobre um cluster Hadoop.
E) O GraphX é um conjunto de API's para grafos e computação paralela que não se integra com o núcleo do Spark, necessitando de sistemas externos para funcionar.

",C,"

A alternativa A está incorreta pois o Spark SQL é totalmente capaz de lidar com dados não estruturados, permitindo a realização de consultas SQL em diversos tipos de fontes de dados.
A alternativa B está incorreta porque, além de processar dados em tempo real, o Spark Streaming também pode ser utilizado para analisar dados históricos, desde que esses dados estejam acessíveis.
A alternativa C está correta pois o MLib é realmente a biblioteca voltada para machine learning dentro do ecossistema Spark, oferecendo uma gama de algoritmos de aprendizado de máquina, como classificação, regressão, agrupamento, entre outros.
A alternativa D está incorreta porque a arquitetura Spark é projetada para ser altamente compatível com o Hadoop, e é comum executar o Spark em clusters Hadoop, utilizando o YARN como gerenciador de recursos.
A alternativa E está incorreta visto que o GraphX é efetivamente integrado ao Spark, proporcionando uma API para a manipulação de grafos e computação em paralelo dentro do próprio ecossistema Spark.",7294767
tópico 0,Conceitos de processamento massivo e paralelo,"Questão: Em aplicações que demandam elevado poder de processamento e análise de grandes volumes de dados, o uso de técnicas de processamento massivo e paralelo pode ser essencial para alcançar resultados em tempo hábil. Neste contexto, qual das seguintes opções melhor descreve uma característica fundamental do modelo MapReduce, amplamente utilizado em processos de computação em larga escala?

A) Reduz a necessidade de sincronização entre as tarefas, pois cada tarefa de mapeamento (Map) pode ser executada independentemente.
B) Permite o compartilhamento de memória entre diferentes tarefas para acelerar o processamento e a transferência de dados.
C) Exige que todos os processos de redução (Reduce) sejam executados sequencialmente para garantir a integridade dos resultados finais.
D) Restringe o processamento e a análise de dados a uma quantidade limitada de nós, favorecendo o uso de algoritmos simples para manter a eficiência.
E) Depende exclusivamente de hardware especializado de alto desempenho para distribuir e gerenciar as tarefas de processamento.

",A,"

Explicação dos itens:

A) Correta. O modelo MapReduce divide o processamento de dados em duas fases principais, Map e Reduce. Na fase de mapeamento, o processamento é feito de maneira independente em diferentes conjuntos de dados, o que reduz a complexidade da sincronização entre tarefas.

B) Incorreta. O modelo MapReduce geralmente não compartilha memória entre tarefas; ele utiliza o sistema de arquivos distribuídos para intermediar os dados entre as tarefas de Map e Reduce.

C) Incorreta. As tarefas de redução não precisam necessariamente ser executadas de maneira sequencial. Várias tarefas de Reduce podem operar em paralelo, processando diferentes conjuntos de dados intermediários resultantes da fase de Map.

D) Incorreta. O mapa-reduce é projetado para escalar horizontalmente, ou seja, pode adicionar mais nós ao sistema para aumentar a capacidade de processamento, sem uma limitação pré-definida no número de nós.

E) Incorreta. Embora o desempenho possa ser melhorado com hardware especializado, o MapReduce é projetado para rodar em clusters de computadores comuns, não necessitando exclusivamente de equipamentos de alto desempenho especializados.",189247
tópico 0,Ingestão de dados em lote (batch),"Questão: A incorporação de dados em lote (batch) é uma prática comum em grandes organizações que precisam processar volumes significativos de informações em períodos específicos. Ao planejar a ingestão de dados em lote para um sistema de armazenamento de dados de uma grande corporação, qual das seguintes considerações é MENOS relevante para a eficiência do processo?

A) A capacidade de cada nó de processamento em termos de CPU e memória para manipulação dos dados.
B) A largura de banda da rede e a latência na transferência de dados entre as origens dos dados e o sistema de armazenamento.
C) A compatibilidade do esquema dos dados de origem com o esquema do sistema de armazenamento de dados.
D) A frequência com que os dados são alterados nas fontes originais de dados, para ajustar o agendamento da ingestão.
E) A estética visual das ferramentas de monitoramento do processo de ingestão de dados, para o uso dos analistas de dados.

",E," 

Explicação dos itens:

A) A capacidade de cada nó de processamento é altamente relevante porque pode limitar a quantidade de dados que podem ser processados em um lote e influenciar no tempo de processamento.

B) A largura de banda da rede e a latência são críticas para garantir que os dados sejam transferidos de maneira eficiente e sem demoras excessivas, o que pode impactar diretamente a janela de tempo da ingestão em lote.

C) A compatibilidade do esquema é fundamental para que os dados possam ser incorporados sem necessidade de transformações extensivas, que poderiam retardar o processo de ingestão.

D) A frequência de alteração dos dados pode impactar a estratégia de agendamento da ingestão em lote para minimizar desatualizações e otimizar a relevância dos dados no sistema de armazenamento.

E) A estética visual das ferramentas de monitoramento, embora possa melhorar a experiência do usuário, é a menos relevante das opções apresentadas no que se refere à eficiência operacional do processo de ingestão de dados em lote. O que realmente importa são as funcionalidades e a precisão das ferramentas na detecção e comunicação de problemas no processo de ingestão.",432257
tópico 0,Processamento distribuído,"Questão: No contexto do processamento distribuído, considere um sistema que utiliza o modelo de consistência causal para a replicação de dados. Esse sistema está sujeito a diferentes condições e limitações que afetam o comportamento e desempenho do algoritmo de replicação. Com base nisso, analise as afirmativas a seguir e identifique a opção que apresenta uma descrição correta deste modelo de consistência:

I. O modelo de consistência causal garante que operações de escrita relacionadas causalmente sejam vistas por todos os nós do sistema na mesma ordem.
II. Em sistemas que empregam consistência causal, não existe a necessidade de sincronização entre relógios de máquinas distintas, pois a causalidade pode ser determinada sem referências temporais globais.
III. A consistência causal não é suficiente para resolver problemas que exigem uma forte sincronização, como controle de concorrência em sistemas transacionais.

Assinale a alternativa correta.

A) Apenas I e II estão corretas.
B) Apenas I e III estão corretas.
C) Apenas II e III estão corretas.
D) Todas estão corretas.
E) Nenhuma está correta.

",B," 
A alternativa correta é a letra ""B"". Vamos explicar cada item:

I. Correto. O modelo de consistência causal assegura que se uma operação de escrita A acontece antes de uma operação de escrita B, e há uma relação causal entre elas, então todos os nós do sistema irão observar A antes de B. Isso é crucial para manter a consistência lógica dos dados replicados.

II. Incorreto. Embora a consistência causal não exija a precisão de relógios sincronizados como um modelo de consistência sequencial global (como a consistência forte), ela requer algum mecanismo de rastreamento de causalidade, como vetores de versão, para determinar a ordem das operações. Portanto, alguma forma de controle temporal é necessária para manter a relação causal entre eventos.

III. Correto. A consistência causal é menos restritiva que a consistência forte (também chamada de linearizabilidade), que requer que todas as operações sejam vistas na mesma ordem por todos os nós. Em cenários que exigem sincronização precisa, como sistemas transacionais e bases de dados onde transações concorrentes devem ser serializáveis, a consistência causal pode não ser suficiente para evitar conflitos e garantir a integridade dos dados.",807960
tópico 0,"Arquitetura de cloud computing para ciência de dados (AWS, Azure, GCP)","Questão: Em uma empresa de médio porte que foca em soluções de ciência de dados, um arquiteto de soluções foi encarregado de escolher a plataforma de cloud computing mais adequada para hospedar um pipeline de machine learning escalável e com alta disponibilidade. Dentre as opções AWS, Azure, e GCP, ele deve levar em consideração a facilidade de integração com ferramentas de análise de dados, o suporte a serviços de treinamento e execução de modelos de aprendizado de máquina e a flexibilidade de gerenciamento de recursos computacionais. Qual das seguintes opções é a mais adequada para atender às necessidades da empresa, considerando um contexto onde a integração com serviços de análise e machine learning são cruciais?

A) AWS, devido ao provedor oferecer o Amazon SageMaker que permite a construção, treinamento e implantação de modelos de machine learning de maneira fácil e integrada com outros serviços da AWS.
B) Azure, pois a Microsoft fornece o Azure Machine Learning, uma plataforma de nuvem que apoia a ciência de dados com serviços de construção, treinamento e implantação de modelos, além de uma ampla gama de serviços cognitivos.
C) GCP, já que o Google Cloud oferece o AI Platform que facilita o desenvolvimento de modelos de machine learning em larga escala, proporcionando um ambiente bem integrado com as ferramentas de análise de dados do BigQuery e TensorFlow.
D) AWS, porque é a única plataforma que oferece soluções integradas de IoT que são essenciais para a análise de dados em tempo real e machine learning em projetos de ciência de dados.
E) Azure, em virtude de ser o único provedor de cloud computing que suporta máquinas virtuais com GPUs para o processamento intensivo de treinamento de modelos de aprendizado de máquina.

",A," 

Cada uma das opções menciona características importantes das respectivas plataformas de cloud computing, no entanto, a questão pede uma escolha baseada na integração com ferramentas de análise e machine learning, o que torna a alternativa A a correta. O Amazon SageMaker é conhecido por sua facilidade de integração e pelo amplo alcance de serviços ao redor, unindo funcionalidades de análise de dados e machine learning. Já a alternativa B também é uma opção viável devido ao Azure Machine Learning, porém a questão não deixa claro que a Microsoft é preferível sobre a AWS para esse contexto. A alternativa C é uma afirmação correta sobre o que o GCP oferece, mas não a torna a mais adequada sem um contexto específico que destaque uma vantagem sobre as outras. A alternativa D está incorreta, pois as soluções integradas de IoT são importantes, mas não são únicas da AWS, e também não são o foco principal da questão. A alternativa E é incorreta porque tanto AWS quanto GCP também oferecem máquinas virtuais com GPUs.",2311490
tópico 0,"Ingestão de dados estruturados, semiestruturados e não estruturados","Questão: No contexto de Big Data e análises de dados avançadas, a ingestão de dados representa um aspecto crítico onde diferentes formatos de dados devem ser tratados para possibilitar uma análise efetiva. Considere os três principais formatos de dados: estruturados, semiestruturados e não estruturados. Qual das seguintes opções descreve corretamente as características destes formatos de dados e o método mais apropriado de ingestão?

A) Dados estruturados, tais como informações armazenadas em um banco de dados relacional, devem ser ingeridos utilizando técnicas simples de transferência de arquivos.
B) Dados semiestruturados, como arquivos JSON ou XML, requerem uma análise sintática detalhada para transformação em um esquema pré-definido antes da ingestão.
C) Dados não estruturados, incluindo imagens, vídeos e textos, podem ser diretamente ingeridos em sistemas de processamento de dados sem nenhuma pré-processamento.
D) Dados estruturados muitas vezes necessitam de um processo de Extract, Transform, Load (ETL) para a sua ingestão, com o objetivo de garantir que os dados estejam em conformidade com o esquema do sistema de destino.
E) Dados semiestruturados não necessitam de qualquer forma de transformação ou esquematização, visto que possuem uma estrutura inerente altamente compatível com sistemas de análises de dados.

",D," Explicação dos itens:

A) Dados estruturados geralmente requerem mais do que simples transferência de arquivos. Eles precisam de um processo de ETL para garantir a integridade dos dados e conformidade com o esquema de destino.

B) Embora dados semiestruturados possuam uma estrutura interna, muitas vezes a transformação é necessária para corresponder ao esquema de destino, mas não há uma única maneira de análise sintática, pois isso depende dos requisitos específicos e da finalidade da ingestão.

C) Dados não estruturados frequentemente requerem alguma forma de processamento e análise para transformá-los em um formato utilizável para processamento de dados. Isto pode incluir extração de texto, reconhecimento de imagem, etc.

D) Esta é a resposta correta. Dados estruturados comumente passam por um processo de ETL que envolve sua extração de uma fonte, transformação para se adequar ao esquema da base de dados de destino, e finalmente carregamento na base de destino.

E) Dados semiestruturados geralmente necessitam de algum nível de transformação, já que, embora contenham elementos de estrutura, eles podem não se alinhar perfeitamente com o esquema do sistema de análise de dados. A estrutura inerente facilita a análise, mas não elimina a necessidade de transformação ou esquematização.",5411954
tópico 0,Ingestão de dados em lote (batch),"Questão: Na arquitetura de Big Data, os sistemas de processamento de dados em lotes são essenciais para lidar com grandes volumes de informação de forma eficiente. Ao projetar um sistema para ingestão de dados em lote, alguns aspectos devem ser considerados para garantir um desempenho adequado e uma alta disponibilidade. Qual das seguintes opções NÃO é uma consideração crítica ao projetar um sistema de ingestão de dados em lote?

A) Capacidade de processamento em paralelo para acelerar a ingestão de grandes conjuntos de dados.
B) Uso de esquemas de compressão de dados para otimizar o uso do armazenamento e reduzir os custos.
C) Tempo real de atualização dos dados para garantir a menor latência possível na ingestão dos dados.
D) Tolerância a falhas para assegurar a integridade dos dados em caso de falhas no sistema.
E) Escalabilidade para acomodar o crescimento no volume de dados ao longo do tempo.

",C,"

A questão aborda aspectos importantes do desenho de um sistema de ingestão de dados em lote. A alternativa A é importante, pois o processamento em paralelo é um recurso crítico para acelerar o manejo de grandes volumes de dados. A alternativa B é relevante, pois a compressão de dados pode melhorar o uso do armazenamento e ajudar na redução de custos. A alternativa D é fundamental porque a tolerância a falhas garante a continuidade do negócio e a integridade dos dados. A alternativa E é essencial, uma vez que sistemas de Big Data devem ser capazes de escalar conforme o aumento do volume de dados. Entretanto, a alternativa C menciona ""tempo real de atualização dos dados,"" que não é um requisito para a ingestão de dados em lotes, pois esse processo trata de dados que são coletados e processados em intervalos, e não necessariamente em tempo real.",7631252
tópico 0,Conceitos de processamento massivo e paralelo,"Questão: Em cenários de Big Data, processamento massivo e paralelo são fundamentais para viabilizar análises em grandes volumes de dados. Dentre as arquiteturas e ferramentas utilizadas para esse fim, destaca-se o framework Apache Hadoop, que implementa o modelo de programação MapReduce. Considerando o funcionamento do modelo MapReduce e seus componentes no ecossistema Hadoop, analise as afirmativas a seguir:

I. O procedimento Map é responsável pela filtragem e ordenação dos dados, enquanto o Reduce realiza a agregação dos resultados, funcionando essencialmente de maneira sequencial após a etapa Map.

II. O Hadoop Distributed File System (HDFS) é projetado para armazenar volumes massivos de dados de maneira distribuída, permitindo o processamento paralelo dos dados por meio de alta disponibilidade e tolerância a falhas.

III. A fase de Shuffle and Sort ocorre após a fase de Map e antes da fase de Reduce, envolvendo a organização dos dados de saída do Map em grupos de acordo com as chaves intermediárias geradas, facilitando seu processamento subsequente pelo Reduce.

IV. Em um ambiente Hadoop, todas as operações de processamento de dados necessitam ser escritas no modelo MapReduce, excluindo a possibilidade de uso de outras abstrações e linguagens de consulta, como Apache Pig e Hive.

É correto o que se afirma em:

A) I, II e III, apenas.
B) II, III e IV, apenas.
C) I e IV, apenas.
D) II e III, apenas.
E) Todas as afirmativas estão corretas.

",A,"

Explicações dos itens:

I. Correta. O Map é a primeira fase do MapReduce, onde cada tarefa de mapeamento processa um bloco de dados de entrada, realiza o filtramento e sort, e prepare as informações em pares chave/valor. A fase de Reduce, que é essencialmente a segunda parte do processo, é onde a agregação ou sumarização dos dados é realizada.

II. Correta. O HDFS é o sistema de arquivos distribuídos do Hadoop, projetado para armazenar grandes volumes de dados distribuídos entre vários nodos, o que permite o processamento paralelo e oferece alta disponibilidade e resistência a falhas.

III. Correta. A fase de Shuffle and Sort é o processo intermediário entre a fase de Map e de Reduce. Durante o Shuffle, os pares chave/valor produzidos pela fase Map são transferidos para a máquina onde será executada a tarefa de Reduce correspondente. Já o Sort é o processo de ordenação desses pares antes da execução do Reduce.

IV. Incorreta. A afirmação IV é falsa porque o Hadoop permite o uso de outros paradigmas de processamento e linguagens além de MapReduce. Apache Pig, por exemplo, é uma plataforma para análise de grandes conjuntos de dados que consiste em uma linguagem de alto nível para expressar programas de processamento de dados com Hadoop, e Hive é um armazenamento de dados em Hadoop que fornece uma abstração semelhante ao SQL para consultas. Isso mostra que o Hadoop suporta múltiplas abstrações para processamento de dados.",6964925
tópico 0,"Arquitetura de cloud computing para ciência de dados (AWS, Azure, GCP)","Questão:

Em relação à arquitetura de cloud computing para ciência de dados, é fundamental escolher o provedor e os serviços corretos que atendam às necessidades do projeto. Considere que uma empresa de médio porte busca uma solução de nuvem para desenvolver seus pipelines de dados e machine learning de forma escalável e custo-efetiva. Com base nas características dos principais provedores de serviço de nuvem, qual das seguintes alternativas apresenta uma combinação de serviços que melhor atenderia às necessidades da empresa?

A) AWS com Amazon S3 para armazenamento de dados, AWS Glue para catalogação e ETL, e Amazon SageMaker para construção e treinamento de modelos de machine learning.
B) Azure com Azure Blob Storage para hospedagem de sites estáticos, Azure Data Factory para orquestração de workflows de dados, e Azure Machine Learning Studio para construção e treinamento de modelos de machine learning.
C) GCP com Google Cloud Storage para execução de funções serverless, Google Dataflow para processamento de stream e batch data, e Google AI Platform para análise de dados e treinamento de modelos de machine learning.
D) AWS com Amazon EC2 para computação elástica, AWS Lambda para processamento de eventos em tempo real, e AWS DeepLens para aprendizado de máquina profundo e visão computacional.
E) GCP com Google BigTable para banco de dados NoSQL, Google Dataprep para preparação de dados, e Google AutoML para treinamento e implantação de modelos de machine learning com conhecimento limitado de machine learning.

",A,"

Explicação dos itens:

A) Correto. Amazon S3 é um serviço eficiente para armazenamento de dados. AWS Glue oferece serviços de ETL e catalogação que facilitam o gerenciamento dos dados, enquanto o Amazon SageMaker fornece uma ampla gama de ferramentas para construir, treinar e implantar modelos de machine learning de forma escalável e com custo otimizado.

B) Incorreto. O Azure Blob Storage não é otimizado para hospedagem de sites estáticos, mas para armazenamento de dados. Azure Data Factory é uma boa escolha para orquestração de workflows de dados, porém a descrição do cenário pede funções mais focadas em ciência de dados e machine learning como um todo.

C) Incorreto. Google Cloud Storage é adequado para armazenamento de dados, mas não executa funções serverless, o que seria um papel do Google Cloud Functions. Google Dataflow é uma ótima solução para processamento de dados e Google AI Platform é uma plataforma adequada para análise de dados e treinamento de modelos, mas a questão enfatiza a necessidade de uma solução de armazenamento de dados junto com ETL e machine learning.

D) Incorreto. Amazon EC2 oferta computação elástica, mas não é específico para ciência de dados. AWS Lambda lida com processamento de eventos em tempo real e é serverless, mas não é a ferramenta mais centrada em ciência de dados. AWS DeepLens é uma câmera de deep learning específica que não é citada no cenário como uma necessidade da empresa.

E) Incorreto. Google BigTable é um serviço de banco de dados eficiente, mas a solução não é direcionada ao armazenamento de dados genéricos como parte de um pipeline de ciência de dados. Google Dataprep pode ajudar na preparação de dados, mas a questão pede por soluções de ETL. Google AutoML é útil para treinar modelos sem conhecimento aprofundado, mas não atende integralmente à demanda por uma arquitetura completa de dados e machine learning.",1284958
tópico 0,Armazenamento de big data,"Questão: No contexto de sistemas distribuídos para armazenamento e processamento de grandes volumes de dados, o Hadoop Distributed File System (HDFS) é uma opção amplamente utilizada. A respeito das características e funcionalidades do HDFS, avalie as afirmações a seguir:

I - O HDFS apresenta um modelo de consistência forte, onde a escrita ou atualização de um arquivo é imediatamente visível para todos os leitores.

II - O HDFS foi projetado para lidar com falhas de hardware de maneira transparente, replicando automaticamente os blocos de dados entre diferentes nós no cluster.

III - O HDFS suporta arquivos de tamanho muito grande, distribuindo-os em múltiplos blocos de dados armazenados em vários nós, mas não permite a modificação de partes de arquivos, exigindo a reescrita do arquivo completo em caso de alterações.

IV - A arquitetura do HDFS segue o modelo mestre/escravo, no qual um único NameNode gerencia o namespace do sistema de arquivos e regula o acesso aos arquivos por clientes.

A alternativa que contém todas as afirmações corretas é:

A) I e III
B) II e IV
C) I, II e III
D) II, III e IV
E) I, II, III e IV

",B,"

Explicação dos itens:

I - O HDFS não apresenta um modelo de consistência forte. Ele segue o modelo de consistência eventual, onde após a conclusão das operações de escrita, os novos dados tornam-se visíveis para novos leitores, não imediatamente para todos os atuais leitores.

II - É uma afirmativa correta. O HDFS foi projetado para ser resistente a falhas, replicando blocos de dados em diferentes nós para garantir a disponibilidade e durabilidade dos dados.

III - Esta afirmação também é correta. O HDFS permite armazenar arquivos de grandes dimensões, que são divididos em blocos e distribuídos pelos nós do sistema. No entanto, uma vez criado, um arquivo no HDFS não pode ter partes dele reescritas ou modificadas; o arquivo deve ser reescrito no seu todo, caso haja a necessidade de alterações.

IV - Correto. O HDFS utiliza um modelo mestre/escravo no qual o NameNode atua como mestre, gerenciando o sistema de arquivos e coordenando o acesso aos arquivos pelos clientes, enquanto os DataNodes agem como escravos, armazenando e recuperando blocos de dados quando solicitado pelo NameNode.

Portanto, as afirmações II e IV estão corretas, o que faz da opção B a alternativa correta.",6643734
tópico 0,Processamento distribuído,"Questão:
Considerando o paradigma do processamento distribuído, assinale a opção que indica uma característica inerente a sistemas distribuídos que os distingue de sistemas de processamento centralizado.

A) Toda comunicação entre processos é realizada através de trocas de mensagens, o que pode incorrer em latências significativas.
B) Os sistemas distribuídos são inerentemente menos seguros que sistemas centralizados devido à maior superfície de ataque.
C) A escalabilidade é limitada pela capacidade de processamento da unidade central, que é um gargalo comum nesse tipo de arquitetura.
D) A falha de um serviço em um nó do sistema distribuído leva inevitavelmente à falha total do sistema, comprometendo toda a operação.
E) O processamento distribuído requer um único sistema operacional que controle todos os nós para manter a coerência e sincronização.

",A,"

Explicação dos itens:
A) Correta. Uma característica fundamental dos sistemas distribuídos é que os diferentes processos comunicam-se por envio de mensagens, o que implica em latências que não estão presentes em sistemas onde os processos comunicam-se através da memória compartilhada, como é mais comum em sistemas centralizados.

B) Incorreta. A segurança é um desafio tanto em sistemas distribuídos quanto em centralizados, mas não é uma característica inerente diferenciadora. Além disso, sistemas distribuídos podem empregar técnicas de replicação e particionamento para melhorar a segurança e a tolerância a falhas.

C) Incorreta. A escalabilidade é frequentemente uma vantagem de sistemas distribuídos, pois eles podem agregar recursos ao longo da rede, diferentemente de sistemas centralizados que estão sujeitos à capacidade de uma única unidade de processamento.
 
D) Incorreta. Sistemas distribuídos são projetados para tolerar falhas, e uma falha em um nó frequentemente não implica na falha de todo o sistema. Eles podem utilizar técnicas de redundância e recuperação para garantir a continuidade do serviço.

E) Incorreta. Sistemas distribuídos geralmente operam sobre múltiplos sistemas operacionais e não requerem um único sistema operacional para gerir todos os nós. Eles são projetados para operar mesmo em ambientes heterogêneos.",5417863
tópico 0,Soluções de big data: Arquitetura do ecossistema Spark,"Questão:

A utilização do Apache Spark em arquiteturas de big data tornou-se cada vez mais comum devido à sua capacidade de processar grandes volumes de dados de forma rápida e eficiente. Para o correto entendimento do seu ecossistema, é fundamental conhecer os componentes principais que constituem a sua arquitetura. Nesse contexto, analise as afirmações abaixo sobre os componentes do Apache Spark e assinale a opção correta.

I. O Spark Core é responsável por fornecer a plataforma de execução básica para todos os outros componentes do Spark. Ele fornece a API fundamental de RDDs (Resilient Distributed Datasets) que é a abstração de dados central do Spark.

II. O Spark SQL permite a execução de consultas SQL diretamente contra o Spark, integrando-se perfeitamente ao BI e às ferramentas de análise de dados, mas não permite o uso da API do dataframe que oferece abstrações mais altas sobre os RDDs.

III. O Spark Streaming é o módulo do Spark para processamento de streams de dados em tempo real. Ele pode processar dados em micro-batches e proporcionar resultados quase em tempo real.

IV. O MLlib é a biblioteca de machine learning do Spark, que fornece múltiplos tipos de algoritmos de aprendizagem de máquina, como classificação, regressão e clustering. Entretanto, só suporta algoritmos em batch, não sendo adequada para contextos de aprendizagem online.

V. O GraphX é o componente que introduz o processing de grafos e análises gráficas no Spark, fornecendo APIs para criação e transformação de grafos, bem como uma biblioteca de algoritmos gráficos comuns.

A) Todas as afirmações estão corretas.
B) Apenas as afirmações I, III e IV estão corretas.
C) Apenas as afirmações I, III e V estão corretas.
D) Apenas as afirmações I, II e IV estão corretas.
E) Apenas as afirmações II, III e V estão corretas.

",C,"

Explicação dos itens:

I. Correta. O Spark Core é de fato a base do ecossistema do Spark, fornecendo a API de RDDs que é utilizada por outros módulos e aplicações.

II. Incorreta. O Spark SQL permite sim o uso de consultas SQL, mas também suporta a API de dataframes, o que amplia a sua funcionalidade para além das capacidades padrões do SQL, integrando com a API do Spark.

III. Correta. O Spark Streaming fornece a capacidade de processamento de dados em tempo real através de micro-batches, possibilitando análises rápidas.

IV. Incorreta. O MLlib não só suporta algoritmos em batch, mas também possui funcionalidades evoluindo para o suporte de aprendizagem online e processamento em streaming, embora o seu forte seja o processamento batch.

V. Correta. O GraphX é parte do ecossistema Spark e oferece ferramentas para trabalhar com grafos e realizar análises, bem como algoritmos para processamento de graph.",661037
tópico 0,"Ingestão de dados estruturados, semiestruturados e não estruturados","Questão: Em uma empresa que lida com grande volume de dados oriundos de diferentes fontes, a equipe de ciência de dados é frequentemente desafiada a realizar a ingestão de dados estruturados, semiestruturados e não estruturados. Considerando as características desses diferentes tipos de dados, qual das seguintes opções descreve uma estratégia eficaz para o processo de integração e armazenamento dessas variedades de dados para análise posterior?

A) Utilizar um banco de dados relacional tradicional como única solução, independentemente do tipo de dado, garantindo integridade e consistência.

B) Adotar um sistema de armazenamento de dados exclusivamente baseado em Hadoop, tendo em vista a sua capacidade de armazenar grandes volumes de dados não estruturados.

C) Implementar um lago de dados (Data Lake) que possa armazenar dados de diferentes formatos e tipos, permitindo processamento e análise flexíveis.

D) Privilegiar o armazenamento de dados em planilhas eletrônicas, devido à familiaridade e facilidade de uso para os analistas de dados, independentemente das limitações de volume e complexidade.

E) Optar por silos de dados dedicados para cada tipo de dado, mantendo-os isolados para simplificar a governança e a segurança dos dados.

",C,"

A) É incorreta porque os bancos de dados relacionais tradicionais não são ideais para armazenar dados não estruturados ou semiestruturados em grande volume, podendo também limitar a flexibilidade do processamento desses dados.

B) É incorreta porque o Hadoop é eficaz para dados não estruturados ou semi-estruturados em grande volume, mas não é a solução ideal para gerenciar dados estruturados, que poderiam ser melhor acomodados em sistemas de bancos de dados relacional ou NoSQL, dependendo dos requisitos específicos.

C) É correta porque um Data Lake suporta o armazenamento de dados estruturados, semiestruturados e não estruturados em um único sistema, permitindo a flexibilidade na análise e no processamento de dados de diferentes formatos.

D) É incorreta porque planilhas eletrônicas não são escaláveis e não são adequadas para armazenar grandes volumes de dados, especialmente quando esses dados são não estruturados ou semiestruturados.

E) É incorreta porque a criação de silos de dados isolados pode aumentar a complexidade e impedir a capacidade de realizar análises integradas que cruzem informações de diferentes tipos de dados, além de complicar a gestão dos dados no longo prazo.",6230399
tópico 0,Ingestão de dados em streaming,"Questão: No contexto de processamento de dados em tempo real, diversos sistemas são projetados para lidar com a ingestão de fluxos de dados contínuos, conhecidos como streaming. Uma empresa de tecnologia precisa processar e analisar dados em tempo real de suas operações on-line. Eles esboçaram vários requisitos para o sistema, incluindo a capacidade de tolerar falhas, escalabilidade horizontal e baixa latência na entrega dos dados. Com base nesses requisitos, qual das seguintes opções de tecnologias e arquiteturas seria a mais adequada para atender às necessidades da empresa?

A) Utilizar um banco de dados relacional tradicional com a implementação de índices e particionamento de tabelas.

B) Implementação de um data warehouse para realizar a ingestão de dados, aplicando esquemas em estrela para a organização dos dados.

C) Aplicação de um sistema de arquivos distribuídos e batch processing para gerenciar o grande volume de dados.

D) Adoção de uma plataforma de processamento de streams como Apache Kafka, associada a um framework como Apache Flink ou Apache Storm.

E) Configuração de um sistema de banco de dados NoSQL orientado a documentos, otimizando consultas por meio de agregações pré-calculadas.

",D,"

Explicação:

A) Um banco de dados relacional tradicional pode não ser capaz de lidar com a alta velocidade e volume de um fluxo de dados em tempo real, além de apresentar dificuldades em termos de escalabilidade horizontal e tolerância a falhas para atender aos requisitos de dados em streaming.

B) Um data warehouse é uma solução adequada para análises complexas e armazenamento de grandes volumes de dados históricos, mas geralmente não é otimizado para processamento de dados em tempo real com baixa latência requerida pela ingestão de dados em streaming.

C) O sistema de arquivos distribuídos e batch processing é projetado para processar grandes volumes de dados em lotes e pode não fornecer a baixa latência necessária para aplicações em tempo real, pois não processa os dados à medida que chegam.

D) Apache Kafka é uma plataforma robusta para processamento de streams que oferece alta throughput, replicação automática para tolerância a falhas e capacidade de trabalhar com dados em tempo real. Quando utilizado em conjunto com frameworks como Apache Flink ou Apache Storm, pode atender aos requisitos de escalabilidade horizontal e baixa latência.

E) Bancos de dados NoSQL orientados a documentos são bons para cenários com esquemas de dados flexíveis e para otimização de consultas específicas, mas não são necessariamente projetados para lidar com o processamento de dados em tempo real e a ingestão de dados em streaming.",2054413
tópico 0,"Ingestão de dados estruturados, semiestruturados e não estruturados","Questão: No contexto de Big Data e análise de dados, é crucial entender as diferenças entre dados estruturados, semi-estruturados e não estruturados, pois cada um requer abordagens distintas de ingestão e processamento. Considerando as características inerentes a cada tipo de dado, assinale a opção que melhor descreve a ingestão de dados estruturados, semiestruturados e não estruturados.

A) Dados estruturados são coletados através de sistemas de gerenciamento de banco de dados relacional (RDBMS), não exigindo transformações adicionais, enquanto os dados semi-estruturados e não estruturados requerem processos avançados de normalização e limpeza antes da ingestão.

B) Dados não estruturados, como imagens e vídeos, são mais facilmente ingestados em sistemas analíticos do que dados estruturados, pois não necessitam de esquemas predefinidos ou modelagens iniciais.

C) A ingestão de dados semiestruturados, como XML e JSON, pode aproveitar esquemas flexíveis e aproveitar o processamento de texto para extrair informações, enquanto a ingestão de dados não estruturados frequentemente depende de análise de texto e reconhecimento de padrões.

D) Tanto dados semiestruturados quanto não estruturados podem ser armazenados em sistemas de arquivos distribuídos, como Hadoop, sem a necessidade de transformações, ao contrário dos dados estruturados, que geralmente necessitam de um esquema definido antes do armazenamento.

E) A ingestão de dados estruturados é mais complexa e demorada do que a de dados não estruturados, pois requer a implementação de algoritmos específicos para a análise de cada tipo de dado estruturado, enquanto que para os dados não estruturados é suficiente o uso de técnicas de Machine Learning genéricas.

",C,"

Explicação dos itens:

A) Esta alternativa está incorreta porque implica que os dados semi-estruturados e não estruturados sempre precisam de normalização e limpeza antes da ingestão, o que não é necessariamente verdadeiro. Depende dos requisitos e do contexto dos dados.

B) Esta alternativa está incorreta, pois imagens e vídeos (dados não estruturados) geralmente requerem processamento adicional como reconhecimento de padrões e análise de texto, além do fato de que a falta de um esquema predefinido pode aumentar a complexidade da ingestão.

C) Esta alternativa está correta porque reflete adequadamente o processo de ingestão para os diferentes tipos de dados. Dados semi-estruturados podem ser processados utilizando esquemas flexíveis e processamento de texto, enquanto dados não estruturados geralmente dependem de reconhecimento de padrões e análise de texto.

D) Esta alternativa está incorreta porque sugere que a ingestão de dados semi-estruturados e não estruturados não requer transformações, o que não é verdade em muitos casos. Além disso, dados estruturados podem sim ser armazenados em sistemas de arquivos distribuídos, mas também podem necessitar de um esquema definido.

E) Esta alternativa está incorreta porque sugere que a ingestão de dados estruturados é mais complexa do que a de dados não estruturados, o que geralmente não é o caso, já que os dados estruturados frequentemente seguem padrões e esquemas bem definidos, o que facilita a análise e a ingestão.",6957448
tópico 0,"Arquitetura de cloud computing para ciência de dados (AWS, Azure, GCP)","Questão: Em uma empresa que lida com grandes volumes de dados e tem a necessidade de implementar técnicas avançadas de ciência de dados e machine learning, a escolha da arquitetura de cloud computing é fundamental. Considerando os serviços oferecidos pelas principais provedoras de cloud - AWS, Azure e GCP - qual das seguintes opções melhor permite uma implementação eficiente de pipelines de dados e modelos de aprendizado de máquina, oferecendo uma combinação de serviços de armazenamento, processamento e análise de dados?

A) Utilização exclusiva de máquinas virtuais com CPUs otimizadas para computação para hospedar todas as etapas do pipeline de dados.
B) Configuração de um grande data lake usando o serviço de armazenamento de objetos e orquestração manual de contêineres para as tarefas de processamento de dados e machine learning.
C) Implementação de um ecossistema de análise de dados na nuvem que integra serviços de data warehousing, processamento de dados sem servidor e AutoML para treinamento e implementação automática de modelos.
D) Uso de um único serviço de banco de dados NoSQL escalável para armazenar e analisar todos os tipos de dados envolvidos, desde dados brutos até resultados de modelos de machine learning.
E) Adoção de uma infraestrutura de cloud híbrida que mantém dados sensíveis em servidores locais, enquanto aproveita a capacidade de processamento em nuvem para executar análises complexas e treinamento de modelos.

",C,"

Explicação dos itens:

A) Máquinas virtuais com CPUs otimizadas podem ser utilizadas para diversas tarefas, mas não são a solução mais eficiente para todas as etapas do pipeline de dados e podem não ser otimizadas para operações específicas de machine learning que requerem GPUs ou outras especializações.

B) Um data lake é útil para armazenar grandes volumes de dados, mas a orquestração manual de contêineres não é a abordagem mais eficiente, pois requer gerenciamento complexo e pode não fornecer as melhores ferramentas específicas para processamento de dados e aprendizado de máquina.

C) Esta opção oferece a melhor solução, pois integra um conjunto de serviços especiais como data warehousing para armazenamento e análise de dados (por exemplo, Amazon Redshift, Google BigQuery, Azure Synapse Analytics), processamento de dados sem servidor (por exemplo, AWS Lambda, Azure Functions, Google Cloud Functions) e plataformas de AutoML (por exemplo, AWS SageMaker, Google Cloud AutoML, Azure Machine Learning) para simplificar e automatizar o processo de treinamento e implementação de modelos.

D) Um banco de dados NoSQL é bom para lidar com grandes volumes e variedades de dados, mas não é o mais indicado para todas as funcionalidades de análise de dados e especialmente para o treinamento de modelos de machine learning, que podem requerer ambientes e ferramentas de processamento mais especializados.

E) A infraestrutura híbrida pode ser uma boa escolha para algumas empresas que precisam manter dados sensíveis no local, mas não aborda integralmente a eficácia dos pipelines de dados e das operações de machine learning, que se beneficiam da integração total com a cloud e serviços especializados.",2678580
tópico 0,Soluções de big data: Arquitetura do ecossistema Spark,"Questão: A arquitetura do ecossistema Apache Spark é conhecida por sua capacidade de processamento de grandes volumes de dados de maneira rápida e eficiente, o que é crucial para aplicações de Big Data. Considerando os componentes do ecossistema Spark, qual das seguintes afirmações melhor descreve o papel do Spark SQL na arquitetura do Spark?

A) O Spark SQL é responsável exclusivamente pela serialização e deserialização de dados, facilitando a integração com sistemas de armazenamento de dados NoSQL.
B) O Spark SQL fornece suporte para a execução de comandos SQL, permitindo a leitura de dados de diversas fontes, como HDFS, Apache Hive e bancos de dados relacionais.
C) O Spark SQL é um módulo dedicado ao processamento de grafos e otimização de consultas baseadas em teoria de grafos, funcionando de forma integrada com o GraphX.
D) O Spark SQL age como um sistema de mensageria dentro do ecossistema Spark, possibilitando a comunicação entre os diversos nodos do cluster.
E) O Spark SQL substitui o núcleo de processamento do Spark, o RDD (Resilient Distributed Dataset), fornecendo um novo modelo de dados baseado em grafos para o processamento distribuído.

",B," 

Explicação dos itens:
A) Alternativa incorreta, pois o Spark SQL não é responsável exclusivamente pela serialização e deserialização de dados. Sua função é mais abrangente, incluindo a execução de consultas SQL.
B) Alternativa correta. O Spark SQL é um componente do Apache Spark que permite a execução de SQL e a leitura de dados de várias fontes. Também suporta a integração com o DataFrame API para processamento de dados estruturados.
C) Alternativa incorreta. O módulo dedicado ao processamento de grafos dentro do ecossistema Spark é o GraphX, não o Spark SQL.
D) Alternativa incorreta. O sistema de mensageria, ou comunicação entre nodos em um cluster Spark, é geralmente gerenciado por outros componentes do ecossistema como o Apache Kafka, e não pelo Spark SQL.
E) Alternativa incorreta. O RDD é o principal abstração de dados do Spark, e o Spark SQL trabalha em conjunto com RDDs. Não fornece um novo modelo de dados baseado em grafos, sendo essa a função do GraphX.",2801359
tópico 0,Conceitos de processamento massivo e paralelo,"Questão:

A computação de alto desempenho (HPC) e os sistemas de processamento massivo e paralelo são fundamentais em diversos campos científicos e de negócios para processar grandes volumes de dados com eficiência. Uma das estratégias-chave para aumentar a capacidade de processamento é a utilização de clusters computacionais. Nesse contexto, qual dos seguintes itens NÃO está diretamente relacionado à otimização do processamento paralelo em clusters?

A) Balanceamento de carga entre os nós de processamento.
B) Utilização de uma rede de alta velocidade para comunicação entre nós.
C) Aumento da frequência de clock dos processadores individuais.
D) Implementação de algoritmos que reduzem a necessidade de comunicação síncrona.
E) Uso de sistemas de arquivos distribuídos para acesso concorrente aos dados.

",C,"

Explicação dos itens:

A) Balanceamento de carga entre os nós de processamento - Correto, pois o balanceamento de carga é crucial para garantir que todos os processadores estejam trabalhando eficientemente e evita que alguns nós fiquem ociosos enquanto outros estão sobrecarregados.

B) Utilização de uma rede de alta velocidade para comunicação entre nós - Correto, pois uma rede de comunicação rápida é essencial para reduzir o tempo de transmissão de dados entre os nós, o que pode ser um gargalo significativo no processamento paralelo.

C) Aumento da frequência de clock dos processadores individuais - Não relacionado diretamente, pois, embora o aumento da frequência de clock possa melhorar o desempenho de um único processador, não aborda os desafios inerentes ao processamento paralelo, como a comunicação e sincronização entre os nós.

D) Implementação de algoritmos que reduzem a necessidade de comunicação síncrona - Correto, pois algoritmos que minimizam a comunicação entre processadores podem melhorar significativamente o desempenho do sistema de processamento paralelo, reduzindo os atrasos causados pela espera de dados de outros nós.

E) Uso de sistemas de arquivos distribuídos para acesso concorrente aos dados - Correto, porque sistemas de arquivos distribuídos permitem que múltiplos nós de processamento acessem os dados simultaneamente sem os gargalos associados a um sistema de arquivos centralizado, o que é uma vantagem em configurações de processamento paralelo.",7583917
tópico 0,Ingestão de dados em lote (batch),"Questão: Em um cenário de análise de grandes volumes de dados, a ingestão de dados em lote (batch) é fundamental para processar informações que não exigem processamento em tempo real. Quando se trata de escolher a estratégia correta para ingestão de dados em batch, qual dos seguintes aspectos NÃO é considerado crucial para garantir a eficácia do processo?

A) A capacidade de lidar com cargas de trabalho variáveis e garantir que os processos de carga possam ser escalados conforme necessário.

B) A frequência de atualização dos dados, garantindo que os dados sejam atualizados com a cadência correta e sem intervalos de tempo que comprometam a análise.

C) O suporte ao processamento de diferentes formatos de dados, sejam estruturados, semi-estruturados ou não estruturados, mantendo a integridade e a precisão.

D) A localização geográfica dos dados, considerando a influência da proximidade dos servidores de dados na latência do processamento de dados em lote.

E) A velocidade da conexão de Internet do usuário final, pois a experiência do usuário em uma aplicação de consumo pode interferir diretamente na ingestão de dados em batch.

",E,"

A ingestão de dados em lote (batch) é uma operação que ocorre no backend de sistemas e aplicações, sendo independente da velocidade da conexão de Internet do usuário final. As alternativas A, B, C e D listam fatores diretamente relacionados à eficiência da ingestão de dados em lote: escalabilidade (A) é crucial para lidar com aumentos de dados; a frequência de atualização (B) garante que os dados sejam recentes para análise; a capacidade de processar diferentes formatos de dados (C) é importante para integridade e adaptação a diversas fontes de dados; e a localização geográfica dos dados (D) pode afetar a latência e o tempo necessário para transferir dados, mas não define a eficácia da ingestão de dados em um cenário onde os dados não precisam ser entregues em tempo real. A velocidade da conexão de Internet do usuário (E), enquanto importante para a experiência do usuário, não tem impacto direto na ingestão de dados em batch, que é uma operação de backend otimizada para eficiência e não depende da interatividade do usuário final.",9349723
tópico 0,Processamento distribuído,"Questão: No contexto de sistemas distribuídos, um dos principais desafios está relacionado ao processamento e gerenciamento de dados de maneira eficiente e confiável. Dentro desse espectro, o Teorema CAP é um conceito fundamental que afirma que, em um sistema distribuído, não é possível atender simultaneamente a mais de duas das seguintes garantias: Consistência (C), Disponibilidade (A) e Tolerância à Partição (P). Considerando esse teorema, um sistema distribuído que priorize Tolerância à Partição e Disponibilidade, pode comprometer qual característica em determinados momentos?

A) Latência
B) Consistência
C) Escalabilidade
D) Eficiência de espaço
E) Integridade dos dados

",B,"

Explicação dos itens:

A) Latência - A latência é o tempo de atraso entre o estímulo e a resposta. Embora a latência possa ser uma preocupação em sistemas distribuídos, ela não é uma garantia diretamente relacionada ao Teorema CAP.

B) Consistência - Esta é a característica que pode ser comprometida, conforme o Teorema CAP, em um sistema que prioriza Disponibilidade e Tolerância à Partição. Isso significa que todos os nós podem não ter a mesma versão dos dados ao mesmo tempo.

C) Escalabilidade - A escalabilidade é a capacidade do sistema de lidar com um crescimento significativo em sua carga de trabalho. Não é uma das garantias diretamente abordadas pelo Teorema CAP, embora possa ser afetada pelas características escolhidas.

D) Eficiência de espaço - A eficiência de espaço se refere à quantidade de dados que podem ser armazenados e é uma consideração de design, mais do que uma garantia de um sistema distribuído em relação ao Teorema CAP.

E) Integridade dos dados - A integridade dos dados se refere à correção e confiabilidade dos dados. Enquanto a consistência, como garantia do CAP, está relacionada à visão dos dados em todo o sistema distribuído, a integridade é mais ampla e fora do escopo do Teorema CAP em si.",7025236
tópico 0,Ingestão de dados em streaming,"Questão: 

No contexto de Processamento de Dados em Tempo Real, a ingestão de dados via streaming desempenha um papel crucial no fornecimento de insights imediatos para tomadas de decisão. Considerando um sistema de processamento distribuído, como o Apache Kafka, que característica NÃO é tipicamente esperada de um mecanismo eficiente de ingestão de dados em streaming?

A) Tolerância a falhas, permitindo a recuperação de dados em caso de falha do sistema.
B) Alta taxa de throughput para suportar o processamento de grandes volumes de dados.
C) Latência mínima entre os dados gerados na fonte e a disponibilidade para processamento.
D) Armazenamento de dados por longos períodos, possibilitando análises históricas retroativas.
E) Capacidade de modificação de esquema de dados em tempo real sem a interrupção do serviço.

",E,"

A opção E é incorreta, pois a capacidade de modificar esquemas de dados em tempo real geralmente não é uma característica dos sistemas de ingestão de dados em streaming. Isso porque os esquemas são frequentemente definidos antes da ingestão e mudanças em tempo real podem causar problemas de compatibilidade e integridade dos dados. Enquanto isso, as opções A, B, C e D são características típicas esperadas de um mecanismo eficiente de ingestão de dados em streaming. A tolerância a falhas (A) é essencial para garantir a confiabilidade, uma alta taxa de throughput (B) para lidar com grandes volumes de dados, baixa latência (C) para entrega quase em tempo real, e capacidade de armazenamento a longo prazo (D) para análises futuras.",5670186
tópico 0,Armazenamento de big data,"Questão: 
Considere uma empresa que precisa processar e analisar grandes volumes de dados não estruturados em tempo real. Para maximizar a eficiência no armazenamento e na recuperação desses dados, a equipe de TI está avaliando diferentes tecnologias de armazenamento. Dentre as opções abaixo, qual é a mais adequada para atender a essa necessidade específica de armazenamento de big data?

A) Sistema de arquivos tradicional (como NTFS ou HFS)
B) Armazenamento orientado a colunas (como Apache Cassandra)
C) Base de dados relacional (como MySQL ou PostgreSQL)
D) Sistema de arquivos de rede (como NFS)
E) Armazenamento em fita magnética

",B,"

Explicação dos itens:

A) Sistema de arquivos tradicional (como NTFS ou HFS) não é ideal para big data, especialmente não estruturados e em tempo real, devido a limitações na escalabilidade e desempenho com grandes volumes de dados.

B) Armazenamento orientado a colunas (como Apache Cassandra) é adequado para cenários de big data com necessidade de análise rápida e processamento de grandes quantidades de dados distribuídos, pois permite eficiência na leitura e escrita de grandes volumes de dados e escalabilidade horizontal.

C) Base de dados relacional (como MySQL ou PostgreSQL) tende a ser menos eficaz para grandes volumes de dados não estruturados e para análises em tempo real devido ao esquema rígido e a maior complexidade no escalonamento horizontal.

D) Sistema de arquivos de rede (como NFS) tem aplicação em compartilhamento de arquivos, mas não é otimizado para cenários de big data devido à latência de rede e problemas de escalabilidade com grandes volumes de dados.

E) Armazenamento em fita magnética é tipicamente usado para backup e arquivamento de longo prazo devido às suas características de baixo custo e alta capacidade, mas não é adequado para acesso em tempo real devido à sua natureza sequencial e baixas velocidades de acesso.",3702103
tópico 0,Processamento distribuído,"Questão: No contexto de sistemas distribuídos, o teorema CAP é um conceito fundamental que representa um limite teórico que afeta a concepção e o desempenho desses sistemas. De acordo com o teorema CAP, é impossível para um sistema distribuído garantir simultaneamente mais de duas das seguintes propriedades: Consistência (C), Disponibilidade (A) e Tolerância a Partição de Rede (P). Uma empresa está projetando um sistema distribuído para gerenciar dados financeiros em tempo real que devem ser consistentes e altamente disponíveis em várias localidades geográficas.

Nesse cenário, qual estratégia deve ser adotada pela empresa levando-se em conta as limitações impostas pelo teorema CAP?

A) Configurar o sistema para enfatizar a tolerância a partições de rede acima das outras propriedades, aceitando a possibilidade de dados eventualmente inconsistentes.

B) Projetar o sistema para ser principalmente consistente e tolerante a partições, mesmo que isso resulte em uma baixa disponibilidade durante falhas de rede.

C) Priorizar a consistência e a disponibilidade, utilizando técnicas para sincronização de dados após falhas de partições, reconhecendo que a tolerância a partições será limitada.

D) Ignorar a Tolerância a Partição de Rede oferecendo uma forte consistência e disponibilidade, uma vez que os dados financeiros não são críticos e podem tolerar alguma latência.

E) Construir uma abordagem híbrida que altera entre consistência e disponibilidade de acordo com a carga do sistema e a detecção de partições, mantendo alguma tolerância a partições.

",C,"

Explicação dos itens:

A) Esta alternativa é incorreta porque a questão enfatiza que o sistema deve ser consistente e altamente disponível. Optar pela tolerância a partições acima de consistência e disponibilidade contraria esses requisitos.

B) Esta alternativa também é incorreta porque, embora a consistência seja uma necessidade, a alta disponibilidade não pode ser sacrificada, especialmente para dados financeiros em tempo real.

C) Esta é a alternativa correta, pois prioriza duas das três propriedades que são mais críticas no cenário descrito: consistência e disponibilidade. A tolerância a partições é limitada, mas pode ser mitigada com técnicas de sincronização pós-falha, o que é uma abordagem viável para muitos sistemas financeiros.

D) Esta alternativa não é apropriada porque ignora a realidade do teorema CAP, que afirma que não é possível ter forte consistência e disponibilidade em um sistema que não é tolerante a partições; além disso, os dados financeiros são críticos e não podem aceitar a latência que acompanha a ausência de tolerância a partições.

E) A alternativa E sugere uma abordagem variável que não é garantida pelo teorema CAP, uma vez que a alteração entre consistência e disponibilidade não resolve o problema de limitação imposta pela necessidade de se renunciar a uma das três propriedades em qualquer momento.",1789180
tópico 0,Armazenamento de big data,"Questão:
O Big Data impõe desafios significativos em termos de armazenamento, processamento e análise de grandes conjuntos de dados. No contexto de armazenamento de Big Data, diferentes tecnologias podem ser aplicadas para atender a essas exigências. Sobre os sistemas de armazenamento distribuído, qual das seguintes alternativas descreve corretamente uma vantagem que contribui para o desempenho no processamento de Big Data?

A) Redução de latência em sistemas distribuídos torna desnecessário o uso de algoritmos de consistência.
B) Armazenamento em cache em um único nó melhora o desempenho do sistema como um todo, eliminando a necessidade de replicação.
C) A replicação de dados entre nós diferentes proporciona alta disponibilidade, resistência a falhas e melhoria no acesso de leitura.
D) Utilização exclusiva de discos rígidos mecânicos (HDDs) é a opção mais econômica e rápida para o armazenamento de Big Data.
E) Centralização do controle de armazenamento facilita o gerenciamento de metadados e aumenta a eficiência no processamento de consultas.

",C,"

Explicação dos itens:
A) Não é correto afirmar que a redução de latência em sistemas distribuídos torna desnecessário o uso de algoritmos de consistência. Na verdade, a consistência é uma parte essencial no design de um sistema distribuído para garantir que todos os nós tenham uma visão atualizada e precisa dos dados.

B) Armazenamento em cache em um único nó pode oferecer melhorias de desempenho localizadas, mas não melhora o desempenho do sistema como um todo, especialmente em um ambiente distribuído. A replicação ainda é importante para garantir a disponibilidade e tolerância a falhas.

C) Este item está correto. A replicação de dados entre diferentes nós do sistema permite que as operações de leitura sejam distribuídas entre vários pontos, melhorando o desempenho e garantindo que o sistema permaneça operacional mesmo após falhas de um ou mais nós, proporcionando alta disponibilidade.

D) Discos rígidos mecânicos (HDDs) são opções mais econômicas em relação aos discos de estado sólido (SSDs), mas eles são mais lentos em termos de velocidade de leitura e escrita. Portanto, não são geralmente considerados a opção mais rápida para armazenamento de Big Data.

E) A centralização do controle de armazenamento pode simplificar o gerenciamento de metadados, mas também pode se tornar um gargalo e um ponto único de falha, reduzindo a eficiência no processamento de consultas em um ambiente de Big Data que exige escalabilidade e alta disponibilidade.",4699284
tópico 0,Ingestão de dados em streaming,"Questão: No contexto do processamento de grandes volumes de dados, a ingestão de dados em streaming tornou-se essencial para a análise em tempo real e tomadas de decisão ágeis em empresas modernas. Suponha que uma companhia deseja implementar um sistema de ingestão e processamento de dados em streaming para monitorar eventos de transações financeiras em tempo real. Qual das seguintes tecnologias é mais adequada para ser utilizada como o componente central desse sistema, capaz de lidar com altas taxas de throughput e garantindo baixa latência no processamento desses eventos?

A) MySQL
B) Hadoop Distributed File System (HDFS)
C) Apache Kafka
D) Microsoft Excel
E) Redis

",C," 

Explicação dos itens:

A) MySQL: Embora seja um sistema de gerenciamento de banco de dados relacional popular, o MySQL não é otimizado para cenários de ingestão de dados em streaming de alta velocidade e volumes massivos, sendo mais adequado para transações ACID tradicionais e consulta de dados estruturados.

B) Hadoop Distributed File System (HDFS): O HDFS é um sistema de arquivos distribuídos que suporta o armazenamento de grandes conjuntos de dados, porém não é projetado para processamento em tempo real de streaming de dados, tendo sido desenvolvido para cargas de trabalho de processamento em lotes.

C) Apache Kafka: Essa é a opção correta, porque o Apache Kafka é uma plataforma distribuída de streaming de eventos, que foi projetada para lidar com altas taxas de throughput e latência baixa, sendo amplamente utilizada para construir pipelines de dados em tempo real e aplicações de streaming.

D) Microsoft Excel: O Microsoft Excel é uma ferramenta de planilha eletrônica que é usada para análise de dados, organização e cálculo, mas não é apropriado para ingestão de dados em streaming ou processamento de eventos em tempo real.

E) Redis: Apesar do Redis ser uma loja de estrutura de dados em memória com suporte para várias estruturas de dados, ele é mais comum para casos de uso como cache e armazenamento temporário e não é especializado na ingestão de dados em streaming como Kafka.",9264310
tópico 0,"Arquitetura de cloud computing para ciência de dados (AWS, Azure, GCP)","Questão:

Ao decidir sobre a arquitetura de cloud computing para uma solução de ciência de dados em grande escala, uma empresa deve ponderar vários fatores chave para garantir que a plataforma escolhida atenda às suas necessidades de processamento de dados, análises avançadas e aprendizado de máquina. Ao escolher entre os líderes de mercado — AWS, Azure e GCP —, quais são os aspectos mais críticos que devem ser considerados para a tomada de decisão? 

A) Apenas a disponibilidade de máquinas virtuais com GPUs.
B) A compatibilidade com as linguagens de programação mais utilizadas pelos cientistas de dados e engenheiros de machine learning.
C) Somente a política de preços e o custo total de propriedade.
D) A capacidade de integração com ferramentas e serviços de dados existentes, a disponibilidade de serviços gerenciados de machine learning e o suporte a contêinerização.
E) Exclusivamente o suporte técnico e os SLAs oferecidos.

",D," 

Breve explicação dos itens:

A) A disponibilidade de máquinas virtuais com GPUs é importante para tarefas intensivas em computação, como treinamento de modelos de aprendizado de máquina, mas não é o único aspecto a ser considerado.

B) A compatibilidade com linguagens de programação é um fator chave, já que a maioria dos cientistas de dados utiliza linguagens como Python e R, mas apenas isso não abrange todas as necessidades de uma arquitetura de cloud para ciência de dados.

C) Enquanto a política de preços e custo total são aspectos críticos da decisão, focar somente no aspecto financeiro pode resultar na escolha de uma plataforma que não atenda bem a todas as necessidades técnicas e escalabilidade requeridas.

D) Este item captura uma visão mais holística, considerando tanto a necessidade de integração com ferramentas existentes quanto a disponibilidade de serviços gerenciados específicos para ciência de dados e aprendizado de máquina, além do suporte a contêinerização, o que é vital para ambientes de desenvolvimento e produção modernos e escaláveis.

E) O suporte técnico e os Acordos de Nível de Serviço (SLAs) são importantes, mas não são os únicos aspectos críticos. Uma infraestrutura robusta de ciência de dados demanda mais do que apenas suporte e garantias de tempo de funcionamento.",4189621
tópico 0,Soluções de big data: Arquitetura do ecossistema Spark,"Questão: Considerando as demandas de processamento de grandes volumes de dados em tempo real e em batch, o Apache Spark se destaca como uma ferramenta chave no ecossistema de Big Data. Sobre a arquitetura do Spark e seus componentes, avalie as seguintes afirmações:

I. O Spark Core é a base fundamental da plataforma, que provê a execução em memória e as capacidades de computação distribuída.
II. O Spark SQL permite a execução de consultas SQL diretamente sobre dados estruturados, mas não é capaz de interagir com fontes de dados externas, como Hive ou bancos de dados relacionais.
III. O Spark Streaming facilita o processamento de streams de dados em tempo real, porém não permite a integração com sistemas de filas de mensagens como Kafka ou Flume.
IV. O MLlib é uma biblioteca do Spark que fornece algoritmos de machine learning escaláveis, mas é restrita ao processamento de dados em lote, não sendo aplicável ao streaming de dados.

Assinale a alternativa que contém todas as afirmações corretas:

A) I, II, III e IV
B) I e II apenas
C) I e IV apenas
D) I apenas
E) III apenas

",D," 

Explicação dos itens:

I. Correta. O Spark Core é realmente a fundação do Apache Spark, que fornece funcionalidades básicas de distribuição e processamento de dados em memória.

II. Incorreta. O Spark SQL permite a execução de consultas SQL e pode sim interagir com fontes de dados externas, incluindo o Apache Hive e bancos de dados relacionais através de JDBC/ODBC.

III. Incorreta. O Spark Streaming é projetado para a integração com sistemas de filas de mensagens, como o Apache Kafka e o Flume, permitindo o processamento de streams de dados em tempo real e oferecendo uma interação fluida com estes sistemas.

IV. Incorreta. Embora o MLlib seja focado no processamento de dados em batch para machine learning, sua integração com o Spark Streaming permite também a aplicação de algoritmos de aprendizado de máquina em dados em movimento, facilitando a criação de sistemas capazes de aprender em tempo real.
",1684826
tópico 0,"Ingestão de dados estruturados, semiestruturados e não estruturados","Questão: Em ambientes de Big Data, dados podem ser classificados como estruturados, semiestruturados ou não estruturados, com base em sua organização e facilidade de processamento e análise. Supondo um sistema de análise de dados corporativos que lida com uma variedade de fontes de dados, qual das opções a seguir melhor caracteriza cada um destes tipos de dados?
   
A) Dados estruturados são aqueles que seguem um esquema fixo, como tabelas SQL e planilhas, enquanto os dados semiestruturados, como JSON e XML, possuem uma organização flexível. Dados não estruturados são completamente aleatórios sem qualquer identificação previamente definida, como sinais de sensores.
   
B) Dados estruturados são integralmente organizados em campos definidos, como registros financeiros em sistemas ERP. Dados semiestruturados, como e-mails, combinam elementos estruturados e não estruturados. Dados não estruturados não possuem um modelo de dados claro ou esquema, como fotos e gravações de vídeo.
   
C) Dados estruturados e semiestruturados são similares, ambos possuem esquemas pré-definidos e são facilmente indexados, como arquivos CSV e HTML. Dados não estruturados são dados binários sem metadados que descrevem seu formato, como executáveis de computador.
   
D) Dados estruturados são sempre em texto puro, o que permite fácil processamento e análise, enquanto dados semiestruturados são tipicamente em formatos binários. Dados não estruturados não seguem nenhum padrão e podem ser encontrados em logs de sistemas.
   
E) Dados estruturados não necessitam de transformação ou processamento antes da análise, como é o caso de bases de dados relacionais. Dados semiestruturados, entretanto, são de difícil manipulação, como arquivos de áudio e vídeo, e dados não estruturados são compostos principalmente por texto, como e-mails e relatórios.

",B,"
Alternativa A: Incorreta porque descreve os dados não estruturados como completamente aleatórios e sem identificação, o que não é necessariamente verdade. Dados não estruturados podem ter um tipo de organização, mas sem um modelo fixo ou formal.

Alternativa B: Correta pois captura a essência de cada tipo de dado. Dados estruturados têm um esquema rígido, como tabelas de banco de dados. Dados semiestruturados são um meio-termo, como e-mails, que apresentam partes estruturadas como campos de cabeçalho, mas também conteúdo livre e não estruturado no corpo da mensagem. Dados não estruturados não possuem uma estrutura de dados clara e podem ser qualquer coisa desde imagens a documentos textuais.

Alternativa C: Incorreta porque sugere que dados estruturados e semiestruturados são similares na indexação e organização, o que não é correto. Também caracteriza os dados não estruturados de maneira equívoca ao associá-los com executáveis de computador, que não são típicos exemplos de dados neste contexto.

Alternativa D: Incorreta porque implica que todos os dados estruturados são em texto puro e ignora que dados semiestruturados também podem estar em formatos de texto, como XML ou JSON.

Alternativa E: Incorreta porque associa dados semiestruturados a arquivos de áudio e vídeo, que são na realidade exemplos de dados não estruturados, e identifica dados não estruturados principalmente como texto, o que ignora outros tipos de dados não estruturados como imagens e vídeos.",6193173
tópico 0,Conceitos de processamento massivo e paralelo,"Questão: Em um cenário onde uma empresa de grande porte deseja processar uma vasta quantidade de dados coletados de suas operações globais, a utilização de processamento massivo e paralelo torna-se vital para otimizar a análise desses dados de forma eficiente. Considerando as arquiteturas e paradigmas comuns neste contexto, qual das seguintes opções descreve uma tecnologia ou técnica inapropriada para o processamento massivo e paralelo de dados?

A) Uso da arquitetura MapReduce para dividir o processamento de dados em tarefas menores distribuídas por diversos nós.
B) Aplicação do modelo de programação paralela MPI (Message Passing Interface) para coordenar operações entre diferentes processadores.
C) Implementação de um sistema de processamento em tempo real, como o Apache Storm, para lidar com fluxos de dados.
D) Utilização de um cluster de servidores com o sistema de armazenamento distribuído HDFS (Hadoop Distributed File System) para tolerância a falhas.
E) Emprego de uma base de dados relacional tradicional com um único servidor para realizar operações massivas de join e agregação em tempo real.

",E,"

A alternativa E é incorreta para o contexto de processamento massivo e paralelo, pois uma base de dados relacional tradicional em um único servidor pode se tornar um gargalo significativo nas operações de join e agregação em grandes volumes de dados, com limitações severas em termos de escalabilidade e desempenho. As demais alternativas são técnicas apropriadas para processamento massivo e paralelo:

A) A arquitetura MapReduce é um paradigma de processamento que se adequa muito bem ao processamento distribuído e é comumente utilizado para processar grandes volumes de dados.
B) MPI é um protocolo padrão usado para comunicação entre nós em uma arquitetura de processamento paralelo, facilitando a coordenação das tarefas distribuídas.
C) Apache Storm é uma plataforma de processamento de dados em tempo real que permite processar fluxos de dados de modo distribuído e eficiente.
D) HDFS é um sistema de arquivos distribuído que é projetado para armazenar e gerenciar grandes conjuntos de dados em clusters de servidores, oferecendo alta disponibilidade e tolerância a falhas.",9324775
tópico 0,Ingestão de dados em lote (batch),"Questão: No contexto de um sistema de processamento de dados em larga escala, a ingestão de dados em lote (batch) desempenha um papel fundamental na pipeline de dados. Qual das alternativas abaixo descreve de forma mais correta o processo de ingestão de dados em lote?

A) A ingestão de dados em lote é caracterizada por seu tempo real e baixa latência, sendo ideal para sistemas que requerem decisões instantâneas baseadas em grandes volumes de dados.

B) Consiste na inserção de dados de forma contínua e incremental, onde cada pequeno lote de dados é processado em paralelo, ideal para cenários onde o volume de dados não é previsível.

C) É o método de processamento e carregamento de grandes volumes de dados, onde os dados são coletados em um período de tempo e processados em conjunto, sendo mais adequado quando os dados podem ser reunidos e processados em intervalos de tempo regulares.

D) A ingestão de dados em lote requer o uso exclusivo de bases de dados NoSQL devido à sua capacidade de lidar com altos volumes de escrita e leitura, tornando-a inadequada para sistemas que utilizam bancos de dados relacionais.

E) Refere-se a uma abordagem onde todos os dados são mantidos temporariamente em memória antes de serem escritos em disco, o que garante a integridade dos dados mas introduz significativa latência no processamento.

",C,"

Explicação dos itens:

A) Incorreto, pois descreve características de ingestão de dados em tempo real (streaming), que se destaca pela baixa latência e processamento imediato de dados.

B) Incorreto, visto que descreve uma forma de processamento de dados mais alinhada com a ingestão em micro-lotes, que se situa entre o processamento em tempo real e o processamento em lote.

C) Correto, pois a ingestão de dados em lote envolve coletar e processar dados em grandes quantidades durante períodos de tempo definidos, característico de sistemas que não necessitam de análises em tempo real.

D) Incorreto, porque a ingestão de dados em lote não é exclusiva a bancos de dados NoSQL. Bancos de dados relacionais também podem ser usados em processamentos em lote, dependendo dos requisitos do sistema.

E) Incorreto, uma vez que manter todos os dados temporariamente em memória antes de escrever em disco não é uma característica específica da ingestão de dados em lote. Além disso, isso não necessariamente introduz latência significativa no processamento, e a latência não é um aspecto central da ingestão em lote.",2124351
tópico 0,Processamento distribuído,"Questão: Em um contexto de processamento distribuído, a consistência de dados entre os nós em um cluster é fundamental para garantir que todos os processos tenham uma visão unificada e atualizada da informação. Considere um sistema que utiliza processamento distribuído para gerenciar grandes volumes de dados. Qual dos seguintes mecanismos é menos adequado para promover a consistência forte em um ambiente com alto volume de transações distribuídas?

A) Uso de um protocolo de consenso como o Raft, que garante que todos os nós concordem com a ordem e o resultado das transações.

B) Implementação de um serviço de locking distribuído, que previne condições de corrida e garante que as atualizações sejam feitas de forma atômica.

C) Aplicação de técnicas de replicação síncrona, onde cada operação de escrita é realizada simultaneamente em múltiplos nós.

D) Utilização de uma estratégia de eventual consistência, que permite atrasos na propagação de atualizações mas reduz a latência em operações de leitura e escrita.

E) Emprego de mecanismos de quorum para leitura e escrita, assegurando que a maioria dos nós concorde com a versão mais atual dos dados antes que as operações sejam efetivadas.

",D," 

A alternativa correta é a letra D. Uma estratégia de eventual consistência, apesar de ser bastante efetiva em sistemas onde a latência é um fator crítico e podem ser tolerados atrasos na propagação das atualizações, é menos adequada para ambientes que requerem consistência forte, especialmente com alto volume de transações distribuídas. Nas alternativas A, B, C e E, os mecanismos apresentados são projetados para garantir a consistência forte, pois eles asseguram que todos os nós no cluster tenham uma visão coerente e atualizada dos dados, seja através de consenso, locking, replicação síncrona ou quoruns de leitura e escrita.",8801138
tópico 0,"Ingestão de dados estruturados, semiestruturados e não estruturados","Questão: Em um cenário de Big Data, a ingestão de dados pode envolver diferentes tipos de formatos. Cada formato de dado requer uma abordagem específica para efetivamente ser processado e analisado. Assinale a alternativa que melhor descreve o tratamento apropriado para a ingestão de dados estruturados, semiestruturados e não estruturados dentro de um ambiente de análise de dados.

A) Dados estruturados devem ser normalizados e armazenados em sistemas de arquivos tradicionais, enquanto dados semiestruturados e dados não estruturados podem ser diretamente depositados em Data Lakes sem qualquer processamento inicial.

B) Dados estruturados são comumente armazenados em bancos de dados relacionais, dados semiestruturados em NoSQL ou sistemas como Hadoop, e dados não estruturados requerem técnicas avançadas de processamento de linguagem natural e reconhecimento de padrões antes do armazenamento.

C) Todos os dados, seja estruturados, semiestruturados ou não estruturados, devem ser convertidos para um formato relacional antes de serem armazenados para permitir uma análise unificada dentro da plataforma de Big Data.

D) A gestão de todos os tipos de dados deve ser realizada por meio de uma única plataforma de gestão de dados mestres (MDM), independentemente de sua estrutura, para garantir consistência e governança.

E) Dados estruturados e semiestruturados podem ser eficientemente armazenados em Data Warehouses otimizados para consultas, enquanto dados não estruturados muitas vezes exigem armazenamento em sistemas que suportam grandes volumes de dados, como Data Lakes, com processamento posterior para extração de informações relevantes.

",B," 

Explicação:

A) Esta alternativa está incorreta porque normalização e armazenamento de dados estruturados em sistemas de arquivos tradicionais não é a prática comum em ambientes de Big Data. Além disso, dados semiestruturados e dados não estruturados frequentemente requerem algum tipo de processamento para se tornarem úteis.

B) Esta alternativa é a mais adequada pois reflete bem a prática comum de ingestão de dados em Big Data. Dados estruturados geralmente são armazenados em bancos de dados relacionais devido à sua natureza organizada. Dados semiestruturados podem ser melhor gerenciados por sistemas NoSQL ou plataformas como o Hadoop que são equipadas para lidar com dados que não seguem um esquema rígido. Dados não estruturados, como texto e imagem, necessitam de processamentos adicionais, como técnicas de NLP ou machine learning para reconhecimento de padrões, respectivamente, antes de serem armazenados.

C) Esta alternativa não é correta porque converter todos os tipos de dados para um formato relacional não é eficiente ou prático. Cada tipo de dado pode precisar de uma estratégia de armazenamento que aproveite suas características específicas para análise em Big Data.

D) Utilizar uma única plataforma de MDM para todos os tipos de dados não é um método recomendado. Dados estruturados, semiestruturados e não estruturados têm necessidades distintas de armazenamento e gerenciamento que geralmente são melhor atendidas por soluções especializadas.

E) Enquanto esta alternativa reconhece que dados não estruturados podem ser armazenados em Data Lakes, ela erra ao afirmar que dados estruturados e semiestruturados são melhor geridos quando armazenados em Data Warehouses otimizados para consultas. Isso não leva em consideração a necessidade de flexibilidade no armazenamento e processamento de dados semiestruturados.",9747222
tópico 0,Armazenamento de big data,"Questão: Considerando os sistemas de gerenciamento de bases de dados projetados para tratar grandes volumes de dados de diversas naturezas, também conhecidos como sistemas de Big Data, qual das seguintes tecnologias NÃO é adequada para o processamento e armazenamento de dados em cenários que exigem alta escalabilidade horizontal, tolerância a falhas e processamento distribuído?

A) Apache Hadoop

B) NoSQL Cassandra

C) Oracle RDBMS

D) MongoDB

E) Google Bigtable

",C,"

Justificativas dos itens:

A) Apache Hadoop: Correto para cenários de Big Data, pois é uma framework que permite o processamento distribuído de grandes conjuntos de dados em clusters de computadores usando modelos de programação simples.

B) NoSQL Cassandra: Correto para cenários de Big Data, uma vez que é um sistema de armazenamento NoSQL distribuído projetado para lidar com grandes quantidades de dados em vários servidores.

C) Oracle RDBMS: Incorreto para este contexto específico, apesar de ser um sistema de gerenciamento de banco de dados robusto, ele é mais indicado para cenários que requerem a integridade de transações e consultas complexas com SQL. Este tipo de sistema não foi projetado primariamente para alta escalabilidade horizontal e pode ter limitações de desempenho em cenários de Big Data quando comparado às outras tecnologias listadas.

D) MongoDB: Correto, uma vez que é um banco de dados NoSQL que oferece alto desempenho, alta disponibilidade e fácil escalabilidade em cenários de Big Data.

E) Google Bigtable: Correto, pois é um serviço de armazenamento de dados NoSQL na nuvem, projetado para lidar com grandes volumes de dados, proporcionando alto desempenho e escalabilidade.",465080
tópico 0,Conceitos de processamento massivo e paralelo,"Questão: No contexto de sistemas de processamento massivo e paralelo, tecnologias como MapReduce e modelos como o Paradigma de Passagem de Mensagens têm um papel fundamental. Considerando esses conceitos e suas aplicações práticas em ambientes onde grandes volumes de dados são processados, analise as seguintes afirmações:

I. MapReduce é um modelo de programação e uma framework de processamento que permite a manipulação de grandes conjuntos de dados em clusters de computadores utilizando uma abordagem paralela e distribuída.

II. O Paradigma de Passagem de Mensagens envolve a execução de vários processos ou threads em paralelo, onde a comunicação e a sincronização entre eles é realizada exclusivamente pelo envio de mensagens.

III. O modelo de MapReduce elimina a necessidade de sincronização entre tarefas, uma vez que as operações de map e reduce são inerentemente atômicas e isoladas.

IV. O Paradigma de Passagem de Mensagens é altamente dependente de sistemas de compartilhamento de memória para estabelecer comunicação, sendo menos eficaz em arquiteturas distribuídas.

Está(ão) correta(s) apenas a(s) afirmação(ões):

A) I e II
B) II e III
C) I, II e III
D) II, III e IV
E) I e IV

",A,"

Explicação dos itens:

I. Correta. MapReduce é um modelo de programação e uma estrutura (framework) projetada para processar grandes conjuntos de dados de forma distribuída e paralela em clusters de computadores, gerenciando a distribuição das tarefas e a coleta dos resultados.

II. Correta. O Paradigma de Passagem de Mensagens é uma estratégia de programação paralela onde os processos ou threads comunicam-se e sincronizam-se por meio do envio de mensagens, sem a necessidade de compartilhamento de memória.

III. Incorreta. MapReduce requer mecanismos de sincronização, especialmente para a fase de reduce, na qual os resultados parciais do map são combinados. As operações não são atômicas e isoladas por natureza; elas dependem da infraestrutura do framework para gerenciar o estado e o progresso das tarefas.

IV. Incorreta. O Paradigma de Passagem de Mensagens não depende de sistemas com memória compartilhada e é, na verdade, bem adaptado para arquiteturas distribuídas, pois a comunicação ocorre através do envio de mensagens, o que pode ser feito sobre redes de computadores.",5878740
tópico 0,Ingestão de dados em lote (batch),"Questão: No contexto de um Data Warehouse em uma organização de grande porte, a ingestão de dados em lote é frequentemente utilizada para alimentar os sistemas de análise com informações provenientes de fontes heterogêneas. Considerando os princípios e as melhores práticas para a ingestão de dados em lote, qual das seguintes opções melhor descreve uma abordagem eficiente para o processamento e incorporação de grandes volumes de dados em uma base regular?

A) Ingerir os dados em lotes pequenos e frequentes, para minimizar o impacto no desempenho do sistema e garantir a consistência dos dados.

B) Utilizar a técnica de ingestão em tempo real para todos os tipos de dados, assegurando que a base de dados esteja sempre atualizada e pronta para análises instantâneas.

C) Implementar um mecanismo de processamento em lote que seja executado em horários de pico de atividades, assegurando que os dados serão imediatamente disponíveis para os usuários.

D) Empregar um agendamento de ingestão de dados em lote durante janelas de baixa atividade do sistema, para evitar sobrecargas e otimizar o processo de ETL (Extract, Transform, Load).

E) Priorizar a ingestão em lote de dados não estruturados, enquanto dados estruturados devem ser processados individualmente para garantir a integridade e precisão.

",D," 

Explicação dos itens:

A) Ingerir dados em lotes pequenos e frequentes pode ser benéfico para alguns casos de uso, mas não necessariamente reflete uma abordagem eficiente para o processamento de grandes volumes de dados em um Data Warehouse, pois pode aumentar a complexidade do gerenciamento de dados e a carga no sistema de processamento.

B) A utilização da técnica de ingestão em tempo real é ideal para dados que exigem atualização instantânea, mas pode não ser prática ou custo-efetiva para todos os tipos de dados em uma organização de grande porte, especialmente quando o volume é muito alto.

C) Executar processos de ingestão em horários de pico contraria as práticas recomendadas, uma vez que não só degradaria o desempenho do sistema durante os períodos de alta demanda, mas também poderia prejudicar a disponibilidade dos dados para os usuários.

D) O agendamento de ingestão de dados durante períodos de baixa atividade no sistema permite que o processamento de ETL seja realizado com menor impacto no desempenho geral do sistema e reduz a probabilidade de sobrecargas, o que o torna uma prática eficiente para o processamento de informações em lote.

E) A priorização de ingestão de dados com base em seu formato (estruturado ou não-estruturado) não é uma estratégia eficaz generalizada. Ambos os tipos de dados podem ser ingeridos em lote e a melhor prática deve ser determinada pelo caso de uso específico e requisitos de processamento de dados.",6952005
tópico 0,Ingestão de dados em streaming,"Questão: Em um ecossistema de Big Data, a ingestão de dados em streaming é um componente vital para garantir que fluxos contínuos de dados sejam coletados, processados e disponibilizados para análise em tempo quase real. Uma empresa deseja implementar um sistema de ingestão de dados em streaming que seja capaz de lidar com picos de carga e garantir a durabilidade dos dados, mesmo na presença de falhas de componente. Qual das seguintes tecnologias seria mais apropriada para atender a essa demanda?

A) Sistema de arquivos tradicional montado em um único servidor, com scripts personalizados para lidar com a ingestão de dados.
B) Banco de dados relacional clássico com alto desempenho em escrita e leitura, usando esquemas otimizados para operações transacionais.
C) Solução de banco de dados NoSQL orientada a colunas, projetada para escalonamento horizontal e forte consistência de gravação em múltiplos nós.
D) Plataforma de processamento de streams como Apache Kafka, que oferece alta disponibilidade, tolerância a falhas e capacidade de lidar com volumes variáveis de dados.
E) Sistema de armazenamento em nuvem baseado em objetos com capacidade de expansão automática e política de replicação entre vários data centers.

",D," 

Explicação dos itens:
A) Um sistema de arquivos tradicional não é projetado para lidar com a ingestão de dados em streaming de alta velocidade e pode se tornar um gargalo em termos de I/O e escalabilidade, além de não oferecer a resistência necessária a falhas.
B) Um banco de dados relacional pode enfrentar problemas de desempenho sob altas cargas de escrita de um fluxo de streaming, e esquemas otimizados para operações transacionais geralmente não são ideais para lidar com as grandes quantidades de dados não estruturados ou semi-estruturados comumente encontrados em cenários de streaming.
C) Bancos de dados NoSQL orientados a colunas podem ser adequados para casos de uso específicos, onde a análise em tempo real não é necessária, e eles se concentram mais na eficiência de leitura em grandes volumes de dados, do que em ingestão de alta velocidade.
D) Apache Kafka é uma plataforma distribuída projetada especificamente para a ingestão e processamento de grandes volumes de dados em streaming. Sua arquitetura garante alta disponibilidade, durabilidade dos dados e tolerância a falhas, tornando-a uma escolha apropriada para esse cenário.
E) Enquanto os sistemas de armazenamento em nuvem baseados em objetos são altamente escaláveis e confiáveis, eles não são normalmente otimizados para a ingestão de alta velocidade de dados em streaming e podem apresentar latências maiores na disponibilização dos dados para processamento em tempo real.",8426326
tópico 0,Soluções de big data: Arquitetura do ecossistema Spark,"Questão: Em um cenário de processamento de big data onde o tempo de resposta é um fator crítico, uma companhia opta por utilizar o Apache Spark em sua arquitetura devido às suas características de alta performance para processamento em memória. Considerando o ecossistema do Spark, qual componente desempenha um papel fundamental na otimização de tarefas e na construção de planos de execução lógica eficientes, permitindo o processamento de dados de forma mais rápida e eficaz?

A) Spark SQL
B) Spark Streaming
C) Spark Core
D) MLlib
E) GraphX

",A," 

Explicação dos itens:

A) Spark SQL: Correto. É o módulo do Apache Spark que suporta consultas SQL e a execução de operações SQL-like sobre DataFrames, otimizando a execução de tarefas através do Catalyst Optimizer que cria planos de execução lógica eficientes.

B) Spark Streaming: Incorreto. Este componente permite o processamento de fluxos contínuos de dados em tempo real, mas não está diretamente relacionado com a otimização de planos de execução de tarefas.

C) Spark Core: Incorreto. Embora seja o módulo base do Spark que fornece funcionalidades de processamento distribuído fundamentais, como abstração de RDDs e scheduling de tarefas, não é o principal responsável pela otimização de planos de execução lógica.

D) MLlib: Incorreto. MLlib é a biblioteca de machine learning do Spark, que fornece várias ferramentas para algoritmos de machine learning, mas não atua diretamente na otimização de planos de execução para processamento de dados.

E) GraphX: Incorreto. É o componente do Spark para processamento de grafos e análise de grafos distribuídos. Embora ofereça funcionalidades valiosas, seu foco não é a otimização de tarefas e planos de execução lógica no ecossistema do Spark.",6168943
tópico 0,"Arquitetura de cloud computing para ciência de dados (AWS, Azure, GCP)","Questão:
Considere que uma empresa de ciência de dados está avaliando a migração de seu ambiente de trabalho on-premises para uma solução de cloud computing, a fim de otimizar a execução de suas cargas de trabalho de Big Data e machine learning. Entre as opções de provedores de serviços em nuvem, estão AWS (Amazon Web Services), Azure (Microsoft) e GCP (Google Cloud Platform). Tendo em vista os serviços específicos para ciência de dados oferecidos por esses provedores, qual das seguintes alternativas apresenta uma associação correta entre o serviço e seu respectivo provedor?

A) Azure Machine Learning Studio - GCP
B) Google BigQuery - AWS
C) Amazon SageMaker - GCP
D) Azure HDInsight - Microsoft
E) Google Cloud Dataflow - AWS

",D," 
Alternativa A está incorreta porque o Azure Machine Learning Studio é um serviço da Microsoft, não do GCP. A alternativa B também é falsa, uma vez que o Google BigQuery é um serviço de big data no GCP e não no AWS. A alternativa C está errada porque o Amazon SageMaker é um serviço de machine learning da AWS, não do GCP. A alternativa E é incorreta já que o Google Cloud Dataflow é um serviço para processamento de dados em tempo real e batch do GCP e não da AWS. Portanto, a alternativa D é correta, pois o Azure HDInsight é um serviço de processamento de Big Data em nuvem oferecido pela Microsoft, que inclui ferramentas populares como Hadoop, Spark, Kafka e HBase.",5515385
tópico 0,Ingestão de dados em lote (batch),"Questão: Em um sistema de processamento Big Data, a ingestão de dados em lote desempenha um papel crucial na incorporação de vastas quantidades de dados para posterior análise e armazenamento em sistemas como Hadoop ou data warehouses. A implementação de uma solução eficiente de ingestão de dados em lote requer considerações apropriadas sobre as características dos dados e o contexto de uso. Suponha uma empresa que precisa incorporar dados históricos de transações financeiras diariamente de diferentes regionais para um data warehouse centralizado para uma análise de tendências de longo prazo. Qual das seguintes estratégias seria a mais adequada para a ingestão de dados em lote neste cenário?

A) Utilização de streaming de dados em tempo real para proporcionar uma visão atualizada e constante das transações.

B) Emprego de uma ferramenta de ETL (Extract, Transform, Load) para extrair dados ao longo do dia com cargas incrementais frequentes.

C) Implementação de um processo de ETL agendado para a noite, quando os sistemas de produção estão menos ativos, para transferir todos os dados do dia em um único lote.

D) Configuração de uma API de serviços web para permitir consultas pontuais aos dados de transações quando necessárias.

E) Adoção de uma abordagem de Change Data Capture (CDC) para rastrear e replicar apenas as mudanças ocorridas nas bases de dados, em tempo real.

",C,"

A alternativa correta é a letra C, uma vez que se adéqua ao contexto proposto de carregar grandes volumes de dados históricos de transações financeiras diariamente e na otimização do uso dos recursos, uma vez que a carga seria feita durante o período de baixa atividade nos sistemas de produção, reduzindo assim o impacto no desempenho do sistema e garantindo que a análise de tendência possa ser feita com o conjunto completo de dados do dia:

- A alternativa A é inadequada para o cenário, pois o foco é nos dados históricos e não requer uma visão atualizada constante dos dados, como seria o caso de uma análise em tempo real.

- Alternativa B também não é a mais adequada porque as cargas incrementais frequentes não são necessárias para a análise de tendências de longo prazo e podem sobrecarregar o sistema ao longo do dia.

- A alternativa D é imprópria para a ingestão de grandes lotes de dados históricos já que se refere mais a consultas pontuais, o que não se alinha com o objetivo de análise de tendências de longo prazo.

- A alternativa E está mais direcionada para situações onde é necessário manter sincronia em tempo real entre bases de dados, não sendo o ideal para processamento em lote diário de grandes volumes de dados para análise de tendência.",5434443
tópico 0,Soluções de big data: Arquitetura do ecossistema Spark,"Questão: Em um ambiente de Big Data que emprega o Apache Spark como framework principal para processamento de dados em larga escala, qual dos seguintes componentes é responsável por realizar a orquestração de recursos distribuídos, oferecendo uma abstração para o gerenciamento de tarefas computacionais sobre um cluster?

A) Spark SQL
B) Spark Streaming
C) Spark Core
D) Spark MLlib
E) GraphX

",C,"

Explicação dos itens:

A) Spark SQL é um módulo do Spark para processamento de dados estruturados. Ele permite a execução de queries SQL e pode ser utilizado para trabalhar com dados estruturados e semi-estruturados. Portanto, não é o componente responsável pela orquestração de recursos.

B) Spark Streaming é um módulo do Spark projetado para o processamento de streams de dados em tempo real. Ele habilita o processamento de dados em fluxo contínuo, mas não gerencia a alocação de recursos no cluster.

C) Spark Core é o componente fundamental do ecossistema Spark que fornece a base para as funções de paralelização de dados, agendamento de tarefas e gerenciamento de recursos de cluster. Ele é responsável pela orquestração de recursos distribuídos e execução de tarefas em um ambiente de cluster. Este é o componente correto.

D) Spark MLlib é a biblioteca do Spark para aprendizado de máquina (machine learning). Ela oferece vários algoritmos e utilitários para tarefas de aprendizado de máquina, mas não cuida da orquestração de recursos do cluster.

E) GraphX é o componente para o processamento de grafos e análises de grafos utilizando o Spark. Embora possa executar tarefas sobre dados em forma de grafo, não tem como função a orquestração de recursos em um cluster.",2357673
tópico 0,Ingestão de dados em streaming,"Questão:
A ingestão de dados em streaming é um componente crítico na arquitetura de processamento de dados em tempo real. Qual das seguintes opções melhor descreve uma característica desejável de um sistema eficiente de ingestão de dados em streaming?

A) Alta latência e alto throughput para garantir a integridade dos dados em cargas de trabalho variáveis.
B) Capacidade de lidar apenas com dados estruturados, dado que a maioria dos dados em streaming são bem definidos e uniformes.
C) Tolerância a falhas e a capacidade de se recuperar automaticamente de erros sem a perda de dados.
D) Processamento sincronizado de eventos, de modo que a ordem de chegada dos eventos seja mantida de acordo com a sua criação.
E) Emissão de eventos em batch, priorizando a inteligência de negócios em relação à velocidade de processamento dos dados.

",C,"

Explicação dos itens:

A) Incorreto. A alta latência não é desejável em sistemas de ingestão de dados em streaming, pois esses sistemas precisam processar e disponibilizar dados quase em tempo real. O alto throughput é importante, mas não à custa de aumentar a latência.

B) Incorreto. Embora a ingestão de dados estruturados seja importante, um sistema eficiente também precisa ser capaz de lidar com dados não estruturados e semi-estruturados devido à variedade de fontes de dados em streaming.

C) Correto. A tolerância a falhas é essencial em um sistema de ingestão de dados em streaming, pois ele deve ser capaz de lidar com interrupções e continuar operando sem perda de dados. A recuperação automática de falhas garante a confiabilidade do sistema.

D) Incorreto. Processamento assíncrono de eventos é frequentemente preferível em sistemas de ingestão de dados em streaming, pois permite maior flexibilidade e escalabilidade, mesmo que a ordem dos eventos tenha que ser reconstruída posteriormente se necessário.

E) Incorreto. A emissão de eventos em batch pode ser utilizada em determinados contextos, mas não é uma característica desejável para sistemas que precisam processar dados de streaming em tempo real, onde a velocidade e a imediatez do processamento de eventos são críticos.",3119402
tópico 0,"Arquitetura de cloud computing para ciência de dados (AWS, Azure, GCP)","Questão: Em uma análise comparativa entre os serviços de computação em nuvem fornecidos pelas líderes de mercado AWS, Azure e GCP, um cientista de dados está avaliando as opções de serviços de orquestração de fluxo de trabalho para processamento de dados em larga escala. Qual dos seguintes serviços é incorreto associar à respectiva plataforma de cloud computing?

A) AWS Step Functions - AWS
B) Azure Data Factory - Azure
C) Google Cloud Dataflow - GCP
D) Azure Kubernetes Service - GCP
E) AWS Glue - AWS

",D,"

Explicação:

A) AWS Step Functions é um serviço da AWS que permite coordenar múltiplos serviços da AWS em fluxos de trabalho para aplicações e microsserviços, portanto está corretamente associado.

B) Azure Data Factory é um serviço de integração de dados oferecido pela Microsoft Azure, utilizado para criar, agendar e gerenciar fluxos de trabalho de ETL/ELT, consequentemente está associado corretamente.

C) Google Cloud Dataflow é um serviço totalmente gerenciado para processar dados em tempo real e em lote, parte da Google Cloud Platform, o que torna a associação correta.

D) Azure Kubernetes Service (AKS) é um serviço gerenciado de Kubernetes da Microsoft Azure, não do Google Cloud Platform (GCP). Portanto, a associação está incorreta.

E) AWS Glue é um serviço de ETL gerenciado da AWS, que facilita a preparação e carregamento de dados, corretamente associado à AWS.",8959586
tópico 0,Processamento distribuído,"Questão:  

Considere um sistema de processamento distribuído que implementa o modelo de consistência sequencial. Nele, processos distintos executam em nós separados e podem compartilhar variáveis. Dado este cenário, analise as assertivas abaixo a respeito das garantias dadas por um sistema que adere ao modelo de consistência sequencial:

I. As operações de leitura e escrita ocorrem de maneira atômica em cada variável compartilhada.
II. Se uma operação de escrita 'A' ocorre antes de uma operação de escrita 'B' no mesmo processo, qualquer processo que leia a variável modificada visualizará a operação 'A' antes da 'B'.
III. Os resultados das operações de escrita de um mesmo processo são imediatamente visíveis para todos os outros processos do sistema.
IV. As operações em variáveis compartilhadas por diferentes processos aparecem em uma ordem única, mantendo a consistência do ponto de vista de qualquer processo.

Qual das alternativas abaixo contém apenas assertivas verdadeiras para um sistema de processamento distribuído com modelo de consistência sequencial?

A) I, II e IV.
B) I, III e IV.
C) II e III.
D) I e II.
E) I, II, III e IV.

",A,"

Explicação dos itens:

I. Correto. Em um modelo de consistência sequencial, leituras e escritas são atômicas, significando que uma operação de escrita será realizada por completo antes que outra operação possa ocorrer, garantindo que não haverá intercalação de operações.

II. Correto. O modelo sequencial garante que se uma escrita acontece antes de outra no mesmo processo, essa ordem será preservada na visão de todas as operações subsequente em outros processos que acessam essa variável.

III. Incorreto. A visibilidade das operações de escrita não é necessariamente imediata em todos os processos devido à latência na comunicação e difusão das atualizações entre nós distribuídos.

IV. Correto. Uma característica fundamental do modelo de consistência sequencial é que ele impõe uma ordem única na visão de leituras e escritas de todas as variáveis compartilhadas de forma que pareça ser sequencial e coesa do ponto de vista de qualquer processo, mesmo que as operações sejam realizadas em paralelo.

Portanto, a resposta que inclui apenas assertivas verdadeiras é a (A): I, II e IV.",8902075
tópico 0,Armazenamento de big data,"Questão: Em contextos de Big Data, a eficiência e a escalabilidade do armazenamento de dados são críticas para o suporte ao processamento analítico. Considerando os sistemas de gerenciamento de banco de dados tradicionais e os sistemas projetados para Big Data, é incorreto afirmar que:

A) Sistemas como Hadoop Distributed File System (HDFS) são projetados para lidar com grandes volumes de dados distribuídos em vários nós, oferecendo mecanismos de tolerância a falhas.
B) Sistemas de gerenciamento de banco de dados tradicionais geralmente seguem o modelo ACID, que se preocupa com a atomicidade, consistência, isolamento e durabilidade das transações de dados.
C) Sistemas voltados para Big Data frequentemente utilizam o modelo BASE, que enfatiza a disponibilidade, tolerância a partições, e eventual consistência, em detrimento de garantias estritas de consistência a todo momento.
D) Banco de dados NoSQL, como o Cassandra e o MongoDB, foram desenvolvidos especificamente para atender aos requisitos de consistência transacional e esquemas rígidos dos ambientes de Big Data.

E) O uso de sistemas de processamento em memória, como o Apache Spark, permite a análise de dados em tempo real, aproveitando a velocidade superior de leitura e escrita em comparação ao acesso a discos rígidos.

",D,"

Explicação dos itens:

A) Correto. O HDFS é de fato projetado para operar com grandes conjuntos de dados distribuídos, oferecendo mecanismos robustos de tolerância a falhas, como replicação de dados entre os nós do cluster.

B) Correto. Bancos de dados tradicionais seguem o conjunto de propriedades ACID para garantir transações confiáveis, o que é característico nesses sistemas.

C) Correto. O modelo BASE é comum em sistemas projetados para Big Data e, ao contrário do modelo ACID, enfatiza a disponibilidade e a eventual consistência, reconhecendo que nem sempre é possível manter uma consistência imediata em sistemas distribuídos em larga escala.

D) Incorreto. Bancos de dados NoSQL, como Cassandra e MongoDB, são projetados exatamente para flexibilizar as restrições de consistência transacional e permitir esquemas mais dinâmicos ou mesmo sem esquemas, satisfazendo assim as necessidades de ambientes de Big Data que lidam com dados variados e em grande volume.

E) Correto. Sistemas como Apache Spark permitem processar grandes volumes de dados em memória, proporcionando uma análise mais rápida do que aquela possível com acesso baseado em discos rígidos. Isso é particularmente útil para análises em tempo real ou quase em tempo real.",9787037
tópico 0,Conceitos de processamento massivo e paralelo,"Questão: Em ambientes de computação que requerem o processamento de grandes volumes de dados, o processamento massivo e paralelo tornou-se uma estratégia essencial para melhorar a performance e o tempo de resposta das aplicações. Nesse contexto, qual das seguintes alternativas descreve corretamente uma tecnologia ou técnica utilizada para processamento massivo e paralelo de dados?

A) O modelo ACID garante transações de banco de dados sequenciais para manter a consistência dos dados em sistemas distribuídos.
B) O MapReduce é um framework de programação para processamento de dados em larga escala, operando sobre clusters de computadores e dividindo o trabalho em um conjunto de tarefas independentes.
C) Bancos de dados NoSQL favorecem a consistência imediata dos dados sobre a disponibilidade e tolerância a partições, sendo pouco adequados para ambientes que requerem processamento paralelo.
D) A programação orientada a eventos utiliza alta concorrência e paralelismo, por meio da execução de múltiplos threads para lidar com eventos ao mesmo tempo em sistemas de processamento de transações online (OLTP).
E) Algoritmos sequenciais são melhorados automaticamente pelo sistema operacional para realizar processamento paralelo em hardware multicore sem a necessidade de intervenção do desenvolvedor de software.

",B," Explicação dos itens:
A) Incorreto. O modelo ACID (Atomicidade, Consistência, Isolamento e Durabilidade) é focado em propriedades de transação de banco de dados e não está diretamente ligado ao processamento massivo e paralelo de dados.
B) Correto. O MapReduce é de fato uma técnica/tecnologia utilizada para processamento de dados em grande escala, permitindo a distribuição e processamento em paralelo em um cluster de computadores.
C) Incorreto. Bancos de dados NoSQL foram projetados para escalar horizontalmente e são frequentemente usados em ambientes que requerem processamento paralelo. Eles frequentemente favorecem a disponibilidade e tolerância a partições (Teorema CAP) em detrimento da consistência imediata.
D) Incorreto. A programação orientada a eventos não é sinônimo de processamento massivo e paralelo, embora possa ajudar na concorrência. Essa abordagem se concentra mais na eficiência da execução de programas que lidam com muitos eventos de I/O.
E) Incorroto. Algoritmos sequenciais não são automaticamente melhorados para execução em paralelo; essa é uma tarefa que costuma requerer redesign ou reestruturação do código pelo desenvolvedor para aproveitar eficientemente recursos multicore.",9890009
tópico 0,Processamento distribuído,"Questão: No contexto de processamento distribuído, a consistência dos dados entre múltiplas réplicas que estão geograficamente distribuídas é um desafio considerável. Diversos modelos de consistência têm sido propostos para garantir que operações concorrentes não violem a integridade dos dados. Qual dos seguintes modelos de consistência é conhecido por permitir que réplicas gravem atualizações independentemente e então propaguem essas mudanças eventualmente, tolerando assim disparidades temporárias, mas garantindo a convergência a um estado consistente em momentos de inatividade?

A) Consistência Sequencial
B) Consistência Forte
C) Consistência Eventual
D) Consistência Causal
E) Consistência de Leituras Monótonas

",C,"

Explicação dos itens:

A) Consistência Sequencial - Errado. Este modelo exige que as operações de um mesmo processo sejam vistas por todos os processos na mesma ordem em que foram emitidas, mas não necessariamente em tempo real.

B) Consistência Forte - Errado. Nesse modelo, uma vez que uma atualização de dado é realizada, todas as tentativas subsequentes de acessar esse dado verão essa atualização. Requer que todas as réplicas vejam as operações na mesma ordem.

C) Consistência Eventual - Correto. Sob consistência eventual, todas as réplicas concordam em algum momento no futuro, desde que não haja novas atualizações. As réplicas podem operar independentemente por algum tempo, permitindo algumas inconsistências que são resolvidas eventualmente.

D) Consistência Causal - Errado. Este modelo garante que operações causalmente relacionadas sejam vistas por todos os nós na mesma ordem. Ou seja, se uma operação A causou uma operação B, a operação A será vista antes de B por todos os nós.

E) Consistência de Leituras Monótonas - Errado. Este modelo garante que, se uma leitura de dado específico ocorreu em um ponto do tempo, quaisquer leituras subsequentes por esse mesmo processo retornarão os mesmos dados ou uma versão mais recente deles.",1541843
tópico 0,Ingestão de dados em streaming,"Questão: Em um ecossistema de dados moderno, a capacidade de processar e analisar fluxos de dados em tempo real é fundamental para diversas aplicações, como monitoramento de redes sociais, análise de fraudes financeiras e rastreamento de sensores em IoT. Considerando este contexto, qual das seguintes tecnologias NÃO é adequada para lidar com ingestão de dados em streaming e processamento em tempo real?

A) Apache Kafka
B) Apache Storm
C) MySQL
D) Apache Flink
E) Amazon Kinesis

",C,"

Explicação dos itens:

A) Apache Kafka: É uma plataforma de streaming de dados distribuída que é capaz de publicar, assinar, armazenar e processar streams em tempo real. Está correta para ingestão de dados em streaming.

B) Apache Storm: É um sistema de computação distribuída para processar fluxos de dados em tempo real. Funciona perfeitamente para cenários de processamento de streams em larga escala.

C) MySQL: É um sistema de gerenciamento de banco de dados relacional (RDBMS) baseado em SQL. Embora seja possível inserir dados em tempo real, ele não é projetado para processamento de streaming ou ingestão em alta velocidade, como os demais sistemas especializados em streams.

D) Apache Flink: É um framework e mecanismo de processamento de fluxo de dados aberto que oferece capacidades para processar dados em tempo real e batch de forma eficiente.

E) Amazon Kinesis: É um serviço de streaming de dados na nuvem oferecido pela Amazon Web Services (AWS) que permite processar e analisar dados em tempo real em grande escala.

A alternativa C é a correta, pois o MySQL é um sistema de gerenciamento de banco de dados que não se especializa em ingestão e processamento de dados em streaming como as outras opções citadas, que são ferramentas específicas para esse tipo de tarefa.",9731458
tópico 0,Armazenamento de big data,"Questão: Na implementação de sistemas voltados para o armazenamento e processamento de Big Data, diversas tecnologias podem ser empregadas a fim de gerenciar volumes massivos e heterogêneos de dados. Considerando os sistemas de armazenamento distribuído e as características desejáveis para o manejo eficiente de Big Data, qual das seguintes alternativas melhor descreve uma tecnologia que permite o armazenamento distribuído orientado a colunas e é amplamente utilizada nesse contexto?

A) MongoDB
B) Redis
C) Cassandra
D) MySQL
E) Oracle Database

",C,"

A) MongoDB é um banco de dados NoSQL orientado a documentos, não a colunas, o que o invalida para a resposta correta.
B) Redis é um armazenamento de estrutura de dados em memória, conhecido por sua velocidade e não é orientado a colunas.
C) Cassandra é a resposta correta porque é um banco de dados NoSQL altamente escalável e projetado para gerenciar grandes quantidades de dados distribuídos por muitos servidores, sem pontos de falha únicos e oferece um armazenamento orientado a colunas.
D) MySQL é um sistema de gerenciamento de banco de dados relacional (RDBMS) tradicional e não foi projetado especificamente para o armazenamento distribuído de Big Data.
E) Oracle Database é um RDBMS amplamente utilizado no mercado, mas assim como o MySQL, não é especializado em armazenamento distribuído orientado a colunas para Big Data.",2935059
tópico 0,Conceitos de processamento massivo e paralelo,"Questão: No campo de processamento de dados em larga escala, a utilização de algoritmos e sistemas capazes de processar e analisar grandes volumes de informação de forma rápida e eficiente é crucial. Entre as tecnologias que viabilizam o processamento massivo de dados, algumas se destacam devido ao seu design e escalabilidade. Considerando essas tecnologias, assinale a opção que corretamente identifica uma característica essencial do modelo MapReduce.

A) O modelo MapReduce permite que o mesmo dado seja processado simultaneamente por diferentes nós, garantindo redundância e confiabilidade na recuperação de dados.

B) No MapReduce, o processo de 'reduce' é executado em paralelo com 'map', acelerando o processamento ao dividir as tarefas de maneira equitativa entre os nós disponíveis.

C) A fase de 'shuffle' no MapReduce é opcional e pode ser eliminada para otimizar o desempenho quando os dados já estão pré-agrupados.

D) O MapReduce depende fortemente de Sistemas de Arquivos Distribuídos, como o HDFS, para gerenciar o armazenamento de dados de forma eficiente em todo o cluster.

E) O modelo MapReduce é inerentemente sequencial e não permite o processamento em paralelo de tarefas, concentrando o processamento em um único nó para garantir a integridade dos dados.

",D,"

Explicação dos itens:

A) Incorreto. O MapReduce processa dados em diferentes nós de maneira distribuída, mas não processa o mesmo dado simultaneamente para garantir redundância. Esta característica é relacionada a sistemas que implementam replicação de dados, que é um conceito diferente.

B) Incorreto. A fase de 'reduce' apenas começa após a conclusão da fase de 'map' e não é executada em paralelo com ela. Primeiro todos os 'maps' são processados, e depois ocorre a etapa de 'shuffle' seguida pelo 'reduce'.

C) Incorreto. A fase de 'shuffle' é crucial no processamento MapReduce porque é nela que os dados são reorganizados entre os nós para que os resultados do processo de 'map' possam ser agrupados corretamente antes da fase de 'reduce'.

D) Correto. MapReduce é uma técnica intimamente ligada a Sistemas de Arquivos Distribuídos como o Hadoop Distributed File System (HDFS). Esses sistemas de arquivos desempenham um papel fundamental na distribuição e armazenamento de dados de maneira eficiente ao longo de um cluster, que é essencial para o funcionamento do MapReduce.

E) Incorreto. O modelo MapReduce é projetado para processar dados de forma paralela, distribuindo a carga de trabalho entre vários nós de um cluster para aumentar a eficiência e o desempenho. A afirmativa contradiz diretamente o princípio básico do MapReduce.",6806425
tópico 0,"Arquitetura de cloud computing para ciência de dados (AWS, Azure, GCP)","Questão:
A utilização de serviços de cloud computing para ciência de dados tem se ampliado consideravelmente nos últimos anos. Plataformas como AWS, Azure e GCP oferecem uma gama variada de serviços para armazenamento, processamento e análise de grandes volumes de dados. Considerando as características desses serviços, qual das seguintes afirmações é verdadeira a respeito das capacidades oferecidas por essas plataformas em relação à ciência de dados?

A) AWS Redshift é um serviço de armazenamento de objetos de alta disponibilidade e durabilidade, ideal para hospedar grandes datasets para ciência de dados.
B) Microsoft Azure Machine Learning Studio é uma plataforma colaborativa baseada na nuvem que permite a construção, teste e implantação de modelos de machine learning sem a necessidade de programar.
C) GCP BigTable é um banco de dados relacional altamente escalável que suporta cargas de trabalho de análise em tempo real para aplicações de ciência de dados.
D) AWS SageMaker é uma plataforma que unicamente possibilita o armazenamento de grandes volumes de dados, mas não permite a construção e treinamento de modelos de machine learning.
E) Azure HDInsight é uma ferramenta exclusiva para processamento de dados em tempo real, que não suporta aplicações de batch processing comumente utilizadas em ciência de dados.

",B,"

Explicação dos itens:

A) AWS Redshift é, na verdade, um serviço de data warehousing e não um serviço de armazenamento de objetos. O serviço de armazenamento de objetos da AWS é o S3, que é conhecido por sua alta disponibilidade e durabilidade.

B) Microsoft Azure Machine Learning Studio realmente é uma plataforma baseada em nuvem que oferece um ambiente colaborativo, onde é possível construir, testar e implantar modelos de machine learning, fornecendo uma interface visual que permite que façam isso sem necessariamente escrever códigos.

C) GCP BigTable é um banco de dados NoSQL, e não relacional, projetado para lidar com grandes volumes de dados, oferecendo suporte ao processamento analítico, mas não é especificamente otimizado para transações em tempo real, como sugerido na afirmação.

D) AWS SageMaker é uma plataforma da AWS que permite o provisionamento de instâncias para construir, treinar e implantar modelos de machine learning de forma fácil e integrada, e não serve apenas como um serviço de armazenamento.

E) Azure HDInsight é um serviço que permite processamento de dados tanto em batch quanto em tempo real, utilizando diferentes frameworks como Hadoop, Spark, e Kafka, portanto, a afirmação de que é exclusivo para processamento em tempo real não procede.",8304710
tópico 0,Soluções de big data: Arquitetura do ecossistema Spark,"Questão: Nas soluções de big data, a arquitetura do ecossistema Apache Spark é fundamental para processar grandes volumes de dados de forma eficaz. Considere os seguintes componentes do Spark e suas respectivas funcionalidades:

I. Spark SQL: facilita a execução de consultas SQL para trabalhar com dados estruturados.
II. MLLib: biblioteca projetada para processamento de dados semi-estruturados e não estruturados.
III. GraphX: oferece APIs para manipulação de grafos e computação paralela de grafos.
IV. Spark Streaming: habilita o processamento de fluxos contínuos de dados em tempo real.

Com base nessas funções, quais componentes estão corretamente associados às suas funcionalidades?

A) Apenas I e III
B) Apenas II e IV
C) Apenas I, II e III
D) Apenas I, III e IV
E) I, II, III e IV

",C,"

Explicação dos itens:

I. Spark SQL: Está corretamente associado à sua funcionalidade, pois é utilizado para executar consultas SQL sobre dados estruturados e suporta diversas fontes de dados.

II. MLLib: A descrição está incorreta. MLLib é uma biblioteca de aprendizado de máquina do Spark projetada para simplificar algoritmos de machine learning, como classificação, regressão, clustering e filtragem colaborativa, e não para processamento de dados semi-estruturados ou não estruturados.

III. GraphX: Está corretamente associado à sua funcionalidade, pois é o componente do Spark focado em processamento de grafos e computações paralelas em grafos.

IV. Spark Streaming: A descrição está correta e é o componente do Spark que possibilita o processamento de dados em tempo real, mas a opção correta não deve incluir o item II.

A opção C é a correta, pois apenas I e III estão corretamente associados às suas funcionalidades descritas na questão.",4157554
tópico 0,Ingestão de dados em lote (batch),"Questão: Sobre a ingestão de dados em lote (batch) em um ambiente de Big Data, é INCORRETO afirmar que:

A) O processo de batch processing é adequado para análises complexas sobre grandes volumes de dados já armazenados, onde a latência não é uma preocupação crítica.
B) A ingestão de dados em lote é geralmente realizada em intervalos de tempo programados, e não em tempo real, sendo frequentemente utilizada em sistemas de relatórios diários ou mensais.
C) O Apache Hadoop é um exemplo de framework que permite a ingestão e processamento de grandes conjuntos de dados em lote de forma distribuída e escalável.
D) No processamento em lote, os dados são coletados, processados e armazenados instantaneamente assim que são gerados, garantindo que as informações estejam sempre disponíveis em tempo real.
E) Ferramentas como Apache Spark e Apache Flink podem ser utilizadas para realizar tanto processamento de dados em lote quanto em streams, embora tenham características operacionais distintas.

",D,"

Explicação dos itens:

A) Correto. O processamento em lote é ideal para situações onde pode-se tolerar alguma latência e é necessário processar grandes volumes de dados.
B) Correto. A ingestão de dados em lote acontece em intervalos programados, não sendo feita em tempo real, o que é característico de sistemas de processamento em stream.
C) Correto. O Apache Hadoop é um framework amplamente reconhecido para processamento distribuído e análise de conjuntos de dados grandes em um ambiente de computação distribuída.
D) Incorreto. No processamento em lote, os dados não são processados instantaneamente após sua geração. Ao invés disso, eles são acumulados e processados em um intervalo de tempo definido.
E) Correto. O Apache Spark e o Apache Flink oferecem suporte a processamento de dados tanto em batch quanto em stream (processamento em tempo real), com abordagens e recursos distintos.",7081262
tópico 0,Armazenamento de big data,"Questão: A gestão de grandes volumes de dados, conhecida como Big Data, traz novos desafios para os sistemas de armazenamento de informação. Dentre os modelos de armazenamento projetados para lidar eficientemente com as características de Big Data, como volume, variedade, velocidade e veracidade, qual das opções a seguir representa a tecnologia que foi especificamente desenhada para processar grandes volumes de dados não estruturados ou semi-estruturados em clusters de hardware commodity?

A) Sistema de arquivos NTFS
B) Banco de dados relacional SQL Server
C) Sistema de arquivos Hadoop Distributed File System (HDFS)
D) Banco de dados NoSQL CouchDB
E) Disco de Estado Sólido (SSD) com interface NVMe

",C," 

Explicação dos itens:
A) O Sistema de arquivos NTFS é um sistema de arquivos usado por sistemas operacionais Windows e não é especificamente desenhado para processar grandes volumes de dados não estruturados, pois é mais adequado para sistemas de arquivos locais.

B) O Banco de dados relacional SQL Server é um sistema gerenciador de banco de dados relacional e não foi projetado para lidar com os grandes volumes de dados não estruturados típicos do Big Data, pois possui um esquema rígido que limita a variedade dos dados.

C) O Sistema de arquivos Hadoop Distributed File System (HDFS) foi desenhado para armazenar dados de forma distribuída, permitindo que grandes volumes de dados não estruturados ou semi-estruturados sejam processados paralelamente em clusters de hardware commodity, lidando efetivamente com as características do Big Data.

D) O Banco de dados NoSQL CouchDB é projetado para armazenar dados não estruturados ou semi-estruturados, oferecendo flexibilidade em termos de esquema de dados, porém não é especificamente destinado ao processamento de grandes volumes de dados em hardware commodity como o HDFS.

E) Um Disco de Estado Sólido (SSD) com interface NVMe é um dispositivo de armazenamento de alta performance, mas não um sistema projetado especificamente para processar grandes volumes de dados não estruturados em ambiente distribuído.",7411421
tópico 0,"Arquitetura de cloud computing para ciência de dados (AWS, Azure, GCP)","Questão:
A adoção de arquitetura em nuvem para ciência de dados tem crescido substancialmente devido às vantagens como escalabilidade, flexibilidade e eficiência no processamento de grandes volumes de dados. Considerando os serviços de três grandes provedores - AWS (Amazon Web Services), Azure (Microsoft) e GCP (Google Cloud Platform) - analise as seguintes afirmativas relacionadas a serviços de armazenamento e processamento de dados:

I. O AWS Redshift é uma solução de armazém de dados (data warehousing) que permite a execução de consultas complexas em dados estruturados e semi-estruturados com alta performance.

II. O Azure HDInsight é um serviço que proporciona a implementação facilitada de clústeres Hadoop, Spark, HBase e Storm, destinando-se ao processamento de dados em larga escala em ambientes de nuvem.

III. O GCP BigQuery é um serviço de data warehousing totalmente gerenciado e sem servidor, otimizado para análises de big data e suporta SQL padrão para consultas.

IV. O Azure Data Lake é uma solução exclusiva da GCP que oferece armazenamento seguro e massivo para dados estruturados, semi-estruturados e não estruturados com capacidades de análise integradas.

Estão corretas somente as afirmativas:

A) I e II.

B) II e IV.

C) I, II e III.

D) II, III e IV.

E) I, III e IV.

",C,"

Explicação dos itens:

I. Correta. O AWS Redshift é um serviço de data warehousing oferecido pela Amazon que permite análise de grandes volumes de dados com velocidade e eficiência.

II. Correta. Azure HDInsight é um serviço da Microsoft Azure focado no processamento de dados de alta performance e suporta diversas tecnologias, como Hadoop e Spark.

III. Correta. GCP BigQuery é um serviço de data warehousing totalmente gerenciado, otimizado para análises rápidas e escaláveis em grandes conjuntos de dados e usa SQL padrão.

IV. Incorreta. Azure Data Lake é uma solução da Microsoft Azure e não do GCP. Oferece armazenamento em grande escala para quaisquer tipos de dados, analíticos e operacionais, e é integrado com serviços de análise.

Portanto, as afirmativas I, II e III estão corretas, fazendo da opção C a resposta certa.",921583
tópico 0,Processamento distribuído,"Questão: No processamento distribuído, diversas estratégias podem ser adotadas para implementar e gerenciar o ambiente de execução em múltiplas máquinas. Considerando uma aplicação distribuída que necessita de tolerância a falhas e consistência de dados, qual das seguintes técnicas é mais adequada para garantir a correta sincronização e estado consistente entre os nodos da aplicação?

A) Uso de um serviço de diretório leve, como o LDAP, para gerenciar recursos distribuídos.
B) Emprego de algoritmos de ordenação externa para lidar com grandes volumes de dados.
C) Aplicação de checkpointing periódico e mecanismos de recuperação para reestabelecer o estado do sistema.
D) Utilização de balanceamento de carga através de um algoritmo round-robin para distribuição de tarefas.
E) Implementação de técnicas de hashing distribuído para a localização eficiente de dados.

",C,"

Alternativa A está incorreta porque o LDAP (Lightweight Directory Access Protocol) é mais utilizado para gerenciar informações de diretório, como credenciais e perfis de usuários, mas não é uma técnica projetada especificamente para sincronização ou consistência de estado.

Alternativa B está incorreta pois algoritmos de ordenação externa são usados para ordenar grandes conjuntos de dados que não cabem na memória, mas não oferecem mecanismos para manter a consistência do estado entre os nodos de uma aplicação.

Alternativa C está correta porque a técnica de checkpointing envolve salvar o estado de um sistema em intervalos regulares, o que permite a recuperação do último estado conhecido em caso de falhas, proporcionando tolerância a falhas e consistência de dados.

Alternativa D está incorreta pois, apesar de o algoritmo round-robin ajudar na distribuição equitativa de tarefas e no balanceamento de carga entre os nodos, ele não fornece ferramentas para sincronização ou manutenção de estado consistente entre os nodos.

Alternativa E está incorreta porque, apesar de o hashing distribuído ser eficiente na localização de dados em um sistema distribuído, ele não aborda diretamente as questões de tolerância a falhas e consistência de estado entre os nodos de uma aplicação distribuída.",9714453
tópico 0,Conceitos de processamento massivo e paralelo,"Questão:
A capacidade de processar e analisar grandes volumes de dados tem se tornado cada vez mais essencial para empresas de diversos setores. Neste contexto, o processamento massivo e paralelo surge como uma solução para lidar com essa demanda. Entretanto, para realizar o processamento paralelo de maneira eficiente, é crucial compreender a diferença entre os diversos modelos e abordagens. Considere as seguintes afirmações sobre processamento massivo e paralelo:

I. O modelo MapReduce divide o problema em muitas pequenas tarefas que podem ser processadas de forma independente e paralela, posteriormente combinando os resultados para formar o output desejado.

II. O processamento SIMD (Single Instruction, Multiple Data) executa a mesma operação em múltiplos pontos de dados simultaneamente, sendo adequado para operações vetoriais mas limitado pela sincronização entre os elementos de processamento.

III. Em um paradigma de memória compartilhada, todos os processadores acessam e escrevem em uma única memória global, o que pode levar a condições de corrida se não for gerenciado adequadamente.

IV. O modelo de passagem de mensagens é inerentemente livre de condições de corrida, uma vez que os processos comunicam-se apenas por meio do envio e recebimento de mensagens, sem compartilhar estado.

Assinale a opção que contém apenas as afirmações CORRETAS:

A) I e III
B) I, II e IV
C) II, III e IV
D) I, II e III
E) Todas as afirmações são corretas.

",B,"

Explicação dos itens:

I. Correto. O modelo MapReduce de fato divide trabalhos em tarefas menores que são processadas de maneira independente e em paralelo, sendo uma característica central do modelo.

II. Correto. SIMD é um modelo de processamento onde uma única instrução é aplicada a múltiplos dados simultaneamente. Isso é comum em operações vetoriais, como as encontradas em processadores gráficos, mas a necessidade de sincronizar os dados é uma das limitações do modelo.

III. Incorreto. Apesar da descrição ser geralmente verdadeira sobre a memória compartilhada e o possível problema de condições de corrida, isso não é uma afirmação exclusivamente correta, pois a gestão adequada e mecanismos de sincronização podem evitar condições de corrida, não sendo, portanto, uma consequência inevitável.

IV. Correto. No modelo de passagem de mensagens, cada processo ou thread opera de forma independente, comunicando através do envio e recebimento de mensagens. Isso pode reduzir ou eliminar as condições de corrida, já que o estado não é compartilhado diretamente.

Assim, as alternativas I, II e IV são as corretas, tornando a letra B a resposta correta.",4763558
tópico 0,Ingestão de dados em lote (batch),"Questão: No contexto de sistemas de processamento de dados, a ingestão de dados em lote (batch) desempenha um papel fundamental na manipulação de grandes volumes de informações de forma eficiente. Analisando as características dos sistemas de processamento de dados em lote, avalie as seguintes afirmações:

I. Os sistemas de ingestão de dados em lote são idealmente utilizados quando os dados não exigem processamento em tempo real e pode ser tolerada uma latência na disponibilização dos resultados.

II. A ingestão de dados em lote é incompatível com sistemas de armazenamento escaláveis como HDFS (Hadoop Distributed File System), dado que o processamento em lote requer armazenamento estático e de tamanho fixo.

III. Em um sistema de processamento em lote, as tarefas são executadas de forma sequencial e podem ser agendadas para execução em intervalos regulares, aproveitando períodos de baixa demanda computacional.

Está(ão) correta(s) a(s) afirmativa(s):

A) Apenas I e II.
B) Apenas II e III.
C) Apenas I e III.
D) Todas as alternativas.
E) Nenhuma das alternativas.

",C,"

Explicação dos itens:

I. Correto. A ingestão de dados em lote é comumente escolhida em cenários onde a imediaticidade do processamento não é crítica. A latência é, portanto, um fator aceitável, e os dados podem ser acumulados ao longo do tempo para serem processados juntos.

II. Incorreto. A afirmação é falsa porque o HDFS e outros sistemas de armazenamento distribuídos são projetados para lidar com grandes volumes de dados e são adequados para processamento em lote. Os dados em um HDFS, por exemplo, são distribuídos em blocos ao longo de uma rede de computadores, permitindo escalabilidade e flexibilidade na capacidade de armazenamento.

III. Correto. Os sistemas de processamento em lote permitem a execução de tarefas de forma sequencial e podem ser configurados para executar em intervalos definidos. Isso otimiza o uso dos recursos de computação e permite que tarefas de grande porte sejam realizadas durante períodos de menor carga nos sistemas.",1551211
tópico 0,Ingestão de dados em streaming,"Questão: Em uma arquitetura de processamento de dados em tempo real, a capacidade de capturar e processar fluxos contínuos de dados - conhecida como ingestão de dados em streaming - é fundamental para garantir que insights possam ser obtidos de forma ágil e eficiente. Considerando as principais tecnologias e estratégias para a ingestão de dados em streaming, avalie as alternativas abaixo e escolha a opção que corretamente descreve uma plataforma robusta e escalável para esse fim:

A) A utilização exclusiva de bancos de dados relacionais tradicionais, devido à sua ampla adoção e confiabilidade histórica, é a abordagem mais indicada para garantir o alto desempenho na ingestão de dados em streaming.

B) O Apache Kafka é um sistema de mensageria que suporta o particionamento e replicação de mensagens, oferecendo tolerância a falhas e permitindo processamento em alta taxa para cenários de ingestão em streaming.

C) A ingestão de dados em streaming dispensa a necessidade de sistemas para controle de fluxo (backpressure), pois a natureza real-time implica a entrega e processamento imediatos de todas as mensagens sem a possibilidade de atrasos ou congestionamentos.

D) Armazenamento de dados na nuvem, como Amazon S3 e Google Cloud Storage, por serem serviços de armazenamento de objetos, são idealmente adequados para a ingestão de dados em streaming sem a necessidade de qualquer sistema intermediário para buffer ou processamento.

E) A utilização de algoritmos de compressão de alta complexidade, que maximizam a taxa de compressão, é sempre recomendável na ingestão de dados em streaming para reduzir a latência e aumentar o throughput do sistema.

",B," 

A alternativa A está incorreta, pois os bancos de dados relacionais não são otimizados para ingestão de dados em streaming de alta velocidade e podem apresentar gargalos de desempenho. A alternativa C também é incorreta, porque sistemas de controle de fluxo, como backpressure, são essenciais para lidar com variações no volume de dados e evitar perda ou sobrecarga do sistema. A alternativa D está incorreta, pois enquanto Amazon S3 e Google Cloud Storage são excelentes para armazenamento de grandes volumes de dados, eles não são otimizados para ingestão de dados em streaming, que normalmente requer tecnologias que possam lidar com taxa de dados em alta velocidade. A alternativa E é incorreta, pois algoritmos de compressão de alta complexidade podem, na verdade, aumentar a latência devido ao tempo adicional necessário para a compressão e descompressão dos dados. A alternativa B é a correta pois o Apache Kafka é projetado especificamente para lidar com ingestão e processamento de dados em streaming, suportando alta taxa de dados e garantindo robustez e escalabilidade através de particionamento e replicação.",4813706
tópico 0,Soluções de big data: Arquitetura do ecossistema Spark,"Questão: Em um cenário de processamento de big data, o ecossistema Apache Spark tem sido amplamente utilizado devido à sua eficiência e capacidade de processamento em memória. A arquitetura do Spark é composta por diversos componentes, que permitem o processamento de grandes volumes de dados de forma distribuída. Qual dos seguintes componentes é responsável por proporcionar APIs de alto nível para operações em DataFrames, com otimizações de execução automáticas, permitindo que usuários construam aplicações de forma eficiente e com uma abstração mais próxima da compreensão de um cientista de dados?

A) Spark Core
B) Spark Streaming
C) Spark SQL
D) MLlib
E) GraphX

",C," Explicação:

A) Spark Core - Representa o componente fundamental sobre o qual todos os outros serviços do Spark são construídos. É responsável pela execução de tarefas básicas, como gerenciamento de memória, monitoramento de tarefas e recuperação de falhas.

B) Spark Streaming - Este componente é utilizado para processamento de streams de dados em tempo real. Ele divide o fluxo de dados contínuos em micro-batches para serem processados pelo Spark Core.

C) Spark SQL - Correta. O Spark SQL fornece uma maneira de escrever consultas SQL para processar dados, juntamente com APIs para trabalhar com DataFrames e Datasets. Além disso, realiza otimizações automáticas de consulta, o que torna o processamento mais eficiente.

D) MLlib - Conhecido como a biblioteca de machine learning do Spark, o MLlib fornece múltiplos tipos de algoritmos e utilidades de aprendizado de máquina, mas não é a ferramenta projetada para otimizar execução de operações em DataFrames em si.

E) GraphX - Este componente é a biblioteca de processamento de grafos dentro do ecossistema Spark. Ele é usado para construir e analisar redes complexas, mas não está diretamente relacionado à otimização de DataFrames ou SQL.",147013
tópico 0,Armazenamento de big data,"Questão: A gestão e o armazenamento eficiente de grandes volumes de dados, ou big data, tornaram-se uma tarefa essencial para organizações que buscam insights a partir da análise de dados. Em um cenário onde uma empresa pretende implementar uma solução de armazenamento escalável e com alta disponibilidade para o seu crescente volume de dados provenientes de diversas fontes, qual das seguintes opções representa a tecnologia mais adequada para tal finalidade?

A) Relational Database Management System (RDBMS) com esquema de tabela fixo.
B) File System Distribuído (como o Hadoop Distributed File System - HDFS).
C) Sistema de Arquivos Simples sem redundância de dados.
D) Banco de Dados baseado em Grafos para otimização de consultas relacionais.
E) Data Lake utilizando um serviço de armazenamento em blocos tradicional.

",B,"

Explicação dos itens:

A) Relational Database Management System (RDBMS) com esquema de tabela fixo - Embora os RDBMS sejam amplamente utilizados, eles não são ideais para cenários de big data devido ao esquema de tabela fixo e limitações de escalabilidade.

B) File System Distribuído (como o Hadoop Distributed File System - HDFS) - Projetado para armazenar grandes volumes de dados de forma distribuída, o HDFS permite o processamento em paralelo e a alta tolerância a falhas, tornando-o a escolha adequada para os requisitos mencionados.

C) Sistema de Arquivos Simples sem redundância de dados - Um sistema de arquivos simples normalmente não oferece os recursos necessários para lidar com big data, especialmente no que se refere a redundância de dados, que é crucial para a alta disponibilidade.

D) Banco de Dados baseado em Grafos para otimização de consultas relacionais - Embora os bancos de dados baseados em grafos sejam úteis para otimizar consultas complexas relacionais, eles não são especificamente projetados para armazenamento em larga escala de big data.

E) Data Lake utilizando um serviço de armazenamento em blocos tradicional - Data Lakes são opções viáveis para armazenar big data, porém a utilização de um serviço de armazenamento em blocos tradicional pode não oferecer a escalabilidade e o desempenho necessários para processamento de grandes volumes de dados.",350100
tópico 0,Ingestão de dados em lote (batch),"Questão: Em um sistema de processamento de dados em larga escala, um analista de dados precisa realizar a ingestão de grandes volumes de informações que não requerem processamento em tempo real. Essa ingestão de dados deve ser eficiente e possibilitar uma futura análise e processamento que podem ser realizados em janelas de tempo pré-determinadas. Considerando os conceitos de sistemas de processamento de dados em lote (batch), qual das seguintes alternativas melhor descreve um sistema eficaz para a realização dessa tarefa?

A) Um sistema que prioriza a ingestão de dados em tempo real utilizando Kafka, para garantir a entrega instantânea de dados para processamento e análise.

B) Um sistema baseado em micro-batches que utiliza ferramentas como Spark Streaming para processar pequenos lotes de dados continuamente.

C) Uma solução que emprega bancos de dados NoSQL para armazenamento imediato e processamento ad-hoc, sem a necessidade de estruturas de dados pré-definidas.

D) Uma arquitetura que utiliza o Hadoop Distributed File System (HDFS) para armazenar dados em lotes, em conjunto com frameworks como MapReduce ou Apache Spark para processamento posterior.

E) Um sistema integrado de mensageria com MQTT (Message Queuing Telemetry Transport), ideal para garantir a entrega e consumo de dados como eventos individuais em sistemas IoT.

",D," 
A alternativa A não é a mais adequada porque o Kafka é tipicamente usado em sistemas que precisam de processamento em tempo real, o que não é o requisito descrito. A alternativa B também não é ideal, pois o Spark Streaming é mais indicado para processamento de dados em tempo quase real (near-real-time), e não para lotes grandes que não requerem processamento imediato. A alternativa C envolve bancos de dados NoSQL que são úteis para armazenar dados sem esquemas fixos, mas não especifica a eficiência na ingestão em lote e o processamento em janelas de tempo predefinidas. A alternativa E fala de MQTT, que é um protocolo de mensagens leve para dispositivos de IoT e não se encaixa na descrição de processamento de dados em lote. A alternativa D descreve uma solução apropriada para ingestão de dados em lote - o HDFS permite o armazenamento de grandes volumes de dados, e o uso posterior de frameworks como MapReduce ou Apache Spark é indicado para processar esses dados em lote, de acordo com as necessidades do analista.",2572665
tópico 0,Conceitos de processamento massivo e paralelo,"Questão: No âmbito da computação de alto desempenho, o processamento massivo e paralelo desempenha um papel crítico na capacidade de resolver problemas complexos e analisar grandes conjuntos de dados em um tempo razoável. Sendo assim, considere as seguintes afirmações a respeito do processamento massivo e paralelo:

I. O processamento paralelo permite que uma tarefa seja dividida em sub-tarefas mais pequenas, que podem ser processadas simultaneamente por múltiplos processadores, resultando em uma redução significativa do tempo total de execução.
II. O processamento massivo refere-se ao uso de um único processador de alta capacidade para realizar computações extensivas em um grande volume de dados, otimizando a utilização de recursos computacionais.
III. O MapReduce é um modelo de programação e uma técnica associada para o processamento paralelo que permite a análise e geração de grandes conjuntos de dados com uma abordagem distribuída, usando clusters de computadores.
IV. Aumento linear no número de processadores em um sistema de processamento paralelo sempre resultará em um aumento linear na velocidade de processamento, devido à ausência de sobrecarga na coordenação entre processadores.

Considerando as afirmações acima, estão corretas:

A) I e II
B) II e IV
C) I e III
D) I, II e IV
E) III e IV

",C,"

Explicação dos itens:

I. Correto. O processamento paralelo de fato divide tarefas em sub-tarefas que são processadas simultaneamente por múltiplos processadores ou núcleos, o que pode levar a uma redução no tempo de execução.

II. Incorreto. O processamento massivo não é caracterizado pelo uso de um único processador de alta capacidade, mas pelo processamento de grandes volumes de dados distribuídos por vários processadores ou sistemas, trabalhando de forma paralela.

III. Correto. MapReduce é um modelo de programação para processamento paralelo em sistemas distribuídos. Ele é projetado para processar grandes volumes de dados em um conjunto de computadores (cluster), utilizando as operações de mapeamento e redução.

IV. Incorreto. A Lei de Amdahl destaca que o aumento no número de processadores não resulta necessariamente em um aumento linear na velocidade devido à sobrecarga de coordenação e comunicação entre os processadores. Existem limitações quanto à escalabilidade em sistemas paralelos.",241532
tópico 0,"Arquitetura de cloud computing para ciência de dados (AWS, Azure, GCP)","Questão:

A integração de serviços de cloud computing trouxe uma nova era para o tratamento e análise de dados massivos na ciência de dados. Diversas soluções são oferecidas por provedores como Amazon Web Services (AWS), Microsoft Azure e Google Cloud Platform (GCP), que oferecem uma gama de ferramentas projetadas para armazenamento, processamento e visualização de dados. Considerando os produtos e serviços oferecidos por esses provedores, qual das seguintes afirmações melhor descreve um cenário apropriado para implementação do serviço de armazenamento de dados em larga escala e análise de big data para uma empresa de médio porte?

A) Utilizar o Amazon S3 para armazenamento de objetos em larga escala com o Amazon Redshift para a análise de dados, pois ambos são altamente escaláveis e integrados com outros serviços de análise da AWS.

B) Configurar uma série de bancos de dados SQL Server em máquinas virtuais no Azure para tirar proveito do licenciamento existente, usando o Azure Table Storage para análise de dados não estruturados.

C) Empregar o Google BigQuery como uma opção serverless para tarefas de análise de dados sem a necessidade de gerenciar a infraestrutura, mas armazenar os dados no Filestore do GCP para desempenho ideal.

D) Instalar e gerenciar um cluster Hadoop no Compute Engine do GCP manualmente, para controle total sobre configurações e segurança, utilizando uma série de discos persistentes para armazenamento de dados.

E) Adotar o Azure Blob Storage para armazenamento de dados em larga escala juntamente com o Azure Databricks para análise de dados de big data, aproveitando um ambiente integrado que oferece análises avançadas e aprendizado de máquina.

",A," 

Explicação dos itens:

A) Correta: O Amazon S3 é um serviço altamente escalável de armazenamento de objetos que se integra bem com outros serviços AWS, como o Amazon Redshift, que é um serviço de armazenamento de dados em warehouse e análise poderoso, o que justifica a escolha para cenários de big data.

B) O Azure Table Storage é um serviço de armazenamento de NoSQL para dados não estruturados, mas a opção não menciona uma solução de análise de big data integrada que seria necessária para uma empresa de médio porte, e o uso de máquinas virtuais pode não ser a escolha mais escalável ou custo-efetiva.

C) O Google BigQuery é uma excelente opção para análise de big data de forma serverless, mas o Google Filestore é otimizado para cargas de trabalho intensivas de computação e não é a solução ideal para armazenar grandes volumes de dados para análise em larga escala.

D) Gerenciar manualmente um cluster Hadoop envolve um nível significativo de complexidade e administração que pode não ser ideal para uma empresa de médio porte. Além disso, essa abordagem poderia diminuir a agilidade e aumentar os custos.

E) O Azure Blob Storage e o Azure Databricks são boas opções para armazenamento e análise de big data, respectivamente. No entanto, a alternativa A é mais completa ao indicar a integração com outros serviços de análise, o que pode ser essencial para a ciência de dados em uma empresa de médio porte.",5847275
tópico 0,Processamento distribuído,"Questão: Em um ambiente de processamento distribuído, várias máquinas trabalham em conjunto para executar aplicações ou resolver problemas complexos. Um dos modelos arquiteturais empregados nesse contexto é o modelo Master-Slave. Esse modelo é caracterizado pela presença de um nó mestre que distribui as tarefas e coordena os nós escravos. Considerando as características e implicações desse tipo de arquitetura, analise as afirmações a seguir:

I. O nó Master é o ponto central do processamento, e, em caso de falha, pode levar a um colapso de todo o sistema.
II. A escalabilidade horizontal é limitada pela capacidade do nó Master em gerenciar o número crescente de Slaves.
III. A descentralização do processamento é uma vantagem do modelo Master-Slave, pois todos os nós operam de maneira autônoma.
IV. A simplificação do processo de gerenciamento de carga de trabalho é possível devido ao papel direcionador e de controle do nó Master.

É correto apenas o que se afirma em:

A) I, II e III
B) I, III e IV
C) I e II
D) II e IV
E) III e IV

",C," 

A explicação dos itens é a seguinte:

I. Correto. O nó Master é um ponto de falha única (Single Point of Failure, SPOF), o que significa que se ele falhar, poderá comprometer todo o sistema, uma vez que os nós Slaves dependem de suas instruções para operar.

II. Correto. A escalabilidade horizontal, que se refere à adição de mais máquinas para melhorar o desempenho, é limitada pela capacidade do nó Master de gerenciar eficientemente um grande número de Slaves. Se o Master não consegue lidar com mais nós, a adição de mais escravos não resultará em ganhos de desempenho.

III. Incorreto. No modelo Master-Slave, a descentralização do processamento não é uma vantagem, pois, apesar de os nós Slaves realizarem o processamento das tarefas de forma distribuída, eles não operam de maneira autônoma; dependem das instruções centrais do nó Master.

IV. Correto. A simplificação do gerenciamento de carga de trabalho é uma vantagem do modelo Master-Slave, pois o nó Master direciona as tarefas e coordena o processamento, o que torna mais fácil gerenciar e controlar o que cada nó Slave está fazendo.

Portanto a alternativa correta é a C), afirmando que apenas as proposições I e II são verdadeiras.",811102
tópico 0,Soluções de big data: Arquitetura do ecossistema Spark,"Questão: No contexto das soluções de Big Data, o Apache Spark destaca-se por sua capacidade de processamento rápido e por sua arquitetura altamente escalável. Considerando os componentes da arquitetura do ecossistema Spark, assinale a opção que melhor descreve a função do componente denominado ""Spark Core"".

A) É responsável por executar o gerenciamento de cluster, incluindo o agendamento e distribuição de tarefas entre os nós do cluster.

B) Provê uma abstração para o armazenamento de dados em memória, conhecida como Resilient Distributed Dataset (RDD), que permite o processamento distribuído de grandes conjuntos de dados de maneira eficiente.

C) É a interface para a integração do Spark com sistemas de armazenamento externos, como HDFS, Amazon S3 e Cassandra.

D) Implementa o modelo de processamento de stream em tempo real, permitindo que o Spark processe dados que estão sendo gerados continuamente.

E) Oferece suporte à análise de dados em larga escala por meio de operações de Machine Learning distribuídas.
",B,"
O ""Spark Core"" constitui o fundamento do Apache Spark, provendo a abstração fundamental de Resilient Distributed Datasets (RDDs). As RDDs são coleções imutáveis de objetos distribuídos através dos nós do cluster que podem ser processadas em paralelo. Elas oferecem tolerância a falhas e são capazes de recuperar dados perdidos automaticamente.

A alternativa A descreve o papel atribuído ao gerenciador de cluster, como o YARN ou o próprio gerenciador autônomo do Spark (Standalone), não ao Spark Core.

A alternativa C confunde o papel do Spark Core com o da Spark SQL e da DataFrame API, que possibilitam a integração com diversos sistemas de armazenamento e consulta de dados estruturados.

A alternativa D refere-se ao Spark Streaming, um componente do ecossistema Spark dedicado ao processamento de fluxos contínuos de dados em tempo real.

A alternativa E descreve as funcionalidades do MLlib, a biblioteca de Machine Learning do Spark, e não o Spark Core.",8026916
tópico 0,Ingestão de dados em streaming,"Questão: Dentro do contexto de Big Data, a ingestão de dados em tempo real ou streaming é um componente crucial para sistemas que requerem análise rápida e tomada de decisão baseada em informações atualizadas. Considerando as estratégias de ingestão de dados em streaming, avalie as seguintes afirmações sobre ferramentas e conceitos relacionados:

I. Apache Kafka é uma plataforma de streaming de eventos que pode ser usada para construir pipelines de dados robustos, capazes de lidar com altas vazões de dados em tempo real.
II. Apache Flume é uma ferramenta desenhada especificamente para ingestão de dados em lote, portanto, não é adequada para cenários que demandam processamento de streaming.
III. O Apache Storm permite o processamento de fluxos de dados em tempo real, mas não garante a entrega exatamente uma vez (exactly-once semantics), o que pode ser uma limitação para aplicações que exigem alta confiabilidade nas transmissões.

Com base nas afirmações, é correto o que se afirma em:

A) I, apenas.
B) II, apenas.
C) I e III, apenas.
D) II e III, apenas.
E) I, II e III.

",C,"

Explicação dos itens:

I. Esta afirmação é verdadeira. O Apache Kafka é uma plataforma de processamento de streams projetada para lidar com altos volumes de dados em tempo real, sendo muito utilizada para construir pipelines de dados resilientes.

II. Esta afirmação é falsa. Embora o Apache Flume seja frequentemente associado com a ingestão de dados para Hadoop, ele também é capaz de processar dados em streaming. Por isso, ele pode ser usado em cenários que demandam tanto processamento de lote quanto processamento em tempo real.

III. Esta afirmação é parcialmente verdadeira. O Apache Storm oferece recursos de processamento de fluxos de dados em tempo real com diferentes garantias de processamento, incluindo at-least-once e exactly-once semantics, dependendo da configuração. No entanto, a frase pode induzir ao erro ao mencionar que não garante a entrega exactly-once, que é uma característica disponível na ferramenta em uma configuração apropriada. Assim, seria mais exato dizer que o Storm tem a capacidade de garantir a entrega exactly-once, mas oferece também outros níveis de garantia para diferentes necessidades.

Portanto, apenas as afirmações I e III estão corretas, considerando a ambiguidade da informação fornecida em III e o erro em II.",7879282
tópico 0,Ingestão de dados em lote (batch),"Questão: Em processos de transformação digital, a ingestão de dados em lote, conhecida como batch processing, é um método de processamento de dados no qual os dados são coletados, inseridos e processados em lotes em intervalos agendados. Qual das seguintes afirmativas é verdadeira em relação à ingestão de dados em lote?

A) A ingestão de dados em lote é mais indicada para situações onde os dados precisam ser processados em tempo real e com baixa latência.
B) Os sistemas de processamento em lote são tipicamente mais complexos e caros que os sistemas de processamento de fluxo contínuo (streaming).
C) A ingestão de dados em lote pode ser complementada por sistemas de processamento de fluxo contínuo para otimizar a análise de dados em cenários híbridos.
D) A ingestão em lote é preferencialmente realizada em bases de dados não relacionais, visto que sistemas relacionais não suportam bem grandes volumes de ingestão de dados periodicamente.
E) Processamento em lote não é apropriado para grandes volumes de dados, sendo limitado a pequenos conjuntos de informações devido à sua alta demanda de recursos.

",C,"

Explicação dos itens:

A) Este item está incorreto, pois a ingestão de dados em lote não é a mais indicada para situações que requerem processamento em tempo real ou com baixa latência. Cenários que exigem processamento em tempo real tendem a utilizar processamento de fluxo contínuo (streaming).

B) Esta opção é falsa, pois não há uma regra geral que determine que um sistema de processamento em lote seja mais complexo ou caro do que sistemas de processamento de fluxo contínuo. A complexidade e o custo podem variar de acordo com a arquitetura do sistema, a escala de dados e os requisitos específicos do projeto.

C) Esta afirmação é verdadeira. A ingestão de dados em lote pode ser efetivamente combinada com processamento de fluxo contínuo para criar soluções híbridas, oferecendo flexibilidade para tratar diferentes necessidades de processamento e análise de dados.

D) Este item é falso. Bancos de dados relacionais podem sim ser usados para ingestão de dados em lote, embora sistemas NoSQL possam oferecer vantagens em cenários que envolvem grandes volumes ou variedade de dados. Não há uma exclusividade do uso de bases não relacionais para processamentos em lote.

E) Esta afirmativa é incorreta. O processamento em lote é frequentemente usado para grandes volumes de dados, justamente porque permite o processamento de grandes quantidades de informações de modo eficiente quando não é necessária a análise em tempo real.",2781522
tópico 0,Armazenamento de big data,"Questão:
Ao planejar uma infraestrutura para armazenamento e processamento de grandes volumes de dados, um cientista de dados deve considerar diferentes tecnologias e abordagens. No contexto do Big Data, cada tecnologia oferece certas vantagens e desvantagens em termos de escalabilidade, desempenho e flexibilidade. Entre as opções a seguir, qual tecnologia é mais adequada para cenários em que dados estruturados e semi-estruturados crescem rapidamente, e há necessidade de acesso em tempo real com baixa latência?

A) Hadoop Distributed File System (HDFS)
B) Relational Database Management System (RDBMS)
C) NoSQL Databases
D) Data Warehousing
E) Network Attached Storage (NAS)

",C," 

Explicação dos itens:

A) Hadoop Distributed File System (HDFS) - É adequado para armazenar grandes volumes de dados, mas não é otimizado para acesso com baixa latência devido ao seu modelo baseado em processamento em lote.

B) Relational Database Management System (RDBMS) - Embora seja muito bom para gerenciar dados estruturados, ele pode ter dificuldades para escalar horizontalmente de forma eficaz quando os volumes de dados são muito grandes.

C) NoSQL Databases - Estas bases de dados são projetadas para armazenar e recuperar grandes volumes de dados estruturados, semi-estruturados e não estruturados. São altamente escaláveis e oferecem acesso em tempo real com baixa latência, o que é ideal para aplicações de Big Data que exigem alto desempenho.

D) Data Warehousing - São excelentes para analisar grandes volumes de dados, mas geralmente não são a melhor escolha para necessidades de acesso em tempo real, pois são otimizados para processamento de consultas complexas em lote, não para baixa latência.

E) Network Attached Storage (NAS) - É bom para adição simples de armazenamento em redes de computadores mas não é projetado especificamente para necessidades de Big Data e acesso de baixa latência aos dados.",7202535
tópico 0,Processamento distribuído,"Questão: Em sistemas de processamento distribuído, o modelo CAP (Consistência, Disponibilidade e Tolerância à Partição) oferece um quadro teórico para compreensão das limitações do armazenamento distribuído. De acordo com o teorema CAP, é impossível para um sistema de computação distribuída fornecer simultaneamente mais que duas das três garantias seguintes:

A) Consistência instantânea em toda a rede, Disponibilidade para leitura/escrita em todas as circunstâncias e Tolerância a pelo menos um nó falhando.
B) Concorrência de transações, Persistência de dados mesmo em caso de falhas e Tolerância à latência em todas as operações.
C) Integração contínua de dados, Disponibilidade em cenários de rede fragmentada e Suporte a múltiplos modelos de consistência.
D) Consistência de dados entre todos os nós a qualquer momento, Disponibilidade para todas as solicitações e Performance otimizada para cada nó.
E) Consistência forte em todas as operações, Disponibilidade constante para todas as operações e Tolerância a partições de rede arbitrárias.

",E,"

O teorema CAP estabelece que em um sistema distribuído é possível fornecer no máximo duas das três garantias propostas: consistência (todos os nós veem os mesmos dados no mesmo momento), disponibilidade (garantia de que cada solicitação recebe uma resposta sobre se foi bem sucedida ou falhou) e tolerância a partições (o sistema continua a operar apesar de um número arbitrário de mensagens ser perdidas ou atrasadas pela rede). A alternativa E é a correta, pois ""Consistência forte"" refere-se a todos os nós tendo a mesma visão dos dados, ""Disponibilidade constante"" indica que o sistema sempre atende às operações e ""Tolerância a partições de rede arbitrárias"" indica que o sistema pode lidar com falhas na comunicação entre subconjuntos de nós. Todas as outras alternativas ou descrevem conceitos que não estão diretamente relacionados ao teorema CAP ou mencionam condições que não fazem parte do mesmo. 

- Alternativa A: Introduz ""Consistência instantânea"" em vez de ""Consistência forte"", mas é incorreta por afirmar a possibilidade de fornecer ""Disponibilidade para leitura/escrita em todas as circunstâncias"", o que não seria possível ao mesmo tempo que as outras duas condições do CAP.
- Alternativa B: Esta opção introduz ""Concorrência de transações"" e ""Persistência de dados"", conceitos que são importantes em processamento distribuído, porém, não especificamente descritos pelo teorema CAP.
- Alternativa C: Menciona ""Integração contínua de dados"" e ""Suporte a múltiplos modelos de consistência"", o que foge ao escopo do teorema CAP, que fala apenas de consistência forte.
- Alternativa D: Aborda ""Performance otimizada para cada nó"", o que não tem relação com as três características fundamentais discutidas no teorema CAP. Além disso, a alternativa falha ao sugerir que todas as solicitações possam ser atendidas, o que não é garantido quando se opta por forte consistência e tolerância a partições, já que pode comprometer a disponibilidade.",3305750
tópico 0,Soluções de big data: Arquitetura do ecossistema Spark,"Qual das seguintes afirmações melhor descreve a função do Spark Streaming dentro da arquitetura do ecossistema Spark?

A) Spark Streaming é uma extensão do core Spark API que habilita a análise e processamento de dados em lotes, otimizado para trabalhos que não exigem processamento em tempo real.

B) É um sistema de arquivos distribuídos que oferece alta vazão de acesso aos dados da aplicação e é utilizado pelo Spark para armazenagem de dados.

C) Spark Streaming é uma ferramenta para processamento de fluxos de dados em tempo real, integrada ao Spark, que permite o processamento contínuo de dados à medida que são recebidos.

D) Representa um motor de consulta distribuída que permite a execução de consultas SQL relacionais sobre dados estruturados e semi-estruturados no Spark.

E) É um módulo do Spark dedicado à execução de grafos iterativos e algoritmos de aprendizado de máquina de forma distribuída e otimizada.

",C,"

A opção A está incorreta porque o Spark Streaming é focado em dados em tempo real, não em lotes. A opção B descreve o Hadoop Distributed File System (HDFS) e não uma funcionalidade do Spark. A opção D refere-se ao Spark SQL, que serve para executar consultas SQL sobre conjuntos de dados no Spark. A opção E descreve o GraphX e o MLlib, que são módulos para processamento de grafos e aprendizado de máquina, respectivamente, dentro do ecossistema Spark. A opção C é correta porque o Spark Streaming é um componente do Apache Spark focado no processamento de dados em tempo real por meio de micro-lotes, permitindo análises de fluxo contínuo.",2471927
tópico 0,"Arquitetura de cloud computing para ciência de dados (AWS, Azure, GCP)","Questão: Em um ambiente de ciência de dados onde a escalabilidade e a análise de grandes volumes de dados são essenciais, a escolha da arquitetura de cloud computing é crítica para o desempenho e custo-efetividade dos projetos. Considerando as soluções de cloud computing da AWS, Azure e GCP, qual das seguintes opções oferece uma combinação de serviços que se destacam para o processamento analítico de grandes volumes de dados?

A) AWS com Amazon S3 para armazenamento de dados, Amazon EC2 para computação e Amazon EMR para processamento de big data.

B) Azure com Azure Blob Storage para armazenamento de dados, Azure Functions para computação e Azure HDInsight para processamento de big data.

C) GCP com Google Cloud Storage para armazenamento de dados, Google Compute Engine para computação e Google Dataproc para processamento de big data.

D) AWS com Amazon RDS para armazenamento de dados, Amazon ECS para computação e AWS Glue para processamento de big data.

E) Azure com Azure Table Storage para armazenamento de dados, Azure Virtual Machines para computação e Azure Machine Learning service para processamento de big data.

",C,"

Explicação dos itens:

A) Amazon S3, EC2 e EMR são de fato poderosos serviços oferecidos pela AWS para armazenamento, computação e processamento de big data, respectivamente. No entanto, a combinação mencionada na alternativa C é mais especializada em termos de análise de big data.

B) Azure Blob Storage, Azure Functions e Azure HDInsight são também adequados para o cenário descrito. No entanto, Azure Functions é mais orientado para a execução de código sem servidor em resposta a eventos e não é tão focado em tarefas de computação intensiva típicas da ciência de dados.

C) Esta é a opção correta. Google Cloud Storage é uma escolha robusta para armazenamento de dados, Google Compute Engine oferece poder computacional escalável e o Google Dataproc é um serviço rápido, fácil de usar e totalmente gerenciado para processamento de big data baseado em Hadoop e Spark, altamente apropriado para a ciência de dados.

D) Amazon RDS é um serviço de banco de dados relacional e não é o mais indicado para armazenamento de grandes volumes de dados não estruturados, que são comuns em ciência de dados. Amazon ECS é um serviço de orquestração de contêineres e não é primariamente usado para computação de alto desempenho. AWS Glue é um serviço de integração de dados, e não está exclusivamente focado em processamento de big data.

E) Azure Table Storage é mais adequado para armazenamento de dados estruturados e não é ideal para as demandas de armazenamento típicas da ciência de dados. Azure Virtual Machines pode fornecer computação, mas não é tão específico para ciência de dados quanto algumas das outras opções de máquinas virtuais otimizadas para isso. Azure Machine Learning service é uma plataforma de aprendizado de máquinas e não é voltada para processamento de big data como tal.",337968
tópico 0,Conceitos de processamento massivo e paralelo,"Questão: A arquitetura de sistemas distribuídos para processamento massivo e paralelo de dados tem seu desempenho otimizado por meio do uso de técnicas e ferramentas específicas para lidar com grandes volumes de dados e tarefas computacionais complexas. Considerando o cenário de processamento de grandes volumes de dados em tempo real, qual das seguintes afirmações melhor descreve a abordagem que é mais eficiente para garantir escalabilidade e tolerância a falhas?

A) Utilização de um único servidor centralizado de alta performance para realizar o processamento dos dados, uma vez que os núcleos de processamento e a memória são compartilhados, garantindo assim maior velocidade de comunicação entre processos.

B) Implementação de processamento em batch, onde os dados são processados em lotes a intervalos regulares de tempo, permitindo alta taxa de throughput e eficiência no uso dos recursos computacionais.

C) Emprego de um modelo de processamento paralelo utilizando GPUs para realizar cálculos matemáticos intensivos, aproveitando a arquitetura de milhares de núcleos para processar tarefas de forma simultânea.

D) Aplicação de técnicas de MapReduce e sistemas de processamento de fluxo de dados (data streaming) como Apache Kafka, visando um processamento distribuído que permite a análise contínua e em tempo real de dados.

E) Emprego de sistemas de arquivos distribuídos, como o Hadoop Distributed File System (HDFS), para armazenamento redundante, sem, contudo, realizar qualquer forma de processamento paralelo ou distribuído dos dados.

",D,"

Explicação dos itens:

A) A utilização de um único servidor centralizado cria um ponto único de falha e é inadequada para ambientes que requerem escalabilidade horizontal, pois não distribui a carga entre múltiplos nódulos.

B) O processamento em batch não é o ideal para dados em tempo real, pois envolve atrasos inerentes ao processamento em lotes e não atende a necessidades de processamento contínuo e instantâneo.

C) O uso de GPUs é adequado para cálculos matemáticos intensivos e pode ser uma parte do processamento massivo, mas por si só não aborda questões de distribuição de carga e tolerância a falhas no processamento de grandes volumes de dados.

D) A aplicação de técnicas como MapReduce e data streaming permite processar grandes quantidades de dados de maneira distribuída e em tempo real, oferecendo uma alta escalabilidade e tolerância a falhas, sendo assim a escolha mais eficiente para cenários de grande volume de dados.

E) Sistemas de arquivos distribuídos como o HDFS são essenciais para armazenar grandes volumes de dados de forma redundante, mas não são suficientes por si só para processamento paralelo e distribuído, que é uma característica necessária para análise de dados em tempo real.",5634542
tópico 0,Ingestão de dados em streaming,"Questão: A ingestão de dados em streaming é um componente crítico em sistemas de processamento em tempo real. Vários sistemas de análise de dados, como Apache Kafka, Apache Storm e Apache Flink, são empregados para lidar com grandes volumes de dados em movimento rápido. Qual das seguintes afirmações descreve mais precisamente uma vantagem do Apache Flink em comparação com o Apache Storm para ingestão de dados em streaming?

A) O Apache Flink suporta processamento exato-uma-vez (exactly-once) de forma mais eficiente, reduzindo a possibilidade de dados duplicados.
B) O Apache Storm oferece latências mais baixas para mensagens em comparação ao Apache Flink, tornando o Storm mais adequado para cenários de tempo real.
C) O Apache Flink é menos escalável do que o Apache Storm, o que pode ser um fator limitante para grandes volumes de dados.
D) O Apache Flink é uma plataforma de processamento de eventos complexos (CEP), enquanto o Apache Storm não oferece suporte a essa funcionalidade.
E) O Apache Storm suporta nativamente o processamento de gráficos complexos através do Apache Giraph, enquanto o Apache Flink não.

",A,"

Explicação dos itens:

A) Correto. O Apache Flink oferece um forte garantia de entrega exato-uma-vez (exactly-once), o que se traduz em um controle mais preciso sobre a consistência dos dados e idempotência em operações. Isso é vantajoso para evitar a duplicação de dados em comparação ao Apache Storm, que também suporta esse tipo de garantia, mas de forma menos eficiente.

B) Incorreto. Na verdade, o Apache Flink é conhecido por ter latências muito baixas, comparáveis ou até melhores do que o Apache Storm. Além disso, o Storm foi historicamente utilizado para processamento em tempo real, mas o Flink oferece uma abordagem mais moderna e eficaz com melhor desempenho de latência.

C) Incorreto. O Apache Flink é altamente escalável e adequado para grandes volumes de dados, assim como o Apache Storm. Ambas as plataformas são projetadas para escalar horizontalmente e lidar com altas taxas de transferência de dados.

D) Incorreto. Enquanto o Apache Flink possui funcionalidades avançadas para o processamento de eventos complexos (CEP), o Apache Storm também pode ser usado para esse tipo de processamento, embora com um conjunto de funcionalidades diferenciado e talvez menos sofisticado.

E) Incorreto. O Apache Flink tem suporte para processamento de gráficos através da biblioteca Gelly, enquanto que a integração do Apache Storm com o Apache Giraph não é uma característica padrão do Storm para processamento de gráficos; geralmente, isso requer uma configuração e integração adicional.
",4406267
tópico 0,Processamento distribuído,"Questão: Em um sistema de processamento distribuído, a coordenação de processos é um desafio crítico para garantir a eficiência e consistência das operações. Considerando um cenário onde há múltiplas instâncias de um serviço trabalhando com dados distribuídos e replicados, qual dos seguintes algoritmos ou técnicas é mais adequada para garantir exclusão mútua em operações críticas, mantendo a consistência dos dados replicados?

A) Round Robin
B) Eleição de Bully
C) Algoritmo de Lamport
D) MapReduce
E) Algoritmo de Raft

",C,"

Explicação dos itens:

A) Round Robin: É um algoritmo utilizado para agendamento de tarefas, mas não garante exclusão mútua em sistemas distribuídos e consistência de dados replicados.

B) Eleição de Bully: É um algoritmo usado para eleição de coordenadores em sistemas distribuídos, porém não trata do problema de exclusão mútua diretamente em operações críticas.

C) Algoritmo de Lamport: Também conhecidos como relógios lógicos de Lamport, é utilizado em sistemas distribuídos para garantir a ordem de eventos e pode ser empregado para implementar exclusão mútua, ajudando na consistência dos dados replicados.

D) MapReduce: É um modelo de processamento utilizado para processar grandes volumes de dados de forma paralela e distribuída, mas não lida com exclusão mútua em operações críticas.

E) Algoritmo de Raft: Este é um algoritmo de consenso usado em sistemas distribuídos, porém, seu objetivo principal é gerenciar um log de comandos distribuídos, não sendo específico para exclusão mútua em operações com dados replicados.",4632410
tópico 0,Armazenamento de big data,"Questão: Considerando a exigência de manusear volumes massivos de dados, o uso de sistemas distribuídos para o armazenamento de big data se faz essencial para garantir eficiência e escalabilidade. Um cientista de dados responsável por elaborar uma estrutura de armazenamento de big data deve levar em consideração vários aspectos para garantir a performance e a confiabilidade do sistema. Dentre as tecnologias e conceitos listados a seguir, qual NÃO se relaciona diretamente com o armazenamento e processamento eficiente de big data em ambientes distribuídos?

A) Hadoop Distributed File System (HDFS) - projetado para armazenar grandes volumes de dados em clusters de computadores.

B) MapReduce - utilizado para processamento paralelo e distribuído de grandes conjuntos de dados.

C) Spark - um framework de processamento de dados distribuídos que melhora a velocidade de tarefas de big data.

D) Online Transaction Processing (OLTP) - utilizado para o processamento de transações online interativas e eficientes.

E) NoSQL Databases - bancos de dados projetados para escalar horizontalmente e lidar com diferentes tipos de estruturas de dados não relacionais.

",D," 

Explicação dos itens:

A) Hadoop Distributed File System (HDFS) é uma parte essencial do ecossistema Hadoop e é amplamente usado para armazenar grandes quantidades de dados de forma distribuída, permitindo alta taxa de transferência de dados entre os nodos do cluster.

B) O MapReduce é um modelo de programação e uma estrutura associada à processagem e geração de grandes conjuntos de dados com um modelo de computação paralela e distribuída, intimamente ligado ao Hadoop.

C) O Spark é um sistema de processamento de big data que melhora significativamente a velocidade de aplicações em relação ao modelo tradicional do Hadoop MapReduce e é utilizado para processamento em memória.

D) Online Transaction Processing (OLTP) é um sistema focado na gestão de transações de banco de dados, otimizado para processamento de operações diárias de negócio, que não é projetado especificamente para lidar com as complexidades e características do big data.

E) Bancos de dados NoSQL são projetados para escalar de maneira elástica e são uma opção comum para armazenamento de big data, suportando várias estruturas de dados, como documentos e grafos, e são uma boa opção para lidar com a variedade de dados não estruturados ou semi-estruturados.",1112937
tópico 0,Soluções de big data: Arquitetura do ecossistema Spark,"Questão: Em ambientes de processamento de grandes volumes de dados, o Apache Spark se destaca como uma ferramenta essencial no ecossistema de big data. Com relação à arquitetura do Apache Spark, assinale a alternativa correta sobre o seu funcionamento e seus componentes principais.

A) Spark SQL é uma ferramenta auxiliar que só pode ser utilizada para consultas em bancos de dados NoSQL, não sendo compatível com fontes de dados que utilizam o SQL como linguagem de consulta.
B) O Spark Streaming é incapaz de processar dados em tempo real, pois foi projetado apenas para o processamento de lotes (batch) de dados que são coletados em intervalos regulares de tempo.
C) O MLib é uma biblioteca do Spark focada em aprendizado de máquina que fornece algoritmos de machine learning escaláveis e de alto desempenho, que podem trabalhar tanto com dados armazenados em disco quanto em memória.
D) O GraphX permite a execução de algoritmos gráficos e manipulação de grafos de maneira otimizada, mas requer que todos os dados sejam previamente convertidos para o formato Hadoop HDFS para que sejam processados.
E) O Resilient Distributed Dataset (RDD) é um conjunto de dados distribuídos imutável que pode ser processado em paralelo, mas devido à sua natureza volátil, não pode persistir os dados em memória entre diferentes operações.

",C,"

Explicação:

A) A afirmação é incorreta. Spark SQL é um módulo do Apache Spark para trabalhar com dados estruturados e semiestruturados, e oferece suporte para diversos formatos de dados, incluindo bancos de dados que utilizam SQL.

B) Esta afirmação está incorreta. O Spark Streaming é uma extensão do núcleo do Spark que permite processamento de dados em tempo real (streaming), além de processamento em lotes.

C) Esta é a alternativa correta. MLib é a biblioteca de machine learning do Apache Spark, que oferece diversas ferramentas e algoritmos para aprendizado de máquina, trabalhando com dados na memória e em disco de forma eficiente.

D) A afirmação está incorreta. Embora o GraphX seja uma biblioteca para manipulação de grafos e execução de algoritmos gráficos no Spark, ele não requer que os dados sejam convertidos para o formato Hadoop HDFS.

E) Esta afirmação é parcialmente correta em afirmar que RDDs são coleções de dados imutáveis que podem ser processadas em paralelo, mas está equivocada ao dizer que não podem persistir dados em memória. RDDs são projetados para tirar vantagem da persistência em memória para operações interativas e rápidas.",4608595
tópico 0,Ingestão de dados em lote (batch),"Questão: No contexto de processamento de dados em grandes sistemas de informação, a ingestão de dados em lote (batch) é uma estratégia fundamental para o armazenamento e a análise de grandes volumes de dados. Considerando as características e as melhores práticas para a ingestão de dados em lote em um Data Warehouse, analise as seguintes afirmações:

I. A ingestão de dados em lote prioriza a latência em detrimento do throughput, visando o processamento de dados em tempo real.

II. Durante a ingestão de dados em lote, os dados são frequentemente coletados de múltiplas fontes antes de serem processados e armazenados, o que pode incluir o uso de mecanismos de ETL (Extract, Transform, Load).

III. Um dos desafios da ingestão de dados em lote é o gerenciamento de cargas de trabalho, que deve balancear a utilização eficiente dos recursos computacionais com a necessidade de processar os dados dentro de uma janela de tempo adequada.

IV. A ingestão de dados em lote não beneficia-se de mecanismos de paralelização, visto que o processamento sequencial é intrínseco a esse método.

Assinale a opção correta:

A) Apenas as afirmações I e II estão corretas.
B) Apenas as afirmações II e III estão corretas.
C) Apenas as afirmações III e IV estão corretas.
D) Apenas a afirmação II está correta.
E) Todas as afirmações estão corretas.

",B,"

Explicação dos itens:

I. Incorreta. A ingestão de dados em lote é conhecida por priorizar o throughput, ou seja, a quantidade de dados que pode processar em determinado tempo, em contraponto ao processamento em tempo real, que enfatiza a latência baixa.

II. Correta. A ingestão de dados em lote geralmente envolve a coleta de dados de diversas fontes e pode utilizar processos de ETL para transformar e carregar os dados em um Data Warehouse.

III. Correta. O gerenciamento de cargas de trabalho é um aspecto importante da ingestão de dados em lote, onde é necessário otimizar o uso de recursos enquanto atende-se às janelas de processamento estabelecidas.

IV. Incorreta. A ingestão de dados em lote pode se beneficiar substancialmente de mecanismos de paralelização, especialmente quando se lida com grandes volumes de dados, possibilitando o processamento mais eficaz através da distribuição de cargas de trabalho.",3320473
tópico 0,"Arquitetura de cloud computing para ciência de dados (AWS, Azure, GCP)","Questão: Em um projeto de ciência de dados de larga escala, é essencial escolher uma arquitetura de cloud computing que otimize o processamento de grandes volumes de dados, permitindo análises complexas e a construção de modelos preditivos. Considere as seguintes características das plataformas de cloud computing:

I. O serviço Azure HDInsight, um serviço de análise de dados baseado em open-source que fornece clusters Hadoop, Spark, R Server, HBase, e Storm.
II. O AWS SageMaker, uma plataforma de machine learning que permite aos cientistas de dados e desenvolvedores construir, treinar e implantar modelos de machine learning rapidamente.
III. O Google Cloud Dataproc, um serviço rápido de processamento de dados em batch e stream que permite executar cluster Hadoop e Spark.
IV. A funcionalidade AWS Lambda, que permite rodar código sem provisionar ou gerenciar servidores, executando o código apenas quando necessário e escalando automaticamente.

Com base nesse cenário, qual a ferramenta mais apropriada para um cientista de dados que precisa processar um grande conjunto de dados não estruturados, realizar análises exploratórias de dados e construir modelos de machine learning com a capacidade de escalar automaticamente conforme a demanda?

A) I. Azure HDInsight
B) II. AWS SageMaker
C) III. Google Cloud Dataproc
D) IV. AWS Lambda

",B,"

Explicação dos itens:

I. Azure HDInsight é uma ferramenta que fornece serviços de análise de dados e suporta várias tecnologias de big data como Hadoop e Spark. É útil para processar dados em larga escala, mas não é especializado em machine learning e pode não oferecer a mesma conveniência para criar e implantar modelos de ML.

II. AWS SageMaker é uma plataforma que engloba todo o ciclo de vida de machine learning, desde a preparação e análise de dados até a construção e implantação de modelos. Além disso, gerencia automaticamente a infraestrutura necessária, oferecendo escalabilidade e eficiência.

III. Google Cloud Dataproc é um serviço que permite a execução rápida de jobs de Hadoop e Spark, oferecendo boas capacidades para processamento de dados. Contudo, não é tão focado em machine learning quanto o AWS SageMaker.

IV. AWS Lambda é um serviço de computação que executa código em resposta a eventos e gerencia automaticamente os recursos computacionais. Embora ofereça escalabilidade, não é uma ferramenta específica para ciência de dados e pode não ser ideal para análise de dados e modelagem de ML do ponto de vista da conveniência e das funcionalidades especializadas.
",5138944
tópico 0,Conceitos de processamento massivo e paralelo,"Questão: Em um ambiente de processamento de dados em larga escala, diferentes abordagens são adotadas para realizar computação eficiente sobre grandes volumes de dados. Nesse contexto, uma companhia deseja implementar um sistema capaz de processar grandes conjuntos de dados distribuídos geograficamente, privilegiando escalabilidade e tolerância a falhas.

Considerando os paradigmas de processamento massivo e paralelo, qual das seguintes tecnologias é mais adequada para atender essas necessidades?

A) Banco de dados relacional tradicional.
B) Processamento em lote utilizando Hadoop.
C) Processamento em tempo real com sistemas transacionais online.
D) Computação de alto desempenho (HPC) utilizando supercomputadores.
E) Armazenamento em array de discos de rede (NAS).

",B,"

Explicação dos itens:

A) Banco de dados relacional tradicional - Não é a mais adequada para processamento massivo e paralelo de dados distribuídos geograficamente, pois esses bancos não foram desenhados com foco em escalabilidade horizontal e podem ter dificuldades em lidar com grandes volumes de dados e processamento distribuído.

B) Processamento em lote utilizando Hadoop - Correta. O Hadoop é uma plataforma de software que permite o processamento distribuído de grandes conjuntos de dados através de clusters de computadores, oferecendo escalabilidade e uma alta tolerância a falhas. É uma solução adequada para a realização de análises e processamento de dados em larga escala.

C) Processamento em tempo real com sistemas transacionais online - Apesar de ser eficaz para processamento em tempo real, os sistemas transacionais online (OLTP) não são os mais adequados para análise massiva e processamento paralelo de grandes volumes de dados, pois são otimizados para transações rápidas e não para análises complexas.

D) Computação de alto desempenho (HPC) utilizando supercomputadores - O HPC é voltado para tarefas que exigem grandes capacidades de cálculo e não necessariamente para processamento de dados distribuídos e pode representar um alto custo, além de uma complexidade de implementação desnecessária para a tarefa descrita.

E) Armazenamento em array de discos de rede (NAS) - O NAS é uma solução de armazenamento e não uma tecnologia de processamento de dados, portanto, não é adequada para o cenário que requer processamento massivo e paralelo de dados.",9171266
tópico 0,Ingestão de dados em streaming,"Questão: Em um cenário onde uma empresa precisa processar e analisar dados de eventos de sensores em tempo real para tomar decisões rápidas e informadas, qual das seguintes opções é a mais adequada para ingestão de dados em streaming?

A) Banco de Dados Relacional, devido à sua capacidade de garantir a consistência e a integridade dos dados por meio de transações ACID.

B) Hadoop Distributed File System (HDFS), pois oferece armazenamento distribuído e facilita o processamento de grandes volumes de dados estruturados.

C) Apache Kafka, um sistema de mensagens distribuído que oferece alta vazão e baixa latência para processamento de mensagens em tempo real.

D) Apache HBase, ideal para escritas e leituras aleatórias em grandes conjuntos de dados distribuídos, garantindo rapidez no acesso aos dados.

E) Sistema de Arquivos Tradicional, por ser simples de implementar e manter, além de oferecer uma base sólida para manipulação de dados em lotes.

",C,"

Explicação dos itens:

A) Banco de Dados Relacional é projetado principalmente para dados transacionais, e não é otimizado para o alto volume e velocidade que o processamento de streaming de dados requer.

B) Hadoop Distributed File System (HDFS) é mais adequado para processamento de lotes e dados estruturados grandes, mas não é projetado para baixa latência ou processamento em tempo real.

C) Apache Kafka é uma plataforma de streaming de eventos, projetada especificamente para lidar com altas vazões de dados e para conseguir processar e transmitir esses dados em tempo real, o que é ideal para o cenário descrito.

D) Apache HBase é um banco de dados não relacional que opera em cima do Hadoop File System, e enquanto é bom para acesso rápido a grandes volumes de dados, não é especializado em processamento em tempo real de fluxos de dados contínuos.

E) Sistema de Arquivos Tradicional não é adequado para o processamento em tempo real devido à sua incapacidade de lidar com altas vazões de dados em streaming e não fornece as características necessárias para análises em tempo real.",4387531
tópico 0,Ingestão de dados em streaming,"Questão:
A ingestão de dados em streaming é um componente fundamental da arquitetura de dados moderna, especialmente em cenários que requerem análise em tempo real. Na implementação de sistemas que suportam ingestão de dados em streaming, qual das seguintes tecnologias NÃO é tradicionalmente usada para processar e armazenar dados em tempo real?

A) Apache Kafka
B) Apache Hadoop HDFS
C) Amazon Kinesis
D) Apache Flink
E) Apache Storm

",B,"

Explicação dos itens:
A) Apache Kafka: É um sistema de processamento de mensagens distribuído projetado para suportar dados em fluxo (streaming), sendo uma escolha comum para pipelines de dados em tempo real.

B) Apache Hadoop HDFS: O Hadoop Distributed File System (HDFS) é usado principalmente para armazenamento de dados em larga escala e processamento em batch, não sendo otimizado para latência baixa ou processamento de streaming em tempo real.

C) Amazon Kinesis: É um serviço de streaming de dados na nuvem da Amazon Web Services, que permite o processamento de dados em tempo real, similar ao Apache Kafka.

D) Apache Flink: Uma engine de processamento de dados em streaming que permite transformações complexas em tempo real, com suporte para altas vazões e baixa latência.

E) Apache Storm: Um sistema de computação distribuída para o processamento de dados em streaming, capaz de processar milhões de mensagens por segundo, destinado para uso em tempo real.",1428934
tópico 0,Processamento distribuído,"Questão: A implementação de sistemas de processamento distribuído tornou-se cada vez mais comum para enfrentar os desafios de processamento de grandes volumes de dados e computação intensiva. Em relação aos modelos de consistência de dados em um sistema de processamento distribuído, avalie as afirmações a seguir e assinale a opção correta.

I - O modelo de consistência estrita (strong consistency) garante que, após a atualização de um dado, qualquer leitura subsequente refletirá esta mudança, independentemente do nó na rede que está sendo acessado.

II - No modelo de consistência eventual (eventual consistency), as réplicas podem divergir temporariamente, sendo permitido que leituras recentes retornem versões antigas dos dados, mas garantindo sua convergência após um determinado tempo.

III - Consistência causal permite que escritas relacionadas causalmente sejam vistas por todos os nós na mesma ordem, mas não impõe nenhuma ordem entre escritas não relacionadas.

IV - O modelo de consistência sequencial (sequential consistency) exige que as operações de todos os nós sejam vistas na mesma ordem serial global, mesmo que isso possa contrariar a causalidade das operações.

Assinale a opção que apresenta a(s) afirmativa(s) correta(s):

A) Apenas I e II.
B) Apenas II e III.
C) Apenas I, II e III.
D) Apenas II, III e IV.
E) I, II, III e IV.

",C,"

Explicação dos itens:

I - Correta. O modelo de consistência estrita é o mais forte e garante que cada leitura refletirá a escrita mais recente.

II - Correta. A consistência eventual permite divergências temporárias entre as réplicas, garantindo eventualmente que todas as réplicas se atualizarão.

III - Correta. A consistência causal relaciona a visibilidade das operações com a sua causalidade, garantindo que as dependências causais sejam respeitadas em leituras e escritas.

IV - Incorreta. O modelo de consistência sequencial não exige uma ordem serial global que contrarie a causalidade. Ele apenas requer que as operações apareçam em uma ordem sequencial consistente, mas não necessariamente a ordem real em que as operações ocorreram, desde que a sequência de operações seja mantida igual em todos os nós.
",5490937
tópico 0,Soluções de big data: Arquitetura do ecossistema Spark,"Questão:
A plataforma de big data Apache Spark é conhecida por sua capacidade de processar grandes volumes de dados de forma rápida e eficiente, sendo uma ferramenta importante para cientistas de dados, engenheiros de dados e empresas que trabalham com análises avançadas. Sobre a arquitetura do ecossistema Spark, é correto afirmar que:

A) O Spark Streaming é a ferramenta dentro do ecossistema que permite o processamento de batch apenas, não sendo capaz de lidar com fluxos contínuos de dados.

B) O Apache Spark é uma plataforma que opera exclusivamente em memória, o que impede a persistência de dados em disco em caso de falhas ou necessidade de recuperação de dados.

C) O MLib é o componente do Spark focado em aprendizado de máquina (machine learning), oferecendo uma vasta gama de algoritmos e utilitários para construção de modelos preditivos.

D) O GraphX é utilizado para processamento de linguagem natural no ecossistema Spark, fornecendo recursos avançados de análise semântica e morfológica de textos.

E) A arquitetura do Spark não suporta a integração com sistemas de arquivos distribuídos como Hadoop Distributed File System (HDFS), restringindo o seu desempenho em ambientes distribuídos.

",C,"

A alternativa (A) é incorreta porque o Spark Streaming permite o processamento em tempo real, lidando com fluxos contínuos de dados e não apenas processamento em lotes (batch). A alternativa (B) é incorreta, pois, apesar do Spark otimizar o processamento em memória com o conceito de Resilient Distributed Datasets (RDDs), ele ainda permite persistir dados em disco para recuperação e falhas. A alternativa (C) é correta, pois o MLib é o módulo de machine learning do Spark, fornecendo um conjunto de algoritmos de aprendizado de máquina de alta qualidade e fácil uso. A alternativa (D) é incorreta porque o GraphX é o componente do Spark para processamento de graphs e análises de grafos, e não está diretamente ligado ao processamento de linguagem natural. Por fim, a alternativa (E) também é incorreta, porque o Spark foi projetado para se integrar bem com o Hadoop e outros sistemas de arquivos distribuídos, não limitando seu desempenho em ambientes distribuídos.",9797809
tópico 0,"Arquitetura de cloud computing para ciência de dados (AWS, Azure, GCP)","Questão: Em um cenário de ciência de dados, onde a escalabilidade e a segurança dos dados são cruciais, uma empresa optou por uma solução de cloud computing que permite o armazenamento e análise de grandes volumes de dados, bem como a execução de modelos de Machine Learning e Deep Learning de maneira eficiente. Considerando as ofertas de serviços das três grandes provedoras de cloud (AWS, Azure e GCP), qual das seguintes configurações poderia atender melhor às necessidades desta empresa?

A) Utilização do Amazon S3 para armazenamento de dados, Amazon EC2 para computação e processamento e AWS SageMaker para treinamento e implantação de modelos de Machine Learning.

B) Configuração de máquinas virtuais no Microsoft Azure Virtual Machines para armazenamento e processamento de dados, juntamente com Azure Machine Learning Service para o desenvolvimento de modelos.

C) Implementação do Google Cloud Storage para armazenamento, Google Compute Engine para processamento e Google AI Platform para treinamento e implantação de modelos de Machine Learning.

D) Utilização do AWS Glacier para armazenamento de dados a longo prazo, AWS Lambda para processamento serverless e Azure Machine Learning Service para construção e gerenciamento de modelos de Machine Learning.

E) Criação de buckets no Azure Blob Storage para armazenamento, uso do Azure Kubernetes Service para escalabilidade de aplicações e Google AI Platform para treinamento e implantação de modelos.
",A,"
A alternativa correta é a letra A, pois descreve uma configuração de serviços completamente integrados da AWS, líder de mercado, que suportariam todas as necessidades listadas na questão. 

Explicação dos itens:

A) Amazon S3 é um serviço de armazenamento altamente escalável, Amazon EC2 fornece capacidade de computação segura e redimensionável e AWS SageMaker é uma plataforma completa que facilita a criação, treinamento e implantação de modelos de Machine Learning, tornando-a uma escolha adequada para a empresa.

B) Esta alternativa é tecnicamente viável, mas menos específica quanto à escalabilidade e integração entre os serviços oferecidos pela Azure frente à solução da AWS fornecida na Alternativa A.

C) Google Cloud também oferece uma solução integrada de armazenamento, processamento e Machine Learning, mas a Alternativa A é mais alinhada com as necessidades expressas no enunciado e a AWS é conhecida pela sua extensa documentação e suporte, o que pode ser um diferencial.

D) Esta configuração mistura serviços de diferentes provedores (AWS e Azure), o que pode complicar a integração e gestão. Além disso, AWS Glacier é desenhado para armazenamento a longo prazo e baixa frequência, e não para o cenário de ciência de dados onde o acesso rápido aos dados é necessário.

E) Como a Alternativa D, esta opção também mistura serviços de diferentes provedores (Azure e GCP), o que pode gerar complexidade na integração. Além disso, não é a solução mais integrada disponível, em comparação com a Alternativa A.
",6977344
tópico 0,Ingestão de dados em lote (batch),"Questão: Em sistemas de processamento de dados de grande escala, a ingestão em lote (batch) é uma técnica common para a movimentação e processamento de grandes volumes de dados para análises ou armazenamento. No contexto de um data warehouse que opera sob a paradigmática de ingestão de dados em lote, qual das seguintes afirmativas é INCORRETA?

A) A ingestão em lote geralmente é programada para ocorrer em intervalos regulares, como diariamente ou semanalmente, dependendo das necessidades de negócio.

B) Os dados ingeridos por meio de batches são frequentemente provenientes de diversas fontes e requerem processo de limpeza e transformação antes de serem armazenados.

C) Processos em lote são adequados para situações que exigem latência baixa e processamento em tempo real para a tomada de decisão imediata.

D) Ingestão em lote pode resultar em economias de escala, pois o processamento de grandes volumes pode ser mais eficiente em termos de recursos computacionais do que o modelo de processamento de stream.

E) A configuração de jobs em lote normalmente envolve a definição de dependências entre tarefas e a garantia de que os dados sejam carregados na sequência correta.

",C," 
Os itens A, B, D, e E estão corretos, pois destacam características típicas do processamento de dados em lote, como acontece em intervalos regulares (A), a necessidade de limpeza e transformação de dados de diversas fontes (B), a eficiência em escala para grandes volumes (D), e a organização de jobs e dependências (E). O item C é incorreto, porque processos em lote não são adequados para situações que exigem resposta imediata ou baixa latência, uma vez que são concluídos em intervalos de tempo programados, e não em tempo real, o que é característico de processamento de stream.",3804030
tópico 0,Armazenamento de big data,"Questão:
O armazenamento de big data representa um dos maiores desafios tecnológicos na era da informação, devido ao volumoso conjunto de dados estruturados e não estruturados que as empresas e organizações geram continuamente. Pensando nisso, qual das seguintes tecnologias é considerada inapropriada para o armazenamento eficiente de big data?

A) Hadoop Distributed File System (HDFS) 
B) NoSQL databases
C) Sistema de arquivos convencionais
D) Data Lakes
E) Apache Cassandra

",C," 

Explicação dos itens:
A) Hadoop Distributed File System (HDFS) é um sistema de arquivos distribuídos projetado para rodar em hardware de baixo custo, sendo bastante eficaz para armazenamento e processamento de grandes volumes de dados, característica fundamental do big data.

B) NoSQL databases são projetados para armazenar e recuperar grandes volumes de dados distribuídos em um sistema que não adere ao modelo relacional tradicional, tornando-os muito adequados para aplicações de big data que frequentemente envolvem dados não estruturados ou semi-estruturados.

C) Sistema de arquivos convencionais não são projetados para lidar com o tamanho e a complexidade dos conjuntos de dados de big data. Eles podem sofrer com problemas de escalabilidade e performance ao tratar volumes de dados tão grandes e de rápida mudança.

D) Data Lakes são repositórios que permitem armazenar uma vasta quantidade de dados em seu formato bruto e nativo, o que é ideal para big data, já que permite a flexibilidade de armazenar diferentes tipos de dados e a aplicação de esquemas de dados somente quando necessário para análise.

E) Apache Cassandra é um banco de dados distribuído altamente escalável que é projetado para gerenciar grandes quantidades de dados em muitos servidores, oferecendo alta disponibilidade sem ponto único de falha, adequado para cenários de big data.",7829945
tópico 0,Conceitos de processamento massivo e paralelo,"Questão: Na computação de alto desempenho, especialmente no processamento de dados massivos, são empregados conceitos e técnicas que possibilitam a execução eficiente de tarefas computacionais intensivas. Considerando os paradigmas de processamento paralelo e distribuído, analise as afirmativas a seguir e assinale a opção correta.

I - O processamento paralelo é caracterizado pela execução simultânea de múltiplas operações em um único conjunto de dados, o que pode ser facilitado pela utilização de múltiplos núcleos em um único processador.

II - MapReduce é um modelo de programação associado ao processamento paralelo, o qual envolve duas funções principais: ‘Map’, que filtra e organiza os dados, e ‘Reduce’, que realiza a agregação dos resultados após o mapeamento.

III - A consistência eventual é uma estratégia usada em sistemas de processamento paralelo que prioriza a disponibilidade e tolerância a partições em detrimento de garantir a consistência imediata dos dados após uma operação de escrita.

IV - O uso de GPUs (Unidades de Processamento Gráfico) para processamento de dados massivos é um exemplo de computação em grid, onde os recursos computacionais são dispersos geograficamente.

A) Apenas as afirmativas I e II estão corretas.
B) Apenas as afirmativas I, II e III estão corretas.
C) Apenas as afirmativas II e III estão corretas.
D) Apenas as afirmativas I, III e IV estão corretas.
E) Todas as afirmativas estão corretas.

",B,"

Explicação dos itens:

I - Correta. O processamento paralelo se beneficia de arquiteturas com múltiplos núcleos ou múltiplos processadores para realizar operações em paralelo.

II - Correta. MapReduce é um modelo de programação utilizado para processar grandes quantidades de dados em paralelo, dividido principalmente nas etapas de Map (mapeamento) e Reduce (redução).

III - Incorreta. A consistência eventual é um conceito associado a sistemas de processamento distribuído e não paralelo. Ela é uma característica de sistemas que permitem a replicação de dados em diferentes nós, potencialmente levando algum tempo até que todos os nós atinjam um estado consistente.

IV - Incorreta. O uso de GPUs é um exemplo de paralelismo a nível de hardware e é utilizado para processamento de dados de alta intensidade, mas não define a computação em grid. A computação em grid conecta computadores que estão geograficamente dispersos, mas são utilizados principalmente para trabalhar em um único problema computacional como um sistema virtual único.",6386229
tópico 0,Armazenamento de big data,"Questão: Em um cenário de Big Data, a eficácia do processamento e recuperação da informação é crucial para o sucesso das análises realizadas. Nesse contexto, qual das seguintes tecnologias é considerada mais adequada para o armazenamento e gerenciamento de grandes volumes de dados não estruturados ou semi-estruturados, garantindo escalabilidade horizontal e alta disponibilidade?

A) SQLite
B) Hadoop Distributed File System (HDFS)
C) RAID 5
D) Microsoft SQL Server
E) Redundant Array of Independent Disks (RAID) Nível 1

",B,"

Explicação dos itens:

A) SQLite: É um sistema de gerenciamento de banco de dados relacional leve, não é apropriado para Big Data devido à falta de suporte para distribuição e paralelismo.

B) Hadoop Distributed File System (HDFS): É uma tecnologia especialmente projetada para armazenar e gerenciar grandes volumes de dados distribuídos, oferecendo alta disponibilidade e escalabilidade horizontal, tornando-a uma escolha ideal para armazenamento de Big Data. É por isso essa é a resposta correta.

C) RAID 5: RAID 5 é uma configuração de discos que oferece redundância de dados e melhora o desempenho para leitura, mas em contraste com as necessidades de Big Data, não escala bem horizontalmente e não oferece as mesmas vantagens no gerenciamento de grandes volumes de dados não estruturados.

D) Microsoft SQL Server: É um sistema de gerenciamento de banco de dados relacional fornecido pela Microsoft. Apesar de ter capacidades para lidar com grandes volumes de dados, não foi construído especificamente para os desafios associados a Big Data como HDFS.

E) Redundant Array of Independent Disks (RAID) Nível 1: Oferece redundância por meio de espelhamento de disco. RAID 1 não é ideal para Big Data devido a limitações em escalabilidade e eficiência no armazenamento e processamento de dados não estruturados ou semi-estruturados em larga escala.",2661614
tópico 0,Ingestão de dados em lote (batch),"Questão: Em um processo de análise de dados empresariais, a eficiência na ingestão de dados em lote é essencial para garantir que as informações estejam disponíveis para tomada de decisões estratégicas. Dentre as seguintes alternativas, qual descreve melhor a característica que NÃO está associada à ingestão de dados em lote (batch)?

A) Alta latência na disponibilização dos dados após o início da ingestão.
B) Processamento de grandes volumes de dados em intervalos de tempo recorrentes.
C) capacidade de lidar com picos de dados devido à sua natureza agendada.
D) Requerimento de processamento em tempo real para análise imediata dos dados.
E) Possibilidade de otimização e limpeza dos dados antes da análise final ser realizada.

",D," 
A ingestão de dados em lote é caracterizada pelo processamento de grandes volumes de dados de forma programada, em vez de em tempo real. A alternativa A refere-se à alta latência, que é uma característica comum da ingestão em lote devido ao tempo necessário para coletar e processar grandes volumes de dados antes de disponibilizá-los. A alternativa B está correta, pois a ingestão em lote é projetada para lidar com grandes volumes de dados em intervalos definidos. A alternativa C também está correta, indicando que a ingestão em lote pode lidar com picos de dados, pois os dados são coletados e processados em intervalos, ao invés de continuamente. A alternativa E é uma vantagem dessa abordagem, onde há a oportunidade de realizar a otimização e limpeza dos dados. A alternativa D é incorreta para a ingestão de dados em lote, pois está associada à ingestão de dados em tempo real (streaming), onde os dados são processados e analisados imediatamente após a chegada.",5660443
tópico 0,Conceitos de processamento massivo e paralelo,"Questão:

Considerando os conceitos de processamento massivo de dados e computação paralela, analise as afirmativas abaixo e assinale a opção correta.

I. O processamento massivo de dados muitas vezes depende da computação paralela para dividir tarefas complexas em subtarefas menores que podem ser executadas simultaneamente, reduzindo significativamente o tempo de processamento.

II. O modelo MapReduce, utilizado pelo framework Hadoop, é um exemplo de paradigma de programação que facilita a computação distribuída, permitindo o processamento paralelo de grandes volumes de dados em clusters de computadores.

III. No processamento paralelo, a consistência de dados não é uma preocupação, pois cada tarefa paralela opera de forma completamente independente e não há necessidade de coordenar o acesso aos dados entre as tarefas.

IV. GPUs (Unidades de Processamento Gráfico) são exemplo de hardware que permite o processamento paralelo, mas não são adequadas para tarefas de computação de alto desempenho, ficando restritas ao processamento gráfico.

A) Apenas I e II estão corretas.
B) Apenas I, II e III estão corretas.
C) Todas estão corretas.
D) Apenas I, II e IV estão corretas.
E) Apenas II e III estão corretas.

",A,"

Explicação dos itens:

I. Correta. De fato, o processamento massivo de dados se beneficia da computação paralela para gerenciar e processar grandes volumes de informações de forma mais eficiente dividindo as tarefas.

II. Correta. O MapReduce é um modelo de programação que simplifica a computação distribuída, sendo uma ferramenta essencial para processamento massivo de dados no Hadoop.

III. Incorreta. A consistência dos dados é uma questão importante no processamento paralelo. Coordenar o acesso e as atualizações aos dados é crucial para evitar condições de corrida e garantir resultados consistentes.

IV. Incorreta. GPUs são amplamente utilizadas para tarefas de computação de alto desempenho além de processamento gráfico, como na aprendizagem de máquina e análises científicas, devido à sua capacidade de realizar cálculos paralelos de forma eficiente.",5332852
tópico 0,Soluções de big data: Arquitetura do ecossistema Spark,"Questão:
A utilização de tecnologias de Big Data é essencial para lidar com o volume crescente de dados gerados atualmente. O Apache Spark é uma dessas tecnologias, que proporciona um framework de processamento de dados de alta velocidade para aplicações em Big Data. Dentre os componentes do ecossistema Spark listados abaixo, qual é responsável por estender as capacidades do Spark para o processamento de fluxo contínuo de dados em tempo real?

A) Spark SQL
B) Spark Streaming
C) Spark MLlib
D) Spark GraphX
E) Spark Core

",B," Explicação dos itens:

A) Spark SQL - Incorreto. O Spark SQL é o módulo do Apache Spark focado em processamento de dados estruturados e integração com SQL, permitindo a execução de consultas SQL.

B) Spark Streaming - Correto. Spark Streaming é o componente do Apache Spark que permite o processamento de dados em tempo real através de fluxos contínuos, facilitando o desenvolvimento de aplicações que necessitam reagir a dados em tempo real.

C) Spark MLlib - Incorreto. Spark MLlib é a biblioteca de machine learning do Spark, fornecendo diversos algoritmos e ferramentas para construir aplicações de aprendizado de máquinas.

D) Spark GraphX - Incorreto. Spark GraphX é o componente para processamento de grafos e análises gráficas, oferecendo ferramentas para trabalhar com grafos e realizar operações comuns sobre eles.

E) Spark Core - Incorreto. O Spark Core é a base do ecossistema Spark, fornecendo as funcionalidades fundamentais de agendamento de tarefas, gerenciamento de memória, tratamento de falhas, dentre outros. Ele não é especificamente designado para processamento de dados em tempo real, embora forneça as bases sobre as quais o Spark Streaming é construído.",5784374
tópico 0,"Arquitetura de cloud computing para ciência de dados (AWS, Azure, GCP)","Questão: 

Dentro do contexto de arquitetura de cloud computing para ciência de dados, considere que uma organização deseja otimizar a análise de big data e machine learning em uma escala global. A empresa busca utilizar serviços que permitem o processamento de grandes volumes de dados, com um ambiente altamente escalável, flexível e com capacidade de automatizar modelos de predição. Levando em conta os serviços oferecidos pelas principais provedoras de nuvem – Amazon Web Services (AWS), Microsoft Azure e Google Cloud Platform (GCP) –, qual das opções abaixo apresenta a combinação correta de serviços/tecnologias para atender a esses requisitos?

A) AWS - Amazon S3, AWS Glue, Amazon SageMaker e AWS Lambda.
B) Azure - Azure Functions, Azure Cosmos DB, Azure Machine Learning Studio e Azure Blob Storage.
C) GCP - Google BigQuery, Google Cloud Functions, Google ML Engine e Google Cloud Storage.
D) AWS - Amazon DynamoDB, Amazon Redshift, Amazon Lex e Amazon EC2 Container Service.
E) Azure - Azure Table Storage, Azure SQL Data Warehouse, Azure Databricks e Azure HDInsight.

",C," 

Explicação dos itens:

A) A opção A elenca serviços adequados da AWS para análise de big data e machine learning com AWS SageMaker, entretanto, AWS Lambda é mais utilizado para computação sem servidor e não se destina especificamente à análise de big data.

B) Os serviços listados na opção B são válidos serviços de cloud da Azure, contudo, Azure Blob Storage não é otimizado para o tratamento de big data como o Azure Data Lake Storage seria, por exemplo.

C) A opção C é a correta. Ela lista serviços do GCP ideais para análise de big data e machine learning. Google BigQuery permite o processamento de grandes volumes de dados para análise, Google Cloud Functions é um ambiente de execução serverless para construção de microsserviços, Google ML Engine oferece um ambiente para criar, treinar e implementar modelos de machine learning, e Google Cloud Storage é adequado para armazenar grandes quantidades de dados.

D) Amazon DynamoDB é uma base de dados NoSQL e não se destina ao processamento de big data como o Amazon EMR. Amazon Redshift é adequado para data warehousing, mas Amazon Lex é um serviço de reconhecimento de fala e chatbot, e o Amazon EC2 Container Service (agora Amazon ECS) é um gerenciador de contêineres que não se relaciona diretamente com a análise de big data.

E) Azure Table Storage é um armazenamento NoSQL chave-valor que não é ideal para cenários de big data. Azure SQL Data Warehouse (agora parte do Azure Synapse Analytics) é uma solução de data warehousing, e Azure HDInsight e Azure Databricks são serviços apropriados para big data, mas a combinação não abrange o escopo completo requisitado como a opção C.",8072269
tópico 0,Ingestão de dados em streaming,"Questão:
A capacidade de processar e analisar dados em tempo real é cada vez mais crucial em diversos setores como financeiro, telecomunicações, saúde, entre outros. Com isso, a ingestão de dados em streaming torna-se um componente importante na arquitetura de sistemas modernos. Considerando uma arquitura de dados que emprega Kafka para processar fluxos de dados continuamente, qual das seguintes afirmações melhor descreve uma prática recomendada para garantir alta disponibilidade e tolerância a falhas no processo de ingestão de dados?

A) Utilizar somente um tópico com uma única partição, simplificando o gerenciamento do sistema.
B) Configurar um número de partições menor do que o número de consumidores para otimizar o uso dos recursos.
C) Permitir que todos os produtores e consumidores tenham acesso a todas as partições para garantir a máxima flexibilidade.
D) Distribuir as partições uniformemente entre diferentes brokers e criar réplicas dessas partições.
E) Manter os registros de offsets em um banco de dados externo para facilitar a recuperação em caso de falhas.

",D," 

A alternativa D é a correta, pois ao distribuir as partições de forma uniforme entre diversos brokers e criar réplicas de cada partição, você aumenta a tolerância a falhas e a disponibilidade do sistema. Se um dos brokers falhar, as réplicas garantem que os dados ainda estão acessíveis e o processamento pode continuar.

A alternativa A é incorreta porque ter apenas uma partição e um tópico é um ponto único de falha e não escala bem com o aumento do volume de dados.
A alternativa B é incorreta porque geralmente se busca configurar um número de partições igual ou superior ao número de consumidores para permitir uma distribuição eficiente da carga de trabalho e paralelismo na leitura dos dados.
A alternativa C é imprópria, pois não se permite que todos os produtores e consumidores acessem todas as partições indiscriminadamente, para que se possa controlar melhor o acesso aos dados e a distribuição dos mesmos eficientemente entre os consumidores.
A alternativa E é inadequada já que Kafka já gerencia os offsets de forma integrada, e deslocar essa responsabilidade para um banco de dados externo desnecessariamente complicaria o sistema e poderia introduzir latências adicionais.",8785655
tópico 0,Processamento distribuído,"Questão: No contexto de sistemas de processamento distribuído, uma das principais vantagens é a capacidade de escalabilidade horizontal. Isso permite que o sistema expanda seus recursos de acordo com a demanda, adicionando mais nós ao cluster. Entretanto, para que essa expansão seja eficaz, certos princípios e técnicas devem ser aplicados. Qual das seguintes opções NÃO é considerada uma estratégia ou característica essencial para garantir a escalabilidade eficaz em um ambiente de processamento distribuído?

A) Particionamento de dados para distribuir consistentemente a carga de trabalho entre nós.
B) Uso de um algoritmo de eleição para garantir a consistência em ambientes com falha.
C) Manter um estado global persistente que todos os nós devem consultar antes de qualquer operação.
D) Balanceamento de carga para assegurar a distribuição uniforme das solicitações entre os servidores.
E) Tolerância a falhas através de mecanismos de replicação de dados e recuperação automática de erros.

",C," 
Explicação dos itens:

A) Correto: Particionamento de dados permite que o sistema distribua dados e carga de trabalho de maneira eficiente entre diferentes nós, evitando gargalos e pontos únicos de falha.

B) Correto: O uso de algoritmos de eleição é crucial em sistemas distribuídos para garantir que, mesmo na presença de falhas, haja um consenso sobre qual nó deve assumir determinadas funções críticas no sistema.

C) Incorreto: Manter um estado global persistente que necessita ser consultado por todos os nós antes de qualquer operação é uma estratégia que pode levar à criação de gargalos e reduz a escalabilidade, uma vez que centraliza o processamento e cria dependência entre os nós.

D) Correto: O balanceamento de carga é fundamental para assegurar que nenhum nó fique sobrecarregado, permitindo que o sistema como um todo trabalhe de maneira mais eficiente.

E) Correto: A tolerância a falhas é um requisito para sistemas de processamento distribuído, pois garante que o sistema continue operando mesmo que alguns nós falhem, através da replicação de dados e recuperação automática de erros.",2818213
tópico 0,"Arquitetura de cloud computing para ciência de dados (AWS, Azure, GCP)","Questão:
A arquitetura de nuvem para projetos de ciência de dados precisa acomodar uma variedade de serviços para processamento de dados, armazenamento escalável, machine learning e orquestração de workflows. Utilizando a plataforma AWS (Amazon Web Services), um cientista de dados está projetando uma solução end-to-end que necessita das seguintes funcionalidades: coleta e ingestão de grandes volumes de dados em tempo real, processamento e análise desses dados, treino e implementação de modelos de aprendizado de máquina, e a orquestração desses processos de maneira escalável e eficiente. Quais dos seguintes serviços do AWS atenderiam melhor as necessidades listadas, respectivamente?

A) AWS Kinesis, AWS Lambda, AWS SageMaker, AWS Step Functions
B) AWS Glue, AWS Redshift, AWS SageMaker, AWS Batch
C) AWS Kinesis, AWS EC2, AWS ML Services, AWS Data Pipeline
D) AWS Data Pipeline, AWS DynamoDB, AWS DeepRacer, AWS CloudFormation
E) AWS Direct Connect, AWS Athena, AWS SageMaker, AWS OpsWorks

",A,"

Explicação dos itens:
A) AWS Kinesis é um serviço destinado à coleta e análise de dados em tempo real, ideal para a ingestão de grandes volumes de dados. AWS Lambda é um serviço de computação que executa código em resposta a eventos, sendo útil para processamento e análise de dados sem a necessidade de gerenciar servidores. AWS SageMaker oferece a possibilidade de construir, treinar e implementar modelos de machine learning de maneira acessível. AWS Step Functions permite a coordenação de workflows de componentes de microserviços, incluindo tarefas de machine learning, de forma visual e com controle de fluxo de execução.

B) AWS Glue é um serviço de ETL (extração, transformação e carga), enquanto o AWS Redshift é um armazém de dados, ambos são mais adequados para processos de transformação de dados em lotes do que para processamento em tempo real. AWS Batch é mais focado na execução de tarefas de computação em lote ao invés de orquestração de processos complexos.

C) AWS EC2 fornece capacidade computacional sob demanda, porém não é otimizado para processamento de dados em tempo real, o que o AWS Kinesis faz melhor. AWS ML Services é uma gama de serviços de aprendizado de máquina, mas não corresponde a um serviço específico da AWS. AWS Data Pipeline é um serviço de orquestração mais voltado para o deslocamento e transformação de dados em lote do que para orquestração geral.

D) AWS Data Pipeline pode facilitar o movimento de dados, mas não é o melhor para ingestão em tempo real. AWS DynamoDB é um banco de dados NoSQL, o que não atende diretamente à necessidade de processamento e análise de dados em tempo real. AWS DeepRacer não é um serviço de machine learning, mas sim uma plataforma de corrida autônoma para fins educacionais relacionados a aprendizado por reforço. AWS CloudFormation é voltado para a criação e gerenciamento de recursos da AWS com templates, mas não para orquestração de processos de ciência de dados.

E) AWS Direct Connect é usado para estabelecer uma conexão de rede dedicada de um ambiente on-premises para a AWS, não está relacionado à ingestão de dados em tempo real. AWS Athena é um serviço de consulta interativo, excelente para análises sobre dados já armazenados, mas não para ingestão ou processamento em tempo real. AWS OpsWorks é um serviço de gerenciamento de configuração que não é específico para orquestração de workflows de ciência de dados.",4738886
tópico 0,Processamento distribuído,"Questão: No contexto de sistemas de processamento distribuído, diversos algoritmos são utilizados para gerenciar a comunicação, a consistência dos dados e a coordenação entre os nós. Considere um sistema com um conjunto de processos que colaboram entre si, onde cada um executa em um nó de um cluster distribuído e precisa manter uma visão consistente do estado compartilhado do sistema. As estratégias adotadas precisam assegurar tanto a exclusão mútua em seções críticas quanto a ordem dos eventos em todo o sistema. Neste cenário, qual dos seguintes algoritmos é apropriado para garantir que as operações sejam realizadas na ordem correta, mesmo na presença de falhas nos nós e comunicação assíncrona?

A) Algoritmo de Lamport's Bakery para exclusão mútua.
B) Eleição de líder com o algoritmo do Bully.
C) Algoritmo de Dijkstra para o problema do caminho mínimo.
D) Relógios Vetoriais para ordenação de eventos.
E) Protocolo Simplex para transmissão de dados.

",D,"

Explicação dos itens:

A) Algoritmo de Lamport's Bakery para exclusão mútua. - Este algoritmo é eficaz para garantir a exclusão mútua em sistemas distribuídos, mas não trata da ordenação de eventos entre os processos do sistema.

B) Eleição de líder com o algoritmo do Bully. - Este processo é utilizado para eleger um líder num conjunto de processos distribuídos, porém não está diretamente relacionado com a manutenção da ordem dos eventos no sistema.

C) Algoritmo de Dijkstra para o problema do caminho mínimo. - O algoritmo de Dijkstra resolve o problema do caminho mínimo em grafos e não é aplicado para ordenação ou coordenação de eventos em sistemas distribuídos.

D) Relógios Vetoriais para ordenação de eventos. - Os relógios vetoriais são utilizados em sistemas distribuídos para manter a ordenação dos eventos de forma consistente em todo o sistema, mesmo diante de falhas e comunicação assíncrona. É a opção correta para o contexto descrito na questão.

E) Protocolo Simplex para transmissão de dados. - O protocolo Simplex é um método de comunicação unidirecional e não aborda a questão da ordenação de eventos em sistemas distribuídos.",9521090
tópico 0,Soluções de big data: Arquitetura do ecossistema Spark,"Questão: Considerando o ecossistema de big data, especialmente no que diz respeito ao Apache Spark, qual componente arquitetônico é responsável pela distribuição e manutenção do estado dos dados e tarefas computacionais entre os clusters, garantindo simultaneamente a tolerância a falhas e a eficiência no processamento distribuído?

A) Spark Streaming
B) Spark Core
C) Spark SQL
D) MLlib
E) GraphX

",B,"

Explicação dos itens:

A) Spark Streaming: Errado, pois é o módulo do Spark utilizado para processamento de dados em tempo real (streaming), e não para a gerência da distribuição e manutenção do estado dos dados.

B) Spark Core: Correta, é a base do ecossistema Spark, fornecendo a plataforma fundamental sobre a qual todos os outros componentes funcionam. O Spark Core contém a funcionalidade para gerenciamento de tarefas, memória e tolerância a falhas.

C) Spark SQL: Errado, pois é usado para processamento de dados estruturados e suporte a SQL, integrando-se com o Spark Core, mas não é responsável pela manutenção do estado dos dados e tarefas.

D) MLlib: Errado, é a biblioteca de aprendizado de máquina (machine learning) do Spark para tarefas de ciência de dados, e não para gerenciamento da distribuição de dados e tarefas.

E) GraphX: Errado, pois é a API de processamento de grafos do Spark, permitindo a criação, transformação e análise de grafos, mas não é o componente responsável pela distribuição e manutenção do estado dos dados nos clusters.",5229533
tópico 0,Conceitos de processamento massivo e paralelo,"Questão:

As arquiteturas de sistemas para processamento massivo e paralelo são essenciais para lidar com grandes volumes de dados e operações complexas em muitos campos como finanças, ciência de dados, bioinformática e muitos outros. Sobre essas arquiteturas, avalie as seguintes afirmativas e escolha a opção correta:

I) O processamento em lote (batch processing) geralmente é mais apropriado para tarefas que exigem processamento em tempo real devido à sua natureza instantânea de processamento.
II) O processamento paralelo permite que múltiplas operações sejam executadas ao mesmo tempo, o que pode reduzir o tempo necessário para processar grandes quantidades de dados.
III) MapReduce é um modelo de programação e uma técnica de execução associada ao processamento paralelo que ajuda a processar grandes conjuntos de dados de forma distribuída em clusters de computadores.
IV) O processamento de fluxo contínuo (stream processing) é tipicamente implementado para processar dados em tempo real, onde os dados são processados assim que chegam, diferente do processamento em lote que espera a acumulação de dados.

A) Apenas as afirmativas II e III estão corretas.
B) Apenas as afirmativas II, III e IV estão corretas.
C) Apenas as afirmativas I, II e III estão corretas.
D) Todas as afirmativas estão corretas.
E) Apenas as afirmativas I, III e IV estão corretas.

",B," 

Explicação:

I) Esta afirmativa está incorreta porque o processamento em lote não é projetado para tarefas que requerem processamento em tempo real; ao contrário, ele é ideal para processar grandes volumes de dados onde não é necessária uma resposta imediata.

II) Esta afirmativa está correta. O processamento paralelo permite executar múltiplas operações simultaneamente, o que pode reduzir significativamente o tempo de processamento de grandes volumes de dados, aumentando a eficiência.

III) Esta afirmativa está correta. MapReduce é realmente um modelo de programação e um framework para processar grandes conjuntos de dados de forma paralela e distribuída em clusters de computadores. Ele é largamente utilizado em sistemas como o Hadoop.

IV) Esta afirmativa está correta. O processamento de fluxo contínuo ou stream processing é utilizado para processar dados em tempo real, processando-os à medida que são recebidos, o que é diferente do processamento em lote que acumula dados antes de processá-los.",136730
tópico 0,Armazenamento de big data,"Questão: No contexto de big data, o armazenamento adequado dos dados coletados é fundamental para garantir eficiência no processamento e na análise das informações. Em cenários que envolvem volumes massivos de dados não estruturados e estruturados, as soluções tradicionais de armazenamento frequentemente se mostram inadequadas. Com base nesse contexto, qual das seguintes opções melhor representa uma tecnologia de armazenamento projetada para lidar eficientemente com as demandas de big data?

A) Sistemas de arquivos tradicionais, como o NTFS e o HFS+.
B) Bancos de dados relacionais, como o MySQL e o Oracle Database.
C) Dispositivos de armazenamento de área de rede (SAN).
D) Sistemas de arquivos distribuídos, como o Hadoop Distributed File System (HDFS).
E) Armazenamento direto conectado (DAS), utilizando interfaces como SATA ou SAS.

",D," Alternativa A refere-se aos sistemas de arquivos tradicionais que não são projetados para lidar com o volume, a variedade e a velocidade dos dados gerados em um cenário de big data. Alternativa B trata de bancos de dados relacionais, que podem apresentar limitações em termos de escalabilidade horizontal e manipulação de grandes variedades de dados não estruturados. Alternativa C menciona dispositivos de armazenamento de área de rede, que embora úteis para compartilhar armazenamento entre múltiplos servidores, não são idealmente otimizados para o processamento distribuído característico do big data. Alternativa D está correta pois o HDFS é um sistema de arquivos distribuídos projetado especificamente para armazenar grandes volumes de dados, permitindo operações em cluster e otimizado para cenários de big data. Alternativa E trata do armazenamento direto conectado, que é limitado pela capacidade de um único servidor e não proporciona a mesma escalabilidade ou capacidade de processamento distribuído necessária para big data.",7844647
tópico 0,Ingestão de dados em streaming,"Questão: No contexto do processamento de dados em streaming, um dos maiores desafios é lidar com o volume e a velocidade da entrada de dados em tempo real. Qual das seguintes plataformas é conhecida por facilitar a ingestão de dados em streaming, permitindo o processamento em tempo real e a análise de grandes fluxos de dados?

A) MySQL
B) Hadoop Distributed File System (HDFS)
C) Microsoft Excel
D) Apache Kafka
E) Adobe Analytics

",D," 

Explicação dos itens:

A) MySQL é um sistema de gerenciamento de banco de dados relacional, que não é especializado em ingestão de dados em streaming ou processamento em tempo real.

B) Hadoop Distributed File System (HDFS) é um sistema de arquivos distribuídos projetado para armazenar dados em larga escala e processá-los em lote, mas não é ideal para cenários que exigem processamento em tempo real.

C) Microsoft Excel é uma ferramenta de planilha eletrônica que não é adequada para a ingestão ou processamento de dados em streaming, sendo mais usada para análise de dados de forma interativa e não contínua.

D) Apache Kafka é uma plataforma de streaming distribuído que permite a ingestão de grandes volumes de dados em tempo real, processamento de streams e integração com diversos sistemas de processamento de dados, sendo uma solução adequada para o desafio apresentado na questão.

E) Adobe Analytics é uma solução para análise de dados de marketing digital e web analytics, não sendo uma ferramenta focada na ingestão de dados em streaming ou processamento em tempo real de grandes volumes de dados.",9865644
tópico 0,Ingestão de dados em lote (batch),"Questão: Em sistemas de processamento de dados, a ingestão de dados em lote (batch) é uma abordagem fundamental para o processamento de grandes volumes de informação de forma eficiente. Em comparação com o processamento de fluxo contínuo (streaming), qual das seguintes afirmações descreve uma característica típica da ingestão de dados em lote?

A) A ingestão de dados em lote é adequada para cenários onde a latência baixa é uma exigência crítica.
B) Os dados em lote são processados em tempo real, garantindo assim a atualização constante dos sistemas.
C) Geralmente, a ingestão de dados em lote envolve a análise de dados que foram acumulados em um período de tempo antes do processamento.
D) A ingestão em lote é inviável para grandes conjuntos de dados, sendo mais eficiente para volumes reduzidos de informação.
E) O processamento de dados em lote elimina a necessidade de bases de dados duráveis, uma vez que os dados são imediatamente processados e descartados.

",C,"

Explicação dos itens:
A) Incorreta. A ingestão de dados em lote não é a abordagem mais adequada quando a exigência é por baixa latência, já que existe um atraso entre a coleta de dados e seu processamento.
B) Incorreta. Esta afirmação descreve uma característica do processamento de fluxo contínuo (streaming), e não do processamento em lote.
C) Correta. A ingestão de dados em lote é caracterizada por processar conjuntos de dados que foram acumulados ao longo de um período definido antes de serem processados em conjunto.
D) Incorreta. A ingestão em lote é muitas vezes a abordagem preferida justamente para grandes conjuntos de dados, uma vez que permite o processamento eficiente em um momento posterior, sem a necessidade de processar cada elemento de dados em tempo real.
E) Incorreta. O processamento em lote não necessariamente elimina a necessidade de bases de dados duráveis. Dados persistentes podem ser necessários para recuperação em caso de falhas ou para análises históricas.",9717319
tópico 0,Ingestão de dados em streaming,"Questão: Na arquitetura de processamento de dados em tempo real, a ingestão de dados em streaming é um componente crítico que permite a absorção e processamento imediato de fluxos contínuos de informações. Considerando um cenário onde se deseja analisar dados de transações financeiras em tempo real para detecção de fraudes, qual das seguintes tecnologias NÃO é tradicionalmente utilizada para ingestão de dados em streaming?

A) Apache Kafka
B) Apache Flink
C) Apache Storm
D) Amazon Kinesis
E) Redis

",B,"

Explicação dos itens:

- A) Apache Kafka: É um sistema de mensageria e streaming de eventos de código aberto, amplamente usado para a ingestão de dados em streaming, sendo uma escolha comum para esse tipo de cenário.
  
- B) Apache Flink: Embora o Apache Flink seja uma poderosa plataforma de processamento de stream, ele é tipicamente utilizado para processar, não para ingerir dados em fluxo. Portanto, ele não é uma resposta correta para a questão da ingestão de dados em si. 
  
- C) Apache Storm: É um sistema de computação distribuída para processar grandes volumes de dados em streaming, também usado para ingestão de dados em caso de cenários onde o processamento em tempo real é necessário.
  
- D) Amazon Kinesis: É um serviço oferecido pela AWS para processamento de grandes fluxos de dados em tempo real, igualmente utilizado para ingestão de dados em streaming.
  
- E) Redis: É um armazenamento de estrutura de dados em memória usado como banco de dados, cache e corretor de mensagens. Embora algumas configurações possam usar Redis para ingestão de mensagens, ele não é uma tecnologia tradicionalmente associada à ingestão de dados em streaming, mas sim mais conhecido por suas capacidades de armazenamento rápido e como cache. No entanto, dentro das opções apresentadas, o Apache Flink é o que menos se relaciona à função de ingestão em si.",9256906
tópico 0,Soluções de big data: Arquitetura do ecossistema Spark,"Questão: A arquitetura do ecossistema Apache Spark oferece uma série de componentes projetados para atender às diversas necessidades de processamento e análise de big data. Cada componente é especializado em uma funcionalidade dentro do processamento de dados em larga escala. Assinale a opção que CORRETAMENTE associa o componente do Spark ao seu propósito principal.

A) Spark SQL - Facilita o processamento de dados estruturados e suporta SQL para a execução de tarefas de ETL e a interação com dados através de comandos SQL.

B) Spark Streaming - É o módulo responsável pela otimização de recursos em ambientes de cluster, assegurando a eficiência na alocação dos recursos de hardware durante o processamento de dados.

C) Spark MLlib - É o serviço de armazenamento e gerenciamento de metadados do ecossistema Spark, gerenciando informações sobre diferentes fontes de dados.

D) GraphX - O propósito desse componente é fornecer um mecanismo de processamento transacional de alta velocidade para operações em bancos de dados relacionais e NoSQL.

E) Spark Core - Trata-se do sistema distribuído de armazenamento de objetos que permite o armazenamento seguro e redundante de grandes volumes de dados em diferentes formatos.

",A,"

Explicações:

A) Correta. Spark SQL é o componente do Apache Spark que permite o processamento de dados estruturados, utilizando SQL e DataFrames para facilitar a execução de tarefas de ETL (Extract, Transform, Load) e também permitindo a interação com dados por meio de comandos SQL.

B) Incorreta. Spark Streaming é o componente responsável por tratar de fluxos de dados em tempo real, não pela otimização de recursos em clusters. A gestão eficiente de recursos em clusters é conduzida pelo Spark’s cluster manager (YARN, Mesos, ou Spark’s standalone cluster manager), e não é um módulo específico do Spark.

C) Incorreta. Spark MLlib é a biblioteca de aprendizado de máquina do Spark que fornece diversos algoritmos e utilidades para machine learning, e não um serviço de gerenciamento de metadados. O serviço relacionado ao gerenciamento de metadados geralmente é provido por componentes externos, como o Apache Hive Metastore, mas não é uma função do MLlib.

D) Incorreta. GraphX é o componente do Spark projetado para o processamento de grafos e análises gráficas, e não um mecanismo de processamento transacional para operações em bancos de dados relacionais e NoSQL.

E) Incorreta. Spark Core é o componente fundamental do Apache Spark que fornece funcionalidades de computação distribuída, como o RDD (Resilient Distributed Dataset), mas não é um sistema de armazenamento de objetos. O armazenamento distribuído de objetos é usualmente fornecido por sistemas como Hadoop Distributed File System (HDFS) ou outros sistemas de armazenamento compatíveis.",3468312
tópico 0,Processamento distribuído,"Questão: 
No contexto de processamento distribuído, há a necessidade de definição de algoritmos e mecanismos para garantir consistência e sincronização entre os diversos nós do sistema. Dentre os modelos de consistência especificados para sistemas distribuídos, qual dos seguintes garante que uma vez que uma nova escrita se torna visível para uma leitura (torna-se ""fresh""), todas as leituras subsequentes (mesmo que em nós diferentes) verão esta escrita ou uma mais recente?

A) Consistência Eventual
B) Consistência Fraca
C) Consistência Sequencial
D) Consistência Forte
E) Consistência Causal

",D,"

Explicação dos itens:

A) Consistência Eventual: Este modelo não garante que as leituras subsequentes verão as escritas mais recentes imediatamente. Ela apenas assegura que se o sistema parar de receber mudanças, eventualmente todos os nós terão os mesmos dados, mas não especifica o tempo que isso pode levar.

B) Consistência Fraca: Similar à consistência eventual, mas geralmente inclui algum tipo de mecanismo que proporciona um grau maior de garantia que um estado será alcançado em um tempo finito, ainda assim, não assegura que todas as leituras posteriores à uma atualização visualizarão esta atualização.

C) Consistência Sequencial: Mantém as operações em uma sequência específica, porém não garante que uma vez uma atualização se torna visível todas as futuras leituras verão esta atualização em diferentes nós, apenas que todas as operações parecerão ocorrer em uma ordem sequencial única.

D) Consistência Forte: Este modelo assegura que assim que uma atualização é percebida por uma leitura em um nó, todas as leituras subsequentes em qualquer outro nó vão observar essa atualização ou uma versão mais atualizada dos dados. Isso implica que todas as cópias do dado no sistema distribuído são instantaneamente atualizadas.

E) Consistência Causal: Este modelo apenas assegura que as atualizações relacionadas causalmente são vistas por todos os nós na mesma ordem. As leituras podem ver atualizações diferentes se elas não são relacionadas causalmente, logo, não garante a visibilidade da escrita mais nova imediatamente após se tornar visível.",7739877
tópico 0,Armazenamento de big data,"Questão:

O armazenamento de big data é crucial para empresas que precisam analisar grandes volumes de informação para suporte à tomada de decisões estratégicas. Dentre os sistemas de armazenamento e processamento de big data, um dos mais populares é o Hadoop, que opera com o modelo de programação MapReduce e o sistema de arquivos HDFS. Com relação a este contexto, é CORRETO afirmar que:

A) O MapReduce permite apenas a execução de tarefas batch, sendo incapaz de processar fluxos contínuos de dados em tempo real.
B) O HDFS é um sistema de arquivos não distribuído, o que limita a eficiência do Hadoop em ambientes de big data.
C) O Hadoop é estritamente dependente de hardware específico de alto desempenho para a implementação de seus clusters.
D) O modelo MapReduce divide as tarefas de processamento de dados em blocos pequenos que podem ser executados em paralelo, aumentando a eficiência.
E) A arquitetura do Hadoop é centralizada, concentrando o processamento e armazenamento de dados em um único ponto, o que reduz a redundância dos dados.

",D,"

Explicação dos itens:

A) Incorreto. O MapReduce é originariamente para processamento batch (processamento de grandes volumes de dados), mas isso não significa que é incapaz de lidar com fluxos contínuos de dados. Com a evolução do ecossistema Hadoop, outras ferramentas como Apache Storm e Apache Spark foram desenvolvidas para lidar com streams de dados em tempo real.

B) Incorreto. O HDFS (Hadoop Distributed File System) é um sistema de arquivos distribuído que permite o armazenamento eficiente de grandes volumes de dados distribuídos através de múltiplos nós, maximizando assim a capacidade de processamento e armazenamento.

C) Incorreto. Um dos benefícios do Hadoop é a sua capacidade de rodar em hardware comum, não exigindo hardware especial ou de alto desempenho. Isso o torna uma opção de custo-efetiva para muitas organizações.

D) Correto. O modelo MapReduce é projetado para processar grandes volumes de dados em um ambiente distribuído, dividindo o trabalho em tarefas menores que podem ser processadas em paralelo, o que melhora consideravelmente a eficiência e a escalabilidade do processamento de dados.

E) Incorreto. A arquitetura do Hadoop é descentralizada, projetada para operar em um cluster distribuído de servidores que armazena e processa dados, aumentando a redundância e oferecendo alta disponibilidade. A arquitetura centralizada é uma característica de sistemas tradicionais de bancos de dados, não do Hadoop.",8275154
tópico 0,Ingestão de dados em lote (batch),"Questão: Em um cenário de processamento de dados em larga escala, a ingestão de dados em lote (batch) é uma prática comum para lidar com grandes volumes de informação de forma eficiente. Considerando os conceitos de arquitetura de dados e os processos de ingestão em lote, qual das seguintes afirmativas é correta acerca das características da ingestão de dados em lote?

A) A ingestão de dados em lote é ideal para cenários onde o processamento em tempo real é crítico, como monitoramento de redes sociais.

B) Os dados coletados durante a ingestão em lote não necessitam de uma área de armazenamento temporário, pois são processados imediatamente.

C) A ingestão de dados em lote geralmente lida com dados que foram coletados e armazenados ao longo de um período definido antes de serem processados todos de uma vez.

D) Em sistemas que utilizam a ingestão de dados em lote, a latência é tipicamente mais baixa quando comparada aos métodos de processamento de fluxos (stream).

E) A ingestão de dados em lote é mais eficiente e rápida que a ingestão de dados em tempo real independentemente do volume ou da complexidade dos dados envolvidos.

",C,"

Explicação dos itens:
A) Incorreto. Ingestão em lote não é ideal para processamento de dados que exigem análise em tempo real, como no monitoramento de redes sociais; para isso, seria melhor a ingestão de dados de streaming.

B) Incorreto. A ingestão de dados em lote muitas vezes envolve armazenar os dados temporariamente antes de processá-los, pois o processamento não é imediato, é feito após a coleta de todos os dados necessários.

C) Correto. A ingestão em lote trata de processar grandes volumes de dados que foram acumulados durante um período determinado e então processados de uma só vez, o que é uma característica fundamental desse tipo de ingestão.

D) Incorreto. A ingestão de dados em lote geralmente apresenta uma latência mais alta em comparação com a ingestão de dados em tempo real (streaming), pois os dados são coletados em intervalos, não continuamente.

E) Incorreto. A eficiência e a rapidez da ingestão de dados em lote dependem de vários fatores, e não é possível afirmar que seja mais eficiente e rápida sem considerar aspectos específicos como volume de dados, infraestrutura e requisitos de processamento.",2382910
tópico 0,"Arquitetura de cloud computing para ciência de dados (AWS, Azure, GCP)","Questão: Ao projetar uma arquitetura de cloud computing para ciência de dados utilizando serviços da AWS, um arquiteto de soluções precisa assegurar a escalabilidade, eficiência na análise de dados em larga escala e a integração com serviços de machine learning. Qual das seguintes combinações de serviços AWS é a mais adequada para atender a esses requisitos?

A) Amazon EC2 para computação escalável, Amazon RDS para gerenciamento de banco de dados relacional e AWS Lambda para execução de código em resposta a eventos.

B) Amazon S3 para armazenamento de dados, Amazon Redshift para data warehousing e Amazon SageMaker para construção e treinamento de modelos de machine learning.

C) Amazon Lightsail para hospedagem de aplicações, Amazon DynamoDB para banco de dados NoSQL e Amazon Rekognition para análise de imagem e vídeo.

D) AWS Elastic Beanstalk para implantação e gerenciamento de aplicações, Amazon Athena para consultas SQL em dados no S3 e AWS DeepRacer para corridas de modelo de machine learning autônomo.

E) Amazon Kinesis para análise em tempo real de streaming de dados, Amazon Neptune para banco de dados de grafos e AWS Glue para ETL (Extract, Transform, Load).

",B,"

Explicação dos itens:

A) Amazon EC2 e AWS Lambda são serviços de computação que oferecem escalabilidade, mas o Amazon RDS é voltado para bancos de dados relacionais e não é especificamente otimizado para análises de dados em larga escala ou integração direta com serviços de machine learning.

B) Amazon S3 oferece armazenamento em larga escala e é frequentemente utilizado como um data lake por sua capacidade e flexibilidade, Amazon Redshift é um serviço de data warehousing projetado para análises de dados complexas e de alto desempenho, e Amazon SageMaker é uma plataforma totalmente gerenciada que permite aos cientistas de dados criar, treinar e implantar modelos de machine learning facilmente, atendendo aos requisitos da questão.

C) Amazon Lightsail é mais voltado para aplicações simples e pequenos negócios, Amazon DynamoDB para banco de dados NoSQL oferece performance em aplicações que necessitam de baixa latência, e Amazon Rekognition oferece serviços de análise de imagem e vídeo, não cobrindo diretamente os requisitos de análise de dados em larga escala e integração com serviços de machine learning de forma abrangente.

D) AWS Elastic Beanstalk simplifica o processo de implantação, mas não é especificamente para ciência de dados, Amazon Athena permite consultas SQL diretamente no S3, mas a questão pede uma solução completa que também inclua machine learning, e AWS DeepRacer é um projeto para aprender machine learning por meio de competições de corrida de carros autônomos e não uma ferramenta de ciência de dados per se.

E) Amazon Kinesis é projetado para análise em tempo real de grandes fluxos de dados, Amazon Neptune para banco de dados de grafos e AWS Glue para ETL são poderosos para suas respectivas funções, mas não formam uma combinação tão direta e eficaz para ciência de dados e análise de dados em larga escala quanto a opção B.",6483661
tópico 0,Conceitos de processamento massivo e paralelo,"Questão: Em um cenário onde uma empresa de processamento de dados precisa lidar com volumes crescentes de informações, a utilização de técnicas de processamento massivo e paralelo torna-se essencial para manter a eficiência e a velocidade na obtenção de resultados. Analisando as características do processamento massivo e do processamento paralelo, marque a alternativa INCORRETA:

A) O processamento massivo envolve a análise e manipulação de grandes volumes de dados de forma distribuída, geralmente aplicado em cenários de big data.

B) Processamento paralelo consiste no uso simultâneo de múltiplos recursos computacionais para resolver um problema computacional, o que pode ser realizado tanto em uma única máquina com múltiplas unidades de processamento quanto em um cluster de máquinas.

C) O MapReduce é um modelo de programação e uma implementação associada projetada para processar grandes volumes de dados de forma paralela, distribuída e escalonável em clusters de computadores.

D) Processos de ETL (Extract, Transform, Load) são intrinsecamente paralelos e não se beneficiam de arquiteturas de processamento massivo.

E) Tanto o processamento massivo quanto o paralelo se beneficiam de sistemas de gerenciamento de banco de dados que suportam operações distribuídas para manter o desempenho quando lidando com grandes conjuntos de dados.

",D,"

A alternativa A está correta, pois descreve o processamento massivo como uma técnica aplicada a big data. A alternativa B também está correta, pois define acertadamente o processamento paralelo e onde pode ser aplicado. A alternativa C está correta ao identificar o modelo de programação MapReduce como uma ferramenta de processamento paralelo e distribuído. A alternativa E é verdadeira, uma vez que o processamento massivo e paralelo realmente se beneficiam de sistemas de gerenciamento de banco de dados que suportam operações distribuídas. A alternativa D é a INCORRETA, pois os processos de ETL podem, na verdade, ser otimizados e beneficiados significativamente através de técnicas de processamento massivo e paralelo, reduzindo o tempo de carga e transformação de grandes volumes de dados.",1074807
