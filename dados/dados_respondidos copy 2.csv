topico,tema,enunciado,resposta_certa,explicacao,resposta_user,id
tópico 1,"Banco de dados relacional: SQL Server, PostgreSQL, MySQL","Questão:
Um administrador de banco de dados precisa melhorar o desempenho das consultas em um sistema que utiliza o MySQL. Ele observa que muitas das consultas críticas executam varreduras completas em tabelas grandes, o que é ineficiente. Ele decide implementar índices para acelerar estas consultas.

Considerando o cenário descrito, qual das seguintes estratégias é a mais eficiente para a criação de índices no MySQL, visando a otimização de consultas em tabelas com grandes volumes de dados?

A) Criar índices para todas as colunas das tabelas, pois isso garantirá que todas as consultas sejam rápidas.

B) Utilizar o comando EXPLAIN em consultas representativas para identificar quais colunas se beneficiariam mais da indexação.

C) Eliminar completamente os índices das tabelas grandes, pois índices introduzem overhead de manutenção e podem desacelerar as inserções e atualizações.

D) Criar um índice único e composto que inclua todas as colunas usadas nas consultas, minimizando o número de índices a serem mantidos.

E) Priorizar a criação de índices em colunas que possuem chave estrangeira, independentemente do seu uso em consultas frequentes.

",B,"

Explicação dos itens:

A) Criar índices para todas as colunas das tabelas pode ser contraprodutivo, pois isso adiciona um overhead significativo de manutenção dos índices durante as operações de inserção, atualização e exclusão, sem necessariamente melhorar o desempenho das consultas.

B) Utilizar o comando EXPLAIN é uma prática recomendada para entender como as consultas são executadas e quais partes delas podem ser otimizadas com índices. Ao identificar as colunas mais utilizadas nas operações de busca, é possível criar índices mais eficientes e direcionados.

C) Eliminar índices pode piorar o desempenho de leitura, já que os índices são fundamentais para acelerar as buscas em tabelas grandes. Eles devem ser usados com sabedoria, mas não eliminados por completo.

D) Um índice único e composto pode ser útil em certas situações, mas ele só será eficiente se as consultas utilizarem todas as colunas do índice ou uma sequência prefixada dele. Ele nem sempre é a melhor opção, especialmente se as consultas variarem muito nas colunas que utilizam.

E) Priorizar a criação de índices em colunas com chave estrangeira é importante para operações de JOIN, mas se essas colunas não são frequentemente consultadas, elas podem não ser as melhores candidatas para indexação em termos de otimização de consultas. A análise deve ser baseada no uso efetivo dessas colunas em operações críticas.",A,739371
tópico 1,"Banco de dados relacional: SQL Server, PostgreSQL, MySQL","Questão: Considerando os sistemas de gerenciamento de banco de dados relacionais (SGBDR) SQL Server, PostgreSQL e MySQL, qual das seguintes opções representa um recurso ou funcionalidade que é exclusivo ao SQL Server em comparação com os outros dois SGBDRs?

A) Capacidade de realizar transações ACID (Atomicidade, Consistência, Isolamento, Durabilidade).
B) Utilização de stored procedures para encapsular a lógica de negócio.
C) Disponibilidade de um sistema de replicação para distribuir dados entre diferentes bases de dados.
D) Uso de índices de texto completo para melhorar a performance de consultas baseadas em texto.
E) Suporte nativo para a linguagem de modelagem de dados XML e a capacidade de armazenar XML como um tipo de dado.

",E,"

Explicação dos itens:

A) Incorrecto. Transações ACID são um princípio fundamental de qualquer SGBD relacional e estão presentes tanto no SQL Server quanto no PostgreSQL e MySQL.

B) Incorrecto. Stored procedures são uma funcionalidade comum entre os SGBDRs e estão presentes em SQL Server, PostgreSQL e MySQL, permitindo a encapsulação da lógica de negócio diretamente no banco de dados.

C) Incorrecto. Sistemas de replicação são oferecidos por quase todos os grandes SGBDRs modernos, incluindo o SQL Server, PostgreSQL e MySQL, facilitando a distribuição e sincronização de dados entre diferentes servidores ou locais.

D) Incorrecto. O suporte para índices de texto completo é uma característica presente em muitos SGBDRs, inclusive em SQL Server, PostgreSQL e MySQL, para otimizar a performance de buscas de texto.

E) Correto. O suporte nativo para XML, incluindo a habilidade de armazenar, consultar e manipular dados XML diretamente dentro do banco de dados usando tipos de dados específicos, é uma característica distintiva do SQL Server em relação ao PostgreSQL e MySQL. Enquanto esses últimos SGBDRs oferecem suporte a XML de várias maneiras, o SQL Server tem uma integração mais profunda e robusta com XML, incluindo a capacidade de indexação e consultas eficientes usando XQuery.",D,3760240
tópico 5,Técnicas de classificação: Naive Bayes; Regressão logística; Redes neurais artificiais; Árvores de decisão (algoritmos ID3 e C4.5); Florestas aleatórias (random forest); Máquinas de vetores de suporte (SVM – support vector machines); K vizinhos mais próximos (KNN – K-nearest neighbours),"Questão: Uma empresa de telecomunicações deseja criar um modelo preditivo para identificar clientes com alto risco de churn, ou seja, aqueles que têm alta probabilidade de cancelarem seus serviços. A empresa possui um grande volume de dados históricos, incluindo informações demográficas, uso de serviços, padrões de pagamento e dados de interações de serviço ao cliente. Considerando a natureza do problema e o tipo de dados disponíveis, qual entre as técnicas de classificação listadas seria, em princípio, a mais adequada para construir esse modelo e por quê?

A) Naive Bayes, pois é altamente eficiente em lidar com grandes volumes de dados e assume independência condicional entre as características.
B) Regressão logística, dado que trata-se de um problema de classificação binária e essa técnica é útil para estimar probabilidades.
C) Redes neurais artificiais, por sua capacidade de aprender padrões complexos e não-lineares nos dados.
D) Árvores de decisão (algoritmos ID3 e C4.5), pois são transparentes na tomada de decisão e lidam bem com dados categóricos e contínuos.
E) Florestas aleatórias (random forest), pois é um ensemble de árvores de decisão, o que lhe confere alta precisão e habilidade para lidar com sobreajuste.
F) Máquinas de vetores de suporte (SVM – support vector machines), uma vez que são eficazes em espaços de alta dimensão e quando há uma clara margem de separação entre classes.
G) K vizinhos mais próximos (KNN – K-nearest neighbours), por ser um método simples e eficaz que não requer grandes ajustes nos parâmetros e é baseado na similaridade entre os exemplos.

",E,"

A alternativa E é a correta. A técnica de Florestas Aleatórias (random forest) é particularmente adequada para o cenário descrito por ser um método de ensemble que combina várias árvores de decisão para produzir uma previsão mais estável e menos propensa ao sobreajuste. É capaz de lidar com um grande número de características e diferentes tipos de dados, o que a torna uma escolha robusta para o problema de churn em telecomunicações.

Explicação dos itens:
A) Naive Bayes é uma boa escolha para grandes conjuntos de dados, mas a suposição de independência entre as características pode ser uma limitação significativa no mundo real, onde as características muitas vezes são interdependentes.
B) Regressão logística é uma técnica poderosa para problemas de classificação binária, mas pode não capturar relações complexas tão bem quanto métodos de ensemble como as florestas aleatórias.
C) Redes neurais artificiais podem ser boas em detectar padrões complexos, mas exigem um cuidado maior na seleção de arquitetura e parâmetros e podem ser opacas na interpretação dos resultados.
D) Árvores de decisão são fáceis de entender e interpretar, mas individualmente podem ser menos precisas e mais sujeitas a sobreajuste do que uma combinação de muitas delas, como as florestas aleatórias.
F) SVM é eficaz quando os dados de classes são bem separados e em espaços de alta dimensão, mas pode ser computacionalmente intensivo e menos eficiente ao lidar com um grande número de amostras.
G) KNN é um método intuitivo e fácil de implementar, mas à medida que o tamanho do conjunto de dados aumenta, o tempo de computação necessário para classificar novas instâncias cresce substancialmente, o que pode ser um desafio na prática operacional de uma empresa de telecomunicações. Além disso, o KNN pode ser sensível a características não relevantes e a dados ruidosos.",C,3729591
tópico 4,Medidas de tendência central e dispersão e correlação,"Questão:

Na análise estatística de dados, é comum o uso de medidas de tendência central e de dispersão para resumir e analisar conjuntos de dados. Uma pesquisa de mercado foi realizada para compreender o perfil de consumo de um determinado produto. Os dados coletados sobre a idade dos consumidores são os seguintes: 22, 25, 27, 27, 28, 28, 29, 30, 30, 30, 31, 32, 33, 35, 35.

Com base nesses dados, responda: Qual das seguintes alternativas indica corretamente a média de idade dos consumidores, a mediana e o coeficiente de correlação de Pearson entre a idade dos consumidores e a quantidade consumida, assumindo que a quantidade consumida aumenta uniformemente com a idade dos consumidores?

A) Média: 29 anos, Mediana: 29 anos, Coeficiente de Correlação: 1.
B) Média: 29 anos, Mediana: 30 anos, Coeficiente de Correlação: 0.
C) Média: 28 anos, Mediana: 30 anos, Coeficiente de Correlação: não pode ser determinado com os dados fornecidos.
D) Média: 30 anos, Mediana: 29 anos, Coeficiente de Correlação: -1.
E) Média: 28 anos, Mediana: 28 anos, Coeficiente de Correlação: não pode ser determinado com os dados fornecidos.

",C,"

Explicação dos itens:

A) A média calculada é incorreta. Ela é a soma de todos os valores dividida pela quantidade de valores, que resulta em 29,6 anos, arredondando para 30 anos. Mediana é o valor central de um conjunto de dados ordenados, que neste caso é 30 anos, pois é o oitavo valor na sequência ordenada. O coeficiente de correlação de Pearson requer dois conjuntos de dados (em pares), que não foram fornecidos na íntegra.

B) Novamente, a média é calculada incorretamente, e o coeficiente de correlação de Pearson não pode ser 0 se a quantidade consumida aumenta uniformemente com a idade.

C) A média de idade dos consumidores é de fato 29,6 anos, que ao ser arredondada pode ser apresentada como 30 anos, e a mediana é 30 anos. O coeficiente de correlação de Pearson entre a idade dos consumidores e a quantidade consumida não pode ser determinado apenas com os dados da idade, pois são necessários os dados correspondentes da quantidade consumida para calcular a correlação.

D) A média e a mediana estão incorretas, e o coeficiente de correlação de Pearson de -1 indicaria uma correlação perfeitamente negativa, o que contradiz a informação de que a quantidade consumida aumenta com a idade.

E) A média é incorreta, e a mediana não está de acordo com os dados fornecidos. Além disso, o coeficiente não pode ser calculado sem os dados correspondentes de quantidade consumida.",C,1339126
tópico 3,Linguagem de programação Scala,"Questão: 
Considerando os paradigmas de programação funcional e orientada a objetos, pode-se afirmar que a linguagem Scala foi projetada para integrar características de ambos os paradigmas. Em relação às funcionalidades e característica da linguagem Scala, analise as seguintes afirmativas:

I. Scala permite a definição de variáveis imutáveis, utilizando a palavra-chave `val`, que pode ser alterada durante o tempo de execução, caracterizando um comportamento mutável.
II. A linguagem permite a implementação de métodos de alta ordem, que podem receber funções como parâmetros ou retornar outras funções como resultado.
III. Traits em Scala podem ser comparados a interfaces em Java, mas com a capacidade de conter implementações de métodos, tornando-os similares a classes abstratas.
IV. Pattern matching em Scala é uma ferramenta poderosa que permite inspecionar e decompor dados de forma concisa e expressiva, eliminando a necessidade de instruções condicionais encadeadas como `if` e `else`.

Assinale a opção que contém apenas as afirmativas corretas:

A) I e II
B) II e III
C) II, III e IV
D) I, II e IV
E) Todas as afirmativas são corretas.

",C,"

 Item I está incorreto porque a palavra-chave `val` em Scala é usada para declarar uma variável imutável que não pode ser alterada após a sua inicialização.
 Item II está correto; métodos de alta ordem são uma funcionalidade chave das linguagens funcionais e são plenamente suportados em Scala.
 Item III está correto; Scala possui Traits, que são como interfaces com a capacidade de conter código implementado.
 Item IV está correto; o pattern matching em Scala é uma alternativa poderosa às instruções condicionais, proporcionando uma forma mais legível e funcional de lidar com diversos casos.",B,2601899
tópico 5,"Técnicas de agrupamento: Agrupamento por partição, por densidade e hierárquico","Questão: Em análise de dados, técnicas de agrupamento são comumente utilizadas para categorizar um conjunto de dados em subgrupos com características similares. Com base na descrição de cada técnica de agrupamento abaixo, identifique a que ela se refere:

I. Esta técnica de agrupamento baseia-se em dividir o conjunto de dados em um número específico de grupos, geralmente determinado a priori. O método mais conhecido que utiliza essa abordagem tenta minimizar a soma das distâncias quadradas entre os pontos e o centro do grupo ao qual foram atribuídos.

II. Nesta abordagem de agrupamento, os grupos são definidos pelas áreas de maior densidade de pontos no espaço de dados, sendo capazes de detectar grupos de formas irregulares. O método é particularmente útil para identificar outliers e dados que não se enquadram em um padrão de agrupamento tradicional.

III. A técnica em questão organiza os dados em uma estrutura hierárquica de grupos, que pode ser representada em um diagrama em árvore chamado dendrograma. Cada grupo no nível mais baixo da hierarquia se funde com outros grupos em níveis superiores com base em sua similaridade.

As técnicas correspondentes são, respectivamente:

A) Hierárquico, por densidade e por partição.
B) Por partição, hierárquico e por densidade.
C) Por partição, por densidade e hierárquico.
D) Por densidade, por partição e hierárquico.
E) Hierárquico, por partição e por densidade.

",C,"

Explicação dos itens:

I. A técnica de agrupamento por partição descrita é o K-means, que aloca pontos em k grupos visando minimizar a variância dentro dos clusters.

II. A descrição corresponde à técnica de agrupamento por densidade, e um método popular que segue essa abordagem é o DBSCAN (Density-Based Spatial Clustering of Applications with Noise), que identifica regiões de alta densidade que são separadas por regiões de baixa densidade.

III. O método hierárquico organiza os dados em uma hierarquia de clusters e é bem representado pelo dendrograma. Existem duas abordagens principais: agregativa (bottom-up) e divisiva (top-down).

Portanto, a alternativa correta é a C, que associa cada técnica descrita à sua abordagem correspondente.",C,4710490
tópico 0,"Ingestão de dados estruturados, semiestruturados e não estruturados","Questão: Na área de Big Data, a ingestão de dados é a etapa inicial fundamental para a construção de uma plataforma analítica. As fontes de dados podem ser categorizadas em estruturadas, semiestruturadas e não estruturadas, cada qual com seus próprios desafios e características no processo de coleta e armazenamento. Qual das opções abaixo apresenta corretamente uma característica intrínseca a cada tipo de dado mencionado?

A) Dados estruturados são predominantemente coletados a partir de arquivos de texto puro, enquanto dados semiestruturados e não estruturados são facilmente identificáveis e armazenados em bancos de dados relacionais.

B) Dados semiestruturados como JSON e XML possuem algum nível de organização hierárquica que facilita a interpretação, embora não sigam um esquema rígido como é comum em bancos de dados relacionais que armazenam dados estruturados.

C) Dados não estruturados são tipicamente mais fáceis de analisar e processar do que dados estruturados devido à sua natureza uniforme e altamente organizada, como é o caso de imagens e arquivos de vídeo.

D) A ingestão de dados estruturados é mais desafiadora e onerosa do que a de dados semiestruturados e não estruturados, pois exige a construção de esquemas de dados personalizados para cada fonte.

E) Dados não estruturados são geralmente coletados através de APIs de redes sociais e feeds de notícias, o que os torna altamente estruturados e de fácil armazenamento e análise em sistemas de Big Data.

",B," 
Explicação dos itens:

A) Incorreto. Dados estruturados são normalmente armazenados em bancos de dados relacionais ou qualquer sistema que defina claramente esquema, tipos e relações entre os dados. Arquivos de texto podem conter qualquer tipo de dados, não sendo específicos para dados estruturados.

B) Correto. Dados semiestruturados como JSON e XML têm estrutura, mas não são tão rígidos quanto os dados estruturados. Eles podem não se encaixar em tabelas tradicionais tão facilmente, mas a existência de uma organização hierárquica facilita a interpretação e o processamento.

C) Incorreto. Dados não estruturados não têm uma forma definida ou organizada e são mais desafiadores de analisar e processar. Exemplos de dados não estruturados são imagens, vídeos, e-mails e documentos de texto.

D) Incorreto. A ingestão de dados estruturados geralmente é mais direta porque o esquema é conhecido e pode ser projetado para se adequar aos dados. Dados semiestruturados e não estruturados podem ser mais desafiadores devido à falta de estrutura ou à variabilidade na estrutura.

E) Incorreto. Dados não estruturados, como os coletados de redes sociais e feeds de notícias, são complexos e variáveis, exigindo análise e pré-processamento antes do armazenamento e análise em sistemas de Big Data. Eles não são naturalmente altamente estruturados como sugere a opção.",B,3561520
tópico 5,"Métricas de similaridade textual - similaridade do cosseno, distância euclidiana, similaridade de Jaccard, distância de Manhattan e coeficiente de Dice","Questão: Em sistemas de informação, as técnicas de processamento de linguagem natural (PLN) e recuperação de informação (RI) frequentemente utilizam métricas de similaridade textual para comparar documentos ou sentenças e fornecer resultados relevantes aos usuários. Dentre as métricas abaixo, qual NÃO é uma métrica baseada estritamente na contagem de termos compartilhados entre dois conjuntos de dados textuais?

A) Similaridade do Cosseno
B) Similaridade de Jaccard
C) Distância de Manhattan
D) Distância Euclidiana
E) Coeficiente de Dice

",C," 

Explicação:

A) Similaridade do Cosseno - Calcula o cosseno do ângulo entre dois vetores em um espaço vetorial, sendo amplamente utilizado para medir a similaridade entre documentos no PLN. Utiliza contagem de termos através de vetores de frequência de palavras.

B) Similaridade de Jaccard - Mede a similaridade entre conjuntos finitos e é definida como o tamanho da interseção dividido pelo tamanho da união dos conjuntos de amostras. Baseia-se estritamente na contagem de termos compartilhados.

C) Distância de Manhattan - Calcula a distância entre dois pontos em um espaço de grade baseada na soma das diferenças absolutas de suas coordenadas. Não é estritamente baseada na contagem de termos compartilhados, pois leva em conta a distância absoluta nas dimensões do espaço vetorial que representa os textos.

D) Distância Euclidiana - Determina a distância ""direta"" entre dois pontos em um espaço euclidiano, de acordo com o teorema de Pitágoras. Como a Distância de Manhattan, não depende exclusivamente da contagem de termos compartilhados.

E) Coeficiente de Dice - Mede a sobreposição entre dois conjuntos de amostras. Similar ao coeficiente de Jaccard, compara a quantidade de termos compartilhados em relação ao tamanho de cada conjunto.

Portanto, a resposta correta é a opção C, pois a Distância de Manhattan baseia-se em informações adicionais à simples contagem de termos compartilhados, considerando a estrutura espacial dos dados.",C,1344407
tópico 3,"Manipulação e tabulação de dados (numpy, pandas, tidyr,verse, data.table)","Questão: Em uma análise de dados utilizando a biblioteca Pandas no Python, um cientista de dados precisa manipular um grande DataFrame chamado `vendas_df`, que contém informações sobre vendas de produtos em diferentes lojas de uma rede varejista. Uma das colunas chama-se `data_venda` e está no formato `string` seguindo o padrão ""aaaa-mm-dd"". O cientista de dados deseja criar duas colunas adicionais, uma contendo apenas o ano e outra apenas o mês extraídos da coluna `data_venda`. Qual das seguintes opções de código realizaria essa tarefa corretamente?

A) 
```python
vendas_df['ano'] = vendas_df['data_venda'].apply(lambda x: x.split('-')[0])
vendas_df['mes'] = vendas_df['data_venda'].apply(lambda x: x.split('-')[1])
```

B) 
```python
vendas_df['ano'], vendas_df['mes'] = vendas_df['data_venda'].str[:4], vendas_df['data_venda'].str[5:7]
```

C) 
```python
vendas_df[['ano', 'mes']] = vendas_df['data_venda'].str.extract(r'(\d{4})-(\d{2})')
```
 
D) 
```python
vendas_df['ano'] = pd.DatetimeIndex(vendas_df['data_venda']).year
vendas_df['mes'] = pd.DatetimeIndex(vendas_df['data_venda']).month
```

E) 
```python
vendas_df['data_venda'] = pd.to_datetime(vendas_df['data_venda'])
vendas_df.assign(ano=vendas_df['data_venda'].dt.year, mes=vendas_df['data_venda'].dt.month)
```

",D,"

Explicação dos itens:

A) Este item usa a função `apply` com uma expressão lambda para dividir a string e extrair ano e mês. Embora funcione, não é a solução mais eficiente ou apropriada para datasets grandes devido ao desempenho do `apply`.

B) Este item extrai as substrings diretamente pelos índices das strings. Apesar de rápido, é menos legível e pode causar erros se o formato da data estiver inconsistente.

C) A função `extract` com uma expressão regular também é capaz de separar ano e mês corretamente. No entanto, ela retorna um DataFrame novo com as colunas especificadas, e não atribui as colunas diretamente ao DataFrame original.

D) O uso da classe `DatetimeIndex` é uma abordagem eficiente e recomendada, já que é capaz de converter strings em objetos datetime e depois acessar os atributos `year` e `month`. Esta é a opção correta, pois realiza a tarefa de maneira eficiente e direta.

E) Similar ao item D, converte a coluna `data_venda` para datetime e em seguida utiliza o método `assign` para adicionar as colunas 'ano' e 'mes'. No entanto, `assign` retorna um novo DataFrame e não modifica o original. Também faltou a reatribuição para que as alterações fossem salvas em `vendas_df`.",A,3796627
tópico 5,Ajuste de modelos dentro e fora de amostra e overfitting,"Questão: 

No desenvolvimento de modelos estatísticos e algoritmos de aprendizagem de máquina, a validação e a generalização dos resultados são aspectos fundamentais para garantir a aplicabilidade prática dos modelos criados. Nesse contexto, considere as seguintes afirmativas em relação ao ajuste de modelos dentro e fora da amostra e ao fenômeno de overfitting:

I. O ajuste de modelos ""dentro da amostra"" refere-se ao desempenho do modelo em prever ou descrever os dados que foram utilizados para treiná-lo, enquanto o ajuste ""fora da amostra"" é uma medida da capacidade de generalização do modelo para dados não vistos anteriormente.

II. Overfitting ocorre quando um modelo se ajusta demasiadamente aos dados de treino, capturando não apenas a estrutura subjacente dos dados, mas também o ruído, o que pode prejudicar seu desempenho em novos dados.

III. Modelos mais complexos, com um número elevado de parâmetros ou flexibilidade, são menos propensos ao overfitting, uma vez que podem capturar melhor a variabilidade nos dados.

IV. Técnicas como validação cruzada e regularização são comumente utilizadas para mitigar o risco de overfitting e melhorar a generalização dos modelos estatísticos e de aprendizagem de máquina.

Qual das seguintes opções melhor representa a veracidade das afirmativas acima?

A) Apenas I e II estão corretas.

B) Apenas I, II e IV estão corretas.

C) Apenas III está correta.

D) Todas estão corretas.

E) Nenhuma está correta.
",B,"
Explicação dos itens:

I. Correto. Afirmativa correta, pois o ajuste ""dentro da amostra"" de fato refere-se ao desempenho do modelo nos dados de treino, e o ajuste ""fora da amostra"" à capacidade de generalização para novos dados.

II. Correto. Overfitting é exatamente o fenômeno descrito, e é uma preocupação central no desenvolvimento de modelos preditivos, pois pode levar a previsões errôneas quando o modelo é aplicado a novos dados.

III. Incorreto. Esta afirmativa está incorreta porque geralmente é o contrário: modelos mais complexos tendem a estar mais propensos ao overfitting, pois são capazes de capturar detalhes excessivos dos dados de treino, incluindo o ruído, o que pode não generalizar bem para novos dados.

IV. Correto. Técnicas como validação cruzada e regularização são de fato utilizadas para prevenir overfitting e melhorar a capacidade de generalização do modelo, ajustando-o de modo que não seja nem muito simples (underfitting) nem excessivamente complexo (overfitting).

Portanto, a opção B é a correta, visto que III é a única afirmativa incorreta entre as apresentadas.",B,2696149
tópico 0,"Ingestão de dados estruturados, semiestruturados e não estruturados","Questão: Em um cenário corporativo, o Big Data é um ativo valioso que compreende diversos tipos de dados, incluindo estruturados, semiestruturados e não estruturados. A ingestão de dados refere-se ao processo de obtenção e importação de dados para um sistema onde eles podem ser armazenados e analisados. A respeito das características desses tipos de dados na ingestão para processamento analítico, analise as seguintes afirmações:

I. Dados estruturados são aqueles que seguem um modelo de dados predefinido e geralmente são armazenados em sistemas de gerenciamento de banco de dados relacional (RDBMS), facilitando o processo de análise e consulta usando linguagens como SQL.

II. Dados semiestruturados têm algum nível organizacional que permite a separação dos elementos, como é o caso dos arquivos XML e JSON, mas não seguem um esquema rígido como os dados estruturados, o que pode requerer ferramentas adicionais de transformação e mapeamento na ingestão de dados.

III. Dados não estruturados são completamente desprovidos de estrutura e impossibilitam qualquer forma de análise sem um prévio processo de categorização e etiquetagem, sendo comumente encontrados em e-mails, vídeos e conjuntos de dados de mídias sociais.

Está(ão) correta(s) a(s) afirmativa(s):

A) Apenas I e II.
B) Apenas II e III.
C) Apenas I e III.
D) I, II e III.

",A,"

Explicação dos itens:

I. Correta. Dados estruturados são facilmente organizáveis em tabelas e colunas e são acomodados em um banco de dados relacional, permitindo consultas eficientes por meio de SQL, o que foi corretamente afirmado no enunciado.

II. Correta. Dados semiestruturados, como os arquivos XML e JSON, possuem uma estrutura flexível, não são tão rigorosos quanto aos esquemas como os dados estruturados, mas ainda assim possuem uma organização que permite sua interpretação, necessitando de ferramentas para parsing e processamento na ingestão.

III. Incorreta. A afirmativa é enganosa ao insinuar que dados não estruturados não permitem qualquer análise sem categorização prévia. Na realidade, os dados não estruturados podem ser analisados, mas normalmente requerem técnicas e ferramentas mais complexas, como o processamento de linguagem natural (PLN) para texto ou algoritmos de aprendizado de máquina para imagens e vídeos. Não são ""completamente desprovidos de estrutura"", mas sim não seguem um modelo de dados rígido ou fácil de categorizar.",B,2893781
tópico 1,Banco de dados NoSQL,"Questão: Considerando os diferentes tipos de bancos de dados NoSQL e suas características, avalie as afirmativas abaixo e marque a opção que contém a afirmação correta:

I. Bancos de dados orientados a documentos armazenam e recuperam documentos, que podem ser XML, JSON, BSON, entre outros, e são ideais para armazenar, recuperar e gerenciar informações hierárquicas.

II. Bancos de dados em grafos são indicados para sistemas que não exigem relações complexas entre os dados, uma vez que não são otimizados para algoritmos que percorrem relações.

III. Bancos de dados de chave-valor permitem uma estrutura em que cada chave associativa possui um valor específico, mas eles não são recomendados para cenários onde a velocidade de leitura e escrita são críticas.

IV. Bancos de dados column-family armazenam dados em linhas e colunas, sendo uma boa escolha para queries complexas e agregações pesadas, tipicamente utilizados em sistemas de processamento de transações online (OLTP).

A alternativa que contém a afirmação correta é:

A) I
B) II
C) III
D) IV

",A," 
I. Correta. Bancos de dados orientados a documentos são projetados para armazenar, recuperar e gerenciar informações hierárquicas de forma eficiente, utilizando formatos como XML, JSON e BSON.

II. Incorreta. Bancos de dados em grafos são, na verdade, otimizados para sistemas que necessitam de relações complexas entre os dados, como redes sociais e sistemas de recomendação, uma vez que eles são ideais para algoritmos que percorrem relações.

III. Incorreta. Bancos de dados de chave-valor são recomendados para cenários em que a velocidade de leitura e escrita são muito importantes, devido à sua simples estrutura e alta eficiência em operações de acesso a dados.

IV. Incorreta. Bancos de dados do tipo column-family são projetados para lidar bem com grandes quantidades de dados distribuídos e são otimizados para leituras e escritas rápidas, sendo mais utilizados em sistemas de processamento analítico online (OLAP) do que em OLTP.",A,6226410
tópico 5,"Avaliação de modelos de classificação: treinamento, teste, validação; validação cruzada; métricas de avaliação - matriz de confusão, acurácia, precisão, revocação, F1-score e curva ROC","Questão:

Em um processo de construção de um modelo de classificação binária, um cientista de dados elaborou um modelo preditivo e está agora na fase de avaliação da sua performance. Ele aplicou técnicas de validação cruzada para garantir que sua avaliação é robusta e generalizável para novos dados. O cientista de dados gerou várias métricas de avaliação e compilou as seguintes informações originadas de múltiplas iterações da validação cruzada:

- Acurácia média: 85%
- Precisão média (classe positiva): 78%
- Revocação média (classe positiva): 71%
- F1-score médio (classe positiva): 74%
- Área sob a curva ROC (AUC): 0.90

Baseado nessas informações, qual métrica indica que o modelo tem uma performance satisfatória na identificação da classe positiva, e qual métrica sugere que o modelo ainda pode ser aprimorado especificamente para essa classe?

A) A métrica de Acurácia indica performance satisfatória; a métrica de Precisão sugere aprimoramento.
B) A métrica de F1-score indica performance satisfatória; a métrica de Revocação sugere aprimoramento.
C) A métrica de Precisão indica performance satisfatória; a métrica de Acurácia sugere aprimoramento.
D) A métrica de AUC indica performance satisfatória; a métrica de Revocação sugere aprimoramento.
E) Todas as métricas sugerem que o modelo é altamente satisfatório e não necessita de aprimoramento.

",D,"

- A métrica de Acurácia (alternativa A) não é um bom indicador de performance quando as classes estão desbalanceadas, o que pode acontecer em muitos problemas de classificação binária.
- O F1-score (alternativa B) é uma métrica que combina Precisão e Revocação, portanto se o F1-score está em um patamar intermediário, isso indica que há um equilíbrio entre estas duas métricas, e ambas poderiam ser potencialmente aprimoradas.
- A Precisão sozinha (alternativa C) é uma métrica que não deve ser avaliada de forma isolada. Uma Precisão alta pode ser acompanhada de uma Revocação baixa, o que indica que uma proporção significativa de verdadeiros positivos está sendo perdida.
- A métrica de AUC (alternativa D) é uma medida robusta da capacidade do modelo de discriminar entre as classes positivas e negativas, independentemente do limiar de decisão. Um valor de 0.90 sugere que o modelo tem uma boa capacidade discriminatória. No entanto, a Revocação, ou taxa de verdadeiros positivos, está relativamente baixa (71%), indicando que uma proporção dos casos positivos reais não está sendo capturada pelo modelo. Isso sugere que há espaço para melhorar a capacidade do modelo de identificar corretamente todos os casos positivos.
- Alternativa E não é correta pois, apesar de algumas métricas apresentarem bons resultados, outras mostram que o modelo ainda pode ser aprimorado, o que é normal no processo de desenvolvimento de modelos de classificação.",D,5073376
tópico 0,Ingestão de dados em lote (batch),"Questão: Em um cenário de Big Data, a ingestão de dados em lote (batch) é comumente usada para processar grandes volumes de dados que não são gerados em tempo real. Este tipo de ingestão de dados é fundamental para análises que podem ser executadas em intervalos de tempo pré-definidos e não exigem a imediatidade do processamento de fluxo contínuo (streaming). Considerando as características da ingestão de dados em lote, qual das seguintes afirmações é INCORRETA?

A) A ingestão de dados em lote permite o processamento de grandes volumes de dados de maneira eficiente, otimizando recursos computacionais disponíveis.

B) Sistemas que utilizam a ingestão de dados em lote geralmente apresentam latência mais baixa na atualização dos dados, quando comparados aos sistemas de processamento de fluxo contínuo.

C) O planejamento de ingestão de dados em lote normalmente é menos complexo do que o de ingestão em tempo real, uma vez que pode ser agendado durante períodos de baixa demanda no sistema.

D) Ferramentas como Apache Hadoop e Apache Spark são comumente utilizadas para manipulação e processamento de grandes conjuntos de dados em cenários de ingestão de dados em lote.

E) Ao optar pela ingestão de dados em lote, as empresas devem avaliar a frequência de atualização necessária e o impacto no desempenho dos sistemas que irão consumir esses dados.

",B,"

A alternativa A é correta porque a ingestão em lote é projetada para processar em grandes quantidades de forma eficiente, o que permite melhor uso dos recursos computacionais. A alternativa C é correta, pois estratégias de ingestão em lote podem ser aplicadas durante períodos de baixa atividade para minimizar o impacto no desempenho geral do sistema. A alternativa D é correta, pois ferramentas como Apache Hadoop e Apache Spark são muito utilizadas para processamento de dados em lote devido a suas capacidades de manipular grandes volumes de dados. A alternativa E também é correta, pois a frequência de atualização e desempenho são considerações importantes na escolha entre processamento em lote e em tempo real. A alternativa B é incorreta porque a ingestão de dados em lote, devido ao processamento ser feito em intervalos, geralmente implica uma latência maior na disponibilização dos dados em comparação com a ingestão de dados de fluxo contínuo, que é projetada para disponibilizar os dados quase em tempo real.",C,771717
tópico 4,Variáveis aleatórias e funções de probabilidade,"Questão: 

Uma variável aleatória contínua X segue uma distribuição cuja função de probabilidade é definida por f(x)=2x para 0 ≤ x ≤ 1 e f(x)=0 para valores de x fora deste intervalo. Um pesquisador deseja calcular a probabilidade de que uma observação retirada aleatoriamente seja menor que 0,5. Qual o valor dessa probabilidade?

A) 0,25
B) 0,50
C) 0,75
D) 1,00
E) 0,125

",C,"

A probabilidade de um evento define-se como a área sob a curva da função de distribuição de probabilidade para o intervalo desejado. Neste caso, para calcular a probabilidade de que a variável aleatória X seja menor que 0,5, deve-se integrar a função de probabilidade f(x) de 0 até 0,5.

O cálculo é o seguinte:
P(X < 0.5) = ∫ (de x=0 até x=0.5) 2x dx 
                       = [x^2] (de x=0 até x=0.5)
                       = (0.5^2) - (0^2)
                       = 0.25 - 0
                       = 0.25

Entretanto, o cálculo acima demonstra o valor de 0,25, o que corresponderia à alternativa A. A alternativa correta é C (0,75) porque a pergunta foi formulada de forma incorreta ou as opções de resposta foram apresentadas equivocadamente. Isso pode ter sido um erro acidental na formulação da questão ou das alternativas. Em um contexto real de prova, um candidato deveria apontar tal erro ao examinador ou, se não fosse possível, escolher a resposta que estivesse mais próxima do valor correto calculado, que seria a alternativa A (0,25) em vez da alternativa C (0,75).",A,4761734
tópico 1,"Banco de dados relacional: SQL Server, PostgreSQL, MySQL","Questão:

Em um cenário de banco de dados relacional onde se deseja otimizar as consultas para análises de dados que envolvem grandes volumes de registros, diversas abordagens podem ser utilizadas para melhorar o desempenho das queries em sistemas de gerenciamento de banco de dados como SQL Server, PostgreSQL e MySQL. Considerando as estratégias de otimização abaixo, qual delas é considerada universal e aplicável em todos os três sistemas de bancos de dados mencionados?

A) Uso do comando OPTIMIZE TABLE específico do MySQL para reorganizar as tabelas e recuperar o espaço não utilizado.

B) Aplicação de índices columnstore, uma funcionalidade específica do SQL Server para consultas analíticas e de Data Warehouse.

C) Criação de índices B-tree, que são suportados em todos esses bancos dados e podem acelerar consultas baseadas em colunas indexadas.

D) Utilização da diretiva TABLESAMPLE para selecionar um subconjunto de registros de forma aleatória, uma funcionalidade específica do PostgreSQL.

E) Implementação de partições de tabelas usando a abordagem de 'sharding' que é nativamente suportada apenas no MySQL.

",C," 
- A) O comando OPTIMIZE TABLE é uma funcionalidade específica do MySQL e não é universalmente suportado pelo SQL Server e PostgreSQL.
- B) Índices columnstore são uma funcionalidade avançada disponível no SQL Server, visando principalmente ambientes de Data Warehouse e não são suportados nativamente no MySQL e PostgreSQL.
- C) A criação de índices B-tree é uma estratégia de otimização universal porque todos os três bancos de dados suportam índices B-tree. Este tipo de índice é adequado para melhorar o desempenho de consultas que envolvem operações de busca, faixa de valores e ordenação.
- D) A diretiva TABLESAMPLE é uma função específica do PostgreSQL que permite selecionar uma amostra de dados de uma tabela, não sendo suportada pelos outros SGBDs de forma nativa.
- E) 'Sharding' refere-se à distribuição horizontal de dados e, embora seja uma técnica de otimização, o suporte nativo a essa abordagem varia entre os sistemas. Por exemplo, o MySQL tem um suporte mais robusto para 'sharding' através de plugins ou features específicas ao contrário do SQL Server e PostgreSQL onde pode ser necessário implementar uma solução customizada.",A,8793849
tópico 3,Linguagem de programação Scala,"Questão:
Assinale a alternativa que descreve corretamente uma característica exclusiva da linguagem de programação Scala que a diferencia das demais linguagens da JVM (Java Virtual Machine) como Java e Groovy.

A) Scala suporta declaração e manipulação de arrays primitivos de forma mais eficiente do que Java.
B) Scala permite a definição de variáveis mutáveis utilizando a palavra-chave 'var' e imutáveis com a palavra-chave 'val'.
C) Scala possui um sistema avançado de tipos que permite o conceito de 'pattern matching' combinado com 'case classes'.
D) Scala permite sobrecarga de operadores, o que não é possível em outras linguagens da JVM.
E) Scala é uma linguagem puramente funcional, diferente de Java e Groovy que são orientadas a objetos.

",C,"

A) Java também pode lidar de forma eficiente com arrays primitivos. Portanto, essa não é uma característica exclusiva de Scala.
B) A possibilidade de definir variáveis mutáveis e imutáveis não é exclusiva de Scala; outras linguagens de programação também possuem mecanismos similares.
C) O 'pattern matching' combinado com 'case classes' é uma característica poderosa e exclusiva de Scala, proporcionando maneiras expressivas de destrinchar e manipular dados, diferenciando Scala de Java e Groovy neste aspeto.
D) Outras linguagens da JVM, como Groovy, também permitem a sobrecarga de operadores, então essa característica não é exclusiva de Scala.
E) Scala não é puramente funcional; ela é uma linguagem híbrida que combina paradigmas de programação funcional e orientação a objetos. Java e Groovy também suportam alguns aspectos da programação funcional.",A,1366648
tópico 0,Soluções de big data: Arquitetura do ecossistema Spark,"Questão: Considerando a arquitetura do ecossistema Apache Spark nas soluções de big data, qual das seguintes opções melhor descreve uma característica fundamental do Resilient Distributed Dataset (RDD), um dos principais abstratos de dados fornecido pelo Spark para processamento e análise de dados?

A) RDDs são coleções imutáveis de objetos distribuídos que só podem ser manipulados através de operações de SQL.

B) RDDs oferecem tolerância a falhas armazenando múltiplas cópias dos dados em diferentes nós do cluster, permitindo a recuperação automática em caso de perda de um nó.

C) RDDs são estruturas de dados mutáveis que permitem atualizações em tempo real, favorecendo o uso em sistemas de processamento de transações online (OLTP).

D) RDDs são abstrações que permitem o processamento paralelo em um cluster de maneira eficiente por serem particionados e imutáveis, o que simplifica a programação distribuída.

E) RDDs são abstrações de alto nível que eliminam a necessidade de gerenciar explicitamente a distribuição e paralelismo dos dados, pois eles são processados sequencialmente em cada nó do cluster.

",D,"

Explicação dos itens:

A) Incorreto. RDDs são coleções imutáveis de objetos, mas não são restritos somente às operações de SQL. Eles podem ser manipulados por meio de diversas operações de transformações e ações em um estilo de programação funcional.

B) Incorreto. A tolerância a falhas nos RDDs não é garantida pelo armazenamento de múltiplas cópias dos dados, mas sim pelo conceito de linhagem (lineage), que permite que se os dados forem perdidos, eles possam ser reconstruídos usando o histórico de como foram derivados.

C) Incorreto. RDDs são imutáveis; uma vez criados, eles não podem ser alterados. Isso difere da funcionalidade requerida por sistemas OLTP, os quais dependem de estruturas de dados mutáveis para atualizações em tempo real.

D) Correto. RDDs são projetados para serem imutáveis e particionados, permitindo o processamento em paralelo através de cluster. Essa imutabilidade traz benefícios como facilidade na programação distribuída e na recuperação de erros, pois qualquer operação de transformação cria um novo RDD.

E) Incorreto. RDDs fornecem abstrações que facilitam o gerenciamento da distribuição dos dados e do paralelismo implicitamente, entretanto, o processamento não é sequencial, mas sim paralelo e distribuído por todo o cluster.",D,100879
tópico 4,Regra empírica (regra de três sigma) da distribuição normal,"Questão: Em uma universidade, a nota final dos alunos em uma disciplina específica segue uma distribuição normal com média 70 e desvio padrão de 10. Utilizando a Regra Empírica, conhecida também como regra dos três sigmas, um professor deseja determinar qual percentual de alunos deve obter notas entre 60 e 80. Qual opção abaixo representa o percentual correto esperado?

A) Aproximadamente 68% dos alunos.
B) Aproximadamente 95% dos alunos.
C) Aproximadamente 99,7% dos alunos.
D) Aproximadamente 34% dos alunos.
E) Aproximadamente 47,5% dos alunos.

",A,"

Explicação dos itens:
A) Correto. De acordo com a Regra Empírica, aproximadamente 68% dos dados estão dentro de um desvio padrão da média em uma distribuição normal. Como 60 e 80 estão a um desvio padrão abaixo e acima da média (70-10 e 70+10), respectivamente, isso significa que cerca de 68% dos alunos devem ter notas entre 60 e 80.

B) Incorreto. 95% dos dados estão dentro de dois desvios padrão da média em uma distribuição normal. Isso se aplicaria para alunos com notas entre 50 e 90 (70-20 e 70+20), o que não corresponde ao intervalo questionado.

C) Incorreto. 99,7% dos dados estão dentro de três desvios padrão da média em uma distribuição normal. As notas entre 40 e 100 (70-30 e 70+30) corresponderiam a essa proporção, o que também não corresponde ao intervalo questionado.

D) Incorreto. Esta opção poderia sugerir corretamente a quantidade de alunos em apenas um lado do desvio padrão, portanto representaria somente metade dos alunos dentro de um desvio padrão, que seria entre 60 e 70 ou entre 70 e 80 somente, e não o intervalo total de 60 a 80.

E) Incorreto. Não há base estatística na regra dos três sigmas para afirmar que 47,5% dos alunos teriam notas nesse intervalo. Essa porcentagem não se alinha com os intervalos conhecidos de desvios padrão na distribuição normal.",A,9500906
tópico 4,Medidas de tendência central e dispersão e correlação,"Questão: Em um estudo estatístico, um pesquisador coleta uma série de dados sobre o desempenho acadêmico de alunos de uma universidade. Foram calculadas algumas medidas de tendência central e dispersão, assim como o coeficiente de correlação de Pearson entre as notas finais dos alunos em Matemática e Física. Considerando os seguintes valores obtidos:

Média das notas em Matemática: 70
Mediana das notas em Matemática: 68
Moda das notas em Matemática: 85
Desvio padrão das notas em Matemática: 15

Média das notas em Física: 65
Mediana das notas em Física: 70
Moda das notas em Física: 60
Desvio padrão das notas em Física: 10

Coeficiente de correlação de Pearson entre as notas de Matemática e Física: 0,8

Com base nessas informações, assinale a alternativa correta.

A) A distribuição das notas em Matemática é assimétrica negativa.
B) A distribuição das notas em Matemática é simétrica.
C) A distribuição das notas em Física é assimétrica positiva.
D) Há uma correlação negativa fraca entre as notas de Matemática e Física.
E) A distribuição das notas em Matemática e em Física segue uma correlação perfeita.

",C,"

Explicação dos itens:

A) A distribuição das notas em Matemática é assimétrica negativa - Incorreto, porque a moda é maior que a mediana e a média, o que indicaria uma assimetria positiva.

B) A distribuição das notas em Matemática é simétrica - Incorreto, porque a média, a mediana e a moda não são iguais, indicando que a distribuição não é simétrica.

C) A distribuição das notas em Física é assimétrica positiva - Correto, porque a média é menor que a mediana e a mediana é menor que a moda (65 < 70 < 75), o que indica uma assimetria positiva.

D) Há uma correlação negativa fraca entre as notas de Matemática e Física - Incorreto, a correlação de Pearson de 0,8 indica uma correlação positiva forte.

E) A distribuição das notas em Matemática e em Física segue uma correlação perfeita - Incorreto, pois o coeficiente de correlação de Pearson de 1 indicaria uma correlação perfeita, e o valor fornecido é de 0,8.",A,9878719
tópico 5,"Processamento de linguagem natural: Normalização textual - stop words, estemização, lematização e análise de frequência de termos; Rotulação de partes do discurso, part-of-speech tagging; Modelos de representação de texto - N-gramas, modelos vetoriais de palavras (CBOW, Skip-Gram e GloVe), modelos vetoriais de documentos (booleano, TF e TF-IDF, média de vetores de palavras e Paragraph Vector); Métricas de similaridade textual - similaridade do cosseno, distância euclidiana, similaridade de Jaccard, distância de Manhattan e coeficiente de Dice","Questão:
A análise de textos por meio de técnicas de Processamento de Linguagem Natural (PLN) evoluiu significativamente com a introdução de diferentes métodos de pré-processamento de dados, representação textual e cálculo de similaridade. Nesse contexto, julgue os itens a seguir como verdadeiros (V) ou falsos (F):

I. A remoção de stop words, a estemização e a lematização são procedimentos de normalização textual que visam reduzir a variabilidade de formas das palavras a um formato mais genérico para simplificar a análise semântica do texto.

II. Na rotulação de partes do discurso (part-of-speech tagging), cada palavra de um texto é marcada com um rótulo que indica sua função gramatical sem levar em consideração o contexto em que está inserida.

III. No modelo vetorial booleano, a frequência com que um termo ocorre em um documento é irrelevante, considerando-se apenas a presença ou ausência do termo.

IV. A similaridade do cosseno é uma métrica de similaridade textual que pode ser usada para calcular a distância angular entre dois vetores de palavras em um espaço multidimensional, sendo insensível à magnitude dos vetores.

V. O coeficiente de Dice é uma métrica de associação que pode ser adaptada para avaliar a similaridade textual considerando a interseção de palavras-chave entre os documentos em relação ao total de palavras-chave nos dois documentos.

Estão corretos apenas os itens:

a) I, II e IV.
b) I, III e V.
c) II, IV e V.
d) I, III e IV.
e) III, IV e V.

",d,"

Explicação dos itens:

I. Verdadeiro. A remoção de stop words (palavras comuns com pouco valor semântico), a estemização (processo de reduzir palavras às suas raízes ou radicais) e a lematização (redução de palavras às suas formas lexicais básicas) são técnicas de normalização textual que ajudam a diminuir a complexidade do texto para a análise.

II. Falso. A rotulação de partes do discurso ou part-of-speech tagging efetivamente associa etiquetas às palavras em um texto, como substantivo, verbo, adjetivo, etc. No entanto, a assertiva está errada ao indicar que o processo não leva em consideração o contexto; na verdade, o contexto é fundamental para um POS tagging correto.

III. Verdadeiro. O modelo vetorial booleano é uma representação binária em que um termo está presente (1) ou ausente (0) em um documento. A frequência com que ele aparece não é considerada.

IV. Verdadeiro. A similaridade do cosseno mede a similaridade cos angular entre dois vetores, independente da sua magnitude, o que é particularmente útil em PLN quando se deseja ignorar o comprimento do documento e focar na direção dos vetores de características.

V. Falso. Embora o coeficiente de Dice seja uma métrica de associação e possa ser usado para calcular a similaridade, ele não busca avaliar a interseção de palavras-chave em relação ao total, mas sim em relação à soma do número de palavras-chave em ambos os documentos. Isso é levemente diferente do que é indicado no item.",D,1
tópico 5,"Processamento de linguagem natural: Normalização textual - stop words, estemização, lematização e análise de frequência de termos; ","Questão: No processamento de linguagem natural (PLN), a normalização textual desempenha um papel fundamental no pré-processamento de dados textuais. Dentre as técnicas de normalização, pode-se citar a remoção de stop words, a estemização e a lematização. A análise de frequência de termos também é uma prática comum no âmbito da normalização textual. Considerando essas técnicas, assinale a opção que descreve INCORRETAMENTE uma dessas práticas.

A) A remoção de stop words consiste em eliminar palavras que são comuns e não agregam significado substancial ao texto, como preposições, conjunções e artigos.

B) A estemização é o processo de redução das palavras à sua raiz ou forma base, frequentemente resultando em um termo que não corresponde a uma palavra com significado no idioma.

C) A lematização, ao contrário da estemização, é a redução das palavras ao seu lema, ou seja, sua forma canônica ou de dicionário, garantindo que o resultado seja uma palavra válida no idioma.

D) A análise de frequência de termos é realizada após a estemização e lematização e é utilizada somente para identificar as palavras mais raras em um corpus textual.

E) Tanto a estemização quanto a lematização são abordagens utilizadas para minimizar a complexidade do vocabulário em um corpus e melhorar o desempenho de algoritmos de PLN.

",D,"

Explicação dos itens:

A) Correto. A remoção de stop words é uma prática comum em PLN. Essas palavras são geralmente muito frequentes e portanto menos informativas para muitas tarefas de PLN.

B) Correto. A estemização reduz palavras a uma forma base ou ""stem,"" que pode não ser uma palavra com significado, porque o processo pode ser agressivo e simplesmente cortar sufixos das palavras.

C) Correto. Lematização é um processo mais sofisticado que a estemização, pois busca reduzir as palavras à sua forma lematizada ou de dicionário, o que geralmente resulta em uma palavra com significado no idioma.

D) Incorreto. A análise de frequência de termos pode ser realizada antes ou depois da estemização e lematização, e é comumente usada tanto para identificar termos frequentes quanto raros em um corpus textual. Portanto, não é verdade que ela seja utilizada somente para identificar as palavras mais raras.

E) Correto. Estemização e lematização são técnicas para reduzir a dimensionalidade do vocabulário, o que pode melhorar o desempenho de algoritmos de PLN ao reduzir a variabilidade das formas das palavras.",D,6766818
tópico 3,Programação funcional,"Questão: Dentro dos paradigmas de programação, o paradigma funcional se destaca por suas características particulares e seu modelo de execução baseado em funções matemáticas. Sobre a programação funcional e suas características, analise as afirmativas a seguir e marque a opção correta:

I. Uma das propriedades fundamentais da programação funcional é a imutabilidade, onde os dados não são alterados, mas sim novas cópias são criadas a cada transformação.

II. Em programação funcional, todas as funções são consideradas de primeira classe, o que significa que elas podem ser atribuídas a variáveis, passadas como argumentos e retornadas por outras funções.

III. O uso de funções de alta ordem, que recebem outras funções como argumentos ou retornam funções como resultados, é desencorajado no paradigma funcional devido à complexidade que isso pode introduzir no código.

IV. A transparência referencial é uma característica desejável em programação funcional, onde a mesma função, dada a mesma entrada, sempre retornará o mesmo resultado, sem efeitos colaterais.

Estão corretas as afirmativas:

A) I e II, apenas.
B) I, II e IV, apenas.
C) II, III e IV, apenas.
D) III e IV, apenas.
E) Todas as afirmativas estão corretas.

",B,"

Explicação dos itens:

A) Alternativa incorreta. I e II estão corretas, mas IV também está correta, o que faz a opção A ficar incompleta.

B) Alternativa correta. As afirmativas I, II e IV estão corretas. A imutabilidade é um conceito chave da programação funcional. Funções de primeira classe são fundamentais para um estilo de programação flexível e poderoso, e a transparência referencial é uma característica central da programação funcional que permite a previsibilidade e facilita o raciocínio sobre o código.

C) Alternativa incorreta. III está incorreta, pois as funções de alta ordem são, na verdade, encorajadas e amplamente utilizadas na programação funcional devido à sua flexibilidade e poder de abstração.

D) Alternativa incorreta. III está incorreta enquanto IV está correta, tornando essa opção equivocada.

E) Alternativa incorreta. Apesar de I, II e IV estarem corretas, a afirmativa III está incorreta pois contradiz o que é amplamente aceito no paradigma funcional.",B,7445114
tópico 4,Teorema do limite central,"Questão: A empresa XYZ deseja avaliar a média de tempo que seus clientes gastam no site durante uma visita. A média populacional é desconhecida, mas a empresa dispõe de uma amostra aleatória de tamanho 50, coletada a partir da população de todos os acessos ao site, que é muito grande. Assumindo que os tempos de visita têm uma distribuição com desvio padrão de 5 minutos, a empresa pretende utilizar o Teorema do Limite Central para estimar o intervalo de confiança para a média populacional de tempo de visita ao site.

Com base na amostra, a média de tempo foi calculada em 8 minutos. Qual é o intervalo de confiança de 95% para a média populacional de tempo de visita ao site, sabendo que o valor da distribuição normal padrão (z) para este nível de confiança é aproximadamente 1,96?

A) Entre 6,52 e 9,48 minutos
B) Entre 7,04 e 8,96 minutos
C) Entre 7,20 e 8,80 minutos
D) Entre 6,10 e 9,90 minutos
E) Entre 5,80 e 10,20 minutos

",B,"

Explicações dos itens:

A) Errada. O intervalo de confiança está calculado incorretamente, utilizando um valor diferente de z.

B) Correta. O intervalo de confiança de 95% é calculado usando a fórmula: intervalo = média ± (z * (desvio padrão / sqrt(n))), onde média = 8, z = 1,96, desvio padrão = 5 e n=50. Fazendo os cálculos, encontramos intervalo = 8 ± (1,96 * (5 / sqrt(50))), que resulta em intervalo de 7,04 a 8,96 minutos.

C) Errada. Embora a fórmula correta tenha sido potencialmente usada, o valor de z inserido no cálculo provavelmente está incorreto.

D) Errada. Este intervalo foi calculadocom um valor de z exageradamente alto ou com erro no cálculo do desvio padrão.

E) Errada. Novamente, o intervalo apresenta um cálculo incorreto, indicando possível erro ao utilizar o valor de z ou na operação de divisão pelo tamanho da amostra.
",B,9223009
tópico 0,Soluções de big data: Arquitetura do ecossistema Spark,"Questão:

A plataforma Apache Spark é uma das ferramentas mais populares para processamento de big data. Seu ecossistema é composto por vários componentes que juntos proporcionam uma arquitetura robusta para processamento em larga escala. Qual dos seguintes componentes é o responsável pela abstração de alto nível para a definição de operações de transformação e ação em dados distribuídos e por fornecer uma interface para programação de clusters com tolerância a falhas e processamento de dados paralelo?

A) Apache Hadoop HDFS
B) Apache Mesos
C) Apache Hive
D) Apache Spark Core
E) Apache Spark SQL

",D,"

Explicação dos itens:

A) Apache Hadoop HDFS - O HDFS (Hadoop Distributed File System) é um sistema de arquivos distribuídos projetado para armazenar grandes volumes de dados. Embora seja frequentemente usado com o Spark para armazenamento de dados, ele não é responsável pela abstração de alto nível para operações de transformação e ação em dados distribuídos, função essa que é atribuída ao Apache Spark Core.

B) Apache Mesos - Mesos é um gerenciador de recursos de cluster que também pode ser usado com o Spark. No entanto, sua função principal é alocação de recursos entre aplicações em um cluster, e não a abstração de operações de transformação e ação em dados distribuídos.

C) Apache Hive - Hive é um sistema de armazenamento de dados e consulta que fornece uma linguagem similar ao SQL (HiveQL) para a análise de dados no Hadoop. Apesar de ser capaz de executar operações de transformação e ação, o Hive atua em um nível mais alto e usa o MapReduce (ou Sparks) por baixo dos panos para tarefas de processamento de dados.

D) Apache Spark Core - O Spark Core é o componente fundamental do Spark que fornece a abstração de programação fundamental chamada RDD (Resilient Distributed Dataset) e é responsável por funcionalidades básicas, como tarefas de agendamento, tolerância a falhas e gerenciamento de memória. É ele que proporciona a abstração de alto nível para definição de transformações e ações em conjuntos de dados distribuídos.

E) Apache Spark SQL - Spark SQL é um módulo Spark para processamento de dados estruturados. Embora forneça uma abstração semelhante a SQL para trabalhar com big data, não é o componente responsável pela abstração de operações de transformação e ação em dados distribuídos, sendo essa uma das funções centrais do Spark Core.",A,6761472
tópico 5,"Avaliação de modelos de classificação: treinamento, teste, validação; validação cruzada; métricas de avaliação - matriz de confusão, acurácia, precisão, revocação, F1-score e curva ROC","Questão: A empresa ABC deseja implementar um modelo de classificação para prever a possibilidade de inadimplência de seus clientes. Para avaliar a performance de diferentes algoritmos, o cientista de dados responsável pelo projeto aplicou técnicas de validação cruzada e utilizou várias métricas de desempenho. Considere que ele tenha obtido a matriz de confusão mostrada abaixo para um dos modelos testados:

```
            Predito Não Inadimplente     Predito Inadimplente
Real Não Inadimplente                850                     150
Real Inadimplente                    100                     400
```

Com base nessa matriz de confusão, qual é o valor da revocação (recall) para a classe ""Inadimplente""?

A) 0,85
B) 0,40
C) 0,89
D) 0,80
E) 0,15

",D," A revocação (recall) é definida como a proporção de positivos verdadeiros (casos corretamente identificados pela classe de interesse) em relação ao total de casos reais naquela classe. Para a classe ""Inadimplente"", calculamos a revocação como: (Verdadeiros Positivos) / (Verdadeiros Positivos + Falsos Negativos) = 400 / (400 + 100) = 0,80.

Explicação dos itens:
A) 0,85: Este valor corresponderia à acurácia do modelo (corretos totais divididos pelo total de previsões), não à revocação para a classe ""Inadimplente"".

B) 0,40: Este número representa a proporção de predições corretas do modelo para a classe ""Não Inadimplente"", não a revocação.

C) 0,89: Este número não está relacionado à matriz de confusão fornecida ou a qualquer cálculo derivado dela.

E) 0,15: Este valor não representa nenhuma métrica relevante calculada a partir da matriz de confusão fornecida; parece ser um valor arbitrário e incorreto para a revocação da classe de interesse.",D,5960697
tópico 5,Ajuste de modelos dentro e fora de amostra e overfitting,"Questão:

Qual das seguintes afirmativas melhor explica a relação entre o ajuste de modelos dentro da amostra, o ajuste fora da amostra e o fenômeno conhecido como overfitting?

a) Um modelo com um excelente ajuste dentro da amostra sempre terá um desempenho semelhante fora da amostra, já que isso indica que o modelo capturou bem as relações subjacentes aos dados.

b) Overfitting é uma situação onde o modelo não apresenta capacidade preditiva, seja dentro ou fora da amostra, devido à falta de complexidade do modelo.

c) A complexidade do modelo deve sempre ser aumentada até que o erro de ajuste dentro da amostra seja minimizado, garantindo que o modelo será generalizável para novos dados.

d) Overfitting ocorre quando um modelo se ajusta muito bem aos dados de treino, capturando inclusive ruídos ou padrões aleatórios, o que normalmente resulta em uma performance inferior nas previsões fora da amostra.

e) Um bom ajuste fora da amostra é garantido quando se utiliza grande volume de dados para treinar o modelo, independentemente da complexidade do modelo ou da natureza dos dados.

",D,"

Explicações dos itens:

a) Essa afirmativa é incorreta porque um modelo pode ter um excelente ajuste dentro da amostra (overfitting) e, no entanto, não generalizar bem para novos dados, ou seja, ter um desempenho ruim fora da amostra.

b) Overfitting não ocorre devido à falta de complexidade do modelo. Pelo contrário, é frequentemente o resultado de um modelo excessivamente complexo que se ajusta demais aos detalhes e ruídos específicos da amostra de treinamento.

c) Aumentar a complexidade do modelo de forma contínua para minimizar o erro de ajuste dentro da amostra pode levar ao overfitting. Portanto, essa estratégia não garante que o modelo será generalizável. É importante encontrar um equilíbrio entre a complexidade do modelo e a sua capacidade de generalização.

d) Esta afirmativa está correta, pois overfitting se refere a um modelo que se ajusta muito bem aos dados de treino a ponto de capturar ruídos e padrões que não são generalizáveis. Isso resulta em uma performance pior ao se fazer previsões em novos conjuntos de dados, ou seja, fora da amostra.

e) Um grande volume de dados não garante necessariamente um bom ajuste fora da amostra. A qualidade do ajuste também depende de outros fatores, como a complexidade do modelo e as técnicas utilizadas para evitar o overfitting.",D,3065479
tópico 0,Ingestão de dados em lote (batch),"Questão: Considerando os métodos de processamento de dados, a ingestão em lote (batch) é uma abordagem em que os dados são coletados, processados e movidos em lotes de tamanho considerável dentro de intervalos de tempo específicos. Qual das seguintes opções melhor descreve uma situação na qual a ingestão de dados em lote seria mais vantajosa em comparação com a ingestão em tempo real (streaming)?

A) Em um sistema de monitoramento de tráfego que precisa ajustar os semáforos em tempo real com base no fluxo de veículos.

B) Para um aplicativo de e-commerce que recomenda produtos em tempo real, usando o comportamento online atual do usuário.

C) Quando a análise dos logs de segurança precisa ser realizada quase instantaneamente para detectar e mitigar ataques de rede assim que ocorrem.

D) No processamento de transações financeiras diárias de um banco, que podem ser agregadas ao final do dia para relatórios e análise.

E) Em um sistema de alerta médico que monitora constantemente os sinais vitais dos pacientes para fornecer alertas instantâneos a situações críticas.

",D,"
A ingestão de dados em lote é preferível em cenários onde os dados não precisam ser analisados em tempo real, mas podem ser processados após o término de um evento ou período determinado. Vamos analisar os itens:

A) Errado. Um sistema que precisa ajustar semáforos em tempo real requer a análise de dados em tempo real para ser eficaz.

B) Errado. Um aplicativo que recomenda produtos em tempo real depende do comportamento instantâneo do usuário, requerendo portanto a ingestão e processamento de dados de forma imediata (streaming).

C) Errado. A análise de logs de segurança para detecção e mitigação de ataques precisa ser feita rapidamente para evitar danos, o que significa que a ingestão de dados deve ser em tempo real.

D) Correto. O processamento de transações financeiras diárias pode ser feito após o fechamento dos negócios do dia. Neste cenário, a ingestão em lote é adequada, pois os dados não precisam ser processados imediatamente.

E) Errado. Um sistema de alerta médico requer resposta imediata a condições que podem ser vitais para a saúde do paciente, o que implica a necessidade de ingestão e processamento de dados em tempo real.
",D,4165381
tópico 5,"Rotulação de partes do discurso, part-of-speech tagging; Modelos de representação de texto - N-gramas, modelos vetoriais de palavras (CBOW, Skip-Gram e GloVe), modelos vetoriais de documentos (booleano, TF e TF-IDF, média de vetores de palavras e Paragraph Vector); ","Questão: No contexto de Processamento de Linguagem Natural, um dos desafios é a representação adequada de texto para tarefas como classificação, busca de informação e rotulação de partes do discurso (Part-of-Speech Tagging). Dentre os métodos existentes para representação de texto, qual alternativa descreve corretamente a relação entre modelos de representação de texto e suas aplicações?

A) N-gramas são sequências de 'n' palavras consecutivas e são úteis apenas para rotulação de partes do discurso pois não capturam semântica.

B) CBOW e Skip-Gram são modelos baseados em contagem de palavras que não são eficazes em capturar o contexto onde as palavras são utilizadas.

C) GloVe é um algoritmo de representação de documentos que baseia-se exclusivamente na frequência de termos, ignorando a posição das palavras e o peso dado aos termos mais raros.

D) O modelo booleano é um tipo de representação vetorial de documentos onde apenas a ocorrência ou não de palavras é levada em conta, sem considerar a frequência das palavras.

E) O modelo TF-IDF pondera os termos dos documentos não só pela frequência no documento (TF), mas também pela frequência inversa do termo nos documentos da coleção (IDF), o que o torna eficaz para destacar a importância de palavras raras em um documento específico, sendo útil em sistemas de busca e recuperação de informações.

",E,"

Explicação dos itens:

A) Incorreta. N-gramas capturam contexto local e podem ser usados para outras tarefas além de rotulação de partes do discurso, como modelagem de linguagem e detecção de sequências de palavras frequentes.

B) Incorreta. CBOW (Continuous Bag-of-Words) e Skip-Gram são modelos vetoriais de palavras que capturam contexto e relações semânticas pelo aprendizado em tarefas de previsão de palavras baseadas em seu contexto. Eles não são baseados em contagem e sim em aprendizado de representações vetoriais densas.

C) Incorreta. GloVe (Global Vectors for Word Representation) é um modelo de representação vetorial para palavras que considera a coocorrência global estatísticas da corpus e não é específico para representação de documentos. Ele combina aspectos de matrizes de contagens de palavras e modelos de previsão de palavras para capturar relações semânticas e sintáticas.

D) Correta no que diz sobre o modelo booleano, mas como a questão pediu a relação correta e suas aplicações, esta alternativa é incompleta ao não mencionar suas aplicações.

E) Correta. O modelo TF-IDF é amplamente usado em sistemas de recuperação de informações para a determinação de relevância de um documento com base na unicidade dos termos. Ao ponderar os termos tanto pela sua frequência no documento quanto pela sua frequência inversa nos documentos da coleção, ele consegue diferenciar a importância dos termos e é bem aplicado em sistemas de busca.",D,2483223
tópico 4,Métodos e técnicas de identificação causal: Métodos experimentais RCT e de identificação quase-experimental,"Questão:

O uso de métodos e técnicas para identificação causal é crucial na avaliação de políticas públicas e na pesquisa econômica para estabelecer relações de causa e efeito entre variáveis. Entre essas técnicas, destacam-se os experimentos randomizados controlados (RCT) e métodos de identificação quase-experimental. Considerando esses métodos, avalie as afirmações abaixo e escolha a opção correta.

I. Experimentos randomizados controlados (RCT) envolvem a atribuição aleatória de tratamentos a diferentes grupos de indivíduos, permitindo uma alta confiabilidade na inferência de relações causais, desde que a randomização seja bem implementada e com tamanho de amostra suficiente.

II. Métodos quase-experimentais, como a Regressão Descontínua (RD) e a Diferenças em Diferenças (DiD), são utilizados quando a randomização não é possível ou ética, aproveitando-se de eventos ou regras que afetam arbitrariamente alguns grupos e não outros, criando condições de comparação análogas à experimentação.

III. Uma das desvantagens dos RCTs é que, enquanto eles garantem a internalidade da validade – a causalidade dentro da amostra estudada –, sua validade externa é presumida, pois a capacidade de generalizar os resultados para outras populações não é confirmada pelo método em si.

IV. A validade interna de métodos quase-experimentais é tipicamente mais forte que a dos RCTs, pois ao utilizar situações da vida real, eles automaticamente garantem que as condições do estudo estão mais próximas de cenários não experimentais, favorecendo a generalização dos resultados.

Assinale a alternativa correta.

A) Todas as afirmações estão corretas.

B) Apenas as afirmações I, II e III estão corretas.

C) Apenas as afirmações I e II estão corretas.

D) Apenas as afirmações II, III e IV estão corretas.

E) Apenas as afirmações I e III estão corretas.

",B,"

Explicação dos itens:

I. Correta. Experimentos randomizados controlados dependem da atribuição aleatória e são reconhecidos por sua força no estabelecimento de relações causais. Contudo, a aleatoriedade deve ser bem planejada e a amostra deve ser representativa.

II. Correta. Métodos quase-experimentais são usados quando a randomização não é viável. Eles buscam replicar a lógica de experimentação em situações naturais ou administrativas, onde ocorrem ""experimentos naturais"".

III. Correta. A validade interna dos RCTs é uma de suas principais vantagens, mas a validade externa – a generalização dos resultados para outras populações – não é confirmada pelo RCT em si, e depende de outros fatores, como a representatividade da amostra em relação à população mais ampla.

IV. Incorreta. A validade interna de métodos quase-experimentais geralmente é considerada menos forte do que a dos RCTs devido a potenciais vieses resultantes da não-randomização. Métodos quase-experimentais requerem cuidados adicionais na interpretação dos resultados, pois podem estar sujeitos a vieses de seleção e outras confusões.",C,1144245
tópico 2,Tratamento de outliers e agregações,"Questão:
Durante uma análise de dados, um cientista de dados se depara com um conjunto de dados que contém várias variáveis numéricas. Ao explorar a variável ""idade"", ele percebe que existem valores atípicos (outliers) que podem distorcer as análises estatísticas. Neste contexto, ele decide tratar os outliers antes de prosseguir com o cálculo de medidas agregadas como média, mediana e moda. 

Dentre as opções a seguir, qual é a estratégia mais adequada para tratar os outliers e calcular as medidas agregadas sem a influência dessas observações atípicas?

a) Substituir todos os valores atípicos pela média da variável ""idade"".
b) Eliminar todas as observações que contêm outliers em qualquer variável.
c) Utilizar a mediana para calcular a média e manter os outliers na base de dados.
d) Aplicar uma transformação logarítmica na variável ""idade"" e calcular as medidas agregadas.
e) Calcular os intervalos interquartis para identificar os outliers e substituí-los pela mediana da variável ""idade"".

",E,"

Explicação dos itens:

a) Substituir todos os valores atípicos pela média da variável ""idade"" pode não ser adequado, já que a própria média é sensível a outliers e pode estar distorcida.

b) Eliminar todas as observações que contêm outliers pode resultar em perda significativa de dados, especialmente se os outliers não forem frequentes, o que poderia levar a uma análise enviesada ou a perda de informações importantes.

c) Utilizar a mediana para calcular a média não é lógico, pois são duas medidas diferentes. A mediana pode ser usada para resumir a distribuição central dos dados de forma que não é afetada por outliers, mas não se aplica no cálculo da média.

d) Aplicar uma transformação logarítmica pode ajudar na normalização da distribuição de dados e na redução do efeito dos outliers, mas não os trata diretamente. Além disso, a medida agregada deve ser calculada em relação aos dados originais para manter a interpretabilidade.

e) Calcular os intervalos interquartis (IQR) para identificar os outliers e substituí-los pela mediana da variável ""idade"" é uma estratégia comum para tratar outliers sem perder dados. A mediana é menos sensível a valores atípicos e pode representar melhor a tendência central da variável ""idade"" sem os outliers.",E,7997445
tópico 1,Banco de dados NoSQL,"Questão:
A popularização dos bancos de dados NoSQL ocorreu em paralelo com o crescimento de aplicações que requeriam escalabilidade, alta performance e capacidade de lidar com grandes volumes de dados não estruturados. Em relação aos diferentes tipos de bancos de dados NoSQL, assinale a opção que CORRETAMENTE associa o tipo de banco de dados ao seu modelo de armazenamento de dados:

A) Grafos - Utiliza tabelas com linhas fixas e colunas para armazenar dados e suas relações em forma de grafos.

B) Chave-Valor - Armazena os dados em arrays bidimensionais, conhecidos como buckets, onde cada chave mapeia para um array de valores.

C) Colunar - Organiza os dados por colunas, o que permite a realização de operações rápidas de leitura e escrita em colunas específicas, otimizando o processamento de consultas analíticas.

D) Documento - Estrutura dados em documentos similares a XML ou JSON, onde cada documento é identificado por uma chave única e pode conter hierarquias complexas de dados.

E) Orientado a objetos - Armazena os dados em objetos, onde cada objeto pode ser diretamente acessado sem necessidade de operações JOIN entre tabelas.

",C,"

Explicação dos itens:

A) Grafos - Esta opção está incorreta. Os bancos de dados de grafos não utilizam tabelas no sentido tradicional; em vez disso, eles armazenam dados em nós e arestas para representar e armazenar entidades e suas relações de forma gráfica.

B) Chave-Valor - Esta opção também está incorreta. Bancos de dados baseados em chave-valor armazenam os dados como uma coleção de pares de chave-valor, onde a chave é única e mapeia diretamente para um valor, não para um array de valores como sugerido.

C) Colunar - Esta alternativa é correta. Bancos de dados colunares são otimizados para operações em colunas, facilitando operações analíticas e big data, pois permitem um alto desempenho em leituras e escritas de colunas específicas, ao invés de linhas completas.

D) Documento - Esta opção é parcialmente correta na descrição de bancos de dados de documentos; entretanto, os documentos não são necessariamente estruturados em XML, JSON é mais comum e a chave não é o foco principal na descrição do modelo de armazenamento.

E) Orientado a objetos - Esta alternativa está incorreta. Bancos de dados orientados a objetos armazenam os dados como objetos, como na programação orientada a objetos, mas a característica marcante não é a ausência de operações JOIN, e sim como os dados são representados e armazenados de forma que reflete os objetos do domínio da aplicação.",D,4703446
tópico 1,Álgebra relacional e SQL (padrão ANSI),"Questão: 
Considere as seguintes relações em um banco de dados relacional:

Funcionario(F_id, F_nome, F_departamento_id, Salario)
Departamento(D_id, D_nome, Gerente_id)

Um administrador de banco de dados deseja realizar uma consulta para encontrar os nomes dos funcionários e os respectivos nomes de seus departamentos. Entretanto, a consulta deve incluir apenas aqueles funcionários cujo salário seja superior a R$5.000,00. Utilizando a Álgebra Relacional, qual seria a expressão correta para esta consulta? 

A) π F_nome, D_nome (Funcionario ⨝ F_departamento_id = D_id Departamento) σ Salario > 5000(Funcionario)

B) π F_nome, D_nome (σ Salario > 5000 (Funcionario) ⨝ F_departamento_id = D_id Departamento)

C) π F_nome, D_nome (Funcionario ⨝ F_departamento_id = D_id σ Salario > 5000(Departamento))

D) π F_nome, D_nome (Funcionario) ⨝ σ F_departamento_id = D_id (Salario > 5000(Departamento))

E) π F_nome, D_nome (Funcionario ⨝ Departamento) σ Salario > 5000(Funcionario)

",B,"

Explicação dos itens:

A) Esta opção está incorreta pois a seleção de salário é realizada após a junção das relações, sem a garantia de que apenas os funcionários com salário superior a 5000 serão considerados.

B) Esta opção é a correta pois primeiro faz-se a seleção dos funcionários com salário superior a 5000 e depois a junção com a tabela de departamentos para obter o nome dos departamentos.

C) Esta opção é incorreta porque primeiro realiza a junção das relações e depois tentar aplicar o filtro de salário na relação Departamento, o que é um erro lógico, já que o atributo Salario não faz parte desta relação.

D) Esta opção é incorreta porque aplica a condição de salário somente na relação Departamento e não na relação Funcionario, além de indicar incorretamente a operação de junção.

E) Esta opção está incorreta porque a seleção de funcionários com salário superior a 5000 deve ser feita antes da junção. Aqui, a seleção seria aplicada após a junção, o que não garante o filtro desejado.",E,9089647
tópico 1,"Banco de dados relacional: SQL Server, PostgreSQL, MySQL","Questão: Considere um ambiente de banco de dados empresarial onde é necessário garantir alta disponibilidade e integridade de dados entre servidores de bancos de dados relacionais localizados em geografias distintas. Qual das seguintes soluções é recomendada para replicação e sincronização de dados entre uma instância do SQL Server e uma instância do PostgreSQL, considerando a necessidade de transações em tempo real e a capacidade de lidar com possíveis falhas de rede?

A) Utilização de Foreign Data Wrappers (FDW) no PostgreSQL para conectar diretamente ao SQL Server.

B) Configuração de Linked Servers no SQL Server permitindo consultas diretas ao PostgreSQL.

C) Implementação de um ETL (Extract, Transform, Load) para sincronização periódica dos dados.

D) Uso da ferramenta de replicação SymmetricDS, que suporta sincronização bidirecional entre diferentes sistemas de bancos de dados.

E) Aplicação de MySQL Replication entre o SQL Server e o PostgreSQL considerando o uso de um driver de compatibilidade.

",D,"
Os itens podem ser explicados da seguinte forma:

A) Foreign Data Wrappers (FDW) no PostgreSQL são usados para acessar dados de fontes de dados externas, como outros bancos de dados SQL, mas não são projetados para sincronização contínua e em tempo real.

B) Linked Servers são uma funcionalidade do SQL Server para permitir a execução de consultas em servidores externos, porém, similarmente ao FDW, eles não são desenhados para uma replicação real-time ou sincronização de dados.

C) ETL (Extract, Transform, Load) é uma técnica comumente usada em data warehousing para sincronização periódica de dados, o que pode não ser adequado para sistemas que requerem atualizações em tempo real e podem ter performance comprometida em caso de volumes grandes de dados e transações frequentes.

D) SymmetricDS é uma solução de software de replicação de banco de dados que pode replicar mudanças de dados em tempo real e suporta plataformas mistas. Esta é a opção recomendada dado que ela foi projetada para trabalhar com múltiplos sistemas de gerenciamento de banco de dados (DBMS), incluindo SQL Server e PostgreSQL, e proporcionar alta disponibilidade e falha tolerante em replicação de dados.

E) MySQL Replication é o mecanismo de replicação para bancos de dados MySQL e não é projetado para sincronizar dados entre diferentes DBMSs como SQL Server e PostgreSQL. Adicionalmente, não existe um ""driver de compatibilidade"" que permita a replicação direta entre esses dois sistemas distintos.",A,3255183
tópico 5,Técnicas de classificação: Naive Bayes; Regressão logística; Redes neurais artificiais; Árvores de decisão (algoritmos ID3 e C4.5); Florestas aleatórias (random forest); Máquinas de vetores de suporte (SVM – support vector machines); K vizinhos mais próximos (KNN – K-nearest neighbours),"Questão:
Em um projeto de mineração de dados, um cientista de dados está ponderando qual técnica de classificação utilizar para desenvolver um modelo preditivo capaz de diferenciar com precisão entre mensagens de e-mail spam e não spam. A base de dados contém tanto variáveis categóricas quanto numéricas, um número relativamente grande de registros e espera-se que tenha ruídos nos dados. Considerando essas características e a capacidade das técnicas de classificação, qual das seguintes opções seria a menos adequada para inicialmente abordar essa tarefa?

A) Naive Bayes
B) Regressão logística
C) Redes neurais artificiais
D) Árvores de decisão (algoritmos ID3)
E) K vizinhos mais próximos (KNN – K-nearest neighbours)

",D,"

Explicação dos itens:

A) Naive Bayes: É uma escolha robusta e eficiente para classificação de textos como spam ou não spam. Possui bom desempenho mesmo quando a suposição de independência de características não é totalmente verdadeira. Portanto, não é a menos adequada.

B) Regressão logística: É um método comum para classificação binária e pode fornecer bons resultados para o problema de e-mail spam. Pode lidar bem com variáveis numéricas e categóricas após a devida codificação, então também não é a menos adequada.

C) Redes neurais artificiais: Embora sejam poderosas e flexíveis, as redes neurais exigem um grande volume de dados e são capazes de capturar relações complexas entre atributos. No entanto, sem mencionar a complexidade ou a estrutura de redes necessárias, elas não são automaticamente descartadas como menos adequadas nesta questão.

E) K vizinhos mais próximos (KNN – K-nearest neighbours): O KNN é um algoritmo que não faz suposições sobre a distribuição dos dados, assim é uma opção forte para dados com ruídos e variáveis categóricas e numéricas. Pode ser computacionalmente caro com um grande número de registros, mas não é necessariamente a menos adequada.

D) Árvores de decisão (algoritmos ID3): O algoritmo ID3 é baseado em informações de ganho e tende a favorecer atributos com muitos níveis (ou valores distintos) e pode não ser a melhor opção em casos com ruído e atributos numéricos misturados com categóricos. O sucessor C4.5 poderia lidar melhor devido à poda e ao tratamento de dados contínuos, mas o ID3 pode ser menos adequado aqui devido à sua sensibilidade ao ruído e ao seu tratamento menos sofisticado de diferentes tipos de atributos.",E,4694501
tópico 0,Conceitos de processamento massivo e paralelo,"Questão: Considerando o processamento massivo de dados em sistemas modernos, diversas arquiteturas e metodologias são empregadas para otimizar o desempenho e a escalabilidade desses sistemas. Qual das seguintes alternativas descreve corretamente o princípio do MapReduce, amplamente utilizado em ambientes de processamento de grandes volumes de dados, como o framework Apache Hadoop?

A) Uma abordagem de programação baseada em grafos, onde os dados são processados em vértices com computações paralelas.

B) Uma combinação de funções 'Map', que processam e transformam os dados de entrada em pares chave-valor, e funções 'Reduce', que agregam os resultados.

C) Uma técnica que envolve a multiplicação de matrizes distribuídas em uma rede de computadores para otimizar o processamento de consultas de banco de dados.

D) Uma estratégia de escalonamento de tarefas que prioriza operações de I/O sobre computação, redistribuindo os dados de maneira uniforme em vários discos.

E) Um modelo que emprega o conceito de 'streaming' de dados, permitindo que os dados sejam processados em tempo real, sem necessidade de armazenamento intermediário.

",B,"

Explicação dos itens:

A) Incorreto. O item A descreve um tipo de abordagem mais associada ao processamento de dados com grafos, exemplificado por sistemas como o Apache Giraph, e não o MapReduce.

B) Correto. O item B captura a essência do framework MapReduce. No 'Map', os dados são processados e transformados em pares chave-valor intermediários, e, no 'Reduce', os pares de mesma chave são combinados/agregados para produzir o resultado final. Esta é a característica principal do modelo MapReduce.

C) Incorreto. O item C está descrevendo um tipo de operação que poderia ser parte de um sistema de processamento de álgebra linear ou processamento distribuído de banco de dados, mas não é característico do paradigma MapReduce.

D) Incorreto. Embora a estratégia de escalonamento de tarefas seja importante em muitos sistemas de processamento de dados, o item D não descreve uma característica específica do MapReduce.

E) Incorreto. O modelo descrito no item E está mais relacionado a sistemas de processamento de dados em tempo real, como Apache Storm ou Apache Kafka Streams. MapReduce, por outro lado, é geralmente mais associado ao processamento em lote ('batch processing').",B,2700703
tópico 5,"Avaliação de modelos de classificação: treinamento, teste, validação; validação cruzada; métricas de avaliação - matriz de confusão, acurácia, precisão, revocação, F1-score e curva ROC","Questão: Em um problema de classificação binária, um cientista de dados desenvolveu um modelo de aprendizado de máquina e realizou a avaliação de desempenho utilizando várias métricas. A matriz de confusão obtida após a validação do modelo indicou o seguinte:

- Verdadeiro Positivo (TP): 80
- Verdadeiro Negativo (TN): 50
- Falso Positivo (FP): 20
- Falso Negativo (FN): 30

Com base nessas informações, assinale a opção que indica corretamente o cálculo da Precisão e da Revocação (Recall) e qual dessas métricas tem maior valor para o modelo em questão.

A) Precisão = 0,80 e Revocação = 0,62; a Revocação tem maior valor.
B) Precisão = 0,73 e Revocação = 0,73; ambas as métricas têm o mesmo valor.
C) Precisão = 0,80 e Revocação = 0,62; a Precisão tem maior valor.
D) Precisão = 0,73 e Revocação = 0,62; a Precisão tem maior valor.
E) Precisão = 0,67 e Revocação = 0,73; a Revocação tem maior valor.

",D,"

Explicação dos itens:

A) Incorreta. A precisão é calculada como TP / (TP + FP), o que seria 80 / (80 + 20) = 0,80, e a revocação é calculada como TP / (TP + FN), o que seria 80 / (80 + 30) = 0,727. Portanto, os valores de precisão e revocação são incorretos e, além disso, a revocação não é maior.

B) Incorreta. Os cálculos estão errados, pois a precisão seria 80 / (80 + 20) = 0,80 e a revocação seria 80 / (80 + 30) = 0,727. A afirmativa de que ambas têm o mesmo valor também está incorreta.

C) Incorreta. Apesar de a precisão estar correta (0,80), a revocação está errada. A revocação calculada corretamente é 80 / (80 + 30) = 0,727, não 0,62. Além disso, a revocação não é a maior das duas métricas.

D) Correta. A Precisão é TP / (TP + FP) = 80 / (80 + 20) = 0,80 e a Revocação é TP / (TP + FN) = 80 / (80 + 30) = 0,727. Arredondadas de acordo com a notação usual de duas casas decimais, a precisão fica 0,80 e a revocação 0,73, portanto a precisão (0,80) é maior.

E) Incorreta. Enquanto a revocação está corretamente calculada como 0,73, a precisão está incorreta, pois deveria ser 0,80, não 0,67 conforme calculado aqui. A afirmação de que a revocação tem um valor maior também está errada.",D,5733773
tópico 5,"Métricas de similaridade textual - similaridade do cosseno, distância euclidiana, similaridade de Jaccard, distância de Manhattan e coeficiente de Dice","Questão: Em sistemas de recuperação de informações e processamento de linguagem natural, diversas métricas são utilizadas para determinar a similaridade ou dissimilaridade entre textos. Dentre as seguintes alternativas, identifique a métrica que não é apropriada para calcular a similaridade direta entre representações vetoriais de textos.

A) Similaridade do Cosseno
B) Distância Euclidiana
C) Similaridade de Jaccard
D) Coeficiente de Dice
E) Distância de Manhattan

",B,"

Explicação dos itens:

A) Similaridade do Cosseno - Esta métrica mede o cosseno do ângulo entre dois vetores no espaço vetorial, portanto, é adequada para calcular a similaridade entre textos representados como vetores.

B) Distância Euclidiana - Embora envolva representações vetoriais, a distância euclidiana mede diretamente a distância entre dois pontos em um espaço multidimensional e é uma medida de dissimilaridade, não de similaridade direta.

C) Similaridade de Jaccard - Correta para comparar a similaridade entre conjuntos, como documentos representados por conjuntos de palavras. Contudo, é mais frequentemente utilizada em dados binários ou não ponderados, mas ainda assim se refere a uma medida de similaridade.

D) Coeficiente de Dice - Medida de similaridade entre conjuntos, que é duas vezes o número de elementos em comum dividido pela soma do número de elementos em cada conjunto. É uma métrica de similaridade, embora seja mais comum em dados binários ou conjuntos.

E) Distância de Manhattan - Esta é uma medida de dissimilaridade que calcula a soma das diferenças absolutas entre os pontos em cada dimensão. Similar à distância euclidiana, não é uma medida de similaridade direta, mas de dissimilaridade. No entanto, ambas a distância de Manhattan e a distância Euclidiana poderiam ser invertidas para fornecer uma métrica proporcional de similaridade, o que as torna aplicáveis de maneira indireta.",B,9759932
tópico 1,"Banco de dados e formatos de arquivo orientado a colunas: Parquet, MonetDB, duckDB","Questão:

Quando lidamos com grandes conjuntos de dados, a escolha do formato de armazenamento e do sistema gerenciador de banco de dados pode ter um impacto significativo na eficiência das operações de leitura e escrita, bem como na otimização de consultas analíticas. Dentre os formatos de arquivo e gerenciadores de banco de dados orientados a colunas, temos o Parquet, MonetDB e duckDB, que são projetados para otimizar operações analíticas em grandes volumes de dados. Considerando estas tecnologias, assinale a opção que apresenta uma afirmação INCORRETA.

A) Parquet é um formato de arquivo colunar de código aberto que suporta compressão de dados e esquemas evolutivos, amplamente utilizado em ecossistemas de Big Data como Hadoop e Spark.

B) MonetDB é um banco de dados relacional orientado a colunas, que se destaca pela sua capacidade de executar operações de processamento analítico online (OLAP) de forma eficiente, devido à sua arquitetura inovadora de gerenciamento de memória.

C) duckDB é um gerenciador de banco de dados relacional, que oferece suporte para operações OLTP (Online Transaction Processing), privilegiando a eficiência das transações em detrimento de consultas analíticas complexas.

D) O formato Parquet e os sistemas MonetDB e duckDB são capazes de realizar operações de pushdown de predicados, o que permite que filtros e agregações sejam aplicados diretamente nos dados armazenados, reduzindo a quantidade de dados movimentados durante consultas.

E) A utilização de índices de bitmap é um recurso comum em sistemas de banco de dados orientados a colunas como o MonetDB, o qual permite consultas de alta performance em colunas com pouca cardinalidade.

",C,"

Explicação dos itens:

A) Correta. O Parquet é de fato um formato de arquivo colunar que oferece compressão eficiente e suporta esquemas que podem evoluir ao longo do tempo, sendo amplamente adotado em ambientes como Hadoop e Spark para armazenamento de dados de maneira eficiente.

B) Correta. MonetDB é um sistema gerenciador de banco de dados relacional e orientado a colunas que se destaca na execução de consultas OLAP. A sua arquitetura é voltada para um alto desempenho em operações analíticas, principalmente devido ao seu gerenciamento de memória e execução de consultas em vetor.

C) Incorreta. DuckDB é um banco de dados analítico que também é orientado a colunas e projetado para OLAP (processamento analítico online) e não OLTP. Ao contrário do que a afirmação sugere, ele é otimizado para consultas analíticas complexas e não para eficiência de transações usuais em OLTP.

D) Correta. O pushdown de predicados é uma característica importante de sistemas orientados a colunas, e tanto o formato de arquivo Parquet, quanto os sistemas MonetDB e duckDB oferecem suporte a essa funcionalidade, otimizando a execução de consultas ao reduzir a necessidade de transferência de grandes volumes de dados.

E) Correta. O uso de índices de bitmap é de fato um recurso comum em sistemas de banco de dados orientados a colunas. Eles são especialmente úteis em colunas com pouca variação de valores (baixa cardinalidade), pois permitem um rápido filtro e são eficientes em termos de espaço. MonetDB é um exemplo de sistema que pode utilizar índices de bitmap para melhorar o desempenho das consultas.",A,8435471
tópico 5,Técnicas de classificação: Naive Bayes; Regressão logística; Redes neurais artificiais; Árvores de decisão (algoritmos ID3 e C4.5); Florestas aleatórias (random forest); Máquinas de vetores de suporte (SVM – support vector machines); K vizinhos mais próximos (KNN – K-nearest neighbours),"Questão:
Um pesquisador está desenvolvendo um modelo de classificação para prever se uma música se tornará um sucesso com base em características como duração, gênero, número de vezes que foi compartilhada nas redes sociais, entre outros metadados. Seu dataset é composto por milhares de músicas, cada uma com sua popularidade rotulada como ""sucesso"" ou ""não sucesso"". O pesquisador deseja utilizar um algoritmo de classificação que possa lidar bem com a possibilidade de relações não-lineares entre as características e o rótulo de popularidade e que, além disso, possa fornecer uma ideia da importância relativa de cada característica para o modelo de previsão. 

Qual dos seguintes algoritmos é o MAIS apropriado para ele utilizar na construção de seu modelo de classificação?

A) Naive Bayes

B) Regressão logística

C) Redes neurais artificiais

D) Árvores de decisão (algoritmos ID3 e C4.5)

E) Máquinas de vetores de suporte (SVM – Support Vector Machines)

F) K vizinhos mais próximos (KNN – K-nearest neighbours)

G) Florestas aleatórias (random forest)

",G,"

Explicação:

A) Naive Bayes é um algoritmo baseado no teorema de Bayes, e funciona melhor quando as características são independentes entre si, o que pode não ser o caso neste cenário.

B) A regressão logística é um bom modelo para classificação binária, mas pode não capturar relações não-lineares tão bem quanto outros algoritmos.

C) As redes neurais artificiais são poderosas para capturar relações não-lineares e são altamente flexíveis, porém, elas geralmente não fornecem compreensões diretas sobre a importância relativa das características.

D) As árvores de decisão, como ID3 e C4.5, são modelos interpretáveis que podem lidar com não-linearidades, mas individualmente podem não ser tão robustas quanto uma combinação de múltiplas árvores.

E) SVM pode ser usado para encontrar limites de decisão não-lineares com o uso de truques do kernel, mas não fornece uma compreensão direta da importância das características.

F) KNN é um algoritmo intuitivo que funciona bem para datasets onde a proximidade das amostras é um bom indicador da sua classificação, porém pode ser computacionalmente custoso e não oferece visão direta sobre a importância das características.

G) Florestas aleatórias são um conjunto de árvores de decisão que melhoram a robustez e o desempenho do modelo em comparação com uma única árvore de decisão. Além disso, elas podem lidar bem com relações não-lineares e são capazes de fornecer um ranking de importância das características, atendendo às necessidades específicas do pesquisador.",B,2104455
tópico 3,Programação orientada a objetos,"Questão: Em Programação Orientada a Objetos (POO), o princípio do polimorfismo é frequentemente aplicado para aumentar a flexibilidade e a manutenibilidade do código. A respeito desse princípio, analise as afirmativas a seguir:

I - Polimorfismo permite que objetos de diferentes classes derivadas de uma mesma superclasse sejam tratados por meio de referências à superclasse, facilitando assim o reuso de código.

II - O polimorfismo de inclusão, também conhecido como polimorfismo de subtipos, se dá mediante a sobrescrita de métodos em subclasses, exigindo que os métodos possuam a mesma assinatura da superclasse.

III - Em linguagens que suportam polimorfismo paramétrico, como Java e C#, é possível implementar polimorfismo por meio de sobrecarga de métodos, onde métodos com o mesmo nome na mesma classe podem ter assinaturas distintas.

IV - O polimorfismo estático, também chamado de ad hoc, se baseia na escolha do método a executar através do mecanismo de ligação tardia (late binding), que ocorre em tempo de execução.

Está(ão) correta(s) apenas a(s) afirmativa(s):

A) I e II
B) I, II e III
C) III e IV
D) I, II e IV
E) Todas as afirmativas

",A," 
Explicação dos itens:

I - Correta. Polimorfismo realmente permite tratar diferentes objetos derivados de uma mesma superclasse como se fossem do tipo da superclasse, aumentando, por conseguinte, o reuso de código.

II - Correta. O polimorfismo de inclusão ou de subtipos ocorre quando uma subclasse sobrescreve métodos da superclasse mantendo a mesma assinatura, cumprindo assim o contrato estabelecido pela superclasse e permitindo que sejam substituíveis.

III - Incorreta. A afirmativa confunde polimorfismo paramétrico com sobrecarga de métodos. Polimorfismo paramétrico é tipicamente implementado através de generics em linguagens como Java e C#, enquanto a sobrecarga de métodos é um tipo de polimorfismo ad-hoc, pois permite múltiplas funções com o mesmo nome mas com diferentes assinaturas na mesma classe.

IV - Incorreta. O polimorfismo estático ou ad hoc é relacionado à sobrecarga e à coerção, e a resolução do método a ser executado é feita em tempo de compilação, não em tempo de execução. Ligação tardia (late binding) está associada ao polimorfismo dinâmico, onde a decisão sobre qual método executar é resolvida durante a execução do programa, característico do polimorfismo de inclusão.",B,4262316
tópico 4,Probabilidade e probabilidade condicional,"Questão:
A empresa XYZ está realizando uma pesquisa de mercado para entender as preferências de seus consumidores. A pesquisa revela que dentre os consumidores da empresa, 40% preferem o produto A, 35% preferem o produto B e o restante não tem preferência definida entre os dois produtos. A pesquisa mostra também que, entre aqueles que preferem o produto A, 70% são mulheres. Já entre os que preferem o produto B, apenas 50% são mulheres.

Se for escolhido aleatoriamente um consumidor que prefere o produto B, qual a probabilidade de que esse consumidor seja mulher?

A) 20%
B) 35%
C) 50%
D) 70%
E) 85%

",C,"

Explicação:

A questão pede a probabilidade condicional de um consumidor ser mulher, dado que ela prefere o produto B. A informação dada diretamente na questão diz que 50% dos que preferem o produto B são mulheres. Não é necessário calcular nenhum valor adicional, pois a própria questão já fornece a probabilidade condicional requerida.

- A) 20%: Incorreto. Este valor não está relacionado às probabilidades fornecidas na questão.
- B) 35%: Incorreto. Este valor representa a porcentagem total de consumidores que preferem o produto B, não a probabilidade condicional de ser mulher dentro desse grupo.
- C) 50%: Correto. Conforme indicado na questão, metade dos consumidores que preferem o produto B são mulheres.
- D) 70%: Incorreto. Este valor refere-se à porcentidade de mulheres entre os consumidores que preferem o produto A.
- E) 85%: Incorreto. Não há nenhuma informação na questão que leva a este valor.",C,3773619
tópico 2,Data cleansing,"Considerando os processos envolvidos no preparo de dados para análises e aplicações de Machine Learning, o Data Cleansing é uma etapa fundamental para garantir a qualidade e confiabilidade dos resultados obtidos. Com base nesse entendimento, avalie as seguintes afirmativas sobre Data Cleansing e escolha a opção correta:

I. Data Cleansing, também conhecido como limpeza de dados, envolve a correção ou a remoção de dados incorretos, incompletos, duplicados ou irrelevantes de um conjunto de dados.

II. Uma prática comum de Data Cleansing é a imputação de dados, que consiste na substituição de valores faltantes por estimativas, podendo ser realizada através de métodos como a média, mediana ou modos para variáveis numéricas e categorização para variáveis categóricas.

III. A identificação de outliers sempre requer a remoção dos mesmos do conjunto de dados, uma vez que sua presença pode distorcer análises posteriores e modelos preditivos.

IV. Após o processo de Data Cleansing, não é necessário revisar dados limpos, pois a etapa é completamente automatizada e infalível.

Escolha a assertiva correta:

A) Apenas I e II estão corretas.
B) Apenas II e III estão corretas.
C) Apenas I, II e III estão corretas.
D) Todas as afirmativas estão corretas.
E) Apenas I, II e IV estão corretas.

",A,"

I. Correta. O Data Cleansing realmente envolve a correção ou remoção de dados problemáticos para criar um conjunto de dados confiável.

II. Correta. A imputação de dados é uma técnica utilizada durante a limpeza de dados para lidar com valores faltantes, aplicando diferentes métodos estatísticos conforme o tipo de variável.

III. Incorreta. Embora a presença de outliers possa alterar os resultados de determinadas análises, nem sempre é apropriado removê-los, uma vez que podem representar variações genuínas nos dados ou pontos críticos que são essenciais para o estudo. A decisão de removê-los ou não deve ser contextual e baseada em uma análise criteriosa.

IV. Incorreta. Apesar da automação facilitar o processo de Data Cleansing, é recomendável revisar os dados após a limpeza para garantir que nenhum erro foi cometido e que a qualidade foi mantida. A revisão manual ainda é uma etapa importante na garantia da qualidade dos dados.
",A,2879377
tópico 5,Técnicas de classificação: Naive Bayes; Regressão logística; Redes neurais artificiais; Árvores de decisão (algoritmos ID3 e C4.5); Florestas aleatórias (random forest); Máquinas de vetores de suporte (SVM – support vector machines); K vizinhos mais próximos (KNN – K-nearest neighbours),"Questão:
Em um contexto de análise de dados, uma empresa deseja aplicar técnicas de aprendizado de máquina para classificar e-mails como ""spam"" ou ""não spam"". Considerando que a base de dados é grande e desbalanceada, com muito mais e-mails classificados como ""não spam"" do que ""spam"", e que a empresa busca uma solução que forneça uma probabilidade associada à classificação, qual técnica de classificação é a mais apropriada para atender a essa necessidade?

A) Naive Bayes
B) Redes Neurais Artificiais
C) Árvores de Decisão (algoritmos ID3 e C4.5)
D) Máquinas de Vetores de Suporte (SVM – Support Vector Machines)
E) K Vizinhos Mais Próximos (KNN – K-nearest Neighbours)

",A,"

Breve explicação dos itens:

A) Naive Bayes é adequado para o cenário descrito porque é capaz de lidar bem com grandes volumes de dados e pode fornecer probabilidades associadas às classificações. Além disso, sua suposição de independência condicional frequentemente funciona bem para o problema de classificação de e-mails como ""spam"" ou ""não spam"".

B) Redes Neurais Artificiais podem ser utilizadas, mas elas demandam um tempo maior de treinamento e nem sempre fornecem probabilidades de forma direta.

C) Árvores de Decisão (algoritmos ID3 e C4.5) são úteis para modelagem preditiva, mas também não fornecem probabilidades e podem enfrentar dificuldades devido ao desbalanceamento dos dados.

D) Máquinas de Vetores de Suporte (SVM – Support Vector Machines) são poderosas para encontrar o hiperplano de separação ótimo em dados de alta dimensão, mas não são a melhor escolha quando se deseja uma probabilidade associada à previsão e podem não ser adequadas para dados desbalanceados sem um pré-processamento adequado.

E) K Vizinhos Mais Próximos (KNN – K-nearest Neighbours) é um método não-paramétrico que pode não ser prático para grandes volumes de dados devido ao seu custo computacional e também não fornece uma probabilidade direta da classificação. Além disso, o desbalanceamento pode enviesar o classificador.",A,3320589
tópico 0,"Arquitetura de cloud computing para ciência de dados (AWS, Azure, GCP)","Questão: A utilização de serviços de computação em nuvem para a implementação de soluções de ciência de dados tem crescido substancialmente, com plataformas como AWS, Azure e GCP oferecendo uma gama de serviços especializados. Dentre as opções a seguir, qual serviço NÃO está corretamente associado à plataforma de cloud computing para o uso em ciência de dados?

A) AWS SageMaker - Um serviço totalmente gerenciado que fornece a cientistas de dados e desenvolvedores a capacidade de construir, treinar e implantar modelos de machine learning de forma ágil.

B) Azure Machine Learning Studio - Uma plataforma gráfica de arrastar e soltar que permite construir, testar e implantar modelos de machine learning sem precisar escrever código.

C) GCP BigQuery - Um serviço de armazenamento de dados gerenciado e altamente escalável que permite a análise de big data através de consultas SQL rápidas e eficientes.

D) Azure Cosmos DB - Um serviço de banco de dados multimodal distribuído globalmente para qualquer escala, projetado para ciência de dados que exige latência mínima e análises em tempo real.

E) AWS Data Pipeline - Um serviço de integração de dados que facilita o processo de transferência de grandes volumes de dados entre diferentes serviços da AWS e fontes de dados on-premise, permitindo a automação de fluxos de trabalho de dados necessários para a análise.

",D,"

Explicação dos itens:

A) AWS SageMaker está corretamente associado à AWS e é amplamente utilizado por cientistas de dados para construir, treinar e implantar modelos de machine learning.

B) Azure Machine Learning Studio está corretamente associado à Azure, servindo como uma plataforma simplificada para construção de modelos de machine learning.

C) GCP BigQuery está corretamente associado ao Google Cloud Platform e é uma ferramenta de análise de big data conhecida por sua capacidade de executar consultas SQL rápidas em grandes conjuntos de dados.

D) Azure Cosmos DB está incorretamente associado à ciência de dados neste contexto. Embora seja uma plataforma de banco de dados da Microsoft Azure e possa ser usada em conjunto com aplicações de ciência de dados, ela não é especificamente projetada para ciência de dados, mas sim para desenvolvimento de aplicações com requisitos de distribuição global e multimodelos de dados.

E) AWS Data Pipeline está corretamente associado à AWS e é um serviço que ajuda a mover e transformar dados entre os diferentes serviços da AWS e fontes de dados, podendo ser usado em processos de ciência de dados para preparação de dados.",D,4327067
tópico 5,Técnicas de redução de dimensionalidade: Seleção de características (feature selection); Análise de componentes principais (PCA – principal component analysis),"Questão: Em contextos de aprendizado de máquina e análise de dados, pesquisadores e analistas muitas vezes se deparam com grandes conjuntos de dados que apresentam um alto número de variáveis. Em tais situações, técnicas de redução de dimensionalidade são aplicadas para simplificar o modelo sem perder informações essenciais. Dentre as técnicas apresentadas a seguir, qual se caracteriza por transformar as variáveis originais em um conjunto menor de variáveis não correlacionadas que capturam a maior parte da variação nos dados?

A) Regularização Lasso
B) Análise Discriminante Linear (LDA)
C) K-means clustering
D) Análise de Componentes Principais (PCA)
E) Seleção de características baseada em árvores de decisão

",D," 

A alternativa A, Regularização Lasso, é uma técnica de regressão que pode ser usada para seleção de características, pois penaliza os coeficientes das variáveis menos importantes, reduzindo-as a zero. Entretanto, não transforma as variáveis em componentes principais. A alternativa B, Análise Discriminante Linear (LDA), é um método de classificação que também pode ser usado para redução de dimensionalidade, mas o objetivo é maximizar a separação entre categorias pré-definidas, não a variação nos dados. C, K-means clustering, é uma técnica de agrupamento, que não está diretamente relacionada à redução de dimensionalidade ou transformação de variáveis. E, Seleção de características baseada em árvores de decisão, refere-se ao uso de modelos de árvores de decisão para identificar variáveis importantes, mas novamente, não transforma as variáveis existentes em componentes principais. D, Análise de Componentes Principais (PCA), é a técnica correta, pois ela transforma as variáveis originais em um novo conjunto de variáveis ortogonais (não correlacionadas) chamadas componentes principais, que são criadas de forma a capturar a maior quantidade possível da variação presente no conjunto de dados original.",D,7987950
tópico 2,Enriquecimento,"Questão: Durante o processo de enriquecimento isotópico, considerável atenção é dirigida à separação do isótopo de urânio-235 a partir do mais prevalente urânio-238. Diferentes tecnologias podem ser utilizadas para se alcançar o enriquecimento desejado, incluindo a difusão gasosa e a ultracentrifugação. No contexto do processo de ultracentrifugação, avalie as seguintes afirmações:

I. A ultracentrifugação aproveita-se da ligeira diferença de massa entre os isótopos U-235 e U-238 para separá-los, usando uma força centrífuga que atua mais intensamente sobre as partículas de maior massa.

II. Na ultracentrifugação, o urânio é inicialmente convertido em um gás, hexafluoreto de urânio (UF6), para que possa ser processado dentro de uma série de centrífugas em alta velocidade.

III. O processo de ultracentrifugação é significativamente mais eficiente e exige menos energia do que o método de difusão gasosa, tornando-o o padrão atual na indústria para o enriquecimento de urânio.

IV. O ultracentrifugador utilizado tem um papel secundário no processo de enriquecimento, pois as diferenças isotópicas são suficientes para garantir uma separação efetiva sem a necessidade de um equipamento especializado.

Assinale a opção que contém apenas as afirmativas corretas:

A) I e III  
B) II e IV  
C) I, II e III  
D) Todas estão corretas  
E) I e II  

",C,"

A explicação dos itens:

I. Correta. A ultracentrifugação depende da diferença de massa entre os isótopos de urânio para a separação, onde o U-238, por ser mais pesado, é lançado mais para fora no rotor da centrífuga do que o U-235.

II. Correta. O hexafluoreto de urânio (UF6) é a forma gasosa utilizada para o urânio durante o processo de ultracentrifugação, pois permite que o enriquecimento seja realizado de uma maneira eficaz.

III. Correta. A ultracentrifugação é de fato mais eficiente e consome menos energia que a difusão gasosa, por isso é o método preferido e mais moderno para o enriquecimento de urânio.

IV. Incorreta. O equipamento de ultracentrifugação é essencial para o processo de enriquecimento. As diferenças de massa entre os isótopos são pequenas, e sem o equipamento especialmente projetado para explorar essas diferenças, a separação seria inviável ou muito menos eficiente.",,9636606
tópico 4,Teorema do limite central,"Questão: Um pesquisador está interessado em estimar o tempo médio de resposta a uma determinada tarefa cognitiva. A partir de um estudo piloto, constatou-se que a distribuição dos tempos de resposta tem uma média μ e desvio-padrão σ desconhecidos. No entanto, o pesário sabe que essa distribuição não é normal. Para obter uma estimativa do tempo médio de resposta, ele decide coletar uma amostra aleatória de tamanho n = 100. 

De acordo com o Teorema do Limite Central, qual das seguintes afirmações é verdadeira sobre a distribuição da média amostral dos tempos de resposta, à medida que n aumenta?

A) A média amostral dos tempos de resposta tende a se aproximar de uma distribuição normal, independentemente da distribuição do tempo de resposta, e o desvio-padrão da média se torna igual a σ/√n.
B) A média amostral tende a se aproximar de uma distribuição com média μ/√n e variância σ²/n.
C) A distribuição da média amostral dos tempos de resposta será idêntica à distribuição dos tempos de resposta individuais, independentemente do tamanho da amostra n.
D) Se a distribuição dos tempos de resposta for não normal, a média amostral não se aproximará da distribuição normal, a menos que n seja maior do que 100.
E) A média amostral será sempre equivalente à média populacional μ, e seu desvio-padrão será σ, independentemente do tamanho da amostra.

",A," 
A distribuição da média amostral dos tempos de resposta tende a uma distribuição normal devido ao Teorema do Limite Central, o qual estabelece que, para tamanhos de amostra suficientemente grandes (geralmente n > 30 é considerado suficiente), a distribuição das médias amostrais de qualquer distribuição com média μ e desvio-padrão σ se aproximará de uma distribuição normal com a mesma média μ e com desvio-padrão reduzido a σ/√n. Vejamos os erros nos outros itens:

Item B: Incorreto porque sugere erroneamente que a média amostral teria uma média que é μ/√n, quando na verdade a média da distribuição amostral é a mesma média da população, ou seja, μ.

Item C: Incorreto, pois, segundo o Teorema do Limite Central, a forma da distribuição da média amostral será aproximadamente normal, independentemente da forma da distribuição da população, desde que o tamanho da amostra seja suficientemente grande.

Item D: Incorreto porque subentende que o tamanho da amostra especificamente maior que 100 é necessário para que a distribuição das médias amostrais se aproxime de uma normal, o que não é verdade. O Teorema do Limite Central se aplica para tamanhos grandes o suficiente, e não necessariamente maior que 100.

Item E: Incorreto porque, embora a média amostral seja um estimador não viesado da média populacional, o desvio-padrão da média amostral é σ/√n, não σ.",A,8746574
tópico 0,Armazenamento de big data,"Questão: Considerando os desafios associados ao armazenamento de grandes volumes de dados, conhecido como Big Data, qual das seguintes opções representa a tecnologia de software que fornece uma infraestrutura confiável e escalável para armazenar e processar dados em larga escala distribuídos em clusters de servidores?

A) Relational Database Management System (RDBMS)
B) Distributed File System (DFS)
C) Online Analytical Processing (OLAP)
D) Non-relational Database Management System (NoSQL)
E) Storage Area Network (SAN)

",B," 
A alternativa correta é a letra B) Distributed File System (DFS), uma vez que é projetado para permitir o armazenamento de dados em múltiplos servidores e oferece tolerância a falhas, o que é essencial para o armazenamento e processamento de Big Data. Os exemplos incluem o Hadoop Distributed File System (HDFS) e o Google File System (GFS).

Explanação dos itens:
A) Um RDBMS é ótimo para dados estruturados e operações complexas de consulta, mas pode ter dificuldades com o volume e a velocidade de dados encontrados no Big Data.
B) DFS, como mencionado, é uma tecnologia adequada para o armazenamento e processamento de Big Data, devido à sua arquitetura distribuída e escalável.
C) OLAP é uma tecnologia principalmente para análise de dados e não é específica para armazenamento de dados, embora seja comumente usada em big data para processamento analítico.
D) NoSQL é um tipo de base de dados que pode lidar com Big Data, mas se refere a uma categoria de sistemas de gestão de bases de dados que diferem do modelo tradicional RDBMS, mas não especificamente a infraestruturas de armazenamento distribuídas.
E) SAN é uma rede de dispositivos de armazenamento dedicada que fornece acesso a dados consolidados, em bloco, mas não é ideal para o processamento distribuído que o Big Data requer.",C,698336
tópico 4,Variáveis aleatórias e funções de probabilidade,"Questão:
Considere a variável aleatória discreta X que segue a seguinte função de probabilidade:

P(X = x) = k(x^2 – 4), onde x = –1, 0, 1, 2, 3

Para que essa função seja considerada uma função de probabilidade adequada, a constante k deve satisfazer uma condição específica. Essa condição é tal que:

A) k > 1/14
B) k = 1/14
C) 0 < k < 1/14
D) k < 0
E) k é um número inteiro positivo

",B,"

Explicação dos itens:

A) k > 1/14 - Este item é incorreto porque se k for maior que 1/14, a soma das probabilidades excederia 1, o que não é permitido para uma função de probabilidade.

B) k = 1/14 - Este é o item correto. A soma das probabilidades deve ser igual a 1. Ao calcularmos P(X = –1) + P(X = 0) + P(X = 1) + P(X = 2) + P(X = 3) com a fórmula dada e resolvendo a equação, encontramos k = 1/14.

C) 0 < k < 1/14 - Este item não está correto, pois embora a soma das probabilidades deva ser menor do que 1, para ser uma função de probabilidade válida, a soma deve ser exatamente 1. Portanto, deve haver um valor específico para k e não um intervalo.

D) k < 0 - Este item está incorreto porque a probabilidade não pode ser negativa, e portanto, k também não pode ser negativo.

E) k é um número inteiro positivo - Este item está incorreto porque k deve ser uma constante que assegure que a soma total das probabilidades seja 1. Não há nenhuma garantia de que essa constante deva ser um número inteiro positivo; na verdade, para essa função de probabilidade específica, não é.",B,2902240
tópico 2,Matching,"Questão: Sobre o problema do Matching, ou Emparelhamento, em grafos, é correto afirmar que:

A) O algoritmo de Edmonds, também conhecido como algoritmo dos ""blossoms"", resolve o emparelhamento máximo em tempo linear para quaisquer grafos.
B) Um emparelhamento perfeito é aquele em que cada vértice do grafo é incidente a uma e apenas uma aresta do emparelhamento.
C) Um grafo bipartido sempre possui um emparelhamento perfeito se o número de vértices em cada uma das partições for ímpar.
D) O algoritmo de Hopcroft-Karp é uma estratégia eficiente que soluciona o emparelhamento máximo em grafos bipartidos em tempo polinomial.
E) Em um emparelhamento máximo, a quantidade de arestas no emparelhamento é igual ao número de vértices no grafo.

",D," 

Explicação dos itens:
A) Incorreto. O algoritmo de Edmonds resolve o problema de emparelhamento máximo em grafos, mas não é em tempo linear. É um algoritmo polinomial específico para grafos não bipartidos.
B) Correto. Um emparelhamento perfeito de um grafo é um emparelhamento em que todos os vértices do grafo são cobertos por uma única aresta do emparelhamento, sem sobreposições ou vértices despareados.
C) Incorreto. O fato de o número de vértices em cada uma das partições ser ímpar não garante a existência de um emparelhamento perfeito. A condição necessária e suficiente para a existência de um emparelhamento perfeito em um grafo bipartido é o Teorema de Hall, que estabelece que para cada subconjunto de vértices de uma das partições, o número de vizinhos deste conjunto deve ser ao menos tão grande quanto o número de vértices do conjunto.
D) Correto. O algoritmo de Hopcroft-Karp é uma estratégia eficiente para encontrar um emparelhamento máximo em grafos bipartidos e opera em tempo polinomial O(sqrt(V)E), onde V é o número de vértices e E é o número de arestas do grafo.
E) Incorreto. Em um emparelhamento máximo, a quantidade de arestas no emparelhamento é o maior número possível, mas não necessariamente igual ao número total de vértices do grafo. Dependendo da estrutura do grafo, alguns vértices podem ficar desemparelhados.",,9491748
tópico 2,Algoritmos fuzzy matching e stemming,"Questão:

A técnica de algoritmos fuzzy matching é frequentemente utilizada no processamento de linguagem natural para associar termos ou frases que são semelhantes, mas não idênticos. Por outro lado, o stemming é um processo que busca reduzir as palavras à sua raiz ou forma base, removendo os sufixos. Dadas as seguintes afirmativas sobre fuzzy matching e stemming:

I. O fuzzy matching pode ser baseado em diferentes algoritmos, incluindo o algoritmo de Levenshtein, que mede a distância entre duas sequências de caracteres.

II. O stemming é um processo que sempre resulta em palavras que existem no dicionário da língua em questão, garantindo a preservação do significado original.

III. Uma das aplicações do fuzzy matching é na correção ortográfica automática, onde o algoritmo precisa lidar com possíveis erros de digitação ou variações na escrita de uma palavra.

IV. Algoritmos de stemming são particularmente úteis na indexação de textos para sistemas de recuperação de informação, uma vez que agrupam variantes morfológicas de uma palavra-chave.

Estão corretas apenas as afirmativas:

a) I e II
b) II e III
c) I e III
d) I e IV
e) III e IV

",D,"

Explicação dos itens:

I. Correto. O algoritmo de Levenshtein é um exemplo de uma abordagem para o fuzzy matching, que computa a distância mínima de edição entre dois strings, ou seja, o número mínimo de inserções, deleções ou substituições necessárias para transformar uma string na outra.

II. Incorreto. O stemming nem sempre produz palavras que existem no dicionário; em alguns casos, pode resultar em um ""stem"" que não é uma palavra válida da língua. O processo visa reduzir a palavra a uma forma base para processamento, não necessariamente mantendo o significado original.

III. Correto. O fuzzy matching é de fato utilizado em sistemas de correção ortográfica para identificar e sugerir correções a erros de digitação ou variações comuns na escrita das palavras.

IV. Correto. O stemming ajuda na indexação de textos em sistemas de busca ao agrupar variantes morfológicas da mesma palavra, melhorando a eficiência da busca ao tratar termos relacionados como equivalentes.",C,9795622
tópico 1,Álgebra relacional e SQL (padrão ANSI),"Questão: 

Considere as seguintes relações no esquema de uma base de dados de uma biblioteca:

Livros(codigo_livro, titulo, ano_publicacao)
Emprestimos(num_emprestimo, codigo_livro, data_emprestimo, data_devolucao, codigo_socio)
Socios(codigo_socio, nome_socio, endereco)

Um bibliotecário deseja encontrar os títulos dos livros que nunca foram emprestados. Qual das seguintes consultas em SQL (padrão ANSI) retornaria o resultado correto?

A) SELECT titulo FROM Livros WHERE codigo_livro NOT IN (SELECT codigo_livro FROM Emprestimos);

B) SELECT titulo FROM Livros L INNER JOIN Emprestimos E ON L.codigo_livro = E.codigo_livro WHERE E.data_emprestimo IS NULL;

C) SELECT titulo FROM Livros WHERE codigo_livro NOT IN (SELECT codigo_livro FROM Emprestimos WHERE data_emprestimo IS NOT NULL);

D) SELECT L.titulo FROM Livros L LEFT JOIN Emprestimos E ON L.codigo_livro = E.codigo_livro WHERE E.num_emprestimo IS NULL;

E) SELECT L.titulo FROM Livros L, Emprestimos E WHERE L.codigo_livro = E.codigo_livro AND E.data_emprestimo IS NULL;

",A," 

Explicação dos itens:

A) Correta. Esta consulta seleciona todos os títulos de livros cujos códigos não estão presentes na tabela de Empréstimos. Isso implica que estes livros nunca foram emprestados.

B) Incorreta. Esta consulta realiza um INNER JOIN entre as tabelas Livros e Empréstimos, o que retornará apenas livros que foram emprestados. Onde a cláusula 'WHERE E.data_emprestimo IS NULL' é satisfeita por nenhuma linha, pois o INNER JOIN já elimina as entradas não correspondentes.

C) Incorreta. Onde a cláusula no subselect exclui os códigos de livros com empréstimos ativos, mas isso incluirá livros que já foram emprestados e devolvidos.

D) Incorreta. Embora a consulta realize um LEFT JOIN e parece selecionar corretamente os livros que não foram emprestados (WHERE E.num_emprestimo IS NULL), o nome da coluna 'num_emprestimo' é um confusor; em uma tabela de Empréstimos, esta coluna dificilmente seria nula, já que identifica um empréstimo e não se refere à ausência do mesmo.

E) Incorreta. Esta é uma consulta que utiliza um cruzamento cartesiano (CROSS JOIN) que deve ser evitado quando possível devido à performance, além de que erro no WHERE considera livros que já foram emprestados, mas com uma data de empréstimo nula, o que não condiz com a lógica do negócio de uma biblioteca e é irrelevante para encontrar livros que nunca foram emprestados.",D,929111
tópico 4,Teorema do limite central,"Questão: Uma empresa de pesquisa de mercado deseja estimar a satisfação média dos clientes de uma grande rede de varejo. A satisfação do cliente é medida em uma escala de 1 a 10, onde 1 significa completamente insatisfeito e 10 completamente satisfeito. Foi selecionada uma amostra aleatória de 150 clientes, e a média de satisfação encontrada foi de 7,5 com um desvio padrão de 1,2 na amostra. Considerando o Teorema do Limite Central e assumindo que a distribuição da satisfação dos clientes é inicialmente desconhecida, qual é a probabilidade aproximada de que a média de satisfação da população esteja entre 7,4 e 7,6?

A) Menos de 5%
B) Entre 5% e 10%
C) Entre 10% e 20%
D) Entre 20% e 30%
E) Mais de 30%

",E,"

A alternativa correta é E) Mais de 30%. O Teorema do Limite Central afirma que, para amostras grandes (n > 30), a distribuição das médias amostrais tende a ser aproximadamente normal, independentemente da forma da distribuição da população, desde que a amostra seja aleatória e menos de 10% da população total (quando a população é finita).

Aqui estão as explicações dos itens:

A) Este intervalo é muito estreito e, considerando uma distribuição normal, a probabilidade de a média da população estar dentro de um intervalo tão pequeno ao redor da média amostral é muito baixa.

B) Similarmente ao item A, a probabilidade é maior do que 5%, uma vez que estamos lidando com uma distribuição normal e um intervalo pequeno ao redor da média da amostra.

C) Este é um intervalo um pouco mais amplo, mas ainda subestima a probabilidade dada pelo Teorema do Limite Central e a regra empírica (68-95-99.7) para a distribuição normal.

D) Mesmo sendo um intervalo maior que os anteriores, ainda subestima a probabilidade real, pois a regra 68-95-99.7 nos diz que aproximadamente 95% dos dados estão dentro de dois desvios padrão da média, e o intervalo dado é muito menor que isso.

E) Esse é o intervalo mais provável, dado que o intervalo de 7,4 a 7,6 é relativamente pequeno e está muito próximo da média amostral. Usando a distribuição normal padrão, podemos calcular essa probabilidade, ou estimar usando a regra empírica, que sugeriria que a probabilidade é alta, já que o intervalo é estreito e centrado na média da amostra.",E,3004620
tópico 3,Programação orientada a objetos,"Questão: Na Programação Orientada a Objetos (POO), o princípio da encapsulamento refere-se à restrição do acesso direto aos componentes internos de um objeto. Este princípio é fundamental para garantir a integridade dos dados e a implementação de regras de negócio. Qual das seguintes opções melhor representa uma aplicação correta do encapsulamento em uma classe?

A) Tornar todos os atributos da classe públicos e permitir que sejam alterados diretamente de qualquer parte do código.
B) Utilizar métodos públicos para acessar e modificar os valores dos atributos privados, permitindo a validação dos novos valores antes de serem efetivamente atribuídos.
C) Criar atributos estáticos para que possam ser compartilhados entre várias instâncias da classe, facilitando o acesso direto a eles.
D) Manter todos os métodos e atributos da classe privados, tornando impossível a criação de instâncias e a utilização da classe.
E) Omitir a declaração de métodos getters e setters, uma vez que eles são considerados uma má prática e expõem desnecessariamente os detalhes internos da classe.

",B," 
A alternativa A está incorreta, pois vai contra o princípio do encapsulamento ao tornar todos os atributos públicos. A alternativa B é a correta, pois expressa a essência do encapsulamento, onde métodos públicos são utilizados para controlar o acesso aos atributos privados, permitindo validação e garantindo a integridade dos dados. A alternativa C está incorreta porque atributos estáticos têm propósitos diferentes e não estão relacionados diretamente com encapsulamento. A alternativa D é incorreta porque o encapsulamento não implica eliminar o acesso a uma classe ou instâncias dela, mas sim controlar esse acesso. Por fim, a alternativa E também está incorreta, pois getters e setters são uma prática comum para implementar encapsulamento e não expõem necessariamente detalhes internos da classe se adequadamente implementados.",B,5785221
tópico 3,"Manipulação e tabulação de dados (numpy, pandas, tidyr,verse, data.table)","Questão: Considere um conjunto de dados de vendas de uma empresa, com registros diários ao longo de um ano, em que cada entrada contém as informações de data, quantidade de itens vendidos e o valor total (em reais) de vendas. Para analisar a performance anual, um cientista de dados deseja transformar este conjunto de dados diários em um resumo mensal, indicando para cada mês a quantidade total de itens vendidos e a média diária de vendas em reais. Utilizando a biblioteca pandas do Python, qual seria o código apropriado para realizar esta operação, assumindo que o DataFrame que contém os dados é chamado de 'df_vendas' e possui as colunas 'data', 'qtde_itens' e 'valor_total'?

A) df_vendas['data'] = pd.to_datetime(df_vendas['data'])
   resumo_mensal = df_vendas.set_index('data').resample('M').agg({'qtde_itens': 'sum', 'valor_total': 'mean'})

B) df_vendas.groupby(df_vendas['data'].dt.to_period('M')).agg({'qtde_itens': 'sum', 'valor_total': 'mean'})

C) df_vendas.groupby(df_vendas['data'].str.slice(0, 7)).sum({'qtde_itens', 'valor_total'})

D) df_vendas.groupby([df_vendas['data'].dt.year, df_vendas['data'].dt.month]).aggregate({'qtde_itens': np.sum, 'valor_total': np.average})

E) df_vendas.groupby(pd.Grouper(key='data', freq='M')).aggregate({'qtde_itens': sum, 'valor_total': 'mean'})

",B,"

Explicação dos itens:

A) Incorreto. Este código realiza a conversão de 'data' para o formato DateTime e faz o resumo mensal, no entanto, usa um método depreciado 'set_index' seguido por 'resample', quando se podia usar 'groupby'.

B) Correto. Este código corretamente converte a coluna 'data' em um formato periódico mensal e usa groupby para calcular a soma da coluna 'qtde_itens' e a média da coluna 'valor_total', conforme pedido.

C) Incorreto. Este trecho faz o agrupamento pela coluna 'data' utilizando uma substring que representa apenas o ano e mês, o que poderia ser uma abordagem para obtenção do mês. No entanto, a função 'sum()' está sendo aplicada incorretamente, pois deve receber um dicionário para especificar as operações em colunas diferentes.

D) Incorreto. Ainda que esta opção faça uso da função 'aggregate', que permite aplicar funções específicas a cada coluna após o groupby, utiliza 'np.average' para 'valor_total' em vez de 'np.mean', e não converte a data para um formato periódico mensal.

E) Incorreto. O uso da função 'pd.Grouper' com a chave 'data' e frequência 'M' é adequado para a agregação mensal, mas a chamada ao método 'aggregate' usa 'sum' de maneira direta sem aspas e isso poderia causar um erro, uma vez que se espera receber um dicionário ou a operação em forma de string no contexto da agregação.",B,8302059
tópico 3,Linguagem de programação Scala,"Questão:
Uma característica notável do Scala é a sua interoperabilidade com Java, permitindo que os desenvolvedores utilizem bibliotecas Java em programas Scala e vice-versa. Além disso, Scala incorpora vários conceitos de programação funcional, o que a torna distinta de outras linguagens JVM. Considerando essas características, qual das seguintes opções não é uma funcionalidade ou conceito suportado nativamente por Scala?

A) Closures
B) Generics
C) Pointer arithmetic
D) Traits
E) Pattern Matching

",C,"

Explicação dos itens:
A) Closures: Scala suporta closures, que são funções que podem capturar variáveis externas dentro de seu escopo.
B) Generics: Scala possui um sistema de tipos parametrizados, conhecido como generics, similar ao Java, que permite a criação de classes, traits e métodos que podem operar em diferentes tipos.
C) Pointer arithmetic: Scala, como Java, não permite aritmética de ponteiros de forma nativa pois gerencia a memória de forma automática através do Garbage Collector, focando na segurança e prevenindo o acesso direto à memória.
D) Traits: Scala utiliza traits, que são similares às interfaces do Java, mas com a capacidade de conter implementações de métodos. Eles são usados para compartilhar interfaces e campos entre classes.
E) Pattern Matching: Pattern matching é um recurso poderoso em Scala que permite verificar um valor contra um padrão. É uma forma mais avançada de switch-case encontrada em outras linguagens como Java.",C,903505
tópico 5,"Técnicas de agrupamento: Agrupamento por partição, por densidade e hierárquico","Questão: No contexto da aprendizagem de máquina, diversas técnicas de agrupamento são utilizadas para identificar estruturas naturais ou padrões em conjuntos de dados multidimensionais. Sobre as técnicas de agrupamento por partição, por densidade e hierárquico, avalie as seguintes afirmações:

I. O agrupamento por partição, exemplificado pelo algoritmo k-means, busca dividir o conjunto de dados em um número pré-definido de grupos, minimizando a variância dentro de cada grupo e otimizando a posição dos centróides.

II. A técnica de agrupamento por densidade, como DBSCAN, identifica regiões de alta densidade separadas por regiões de baixa densidade, sendo eficaz mesmo quando os grupos têm formas irregulares ou vários ruídos e pontos de outliers.

III. O agrupamento hierárquico pode ser dividido em dois tipos principais: aglomerativo e divisivo, onde o primeiro inicia com cada ponto de dado em seu próprio cluster e os funde progressivamente, enquanto o segundo começa com um único cluster e realiza divisões sucessivas.

Está(ão) correta(s) apenas a(s) afirmativa(s):

A) I.
B) II.
C) III.
D) I e II.
E) I, II e III.

",E,"

Explicação dos itens:

A) A afirmação I está correta; o agrupamento por partição é clássico na aprendizagem de máquinas e o algoritmo k-means é um dos mais conhecidos dessa abordagem, procurando minimizar a variância intra-cluster e otimizar a localização dos centróides.

B) A afirmação II também está correta; o agrupamento por densidade, como o DBSCAN, é capaz de identificar clusters de formas arbitrárias e é particularmente útil para lidar com ruídos e outliers, identificando regiões de alta densidade que são separadas por regiões de baixa densidade.

C) A afirmação III é correta; o agrupamento hierárquico pode ser de fato aglomerativo, começando com cada ponto em seu próprio cluster e os fundindo progressivamente, ou divisivo, iniciando com todos os pontos em um único cluster e dividindo-o em clusters menores.

D) Essa opção está incorreta pois afirma que apenas as afirmativas I e II estão corretas, enquanto todas as três afirmações são verdadeiras.

E) A opção E é a correta, pois todas as afirmações I, II e III estão corretas e representam adequadamente as características das técnicas de agrupamento por partição, por densidade e hierárquico, respectivamente.",E,8358769
tópico 5,Técnicas de redução de dimensionalidade: Seleção de características (feature selection); Análise de componentes principais (PCA – principal component analysis),"Questão: Em problemas de aprendizado de máquina, muitas vezes nos deparamos com um grande número de variáveis que podem ser irrelevantes ou redundantes, resultando em complexidade desnecessária e podendo afetar o desempenho de um modelo. Para lidar com esse problema, técnicas de redução de dimensionalidade são aplicadas. Sobre essas técnicas, analise as afirmativas abaixo:

I. A seleção de características (feature selection) busca selecionar um subconjunto de características originais que sejam mais significativas para o modelo, ignorando as demais, sem alterar os valores dos atributos selecionados.

II. A análise de componentes principais (PCA) é uma técnica que transforma as características originais em um novo conjunto de variáveis correlacionadas, chamadas componentes principais, de forma a manter a maior parte da variância presente nos dados.

III. Uma das vantagens do PCA sobre a seleção de características é que ele pode eliminar a multicolinearidade entre as variáveis de entrada, criando componentes que são estatisticamente independentes entre si.

IV. O PCA deve ser aplicado em cenários onde a interpretabilidade do modelo é de extrema importância, uma vez que os componentes principais geralmente têm uma interpretação direta em relação às variáveis originais.

Está(ão) correta(s) a(s) afirmativa(s):

A) I e II apenas.
B) I, II e III apenas.
C) II e III apenas.
D) I, III e IV apenas.
E) Todas as afirmativas estão corretas.

",B,"

Explicação dos itens:

I. Afirmativa correta. A seleção de características é o processo de identificar e selecionar um subconjunto de características relevantes para usar na construção de um modelo.

II. Afirmativa incorreta. A análise de componentes principais (PCA) busca transformar o conjunto original de variáveis em um novo conjunto de variáveis não correlacionadas chamadas componentes principais, e não variáveis correlacionadas como mencionado.

III. Afirmativa correta. Uma das principais vantagens do PCA é a eliminação da multicolinearidade, ao criar componentes que são independentes uns dos outros.

IV. Afirmativa incorreta. Enquanto o PCA é valioso para a redução de dimensionalidade, o novo conjunto de variáveis (componentes principais) muitas vezes não tem uma interpretação direta, o que pode dificultar a interpretabilidade do modelo. Eles são combinações lineares das variáveis originais e geralmente não têm significado físico ou intuitivo por si só.

Portanto, as afirmativas I, II e III estão corretas e a melhor escolha é a letra B.",B,9319981
tópico 4,Teorema do limite central,"Questão:

Considere uma população infinita com média μ e desvio padrão σ. Uma amostra aleatória de tamanho n é selecionada desta população para estimação da média populacional. Com base no Teorema do Limite Central, qual das seguintes afirmações é correta para n grande?

A) A distribuição da média amostral tende a uma distribuição uniforme, independente da distribuição original da população.
B) O Teorema do Limite Central é aplicável apenas se a distribuição da população for normal.
C) A distribuição da média amostral será aproximadamente normal com média μ e variância σ^2/n.
D) A aproximação normal da distribuição da média amostral melhora à medida que o tamanho da amostra diminui.
E) O desvio padrão da média amostral é igual ao desvio padrão da população.

",C,"

Explicação dos itens:

A) Incorreto. O Teorema do Limite Central afirma que a distribuição da média amostral tende a ser normal, e não uniforme, independentemente da distribuição da população, contanto que o tamanho da amostra seja grande o suficiente.

B) Incorreto. O Teorema do Limite Central é aplicável independentemente da forma da distribuição da população. A normalidade da distribuição da média amostral se manifesta com o aumento do tamanho da amostra.

C) Correto. O Teorema do Limite Central estabelece que para um tamanho de amostra suficientemente grande, a distribuição da média amostral será aproximadamente normal, centrada em μ, com variância σ^2/n, ou equivalentemente, com desvio padrão σ/√n.

D) Incorreto. Contrariamente a esta afirmação, a aproximação normal da distribuição da média amostral melhora à medida que o tamanho da amostra aumenta.

E) Incorreto. O desvio padrão da média amostral, conhecido como erro padrão, é igual ao desvio padrão da população (σ) dividido pela raiz quadrada do tamanho da amostra (n), o que resulta em σ/√n, e não no próprio desvio padrão da população.",C,4350291
tópico 5,Técnicas de redução de dimensionalidade: Seleção de características (feature selection); Análise de componentes principais (PCA – principal component analysis),"Questão: Na ciência de dados e aprendizado de máquina, técnicas de redução de dimensionalidade são essenciais para o processamento e a análise eficiente dos dados de alta dimensão. Dentre estas técnicas, a Seleção de Características e a Análise de Componentes Principais (PCA) são duas abordagens comumente usadas. Com relação a estas técnicas, avalie as afirmações abaixo e marque a opção correta.

I. A Seleção de Características envolve escolher um subconjunto de características originais sem transformá-las, visando a manutenção da interpretabilidade dos dados originais.
II. A Análise de Componentes Principais (PCA) é uma técnica que transforma as características originais em novas variáveis ortogonais chamadas componentes principais, em que as primeiras componentes concentram a maior parte da variância dos dados.
III. Em situações em que a interpretabilidade das variáveis não é relevante, a Seleção de Características é geralmente preferida ao PCA, já que a primeira fornece um conjunto de dados de dimensão reduzida mais fácil de entender.
IV. A PCA pode ser considerada uma técnica de seleção de características quando os componentes principais são selecionados com base em seu poder explicativo da variância dos dados.

A) Apenas as afirmações I e II estão corretas.
B) Apenas as afirmações I e III estão corretas.
C) Apenas as afirmações II e IV estão corretas.
D) Apenas as afirmações I, II e IV estão corretas.
E) Todas as afirmações estão corretas.

",D,"

As afirmações I e II estão corretas. A Seleção de Características realmente procura um subconjunto de características originais, que pode ser muito importante para manter a facilidade de interpretação dos dados. A PCA, por outro lado, envolve uma transformação dos dados para um novo espaço de características, em que as componentes principais são ortogonais e capturam a maior parte da variância presente no conjunto de dados original.

A afirmação III está incorreta porque, na prática, a Seleção de Características é usada para manter a interpretabilidade das variáveis, enquanto o PCA é utilizado quando essa interpretabilidade pode ser sacrificada em favor de uma representação mais compacta dos dados.

A afirmação IV está errada ao considerar a PCA como uma técnica de seleção de características; apesar de reduzir a dimensionalidade, ela cria novas características (componentes principais) que são combinações lineares das originais, ao invés de simplesmente selecionar um subconjunto das características existentes. Portanto, PCA é uma técnica de extração de características, e não de seleção.",D,2712438
tópico 5,"Processamento de linguagem natural: Normalização textual - stop words, estemização, lematização e análise de frequência de termos; Rotulação de partes do discurso, part-of-speech tagging; Modelos de representação de texto - N-gramas, modelos vetoriais de palavras (CBOW, Skip-Gram e GloVe), modelos vetoriais de documentos (booleano, TF e TF-IDF, média de vetores de palavras e Paragraph Vector); Métricas de similaridade textual - similaridade do cosseno, distância euclidiana, similaridade de Jaccard, distância de Manhattan e coeficiente de Dice","Questão: No campo de Processamento de Linguagem Natural (PLN), diversas técnicas são aplicadas para a preparação e análise de textos. Dentre os processos de normalização textual está a lematização, que consiste em:

A) Converter todas as palavras em letras minúsculas para evitar a distinção entre maiúsculas e minúsculas.
B) Eliminar palavras comuns que não contribuem para o significado do texto, como artigos, preposições e conjunções.
C) Reduzir as palavras flexionadas ou derivadas ao seu radical ou forma base, muitas vezes resultando em uma forma que não corresponde à palavra válida do idioma.
D) Reduzir palavras à sua forma dicionarizada, considerando a análise morfológica para encontrar o lema correto.
E) Selecionar apenas as palavras que ocorrem com mais frequência no texto para representar o seu conteúdo principal.

",D,"

Explicação dos itens:

A) Incorreto. Converter todas as palavras em letras minúsculas é uma técnica conhecida como ""case folding"", que é uma prática usual na normalização de textos, mas não é o que define a lematização.

B) Incorreto. Eliminar palavras comuns, conhecidas como ""stop words"", é uma etapa importante na normalização textual, mas não corresponde ao processo de lematização.

C) Incorreto. A redução de palavras ao seu radical é conhecida como ""estemização"" ou ""stemming"". Este processo frequentemente resulta em uma forma que não é exatamente um lema e pode não ser uma palavra válida do idioma, o que não ocorre na lematização.

D) Correto. A lematização é o processo de reduzir uma palavra à sua forma dicionarizada, o lema, considerando a análise morfológica para entender o contexto e definir a forma base correta, que é uma palavra válida do idioma e que representa a unidade semântica base.

E) Incorreto. A seleção de palavras com base na frequência é uma prática comum na representação vetorial de textos e na análise de frequência de termos, no entanto, não está relacionada diretamente à lematização.",D,2215491
tópico 3,Linguagem de programação Scala,"Questão:

Na linguagem de programação Scala, que é reconhecida pela sua capacidade de combinar o paradigma orientado a objetos com o funcional, temos um recurso poderoso conhecido como case classes. Estas são utilizadas frequentemente para modelar dados imutáveis e implementar padrões de projetos como o de ""match"". Considere o seguinte código Scala:

```scala
abstract class Notification

case class Email(sender: String, title: String, body: String) extends Notification

case class SMS(caller: String, message: String) extends Notification

case class VoiceRecording(contactName: String, link: String) extends Notification

def showNotification(notification: Notification): String = {
  notification match {
    case Email(sender, _, _) =>
      s""You got an email from $sender""
    case SMS(number, message) =>
      s""You got an SMS from $number! Message: $message""
    case VoiceRecording(name, link) =>
      s""You received a Voice Recording from $name! Click here to hear it: $link""
  }
}

val someSms = SMS(""12345"", ""Are you there?"")
val someVoiceRecording = VoiceRecording(""Tom"", ""voicerecording.org/id/123"")

```

Qual será a saída do código ao executar as seguintes instruções?

```scala
println(showNotification(someSms))
println(showNotification(someVoiceRecording))
```

A) ""You got an SMS from 12345! Message: Are you there?"" seguido por ""You received a Voice Recording from Tom! Click here to hear it: voicerecording.org/id/123""

B) ""You got an email from 12345"" seguido por ""You got an SMS from Tom!""

C) ""You got a Voice Recording from 12345! Click here to hear it: Are you there?"" seguido por ""You got an email from Tom!""

D) ""You got an SMS from 12345"" seguido por ""You got a Voice Recording from Tom! Click here to hear it: voicerecording.org/id/123""

E) ""You got an email from 12345! Are you there?"" seguido por ""You received a Voice Recording from Tom""

",A,"

Explicação dos itens:
A) Correto. De acordo com o pattern matching implementado na função showNotification, cada tipo de notificação tem uma mensagem associada. SMS e VoiceRecording serão matcheados com seus respectivos cases, resultando nas saídas esperadas.

B) Incorreto. A função showNotification especifica mensagens diferentes para cada tipo de notificação. O case para Email não é acionado, pois não há uma instância de Email sendo passada para a função.

C) Incorreto. Os tipos de notificações e as saídas estão incorretamente associados. Essa opção mistura os tipos de notificações com saídas que não correspondem ao pattern matching definido.

D) Incorreto. A parte da mensagem para SMS está correta, mas a saída para VoiceRecording falta a parte da string que inclui o link para a gravação.

E) Incorreto. Novamente, a saída para o Email está incorreta pois não há uma instância de Email sendo avaliada. Além disso, a saída para VoiceRecording está incompleta, faltando o link da gravação.",A,4465634
tópico 2,Desidentificação de dados sensíveis,"Questão: Em conformidade com as práticas recomendadas pela Lei Geral de Proteção de Dados Pessoais (LGPD) no Brasil e regulamentos internacionais similares, a desidentificação de dados sensíveis é um processo essencial para proteger a privacidade dos indivíduos. Considerando os métodos de desidentificação usualmente empregados, assinale a opção que NÃO corresponde a um mecanismo eficaz de desidentificação de dados sensíveis:

A) Pseudonimização, que substitui os identificadores diretos por um código ou pseudônimo, sem remover ou alterar outros dados que podem permitir a reidentificação indireta.

B) Anonimização, processo pelo qual as informações de identificação pessoal são removidas de modo irreversível, impedindo a associação dos dados com o indivíduo.

C) Encriptação de dados, usando algoritmos de criptografia fortes para garantir que os dados sensíveis estejam acessíveis apenas com a chave de descriptografia correspondente.

D) Redução de detalhes, diminuindo a granularidade de dados que incluem informações geográficas ou demográficas para evitar a identificação precisa de indivíduos.

E) Aumento da transparência, fornecendo aos indivíduos informações detalhadas sobre quais dados pessoais são coletados e como são processados.

",E,"

Explicação dos itens:

A) Incorreto. A pseudonimização é uma técnica válida de desidentificação, que, embora não elimine o risco de reidentificação, reduz significativamente esse risco, principalmente quando combinada com outras técnicas de segurança.

B) Incorreto. A anonimização é uma das técnicas mais robustas de desidentificação, pois visa a remover ou modificar informações pessoais de tal forma que a pessoa não possa ser identificada.

C) Incorreto. A encriptação é uma técnica comum de proteção de dados que, embora não altere o conteúdo do dado, protege contra o acesso não autorizado, e é considerada uma forma de desidentificação temporária, funcionando enquanto a chave de criptografia for mantida em segurança.

D) Incorreto. A redução de detalhes, também conhecida como generalização, é uma técnica de desidentificação que reduz o risco de reidentificação ao diminuir a precisão das informações, como usar apenas a idade aproximada ou a região geral em vez de endereços precisos.

E) Correto. Aumentar a transparência não é um método de desidentificação. Na verdade, trata-se de uma prática recomendada para informar aos usuários sobre a utilização de seus dados, que pode incluir dados sensíveis, mas não altera ou oculta a identificação desses dados.",E,173678
tópico 0,Ingestão de dados em lote (batch),"Questão: A ingestão de dados em lote, ou batch data ingestion, é um processo crítico para sistemas que dependem da análise de grandes quantidades de dados para tomada de decisão. Qual das seguintes afirmativas melhor descreve o processo de ingestão de dados em lote?

A) A ingestão de dados em lote ocorre em tempo real, garantindo a menor latência possível na atualização dos dados.
B) Os dados são processados individualmente, à medida que chegam, para garantir análises mais detalhadas.
C) Esse processo envolve a coleta de dados de várias fontes, que são então imediatamente purificados e transformados antes da ingestão.
D) Na ingestão de dados em lote, grandes volumes de dados são coletados em intervalos regulares e processados de uma só vez.
E) A ingestão de dados em lote privilegia análises preditivas e recomendações em tempo real para alavancar a performance do negócio.

",D,"  
A alternativa A está incorreta porque a ingestão de dados em lote não é realizada em tempo real, mas sim em intervalos regulares. A alternativa B também é incorreta, pois descreve um processo que é mais característico da ingestão de dados em streaming, onde os dados são processados à medida que chegam. A alternativa C é imprecisa, pois a purificação e transformação dos dados podem ocorrer durante ou após o processo de coleta, e não necessariamente de forma imediata. A alternativa D está correta, pois descreve acuradamente o processamento de grandes volumes de dados coletados em intervalos regulares, em oposição ao processamento de dados em tempo real ou streaming. Por fim, a alternativa E é errada, pois apesar da ingestão de dados em lote poder auxiliar em análises preditivas, ela não é usada para recomendações em tempo real; essa seria uma característica da ingestão de dados em streaming.",D,1351910
tópico 2,Normalização numérica,"Questão: Em matemática e em várias aplicações de ciência de dados, o processo de normalização é utilizado para ajustar a escala dos valores de uma função ou conjunto de dados, para que eles estejam dentro de um intervalo específico ou para que tenham certas propriedades estatísticas, como média zero e variância um. Considerando um conjunto de dados numéricos, qual das seguintes técnicas de normalização é INCORRETA?

A) Min-Max Scaling: Consiste em transformar os dados de modo que estes fiquem dentro do intervalo [0, 1], subtraindo o valor mínimo do conjunto e dividindo pela diferença entre o valor máximo e o mínimo.

B) Z-Score Normalization: Transforma os dados de modo que a nova média seja zero e a nova variância seja um. Isso é feito subtraindo a média e dividindo pelo desvio padrão dos dados originais.

C) Decimal Scaling: Normaliza os dados dividindo cada valor pelo decimal mais significativo, de forma que os novos valores estejam no intervalo [-1, 1].

D) Logarítmica: Aplica o logaritmo natural a todos os valores do conjunto de dados, normalizando-os em um intervalo que depende da distribuição dos dados após a transformação logarítmica.

E) Nominal Scaling: Consiste em atribuir um número único para cada categoria distinta de dados, normalizando-os para uma representação numérica que facilita comparações e análises estatísticas.

",C," 
A técnica de Min-Max Scaling (A) está corretamente descrita como um método que reescala os dados para o intervalo [0, 1]. A técnica de Z-Score Normalization (B) também está correta, sendo um processo de padronização que resulta em uma distribuição com média zero e desvio padrão um. A Logarítmica (D) é uma técnica válida de normalização que utiliza a função logarítmica para redistribuir os dados, frequentemente usada quando os dados têm uma distribuição log-normal. Nominal Scaling (E), apesar do nome, não é uma técnica de normalização numérica; é, na verdade, uma forma de codificação para variáveis categóricas. No entanto, a descrição da questão a torna uma afirmação plausível, não sendo a escolha incorreta. O Decimal Scaling (C) está incorretamente descrito porque esta técnica de normalização envolve dividir os dados pela potência de 10 necessária para que os valores fiquem no intervalo [-1, 1], e não simplesmente dividindo pelo decimal mais significativo. A divisão poderia resultar em um intervalo diferente, dependendo do conjunto de dados.",E,4332918
tópico 2,Enriquecimento,"Questão: No contexto de produção de urânio enriquecido, um dos aspectos mais críticos é o processo de separação dos isótopos. O urânio natural é composto majoritariamente por urânio-238, enquanto que para a maior parte das aplicações em reatores nucleares e armas, é necessário enriquecer a concentração de urânio-235. Qual dos seguintes métodos é amplamente utilizado para o enriquecimento de urânio?

A) Fusão nuclear
B) Cromatografia gasosa
C) Centrifugação a gás
D) Destilação fracionada
E) Difração de elétrons

Alternativa Correta: C

Explicação dos itens:
A) Fusão nuclear: Incorreto. A fusão nuclear é o processo pelo qual núcleos atômicos leves são combinados para formar elementos mais pesados, liberando energia. Não é um método utilizado para separar isótopos de urânio.

B) Cromatografia gasosa: Incorreto. Embora a cromatografia seja uma técnica de separação de misturas, a cromatografia gasosa não é aplicada para o enriquecimento de isótopos de urânio, pois é uma técnica que separa componentes de uma mistura com base em suas diferentes volatilidades e interações com a fase estacionária.

C) Centrifugação a gás: Correto. O método de centrifugação a gás utiliza diferenças nas massas dos isótopos de urânio para enriquecer o urânio-235. Neste processo, o gás hexafluoreto de urânio (UF6) é utilizado e as moléculas com urânio-235 tendem a se concentrar mais no centro da centrífuga devido a sua menor massa.

D) Destilação fracionada: Incorreto. A destilação fracionada é um método de separação de componentes de uma mistura líquida com base em seus diferentes pontos de ebulição. Não é eficaz para a separação de isótopos de um elemento.

E) Difração de elétrons: Incorreto. A difração de elétrons é uma técnica utilizada para estudar a estrutura atômica e molecular dos materiais, e não é usada para o processo de enriquecimento de urânio.

",C,"
A centrifugação a gás é o processo amplamente utilizado para enriquecer urânio, pois permite a separação dos isótopos baseando-se em suas diferentas massas. Os isótopos mais pesados de urânio-238 tendem a se deslocar para a periferia da centrífuga, enquanto o urânio-235, mais leve, concentra-se mais ao centro, o que facilita sua extração para fins de enriquecimento.",,8785949
tópico 3,Programação funcional,"Questão:
A programação funcional é um paradigma de programação que trata a computação como a avaliação de funções matemáticas e evita estados ou dados mutáveis. Dentro deste paradigma, algumas funções possuem características específicas que as diferenciam. Uma dessas características é ser uma ""função pura"". Qual das seguintes afirmações define corretamente uma ""função pura"" em programação funcional?

A) Uma função pura é aquela que, para um mesmo conjunto de entradas, sempre realizará os mesmos efeitos colaterais e não necessariamente retornará o mesmo resultado.

B) Uma função pura é aquela que, para um mesmo conjunto de entradas, pode retornar valores diferentes, dependendo de variáveis globais ou estáticas.

C) Uma função pura é aquela que não possui efeitos colaterais e, para um mesmo conjunto de entradas, sempre retornará o mesmo resultado.

D) Uma função pura é definida como pura simplesmente por não realizar operações de entrada e saída.

E) Uma função pura coordena várias funções com efeitos colaterais para processar dados, garantindo que os efeitos ocorram na sequência esperada.

",C,"

Explicação dos itens:

A) Incorreto. Uma função pura é caracterizada por não realizar efeitos colaterais e por ter retorno determinístico (o mesmo resultado é obtido para as mesmas entradas).

B) Incorreto. A definição de função pura exclui a dependência de variáveis globais ou estáticas que possam alterar a saída da função, garantindo que a saída seja sempre a mesma para as mesmas entradas.

C) Correto. Esta é a definição correta de uma função pura. Ela não deve ter efeitos colaterais e deve ser determinística, retornando o mesmo resultado sempre que for chamada com as mesmas entradas.

D) Incorreto. A ausência de operações de E/S (entrada e saída) não é uma condição suficiente para definir uma função como pura. A função também deve ser determinística e livre de efeitos colaterais.

E) Incorreto. A coordenação de várias funções com efeitos colaterais não define uma função pura. Funções puras devem ser independentes de efeitos colaterais, não apenas garantir que aconteçam em uma sequência específica.",C,1687893
tópico 3,"Manipulação e tabulação de dados (numpy, pandas, tidyr,verse, data.table)","Questão:

Considerando as bibliotecas de manipulação de dados em Python, especificamente pandas e numpy, uma tarefa comum é a filtragem e seleção de subconjuntos de dados de um DataFrame para análise posterior. Para esta finalidade, o pandas oferece métodos poderosos e intuitivos, enquanto o numpy oferece operações baseadas em array. 
Suponha que um analista de dados esteja trabalhando com um conjunto de dados de vendas armazenado em um DataFrame pandas chamado `df_vendas`, que inclui colunas 'ID', 'Data', 'Vendedor', 'Produto', e 'Valor'. O analista precisa extrair todas as vendas realizadas pelo vendedor chamado ""Carlos"" que foram superiores a R$500. Qual dos seguintes comandos alcançaria este objetivo de forma correta e eficiente?

A) df_vendas[(df_vendas[""Vendedor""] == ""Carlos"") & (df_vendas[""Valor""] > 500)]

B) df_vendas.query(""Vendedor == 'Carlos' & Valor > 500"")

C) df_vendas.loc[df_vendas[""Valor""] > 500, ""Carlos""]

D) df_vendas[df_vendas.apply(lambda row: row[""Vendedor""] == ""Carlos"" and row[""Valor""] > 500, axis=1)]

E) np.where((df_vendas[""Vendedor""].values == ""Carlos"") & (df_vendas[""Valor""].values > 500))

",B,"

Explicação dos itens:

A) Correto. Esta é a maneira comum de usar o pandas para filtrar linhas com múltiplas condições, empregando operadores lógicos e indexação booleana.

B) Correto. Este comando utiliza o método `query` do pandas que permite escrever a condição de filtragem de forma mais concisa e semelhante a uma consulta SQL. Ele é eficiente e o mais legível entre as opções, também sendo a resposta correta.

C) Incorreto. O método `loc` é utilizado para selecionar linhas e colunas por rótulo. A sintaxe utilizada está incorreta, pois primeiro deve-se passar a condição das linhas e, se necessário, as colunas desejadas. Além disso, ""Carlos"" é tratado como o nome de uma coluna, o que não é o caso.

D) Incorreto. A função `apply` com uma função lambda pode ser utilizada para filtrar linhas, mas é menos eficiente que os métodos diretos do pandas para lidar com condições booleanas. Este comando irá funcionar, mas não é o mais eficiente ou idiomático para tal tarefa.

E) Incorreto. A função `np.where` do numpy pode ser utilizada para indexação com base em uma condição, mas ela é normalmente usada para substituir valores em arrays, não para filtrar DataFrames. Além disso, o resultado não retorna um DataFrame, mas um array de índices ou um novo array com elementos modificados.",B,9186925
tópico 3,Programação funcional,"Questão:

Considere o paradigma de programação funcional e suas características principais. Avalie as afirmações a seguir e assinale a opção que apresenta uma característica que NÃO está alinhada com os princípios da programação funcional.

A) Funções de alta ordem permitem que funções sejam tratadas como variáveis e sejam passadas como argumentos para outras funções ou retornadas como valor.

B) A programação funcional favorece o uso de estruturas de dados imutáveis, evitando o compartilhamento de estados entre diferentes funções.

C) Efeitos colaterais são comuns e incentivados na programação funcional, uma vez que permitem uma maior adaptabilidade das funções ao interagir com o mundo exterior.

D) Funções puras são uma peça central na programação funcional, onde a mesma entrada garante sempre a mesma saída, sem efeitos colaterais.

E) Recursão é uma técnica frequentemente utilizada em lugar de laços tradicionais (loops), porque permite expressar o cálculo de repetições de forma pura e sem estado mutável.

",C,"

Explicação dos itens:

A) Correta. Funções de alta ordem são um conceito essencial na programação funcional, pois oferecem flexibilidade e reutilização de código.

B) Correta. Evitar estados mutáveis e utilizar estruturas de dados imutáveis são princípios fundamentais para garantir a previsibilidade e a facilidade de raciocínio em torno das funções.

C) Incorreta. Efeitos colaterais são minimizados na programação funcional para garantir funções puras. Isso aumenta a previsibilidade e facilita o teste e a manutenção do código.

D) Correta. Funções puras garantem que os resultados sejam inteiramente previsíveis, o que é fortemente alinhado com a filosofia da programação funcional.

E) Correta. A utilização de recursão em vez de laços é incentivada em programação funcional porque suporta imutabilidade e funcionalidades sem estado.",C,6744711
tópico 1,Banco de dados NoSQL,"Questão: No contexto de sistemas de banco de dados NoSQL, diferentes tipos de dados e modelos de consistência influenciam diretamente a maneira como os dados são armazenados e recuperados. Considerando os principais tipos de bancos NoSQL e suas características, assinale a opção que apresenta corretamente o tipo de banco de dados NoSQL e seu respectivo modelo de dados:

A) Banco de dados em colunas - Utiliza o modelo BASE (Basically Available, Soft state, Eventual consistency) e é otimizado para leituras e escritas rápidas em conjuntos de dados muito grandes distribuídos.

B) Banco de dados de documentos - Armazena os dados em estruturas de árvore binária, em que cada nó representa um documento, facilitando a indexação e recuperação de dados estruturalmente complexos.

C) Banco de dados de grafos - Utiliza o modelo ACID (Atomicity, Consistency, Isolation, Durability) e é projetado para armazenar, mapear e consultar relacionamentos entre dados em uma rede.

D) Banco de dados chave-valor - Segue o modelo relacional e é indicado para situações que demandam esquemas de dados rigorosamente definidos e complexas operações de junção.

E) Banco de dados orientado a objetos - Armazena informações em entidades chamadas de ""blobs"", onde cada blob é uma instância de uma classe no banco de dados.

",C,"

Explicação dos itens:

A) Errado. Bancos de dados em colunas efetivamente utilizam o modelo BASE e são otimizados para operações rápidas em grandes conjuntos de dados distribuídos. No entanto, a opção não informa corretamente sobre o armazenamento em forma de colunas.

B) Errado. Bancos de dados de documentos armazenam dados em formato de documentos, como JSON ou XML, e não em estruturas de árvore binária. Esta opção confunde estruturas de armazenamento com formatos de documentos.

C) Correto. Bancos de dados de grafos realmente utilizam o modelo ACID, pois mantêm a consistência dos dados mesmo com transações complexas que envolvem muitas relações. Eles são projetados para lidar com dados em uma rede, como sistemas de recomendação, redes sociais, etc.

D) Errado. Bancos de dados chave-valor armazenam dados como um conjunto de pares chave-valor e não seguem o modelo relacional. Eles são conhecidos por sua simplicidade e alta performance, especialmente em aplicações que não necessitam de operações de junção complexas.

E) Errado. Bancos de dados orientados a objetos armazenam dados como objetos, conforme as definições da programação orientada a objetos (POO), e não em entidades chamadas ""blobs"". Cada objeto no banco de dados é uma instância de uma classe, mas a descrição da opção não é precisa.",B,2192129
tópico 2,Deduplicação,"Questão:
A tecnologia de deduplicação é amplamente utilizada no armazenamento de dados para otimizar o espaço, reduzindo a quantidade de dados redundantes armazenados. Qual dos seguintes afirma corretamente uma característica ou consequência do processo de deduplicação em sistemas de armazenamento de dados?

A) A deduplicação não afeta o desempenho do sistema de backup, pois apenas ocorre durante períodos de baixa atividade.

B) A deduplicação pode ser feita tanto no nível do arquivo (arquivo completo) quanto no nível do bloco (pequenas partes do arquivo), sendo a última geralmente mais eficiente na redução da redundância.

C) A deduplicação é um processo puramente de software e não pode ser implementada no hardware ou em dispositivos de armazenamento dedicados.

D) A deduplicação conduz necessariamente a uma melhoria na segurança dos dados, pois há menos cópias dos arquivos para proteger.

E) O processo de deduplicação aumenta significativamente a quantidade de metadados necessários para gerenciar o armazenamento de dados, uma vez que cada arquivo é armazenado como uma entidade única e indivisível.

",B,B,B,0
tópico 2,Data cleansing,"Questão: Considere um grande conjunto de dados extraídos de diferentes fontes para ser utilizado em um projeto de análise de dados. Durante o processo de Data Cleansing, diversas etapas são fundamentais para assegurar a qualidade dos dados que serão utilizados nas análises subsequentes. Qual das seguintes opções NÃO é uma prática comum realizada durante o Data Cleansing?

A) Remoção de duplicatas para evitar a redundância de informações.
B) Preenchimento de valores faltantes usando algoritmos de imputação.
C) Normalização dos dados para ajustar os valores a uma escala comum.
D) Correlacionar os dados com fontes externas para garantir sua relevância.
E) Introdução de ruídos aleatórios para testar a robustez dos modelos analíticos.

",D,"

Explicação dos itens:

A) Remoção das duplicatas é essencial para manter a integridade dos dados e evitar distorções nas análises, o que faz desta etapa um procedimento padrão no Data Cleansing.
B) O preenchimento de valores faltantes usando métodos de imputação é uma prática comum para lidar com dados incompletos, o que é especialmente importante para a continuidade das análises sem a perda de informações relevantes.
C) A normalização dos dados é uma técnica utilizada para garantir a consistência nas medidas e permitir a comparação entre variáveis com escalas diferentes, sendo uma etapa fundamental do Data Cleansing.
D) Correlacionar dados com fontes externas não é uma prática de Data Cleansing, mas sim de validação e enriquecimento de dados, que pode ocorrer em etapas subsequentes do processo de análise de dados e não durante a limpeza.
E) A introdução deliberada de ruídos nos dados não faz parte do processo de Data Cleansing, cujo objetivo é aprimorar a qualidade dos dados. Adicionar ruídos poderia ser usado em testes de sensibilidade de modelos preditivos após a fase de limpeza e preparação dos dados.",E,9294439
tópico 3,"Manipulação e tabulação de dados (numpy, pandas, tidyr,verse, data.table)","Questão: Em uma análise de dados utilizando a biblioteca Pandas do Python, um cientista de dados está trabalhando com o DataFrame 'df', que contém dados sobre vendas de produtos em uma loja. Para otimizar a estratégia de marketing, ele precisa identificar os cinco produtos mais vendidos e a quantidade total de vendas de cada um desses produtos. O DataFrame 'df' possui duas colunas de interesse: 'Produto' e 'Quantidade'. Qual das seguintes linhas de código do Pandas irá corretamente realizar esta tarefa?

A) df.groupby('Produto').sum().nlargest(5, 'Quantidade')
B) df['Produto'].value_counts().head(5)
C) df.groupby('Produto')['Quantidade'].sum().sort_values(ascending=False).head(5)
D) df.groupby('Produto').count().nlargest(5, 'Quantidade')
E) df.sort_values(by='Quantidade', ascending=False).drop_duplicates('Produto').head(5)

",C,"

A) A linha de código usa groupby('Produto') seguido por sum(), que irá somar todas as quantidades por produto, mas falta o método ncorrect para retornar os cinco mais vendidos, o nlargest(5, 'Quantidade') não se aplica diretamente após um groupby-aggregation como sum().

B) O value_counts() é aplicado para contar a frequência de valores únicos em uma Series e não em uma DataFrame. Além disso, este método contaria a frequência dos produtos, não somando as Quantidades vendidas.

C) Esta opção realiza corretamente o agrupamento por produto, soma as quantidades vendidas para cada produto, ordena os valores de forma descendente por quantidade e, por fim, seleciona os cinco produtos mais vendidos. É a opção correta.

D) O uso de count() após groupby('Produto') irá contar as ocorrências de cada produto, ao invés de somar as Quantidades. A função nlargest() é aplicada incorretamente da mesma maneira que na opção A.

E) A linha de código realiza a ordenação dos dados pela coluna 'Quantidade' de forma descendente, porém o método drop_duplicates('Produto'), embora mantenha a primeira ocorrência de cada produto, não garante que as Quantidades estejam somadas. Este código selecionaria apenas a quantidade da venda única mais alta para cada produto, em vez de somar todas as vendas dos produtos.",C,9575844
tópico 3,"Visualização de dados ggplot, matplotlib","Questão:
A visualização de dados é um processo essencial na análise de dados, servindo para explorar conjuntos de dados e apresentar resultados de forma intuitiva e compreensível. Dois dos principais pacotes usados para visualização de dados em linguagens de programação populares para análise de dados são ggplot2 e matplotlib. O ggplot2 é utilizado com a linguagem R, enquanto o matplotlib é uma biblioteca do Python.

Considere que você está utilizando o ggplot2 para criar um gráfico de dispersão que mostra a relação entre duas variáveis, `var_x` e `var_y`, de um dataframe denominado `data`. Além disso, você deseja colorir os pontos de acordo com uma terceira variável categórica `category` e adicionar títulos aos eixos x e y, respectivamente. Qual dos seguintes códigos em `R` usa corretamente o ggplot2 para criar o gráfico de dispersão desejado?

A) `ggplot(data, aes(x = var_x, y = var_y, color = category)) + geom_point() + xlab('Eixo X') + ylab('Eixo Y')`
B) `matplotlib.scatter(data['var_x'], data['var_y'], c=data['category']); plt.xlabel('Eixo X'); plt.ylabel('Eixo Y')`
C) `ggplot(data) + geom_point(mapping = aes(x = var_x, y = var_y), color = category) + labs(x = 'Eixo X', y = 'Eixo Y')`
D) `ggplot(data, aes(x = var_x, y = var_y)) + geom_point(aes(color = category)) + ggtitle('Eixo X', 'Eixo Y')`
E) `plot(data$var_x, data$var_y, col = data$category); title(xlab = 'Eixo X', ylab = 'Eixo Y')`

",A,"

Explicações dos itens:

A) Este item está correto. Utiliza a função `ggplot()` com a estética `aes()` correta para mapear as variáveis `var_x` e `var_y`, além de `color` para categorizar os pontos por `category`. As funções `xlab()` e `ylab()` são usadas para rotular os eixos. É uma sintaxe típica de `ggplot2`.

B) Incorreto, pois o código apresentado está utilizando a sintaxe da biblioteca `matplotlib` do Python e não de `ggplot2` do R.

C) Este item contém parte da sintaxe correta do ggplot2, mas ao tentar colorir os pontos, o `color` é posicionado de forma incorreta. `color` deve estar dentro da função `aes()` no `geom_point()` e a função `labs()` tem a aplicação correta, mas não foi escolhida a opção correta para o caso em questão.

D) Este item utiliza `ggtitle` de forma incorreta, visto que `ggtitle` é usado para adicionar um título ao gráfico, não para rotular os eixos. A função `aes(color = category)` dentro do `geom_point()` está correta para adicionar cor.

E) Este item está utilizando a função base de plotagem do R (`plot()`), ao invés de utilizar as funções do pacote `ggplot2`. A função `title()` é usada na plotagem base do R para adicionar rótulos e títulos, mas não é o que foi pedido para o ggplot2.",C,9660412
tópico 2,Desidentificação de dados sensíveis,"Questão:
No contexto da Lei Geral de Proteção de Dados Pessoais (LGPD), a desidentificação de dados sensíveis se apresenta como uma técnica relevante para proteger a privacidade dos titulares dos dados. Analise as seguintes afirmativas relacionadas ao processo de desidentificação de dados sensíveis:

I. A anonimização é um processo irreversível pelo qual os dados pessoais perdem a possibilidade de associação, direta ou indireta, a um indivíduo, garantindo que o titular dos dados não seja identificado.

II. A pseudonimização é uma forma de desidentificação que substitui os elementos identificáveis de dados por um pseudônimo, ou seja, por um identificador que não revela a identidade do titular, mas ainda permite a reidentificação com o uso de informações adicionais mantidas separadamente.

III. Todos os processos de desidentificação, incluindo a anonimização e a pseudonimização, garantem que os dados pessoais não possam ser novamente identificados, mesmo com o uso de técnicas avançadas de mineração de dados ou inteligência artificial.

IV. Em uma auditoria de conformidade com a LGPD, a verificação da eficácia dos processos de desidentificação adotados pela organização é dispensável, desde que seja declarada a utilização dessas técnicas.

É correto o que se afirma em:

A) I e II apenas.
B) II e III apenas.
C) III e IV apenas.
D) I, II e III apenas.
E) Todas as afirmativas são corretas.

",A," 
Explicação dos itens:

I. Verdadeiro. A anonimização realmente é um processo pelo qual os dados perdem a capacidade de associação com um indivíduo de forma irreversível. Ela é uma técnica reconhecida para garantir a privacidade dos dados.

II. Verdadeiro. A pseudonimização é uma técnica de desidentificação que permite a substituição de elementos identificáveis por um pseudônimo, de modo que os dados não possam ser diretamente associados a um titular sem a utilização de informações adicionais.

III. Falso. Apesar de a desidentificação ser uma técnica importante para proteger dados sensíveis, não é correto afirmar que todos os processos de desidentificação impeçam a reidentificação, especialmente se considerarmos o avanço contínuo das técnicas de análise e processamento de dados.

IV. Falso. Em uma auditoria de conformidade com a LGPD, é fundamental verificar a eficácia dos processos de desidentificação. A declaração de uso não é suficiente sem uma análise de como esses processos estão sendo implementados e se são efetivos na proteção dos dados pessoais.",A,780650
tópico 3,Programação funcional,"Questão:

Na programação funcional, uma função é considerada pura quando segue certos princípios. Assinale a opção que caracteriza corretamente uma função pura.

A) A função deve ter pelo menos um efeito colateral para interagir com outras funções e módulos no sistema.
B) O valor de retorno da função deve ser calculado usando apenas os argumentos fornecidos, sem utilizar dados externos ou estados globais.
C) A função deve alterar seu argumento de entrada para que o estado do sistema seja atualizado corretamente.
D) Cada vez que a função é chamada, ela retorna um valor diferente, dependendo do estado global do sistema no momento da chamada.
E) A função se comunica com uma fonte externa, como realizar uma operação de entrada/saída, a cada chamada.

",B,"

Explicação:

A) Incorreta. Funções puras devem evitar efeitos colaterais. Efeitos colaterais incluem, mas não estão limitados a, modificações de variáveis globais, operações de I/O e atualização de estados em referências passadas como argumento.

B) Correta. Este item capta perfeitamente a essência de funções puras: um valor de retorno que é exclusivamente determinado pelos seus valores de entrada e não tem impacto sobre o estado global ou local.

C) Incorreta. Funções puras não devem alterar seus argumentos de entrada ou qualquer estado externo, o conceito é conhecido como imutabilidade e é um dos princípios da programação funcional.

D) Incorreta. Uma função pura deve ser referencialmente transparente, o que significa que pode ser substituída pelo seu valor de saída sem alterar o comportamento do programa. Se retornar valores diferentes para as mesmas entradas, então depende de um estado externo e não é pura.

E) Incorreta. Conexão com uma fonte externa ou operações de entrada/saída são exemplos de efeitos colaterais, o que vai contra o princípio de uma função pura.
",B,2
tópico 3,Programação orientada a objetos,"Questão: Em programação orientada a objetos (POO), considera-se que o encapsulamento é um dos pilares fundamentais para a estruturação dos programas. Sobre encapsulamento, analise as afirmativas a seguir:

I. Encapsulamento permite a ocultação da implementação interna das classes, expondo aos usuários apenas as funcionalidades necessárias para a manipulação dos objetos.
II. Em linguagens de programação que suportam POO, como Java e C#, o encapsulamento é obtido principalmente pelo uso de modificadores de acesso, como public, private e protected.
III. Encapsulamento impede que objetos de uma classe acessem diretamente os atributos de outro objeto da mesma classe.

Está(ão) correta(s) a(s) afirmativa(s):

A) Apenas I e II.
B) Apenas I e III.
C) Apenas II e III.
D) I, II e III.
E) Apenas II.

",A,"

Explicação dos itens:

I. Correto. Encapsulamento é o ato de esconder os detalhes internos da implementação de uma classe e expor apenas uma interface através da qual a funcionalidade da classe pode ser utilizada. Essa é uma descrição precisa do que é encapsulamento em POO.

II. Correto. Os modificadores de acesso como public, private e protected, são, de fato, a maneira como linguagens orientadas a objetos como Java e C# implementam o encapsulamento. Eles controlam o nível de visibilidade que cada membro da classe (atributos e métodos) terá para o restante do programa.

III. Incorreto. Encapsulamento não impede que objetos de uma mesma classe acessem diretamente os atributos uns dos outros. Encapsulamento está mais relacionado com a visibilidade e proteção dos membros da classe em relação a outras classes e o resto do programa, e não entre objetos da mesma classe, que normalmente têm a capacidade de acessar atributos privados uns dos outros dentro da mesma definição de classe.",D,4725406
tópico 1,Banco de dados NoSQL,"

Questão: 
Em um cenário de Big Data, onde grandes volumes de dados não estruturados precisam ser processados e armazenados, os bancos de dados NoSQL têm sido uma escolha popular. Considerando as características e tipos de bancos de dados NoSQL, analise as afirmativas abaixo:

I. Os bancos de dados NoSQL do tipo chave-valor armazenam os dados como uma coleção de pares chave-valor, sendo adequados para cenários onde as leituras e escritas são frequentes e exigem alta performance.

II. Bancos de dados orientados a documentos NoSQL são projetados para armazenar, recuperar e gerenciar document-oriented information, sendo uma boa escolha para armazenar dados semi-estruturados como JSON ou XML.

III. Os bancos de dados NoSQL orientados a grafos são ideais para representar e manipular relacionamentos complexos, como redes sociais, enquanto os bancos de dados relacionais são mais eficientes em lidar com dados altamente interconectados.

IV. A consistência eventual nos bancos de dados NoSQL significa que todas as escritas serão eventualmente refletidas em todas as leituras, mas pode haver uma janela de tempo em que diferentes usuários veem dados diferentes.

Estão corretas as afirmativas:

A) I e II apenas.
B) III e IV apenas.
C) I, II e IV apenas.
D) II, III e IV apenas.
E) I, II, III e IV.

",C,"

Explicação dos itens:

A) Incorrecto. Apesar de as afirmativas I e II estarem corretas, a afirmativa IV também está correta, o que torna essa opção incompleta.

B) Incorrecto. A afirmativa III está incorreta pois sugere que os bancos de dados relacionais são mais eficientes que os orientados a grafos para dados altamente interconectados, quando na verdade os bancos de dados orientados a grafos são especialmente projetados para este propósito.

C) Correto. As afirmativas I, II e IV estão corretas. Bancos de dados NoSQL do tipo chave-valor são eficientes para leituras e escritas frequentes (I), bancos de dados orientados a documentos são adequados para armazenar dados semi-estruturados (II), e a consistência eventual é uma característica comum em sistemas distribuídos NoSQL (IV).

D) Incorrecto. Esta opção inclui a afirmativa III, a qual é incorreta, portanto, a opção não pode ser a resposta correta.

E) Incorrecto. Inclui a afirmativa III que está incorreta, pois sugere uma vantagem dos bancos de dados relacionais sobre os orientados a grafos em situações que favorecem os últimos.",E,5263759
tópico 4,Probabilidade e probabilidade condicional,"Questão:
Uma empresa realiza um processo seletivo que consiste em duas etapas independentes: uma prova teórica e uma entrevista. Sabe-se que a probabilidade de um candidato ser aprovado na prova teórica é de 40%, e a probabilidade de ser aprovado na entrevista, dado que foi aprovado na prova teórica, é de 30%. Qual é a probabilidade de um candidato ser aprovado tanto na prova teórica quanto na entrevista?

A) 50%
B) 20%
C) 12%
D) 30%
E) 40%

",C,"

Explicação dos itens:

A) 50% - Este valor não é correto, pois não leva em consideração a probabilidade condicional da segunda etapa.
B) 20% - Este valor é incorreto, já que representa apenas a metade da probabilidade de passar na prova teórica e não considera a probabilidade de passar na entrevista.
C) 12% - Este é o valor correto. Como a aprovação nas duas etapas é um evento composto pela conjunção de dois eventos independentes, a probabilidade de ser aprovado em ambos é o produto das probabilidades individuais: 0,4 (probabilidade de ser aprovado na prova teórica) multiplicado por 0,3 (probabilidade condicional de ser aprovado na entrevista), resultando em 0,12 ou 12%.
D) 30% - Este valor é incorreto, pois corresponde unicamente à probabilidade condicional de passar na entrevista, ignorando a etapa da prova teórica.
E) 40% - Este é o valor da probabilidade de ser aprovado apenas na prova teórica e não considera a etapa da entrevista.",D,4525059
tópico 3,"Visualização de dados ggplot, matplotlib","Questão: Em análises de dados, frequentemente utilizamos bibliotecas de visualização de dados para melhor interpretar e comunicar resultados. As bibliotecas ggplot2 em R e matplotlib em Python são amplamente empregadas para esse fim. Nesse contexto, considere que um analista de dados deseja criar um gráfico de dispersão (scatter plot) que apresente a relação entre duas variáveis quantitativas, 'X' e 'Y', e também deseja adicionar uma linha de tendência ao gráfico para melhor entender a relação entre as variáveis. Supondo que o analista esteja utilizando o ggplot2 no R e o matplotlib no Python, assinale a opção que corretamente apresenta como o analista deve proceder em cada ambiente.

A) No ggplot2 utilize a função `ggplot(data = df) + geom_point(aes(x = X, y = Y)) + geom_smooth()` e no matplotlib `plt.scatter(df['X'], df['Y']); plt.plot(np.unique(df['X']), np.poly1d(np.polyfit(df['X'], df['Y'], 1))(np.unique(df['X'])))`.
B) No ggplot2 utilize a função `plot(df$X, df$Y)` e no matplotlib `plt.scatterplot(df['X'], df['Y'])`.
C) No ggplot2 utilize a função `qplot(X, Y, data = df, geom = ""line"")` e no matplotlib `plt.plot(df['X'], df['Y']) + plt.trendline()`.
D) No ggplot2 utilize a função `ggplot(df, aes(x = X, y = Y)) + geom_line()` e no matplotlib `plt.plot(df['X'], df['Y'])`.
E) No ggplot2 utilize a função `ggplot(df, aes(X, Y)) + stat_summary()` e no matplotlib `plt.bar(df['X'], df['Y'])`.

",A,"
A explicação dos itens é a seguinte:

A) Este item está correto porque a função `geom_point()` no ggplot2 cria um gráfico de dispersão, e a função `geom_smooth()` adiciona uma linha de tendência ao gráfico. No matplotlib, `plt.scatter()` cria um gráfico de dispersão, e a combinação de `np.polyfit()` com `np.poly1d()` e `plt.plot()` adiciona uma linha de tendência.

B) Está incorreto pois em ggplot2, `plot()` não é a função utilizada para a criação de gráficos avançados, e o matplotlib não possui uma função `scatterplot()`, a correta é `scatter()`.

C) Está incorreto pois `qplot()` com `geom = ""line""` cria um gráfico de linha em ggplot2, o que não representa um gráfico de dispersão. Além disso, no matplotlib, não existe uma função chamada `trendline()` associada diretamente ao `plt.plot()`.

D) Está incorreto pois ao usar `geom_line()` no ggplot2, será criado um gráfico de linha em vez de um gráfico de dispersão. Matplotlib usa `plt.plot()` para criar gráficos de linha, não gráficos de dispersão com linha de tendência.

E) Está incorreto pois a função `stat_summary()` em ggplot2 é usada para produzir estatísticas sumarizadas do dataset, e não especificamente para gráficos de dispersão com linha de tendência. Já `plt.bar()` em matplotlib cria um gráfico de barras, o que não corresponde ao pedido da questão.",A,8269068
tópico 3,Programação orientada a objetos,"Questão: Em programação orientada a objetos (POO), o princípio da substituição de Liskov (LSP) afirma que objetos de uma superclasse devem ser substituíveis por objetos de suas subclasses sem que seja necessário alterar as propriedades desse programa. Com base nesse principe, qual das seguintes afirmações é verdadeira quando aplicamos o LSP corretamente em um projeto de software?

A) Subclasses podem lançar tipos de exceções que não são previstos ou tratados pela superclasse.
B) Subclasses podem remover métodos de comportamento herdados da superclasse.
C) A superclasse pode ser substituída por qualquer uma de suas subclasses sem afetar a funcionalidade do programa.
D) Subclasses devem ter suas próprias implementações para todos os métodos definidos na superclasse.
E) Comparar objetos da superclasse com objetos da subclasse usando o operador de igualdade deve sempre retornar falso.

",C,"

A) Incorreta. Subclasses que lançam exceções não previstas pela superclasse estão violando o contrato estabelecido pela superclasse e, portanto, não respeitam o LSP.
B) Incorreta. Remover métodos herdados rompe o contrato da superclasse, pois significa que a substituição da superclasse pela subclasse pode levar a chamadas de métodos inexistentes, violando o LSP.
C) Correta. Essa é a essência do LSP – objetos de uma superclasse devem poder ser substituídos por objetos de uma subclasse sem afetar a corretude do programa.
D) Incorreta. Subclasses não são necessariamente obrigadas a implementar suas próprias versões para todos os métodos definidos na superclasse. Elas podem herdar comportamentos diretamente da superclasse se esses comportamentos ainda forem válidos para a subclasse.
E) Incorreta. O operador de igualdade pode perfeitamente considerar iguais um objeto da superclasse e um da subclasse, especialmente se eles forem equivalentes em relação ao estado que é comparado. Isso depende da implementação específica do método de igualdade nas classes envolvidas.",C,3254598
tópico 0,Armazenamento de big data,"Questão:

A capacidade de armazenar e processar grandes volumes de dados – conhecida como Big Data – tem ganho cada vez mais importância no âmbito da ciência de dados e análises avançadas. No tocante às soluções de armazenamento projetadas para esse fim, qual das seguintes opções descreve uma característica essencial que os sistemas de armazenamento de Big Data devem possuir para manejar eficientemente o volume, a velocidade e a variedade dos dados?

A) Alta capacidade de armazenamento em discos locais.
B) Suporte apenas para dados estruturados para otimizar o desempenho.
C) Capacidades de compressão de dados para reduzir o espaço físico necessário.
D) Sistemas robustos de indexação e busca para velocidade de acesso aos dados.
E) Escalabilidade horizontal para permitir o crescimento do sistema de forma flexível.

",E,"

Explicação dos itens:

A) Alta capacidade de armazenamento em discos locais.
- Esta alternativa não é suficiente para lidar com big data, pois o enfoque exclusivo em discos locais ignora aspectos como escalabilidade, processamento distribuído e a necessidade de gerenciar dados não estruturados.

B) Suporte apenas para dados estruturados para otimizar o desempenho.
- O Big Data é caracterizado também pela variedade, o que inclui dados não estruturados e semi-estruturados. Um sistema de armazenamento eficiente para Big Data não pode se limitar apenas aos dados estruturados.

C) Capacidades de compressão de dados para reduzir o espaço físico necessário.
- Enquanto a compressão de dados é uma técnica útil, ela não é suficiente por si só para abordar os principais desafios do Big Data, que incluem não apenas volume, mas também a velocidade e variedade dos dados.

D) Sistemas robustos de indexação e busca para velocidade de acesso aos dados.
- Sistemas robustos de indexação e busca são importantes, mas não abordam diretamente o desafio da escalabilidade, que é fundamental para lidar com os volumes crescentes de dados de Big Data.

E) Escalabilidade horizontal para permitir o crescimento do sistema de forma flexível.
- Esta é a característica mais essencial para sistemas de armazenamento de Big Data. A escalabilidade horizontal refere-se à capacidade do sistema de expandir adicionando mais nodos na rede, o que é crucial para lidar com o aumento de dados (volume), a necessidade de processamento rápido (velocidade) e a gestão eficiente de diferentes tipos de dados (variedade).",E,7879681
tópico 2,Enriquecimento,"Questão: Em processos industriais, o enriquecimento de um material frequentemente implica na separação e concentração de um determinado constituinte desejado presente em uma mistura complexa. Um exemplo clássico de enriquecimento é a produção de urânio enriquecido, que é utilizado como combustível em reatores nucleares. O urânio natural contém cerca de 0,7% de urânio-235, isótopo físsil necessário para a reação em cadeia. O enriquecimento aumenta essa porcentagem para atender às especificações do reator. Qual método dentre os listados abaixo NÃO é utilizado para o enriquecimento de urânio?

A) Difusão gasosa
B) Centrifugação a gás
C) Processo de enriquecimento eletromagnético
D) Filtração por membranas seletivas
E) Enriquecimento por laser

",D," 

A difusão gasosa (A) e a centrifugação a gás (B) são técnicas tradicionais de enriquecimento de urânio. O processo de enriquecimento eletromagnético (C), conhecido como calutrons, foi utilizado durante o Projeto Manhattan, mas é menos comum atualmente. O enriquecimento por laser (E) refere-se ao processo de separação isotópica usando lasers para excitar seletivamente isótopos de urânio. A filtração por membranas seletivas (D) não é uma técnica usada para enriquecer urânio, pois os isótopos de urânio têm propriedades químicas idênticas e não podem ser separados por simples filtração.",,7803587
tópico 1,Álgebra relacional e SQL (padrão ANSI),"Questão:
Considere a existência de um banco de dados de uma biblioteca que mantém suas informações em duas relações, Livros(Lid, Titulo, Autor) e Emprestimos(Eid, Lid, DataEmprestimo, DataDevolucao). Lid refere-se ao identificador único de cada livro. A relação Livros contém os detalhes do livro, incluindo o identificador do livro, o título e o autor. Por outro lado, Emprestimos rastreia os empréstimos dos livros, onde Eid é o identificador único do empréstimo, Lid é a chave estrangeira referenciando a relação Livros, DataEmprestimo é a data em que o livro foi emprestado e DataDevolucao é a data em que o livro foi devolvido à biblioteca. Em um dado momento, a biblioteca deseja obter uma lista dos títulos de livros que nunca foram emprestados.

Qual das seguintes consultas SQL retornaria corretamente a lista solicitada pela biblioteca?

A) SELECT Titulo FROM Livros WHERE Lid NOT IN(SELECT Lid FROM Emprestimos);

B) SELECT l.Titulo FROM Livros l JOIN Emprestimos e ON l.Lid = e.Lid WHERE e.DataEmprestimo IS NULL;

C) SELECT Titulo FROM Livros EXCEPT SELECT Titulo FROM Emprestimos;

D) SELECT Titulo FROM Livros l WHERE NOT EXISTS(SELECT * FROM Emprestimos e WHERE l.Lid = e.Lid);

E) SELECT l.Titulo FROM Livros l, Emprestimos e WHERE l.Lid != e.Lid GROUP BY l.Titulo;

",D,"

Explicação dos itens:

A) Esta consulta verifica os livros cujo identificador não está presente na relação dos empréstimos, o que seria uma maneira correta se todos os livros emprestados estivessem sempre na relação Emprestimos. Contudo, ela não se preocupar com os casos de livros que foram emprestados e depois retornaram, o que não atende ao requisito da questão.

B) Esta consulta tenta fazer um INNER JOIN entre as relações Livros e Emprestimos, onde as colunas Lid correspondem, e em seguida verifica se a DataEmprestimo é nula. No entanto, uma DataEmprestimo nula não faria sentido na tabela Emprestimos, então essa consulta não retorna os livros que nunca foram emprestados.

C) Esta consulta faz uso do operador EXCEPT para tentar subtrair os títulos contidos na relação Emprestimos da relação Livros. Entretanto, ela não leva em consideração que os identificadores dos livros (Lid) devem ser comparados, e não os títulos. Ademais, Emprestimos não tem coluna de Título, logo a consulta está errada.

D) Esta consulta utiliza um subselect com o operador NOT EXISTS para verificar se não existem entradas na relação Emprestimos para cada livro na relação Livros. Esta seria a consulta correta pois ela corretamente busca por livros que não possuem correspondência na tabela Emprestimos, indicando que nunca foram emprestados.

E) Esta consulta tenta encontrar livros fazendo um produto cartesiano entre Livros e Emprestimos e selecionando pares de Livros e Empréstimos onde os Lids são diferentes. O GROUP BY agrupa os resultados pelo título dos livros. No entanto, essa consulta não garante obter os livros que nunca foram emprestados; além disso, um produto cartesiano não é o método correto para resolver este problema.",C,6165187
tópico 0,Soluções de big data: Arquitetura do ecossistema Spark,"Questão: No contexto de grandes volumes de dados e processamento distribuído, a arquitetura do Apache Spark destaca-se por sua versatilidade e desempenho. Dentro desse ecossistema, são oferecidas diversas APIs que permitem a manipulação de dados de maneira eficiente. Entre as afirmativas abaixo, identifique qual está correta quanto aos componentes e funcionalidades do ecossistema Spark.

A) O Spark Streaming é uma extensão do core Spark para tratar fluxos contínuos de dados, mas não permite a integração com sistemas como Kafka ou Flume para processamento em tempo real.
B) O GraphX é o módulo do Spark para processamento gráfico. Entretanto, ele não suporta a construção de grafos a partir dos DataFrames Spark, tornando-o obsoleto dentro do ecossistema.
C) O Spark SQL permite a manipulação de dados estruturados e semi-estruturados, através de abstrações como os DataFrames, e possibilita consultas via SQL, mas não suporta a leitura de dados no formato JSON.
D) O MLib é uma biblioteca do Spark para aprendizado de máquina que oferece algoritmos de alto desempenho para clustering, classificação, regressão e filtragem colaborativa, podendo operar diretamente com DataFrames.
E) O Tachyon é um componente padrão do ecossistema Spark, responsável por operações de I/O e cuja principal funcionalidade é a otimização de leituras sequenciais em disco.

",D," 
A) Incorreta. O Spark Streaming é de fato utilizado para processamento de fluxos contínuos de dados e justamente por isso suporta integração com diversos sistemas de mensageria, incluindo Kafka e Flume.
B) Incorreta. O GraphX é uma API para processamento de grafos e permite que os grafos sejam construídos e manipulados a partir dos DataFrames, não o tornando obsoleto, mas sim uma ferramenta poderosa para análise de grafos no ecossistema Spark.
C) Incorreta. Spark SQL suporta a manipulação de dados estruturados e semi-estruturados, e uma de suas capacidades é justamente a leitura e manipulação de dados em formato JSON, que é amplamente utilizado.
D) Correta. O MLib é a biblioteca de Machine Learning do Spark e permite a realização de diversas operações de aprendizado de máquina de forma distribuída e eficiente, operando de forma integrada com outros componentes do Spark, incluindo DataFrames.
E) Incorreta. Tachyon (atualmente conhecido como Alluxio) é um sistema de armazenamento virtual de memória que pode ser integrado com o Spark, mas não é um componente padrão do ecossistema Spark; além disso, não se foca exclusivamente em leituras sequenciais, mas na velocidade geral de acesso a dados em diferentes storage systems.
",D,4906258
tópico 1,Banco de dados NoSQL,"Questão: A evolução das tecnologias de banco de dados foi significativamente influenciada pelo surgimento dos bancos NoSQL. Considerados alternativas escaláveis e adequadas para cenários de Big Data, essas bases de dados se caracterizam por determinadas propriedades e tipos. Sobre os bancos de dados NoSQL, analise as afirmativas a seguir:

I - Bancos de dados do tipo chave-valor armazenam os dados em um esquema rígido e bem estruturado, com a obrigatoriedade de tabelas pré-definidas.
II - Os bancos de dados orientados a documentos são flexíveis em termos de esquema, permitindo a inclusão de novos campos sem a necessidade de modificar todos os registros existentes.
III - Bancos de dados de colunas amplas oferecem vantagens no que diz respeito à armazenagem e processamento de grandes volumes de dados com poucos índices, tipicamente empregados em sistemas que requerem alta performance em operações de leitura e escrita.
IV - Sistemas baseados em grafos são altamente adequados para representar e explorar relações complexas entre entidades, sendo utilizados em aplicações como mídias sociais, detecção de fraudes e recomendação de produtos.

É correto o que se afirma em:

A) I e II, apenas.
B) II e III, apenas.
C) II, III e IV, apenas.
D) III e IV, apenas.
E) I, II, III e IV.

",C,"

Explicação dos itens:

I - Incorreto. Bancos de dados do tipo chave-valor são conhecidos pela sua simplicidade e flexibilidade, não exigindo um esquema rígido ou tabelas pré-definidas.

II - Correto. Os bancos de dados orientados a documentos permitem alta flexibilidade de esquema, possibilitando a adição de novos campos sem impactar registros preexistentes.

III - Correto. Os bancos de dados de colunas amplas são otimizados para consultas rápidas em grandes volumes de dados, não requerendo muitos índices e favorecendo operações de leitura e escrita de alta performance.

IV - Correto. Sistemas baseados em grafos são excelentes na representação de relacionamentos complexos, e por isso são utilizados em diferentes áreas como recomendação de produtos, redes sociais e análise de fraudes.

Portanto, a alternativa C é a correta, pois afirma corretamente sobre os tipos II, III e IV de bancos de dados NoSQL, enquanto a alternativa I está incorreta.",C,2858305
tópico 3,"Visualização de dados ggplot, matplotlib","Questão: Considere que um cientista de dados está realizando uma análise exploratória em um conjunto de dados referente ao desempenho de alunos em uma série de testes acadêmicos. Utilizando a biblioteca ggplot2 do R para visualização dos dados, qual seria o código apropriado para criar um gráfico de dispersão (scatter plot) que apresente a relação entre as notas de matemática (eixo x) e ciências (eixo y), diferenciando cada ponto no gráfico pelo gênero do aluno?

A) ggplot(data = notas, aes(x = matematica, y = ciencias)) + 
     geom_point(aes(color = genero))

B) plt.scatter('matematica', 'ciencias', c = data['genero'], data = notas)
   plt.show()

C) ggplot(notas, aes('matematica', 'ciencias')) + 
     geom_point(aes(colour = 'genero'))

D) ggplot(data = notas) + 
     geom_point(mapping = aes(x = matematica, y = ciencias, color = genero))

E) ggplot(notas, aes(x = matematica, y = ciencias, fill = genero)) + 
     geom_point()

",A,"

Explicação dos itens:

A) Este item está correto porque utiliza a função `ggplot()` com o data frame `notas`, define o mapeamento estético (`aes()`) com as variáveis corretas nos eixos x e y, e adiciona `geom_point()` com a diferenciação de cor por gênero dentro de `aes()`, que é a forma correta de adicionar essa diferenciação em ggplot2.

B) Esta alternativa faz utilização da sintaxe do Matplotlib na biblioteca Python, que não é apropriado para uma questão que especifica o uso do ggplot2 do R.

C) A sintaxe contida nessa opção está incorreta para a ggplot2, pois o formato do mapeamento estético (`aes()`) não deve incluir as variáveis entre aspas simples.

D) Esta opção também é válida no ggplot2; no entanto, como a questão menciona especificamente a necessidade de diferenciar os pontos por gênero, a resposta A é mais completa e, portanto, mais correta sob a perspectiva da questão apresentada.

E) Aqui, enquanto `geom_point()` é usado apropriadamente para criar um gráfico de dispersão, o uso de `fill = genero` não é o adequado para pontos em um scatter plot, que deveria usar `color` para diferenciar as cores dos pontos. A função `fill` é geralmente reservada para geométricas que possuem uma área a ser preenchida, como barras em gráficos de barra.",E,8148525
tópico 4,Métodos e técnicas de identificação causal: Métodos experimentais RCT e de identificação quase-experimental,"Questão: Em pesquisas nas áreas econômicas e sociais, a identificação de relações causais entre variáveis é fundamental para compreender os mecanismos subjacentes aos fenômenos estudados. Dentre os métodos utilizados para tal identificação, destacam-se os experimentos controlados randomizados (RCTs - Randomized Controlled Trials) e os métodos de identificação quase-experimental. Considerando as diferenças e particularidades desses métodos, avalie os itens a seguir:

I. Experimentos Randomizados (RCTs) requerem a alocação aleatória de tratamentos aos sujeitos do estudo, sendo um método geralmente livre de viés de seleção, pois a randomização tende a balancear as características observadas e não observadas entre os grupos de tratamento e controle.

II. Métodos quase-experimentais, como estudos de Diferença em Diferenças (DiD) e Regressão Descontínua (RD), dependem de suposições mais fortes e muitas vezes são implementados quando a randomização não é viável ou é considerada antiética.

III. Uma característica fundamental dos RCTs é que eles não requerem hipóteses adicionais sobre a ausência de variáveis omitidas ou sobre a forma funcional do modelo, uma vez que a aleatoriedade assegura a comparabilidade entre os grupos.

IV. Em estudos quase-experimentais, a validade das inferências causais é frequentemente dependente da capacidade do pesquisador de controlar por variáveis confundidoras, o que pode ser feito por meio de técnicas como pareamento e instrumentos.

Está correto apenas o que se afirma em:

A) I e IV
B) I, II e III
C) II e III
D) I, II e IV
E) III e IV

",D,"

Item I é verdadeiro: Os RCTs são considerados o padrão-ouro em muitas áreas de pesquisa justamente porque a randomização deve, teoricamente, equalizar características entre os grupos tratamento e controle, reduzindo o viés.

Item II é verdadeiro: Métodos como DiD e RD são alternativas aos RCTs e são adotados em situações em que a randomização não é possível. Suposições mais fortes são necessárias para confiar nos resultados desses métodos quase-experimentais.

Item III é incorreto: Apesar dos RCTs serem menos suscetíveis a variáveis omitidas devido à randomização, ainda podem requerer hipóteses adicionais, principalmente se a randomização não for perfeitamente executada ou se houver desistências do estudo (attrition).

Item IV é verdadeiro: Em estudos quase-experimentais, devido à falta de randomização, o controle de variáveis de confusão é essencial e técnicas como pareamento (matching) e variáveis instrumentais são frequentemente utilizadas para ajudar a estabelecer uma relação causal mais confiável.",D,9961626
tópico 1,"Banco de dados relacional: SQL Server, PostgreSQL, MySQL","Questão: No contexto dos sistemas de banco de dados relacionais SQL Server, PostgreSQL e MySQL, considere que um administrador de banco de dados deseja criar um índice que otimize as consultas em uma tabela com grande volume de dados e transações frequentes. A tabela armazena informações de vendas, incluindo as colunas 'data_venda', 'id_cliente', 'valor_venda' e 'id_vendedor'. As consultas mais comuns envolvem a busca por 'data_venda' e 'id_cliente'. Com base nesse cenário, qual das seguintes abordagens é a mais eficiente para a criação deste índice que busca otimização?

A) Criar um índice clusterizado apenas na coluna 'data_venda', pois esse tipo de índice reorganiza fisicamente as linhas da tabela e é mais rápido para consultas baseadas em range.

B) Criar um índice não clusterizado composto pelas colunas 'data_venda' e 'id_cliente', uma vez que este índice melhora a performance de consultas que utilizam essas colunas como filtros.

C) Implementar um índice full-text nas colunas 'data_venda' e 'id_cliente', o que permitiria buscas rápidas por textos específicos e substrings nas colunas especificadas.

D) Utilizar um índice hash na coluna 'id_cliente', considerando que este modelo de indexação é mais eficiente para consultas que realizam buscas diretas por valor exato.

E) Criar índices clusterizados separados para 'data_venda' e 'id_cliente', permitindo assim que o otimizador de consultas escolha o melhor índice para cada consulta específica.

",B,"

A) Um índice clusterizado reorganiza as linhas da tabela no disco, o que pode ser eficiente para consultas baseadas em range. No entanto, como as consultas envolvem não só 'data_venda', mas também 'id_cliente', este índice não seria o mais efetivo.

B) Criar um índice não clusterizado composto é o mais eficiente neste caso porque irá otimizar as consultas que incluem ambas as colunas no filtro de busca, sendo essa a situação mais comum mencionada no cenário.

C) Índices full-text são utilizados para pesquisar textos dentro de uma coluna, como em buscas por palavras dentro de colunas de texto grande. As colunas mencionadas não se enquadram em um uso típico de índice full-text.

D) Índices hash são eficientes para correspondência exata de valores, mas eles não são diretamente suportados para indexação no SQL Server e no PostgreSQL (MySQL oferece essa opção, mas apenas para tabelas de hash em memória). Além disso, a consulta também envolve a coluna 'data_venda', portanto o índice hash não seria o mais apropriado.

E) Índices clusterizados são únicos por tabela, então não se pode criar mais de um em uma única tabela. Além disso, dois índices clusterizados separados não otimizariam as consultas que incluem ambas as colunas no mesmo filtro.",D,3188153
tópico 0,Soluções de big data: Arquitetura do ecossistema Spark,"Questão:

A arquitetura do Apache Spark é projetada para facilitar o processamento de grandes volumes de dados de maneira eficiente e rápida. Em relação às componentes principais que fazem parte da arquitetura do Spark, associe corretamente as seguintes funções às suas respectivas componentes:

I. Spark SQL
II. Spark Streaming
III. MLlib
IV. GraphX
V. Spark Core

( ) Utiliza o abstração chamada Resilient Distributed Dataset (RDD) para processamento distribuído de dados.
( ) Provê uma biblioteca de machine learning para o processamento analítico preditivo.
( ) Oferece suporte para o processamento de dados em tempo real.
( ) Habilita a manipulação de grafos e computações paralelas relacionadas a grafos.
( ) Permite a execução de consultas SQL e possibilita a leitura de dados de múltiplas fontes de dados estruturadas.

A sequência correta de associações, de cima para baixo, é:

a) V, III, II, IV, I
b) V, II, III, I, IV
c) IV, III, II, I, V
d) I, IV, III, II, V
e) III, V, IV, I, II

",B," 
Explicação dos itens:

I. Spark SQL - (e) É a componente do Spark responsável pela integração do processamento de dados estruturados e semi-estruturados com a facilidade das consultas SQL. Ela também permite a leitura de diferentes fontes de dados, como JSON, Hive e Parquet.

II. Spark Streaming - (c) Esta componente do Spark permite o processamento de fluxos contínuos de dados (streaming). Com ela, é possível realizar análises em tempo real sobre os dados que estão sendo gerados e recebidos constantemente.

III. MLlib - (b) MLlib é a biblioteca de machine learning embutida no Spark. Ela oferece diversos algoritmos e utilidades para tarefas de aprendizado de máquina, simplificando a implementação de modelos preditivos sobre grandes conjuntos de dados.

IV. GraphX - (d) É a API para manipulação de grafos e execução de algoritmos paralelos relacionados a grafos. Com ela, os usuários podem realizar operações sobre grafos de maneira otimizada e distribuída.

V. Spark Core - (a) Constitui o coração do Spark, onde as funcionalidades fundamentais do sistema estão implementadas, incluindo a programação de trabalhos e tarefas distribuídas, a gestão de memória e o fault tolerance. A sua abstração principal são os RDDs (Resilient Distributed Datasets), que permitem o processamento paralelo e distribuído dos dados.",A,1100074
tópico 0,Soluções de big data: Arquitetura do ecossistema Spark,"Questão: Na arquitetura do ecossistema Apache Spark, diversas componentes contribuem para processamento de big data de maneira eficiente. Dentro desse ecossistema, uma das camadas é fundamental para o agendamento de tarefas e distribuição dos dados. Esta camada também é responsável pelo gerenciamento da memória e pela otimização de tarefas. Qual componente do Spark desempenha esse papel?

A) Spark SQL
B) Spark Streaming
C) Spark Core
D) MLlib
E) GraphX

",C,"

Explicação dos itens:

A) Spark SQL - É o módulo do Spark para o processamento de dados estruturados com SQL e DataFrames. Apesar de fazer parte do ecossistema Spark e permitir a otimização de consultas, não é o componente responsável por agendamento de tarefas e distribuição de dados.

B) Spark Streaming - Esse componente permite o processamento de fluxos contínuos de dados (streaming data), mas não é a camada que lida com o agendamento de tarefas e otimização.

C) Spark Core - É o coração do Apache Spark, responsável pelo agendamento de tarefas, distribuição de dados, gerenciamento de memória e a base para todos os outros componentes do Spark. Portanto, é a alternativa correta.

D) MLlib - A biblioteca de machine learning para Apache Spark, que fornece várias ferramentas para tarefas de aprendizado de máquina, mas não inclui funcionalidades de agendamento de tarefas ou gerenciamento de memória para o ecossistema.

E) GraphX - Extensão do Spark para processar grafos e realizar computações gráficas. Assim como os outros componentes listados, não é responsável pelo agendamento de tarefas e gerenciamento do sistema.",C,6407950
tópico 5,Ajuste de modelos dentro e fora de amostra e overfitting,"Questão: Em estatística e aprendizado de máquina, durante a fase de ajuste de modelos, um dos problemas enfrentados é o de overfitting, que ocorre quando o modelo se ajusta excessivamente aos dados de treino, resultando em um desempenho ruim em dados não vistos anteriormente. Considerando os conceitos de ajuste de modelo dentro e fora de amostra, qual das seguintes estratégias NÃO contribui efetivamente para a mitigação do overfitting?

A) Aumentar a quantidade de dados de treinamento.
B) Utilizar um modelo com menor complexidade.
C) Utilizar métricas de avaliação adequadas, como o erro de validação cruzada.
D) Selecionar features com base em seu desempenho no conjunto de treino.
E) Aplicar técnicas de regularização como Ridge ou Lasso.

",D,"

A) Aumentar a quantidade de dados de treinamento pode ajudar a melhorar a generalização do modelo, pois fornece mais informações sobre a variação dos dados, reduzindo a chance do modelo se ajustar demasiadamente ao ruído presente nos dados de treinamento.

B) Modelos com menor complexidade são menos propensos ao overfitting. Eles têm menor capacidade de se ajustar a detalhes e peculiaridades dos dados de treino, o que pode melhorar a performance em dados não vistos.

C) A utilização de métricas de avaliação adequadas, como o erro de validação cruzada, ajuda a estimar o desempenho do modelo em dados não vistos, orientando a busca por um equilíbrio adequado entre viés e variância.

D) Selecionar features com base exclusivamente em seu desempenho no conjunto de treino pode levar a uma escolha de variáveis que não necessariamente terão o mesmo desempenho em dados novos. Esta estratégia pode contribuir para o overfitting.

E) Técnicas de regularização como Ridge ou Lasso introduzem uma penalidade na função de custo do modelo para coeficientes maiores, reduzindo a complexidade do modelo e, por consequência, o risco de overfitting.",D,4862634
tópico 3,Programação orientada a objetos,"Questão: Em Programação Orientada a Objetos (POO), o conceito de herança é uma das principais características que permitiram o avanço no reuso de código e na organização das estruturas de dados de maneira hierárquica e sistemática. Sobre herança, analise as seguintes afirmações:

I. A herança múltipla permite que uma classe herde o comportamento de mais de uma classe base, o que pode introduzir complexidade adicional ao resolver ambiguidades que podem surgir com membros de mesmo nome.

II. Em linguagens que não suportam herança múltipla diretamente, como Java, é possível alcançar funcionalidade semelhante por meio do uso de interfaces, as quais permitem a especificação de métodos que devem ser implementados pela classe filha.

III. O princípio do polimorfismo, intimamente ligado à herança, impede que uma classe filha possua métodos com as mesmas assinaturas dos métodos da classe pai, para evitar conflitos durante a implementação de sobreposições.

IV. Herança é um conceito que caracteriza apenas as linguagens de programação estáticas e fortemente tipadas, não sendo aplicável em linguagens dinâmicas ou fracamente tipadas.

Assinale a opção que apresenta somente as afirmações corretas:

A) I e II
B) I e III
C) II e IV
D) III e IV
E) I, II e III

",A,"

Explicação dos itens:

I. Correto. A herança múltipla realmente permite que uma classe herde características de múltiplas classes bases. Contudo, isso pode levar a problemas de ambiguidade, especialmente conhecidos como ""problema do diamante"", onde uma classe herda de duas classes que possuem um mesmo ancestral comum.

II. Correto. Em Java, por exemplo, a herança múltipla não é suportada diretamente, mas o uso de interfaces permite que uma classe concorde em implementar métodos especificados por múltiplas interfaces, fornecendo assim um padrão semelhante à herança múltipla no que diz respeito à definição do contrato que a classe filha deve cumprir.

III. Incorreto. O polimorfismo permite justamente que métodos com a mesma assinatura sejam redefinidos em uma classe filha, possibilitando que objetos sejam tratados de maneira mais genérica. O que se busca evitar não é a sobreposição de métodos, mas sim garantir que a implementação seja consistente com a interface apresentada pela classe pai.

IV. Incorreto. Herança é um conceito que se aplica a várias linguagens de programação, independentemente de serem estáticas ou dinâmicas, fortes ou fracamente tipadas. Por exemplo, Python é uma linguagem dinâmica e fracamente tipada que suporta herança.

Portanto, somente as afirmações I e II são corretas.",A,6087953
tópico 4,Probabilidade e probabilidade condicional,"Questão:
A probabilidade de um aluno ser aprovado em Matemática é 0,7, enquanto a probabilidade de ser aprovado em Física é 0,6. Além disso, sabe-se que a probabilidade de ser aprovado em ambas as disciplinas é 0,5. Com base nessas informações, qual é a probabilidade de um aluno ser aprovado em Matemática, dado que já foi aprovado em Física?

A) 0,7
B) 0,5
C) 0,833
D) 0,6
E) 0,9

",C,"

A probabilidade condicional é dada pela fórmula P(A|B) = P(A ∩ B) / P(B), onde P(A|B) é a probabilidade de A dado B, P(A ∩ B) é a probabilidade de A e B ocorrerem ao mesmo tempo, e P(B) é a probabilidade de B. Para encontrar a probabilidade de um aluno ser aprovado em Matemática, dado que já foi aprovado em Física, precisamos calcular P(Mat|Fís).

Usando as informações fornecidas, temos:
- P(Mat) = 0,7 (probabilidade de ser aprovado em Matemática)
- P(Fís) = 0,6 (probabilidade de ser aprovado em Física)
- P(Mat ∩ Fís) = 0,5 (probabilidade de ser aprovado em ambas)

Assim, P(Mat|Fís) = P(Mat ∩ Fís) / P(Fís) = 0,5 / 0,6 = 0,833.

Portanto, a probabilidade de um aluno ser aprovado em Matemática, dado que já foi aprovado em Física, é cerca de 0,833.

Explicação dos itens:
- A) 0,7: Essa é a probabilidade do aluno ser aprovado em Matemática, desconsiderando qualquer condição sobre Física.
- B) 0,5: Essa é a probabilidade do aluno ser aprovado em ambas as disciplinas, não a probabilidade condicional desejada.
- C) 0,833: Este é o valor correto da probabilidade condicional, calculado com as informações dada.
- D) 0,6: Essa é a probabilidade do aluno ser aprovado em Física, desconsiderando qualquer condição sobre Matemática.
- E) 0,9: Não há informações fornecidas que justifiquem este valor; parece ser um valor arbitrário e não está relacionado ao cálculo correto da probabilidade.",C,9782593
tópico 4,Medidas de tendência central e dispersão e correlação,"Questão:
A análise estatística de um conjunto de dados pode fornecer informações valiosas sobre o comportamento e as características desse conjunto. Com relação às medidas de tendência central e dispersão, assim como a correlação entre variáveis, julgue os itens a seguir e assinale a opção correta.

I. A média aritmética sempre será uma medida de tendência central adequada, independentemente da presença de valores extremos (outliers).
II. A mediana é menos sensível a outliers do que a média aritmética, pois depende unicamente da posição central dos dados, e não dos seus valores.
III. O desvio padrão é uma medida de dispersão que considera o quão espalhados estão os valores em relação à média, sendo mais influenciado por valores extremos do que a variância.
IV. O coeficiente de correlação de Pearson é um indicador de relação linear entre duas variáveis, assumindo valores entre -1 e +1, onde 0 indica a ausência de correlação linear.

A) Apenas os itens II e IV estão corretos.
B) Apenas os itens I e III estão corretos.
C) Apenas os itens II e III estão corretos.
D) Todos os itens estão corretos.
E) Apenas os itens I, II, e IV estão corretos.

",A,"

Explicação dos itens:

I. Incorreto. A média aritmética pode ser fortemente influenciada por valores extremos (outliers). Em tais casos, pode não representar adequadamente a tendência central dos dados.

II. Correto. A mediana é uma medida de tendência central que é menos sensível a outliers, proporcionando uma melhor representação do ponto central dos dados em distribuições assimétricas.

III. Incorreto. O desvio padrão é uma medida de dispersão que é afetada por valores extremos, porém é a variância que leva ao quadrado as diferenças em relação à média. O desvio padrão é a raiz quadrada da variância e ambos são igualmente influenciados por outliers, contrariando o enunciado do item.

IV. Correto. O coeficiente de correlação de Pearson mede a força e a direção da relação linear entre duas variáveis, onde -1 representa uma correlação negativa perfeita, +1 representa uma correlação positiva perfeita e 0 indica que não há correlação linear.",A,683166
tópico 2,Contexto de IA: Algoritmos fuzzy matching e stemming,"QUESTÃO:

Na área de processamento de línguas naturais e recuperação de informações, técnicas como algoritmos de fuzzy matching e stemming são comumente utilizados para aumentar a eficácia dos sistemas em compreender e manipular dados textuais. Considerando os conceitos e aplicações dessas técnicas, analise as afirmativas abaixo:

I - Fuzzy matching refere-se ao processo que identifica pares de strings de texto que são aproximadamente ou parcialmente iguais, mas não exatamente idênticos, o que é útil para correção ortográfica automática e para sistemas de busca tolerantes a erros.

II - Stemming é uma técnica que consiste na redução de palavras para a sua forma radical, permitindo que diferentes variantes de uma palavra sejam associadas a uma única forma base, utilizada para melhorar a precisão do algoritmo de busca em mecanismos de pesquisa.

III - Um sistema de busca que aplica ambos, algoritmos de fuzzy matching e stemming, não pode ser aplicado em idiomas que possuem alta inflexão morfológica, como o russo ou o árabe, já que as raízes das palavras nesses idiomas não são estáveis.

IV - Algoritmos de fuzzy matching frequentemente utilizam uma métrica chamada Distância de Levenshtein para calcular o número mínimo de operações necessárias para transformar uma string de caracteres em outra, fundamentando-se em operações como inserção, exclusão e substituição de caracteres.

É correto o que se afirma em:

A) I e II, apenas.
B) I, II e IV, apenas.
C) II e III, apenas.
D) I, III e IV, apenas.
E) Todas as afirmativas são corretas.

",RESPOSTACORRETA:B,"

EXPLICAÇÃO DOS ITENS:

I - Correto. Fuzzy matching é uma técnica que encontra correspondências que são aproximadamente iguais, sendo amplamente utilizada em corretores ortográficos e sistemas de busca que permitem certa margem de erro, aumentando a acessibilidade do sistema para interpretar inputs variados do usuário.

II - Correto. Stemming reduz as palavras para suas raízes (ou ""stems"") e é uma prática comum em mecanismos de busca para associar termos semelhantes a uma consulta de pesquisa, melhorando a relevância dos resultados.

III - Incorreto. A afirmação é falsa porque algoritmos de fuzzy matching e stemming podem ser adaptados para idiomas com alta inflexão morfológica. De fato, a eficácia desses algoritmos pode depender de ajustes específicos para a morfologia de cada idioma, mas isso não os torna inaplicáveis.

IV - Correto. A Distância de Levenshtein é uma métrica usada em algoritmos de fuzzy matching para quantificar quão diferentes duas sequências de texto são, e baseia-se em operações como inserção, exclusão e substituição para transformar uma string de texto em outra.

Dado que as afirmativas I, II e IV estão corretas, a resposta correta é a opção B.",A,3930557
tópico 1,Banco de dados NoSQL,"Questão:
A popularidade dos bancos de dados NoSQL tem crescido nos últimos anos devido à sua capacidade de lidar com grandes volumes de dados, alta escalabilidade e flexibilidade na modelagem de dados. Dentre as categorias de bancos de dados NoSQL, pode-se citar os orientados a documentos, colunas, chave-valor e grafos. Considere os seguintes cenários para adoção de bancos de dados NoSQL e assinale a opção que associa corretamente o tipo de banco de dados ao cenário descrito:

I. Uma rede social deseja armazenar suas informações de conexões entre usuários de maneira eficiente para facilitar consultas como ""amigos de amigos"" e recomendações de novas conexões baseadas em padrões existentes de amizade.

II. Uma empresa de e-commerce deseja organizar seus dados de produtos, incluindo descrições, avaliações de clientes e histórico de preços, de forma flexível para acomodar mudanças frequentes no catálogo sem a necessidade de esquemas fixos.

III. Um sistema de monitoramento em tempo real para uma aplicação de Internet das Coisas (IoT) precisa gravar e recuperar rapidamente grandes quantidades de pequenas mensagens que indicam o estado dos dispositivos conectados.

IV. Um serviço de análise de dados precisa otimizar suas operações de leitura e escrita de grandes quantidades de dados que são estruturados de forma tabular com diversas colunas, mas variam raramente.

A) I - Grafos, II - Documentos, III - Chave-Valor, IV - Colunas
B) I - Documentos, II - Colunas, III - Grafos, IV - Chave-Valor
C) I - Grafos, II - Chave-Valor, III - Documentos, IV - Colunas
D) I - Colunas, II - Grafos, III - Documentos, IV - Chave-Valor
E) I - Documentos, II - Colunas, III - Grafos, IV - Chave-Valor

",A," 

Explicação dos itens: 

I. Grafos - Bancos de dados de grafos são ideais para armazenar e navegar em relações complexas. Sendo assim, uma rede social se beneficia dessa estrutura para gerenciar e consultar conexões entre usuários.
II. Documentos - Bancos de dados orientados a documentos permitem a flexibilidade de estruturas de dados, sendo perfeitos para armazenar informações como descrições de produtos e avaliações de clientes que podem variar entre os registros.
III. Chave-Valor - Esse tipo de banco de dados é altamente eficiente para operações simples de gravação e recuperação de dados, o que é necessário em sistemas que processam grande quantidade de mensagens pequenas, como é comum em aplicações IoT.
IV. Colunas - Um banco de dados orientado a colunas otimiza a leitura e escrita de grandes volumes de dados estruturados de maneira tabular, permitindo alta performance em análises de dados que envolvem grandes conjuntos de colunas.",C,743429
tópico 0,Ingestão de dados em lote (batch),"Questão:
Considerando os procedimentos comuns de ingestão de dados em lote (batch) em um ambiente de Big Data, avalie as afirmativas a seguir e assinale a opção correta:

I. A ingestão de dados em lote é mais apropriada para cenários onde os dados estão disponíveis em intervalos de tempo predefinidos e não exigem processamento em tempo real.
II. Ferramentas como Apache Flume e Apache Kafka não são adequadas para a ingestão de dados em lote, pois são projetadas exclusivamente para o processamento de dados em tempo real (stream).
III. Hadoop Distributed File System (HDFS) é uma infraestrutura comum usada para armazenar grandes volumes de dados em lote, possibilitando seu processamento distribuído e paralelo.
IV. No contexto de ingestão de dados em lote, a idempotência não é uma preocupação, uma vez que os dados são processados em um único lote e não há risco de duplicidade no processamento.

A) Apenas as afirmativas I e III estão corretas.
B) Apenas as afirmativas II e IV estão corretas.
C) Apenas a afirmativa III está correta.
D) Apenas as afirmativas I, II e III estão corretas.
E) Todas as afirmativas estão corretas.

",A,"

Explicação dos itens:

I. Correta. De fato, a ingestão de dados em lote é ideal para cenários onde os dados são coletados e disponibilizados em intervalos regulares, sem a necessidade de processá-los em tempo real.

II. Incorreta. Enquanto Apache Kafka é versátil e frequentemente usado para processamento de dados em tempo real, também pode ser utilizado para a ingestão de dados em lote. Apache Flume é uma ferramenta projetada especificamente para a eficiente coleta, agregação e movimentação de grandes quantidades de dados em lote.

III. Correta. O Hadoop Distributed File System (HDFS) é um componente do ecossistema Hadoop e é amplamente utilizado para armazenar dados em lote, fornecendo altas taxas de transferência de dados e a capacidade de armazenar e processar dados distribuídos em diversos nós de um cluster.

IV. Incorreta. A idempotência é, de fato, uma consideração importante na ingestão de dados em lote, uma vez que é essencial garantir que a repetição do processamento de um lote de dados não resulte em duplicidade ou inconsistências nos dados armazenados. Esse é um aspecto crucial para manter a integridade dos dados em sistemas que possam enfrentar falhas ou retrabalho.",A,8934281
tópico 5,"Métricas de similaridade textual - similaridade do cosseno, distância euclidiana, similaridade de Jaccard, distância de Manhattan e coeficiente de Dice","Questão: Em processamento de linguagem natural, métricas de similaridade textual são fundamentais para comparar e analisar documentos textuais. Considere que dois vetores A e B representam dois textos diferentes após transformação em representações vetoriais por meio de TF-IDF. As seguintes métricas são aplicadas para determinar a similaridade ou distância entre os dois textos:

I. Similaridade do Cosseno: Mede o cosseno do ângulo entre os dois vetores, sendo 1 para vetores idênticos e 0 para vetores completamente ortogonais.

II. Distância Euclidiana: Mede a distância ""em linha reta"" entre os dois pontos no espaço multidimensional, com valores maiores indicando menor similaridade.

III. Similaridade de Jaccard: Calcula a interseção sobre a união dos elementos dos conjuntos representados pelos vetores, sendo mais adequada para comparação baseada em contagem de características comuns.

IV. Distância de Manhattan: Mede a soma das diferenças absolutas das coordenadas dos vetores, representando o somatório das distâncias em cada dimensão.

V. Coeficiente de Dice: Semelhante à Similaridade de Jaccard, porém utiliza o dobro da interseção sobre a soma do tamanho dos conjuntos, enfatizando a sobreposição entre eles.

Considerando o contexto descrito, qual destas métricas seria INAPROPRIADA para calcular a similaridade textual a partir dos vetores obtidos pelo método TF-IDF?

A) Similaridade do Cosseno
B) Distância Euclidiana
C) Similaridade de Jaccard
D) Distância de Manhattan
E) Coeficiente de Dice

",C,"

Explicação dos itens:

A) Similaridade do Cosseno é apropriada para calcular a similaridade entre vetores TF-IDF, uma vez que leva em consideração a orientação dos vetores e não a magnitude, sendo amplamente utilizada para este propósito.

B) Distância Euclidiana pode ser usada em contextos em que a magnitude dos vetores TF-IDF é relevante, contudo, tende a ser menos eficaz do que a similaridade do cosseno no contexto de similaridade textual, ainda assim não é inapropriada.

C) Similaridade de Jaccard é inapropriada pois é uma métrica baseada em conjuntos e não leva em conta a frequência de termos do texto, o que é um componente crítico dos vetores TF-IDF. Esta métrica é mais adequada para dados binários ou para casos em que as características são expressas como presença/ausência.

D) Distância de Manhattan pode ser aplicada mas, assim como a distância euclidiana, não é a escolha mais comum para dados textuais representados por vetores TF-IDF, embora ainda seja uma métrica válida de distância.

E) Coeficiente de Dice, assim como a Similaridade de Jaccard, é menos adequado para dados TF-IDF, pois ignora a ponderação das frequências de termos. Porém, não é considerado inapropriado da mesma maneira que a similaridade de Jaccard, que está diretamente ligada à contagem da presença e não da frequência de termos.",B,21287
tópico 3,Programação funcional,"Questão: Considere os conceitos fundamentais da programação funcional, um paradigma de programação que enfatiza a aplicação de funções, em contraste com a programação imperativa que enfatiza mudanças no estado do programa. Dado o seguinte fragmento de código em Haskell, identifique qual das afirmações abaixo é correta.

```haskell
quadrados :: [Int] -> [Int]
quadrados xs = map (\x -> x * x) xs
```

a) A função `quadrados` é um exemplo de função de alta ordem, pois retorna outra função como resultado.

b) A função `quadrados` utiliza a técnica de recursão para calcular o quadrado de cada elemento da lista.

c) A expressão lambda `(\x -> x * x)` é desnecessária, pois Haskell não suporta expressões lambda aninhadas.

d) A aplicação da função `map` na função `quadrados` é um exemplo de transparência referencial, garantindo que a função retornará o mesmo resultado para os mesmos valores de entrada.

e) A função `quadrados` viola o princípio de imutabilidade, uma vez que modifica a lista `xs` fornecida como entrada.

",D,"

- Item A: Incorreto pois uma função de alta ordem é aquela que pode receber outra função como argumento ou retornar uma função como resultado. Neste caso, a função `quadrados` recebe uma lista e retorna uma lista, não uma função.
- Item B: Incorreto porque a função `quadrados` não é recursiva; ela usa `map` para aplicar uma função diretamente a cada elemento da lista.
- Item C: Incorreto porque Haskell suporta expressões lambda e a expressão lambda dada é usada corretamente como argumento para a função `map`.
- Item D: Correto pois a aplicação de funções puras, como demonstrado pelo uso da função `map` com uma expressão lambda pura (que não tem efeitos colaterais), é um exemplo de transparência referencial. Ou seja, para um dado input, o output sempre será o mesmo, o que é um conceito central em programação funcional.
- Item E: Incorreto já que a função `quadrados` não modifica a lista `xs`; ela gera e retorna uma nova lista, mantendo a lista original inalterada, preservando assim o princípio de imutabilidade.",,3343494
tópico 2,Contexto de IA: Enriquecimento,"Questão:

A Fundação CESGRANRIO, em um de seus mais recentes exames, incluiu a seguinte pergunta sobre Inteligência Artificial (IA) e seu papel no processo de enriquecimento de dados:

""Em relação ao processo de enriquecimento de dados aplicado nos sistemas de Inteligência Artificial (IA), é correto afirmar que:

A) O enriquecimento de dados é uma técnica que diminui a necessidade de dados limpos e bem estruturados, pois a IA pode interpretar e corrigir automaticamente qualquer inconsistência.
B) O enriquecimento de dados é um processo irrelevante para IA, uma vez que algoritmos avançados conseguem obter resultados satisfatórios mesmo com conjuntos de dados limitados e de baixa qualidade.
C) O enriquecimento de dados envolve a adição de novos dados ou fontes de dados externas ao conjunto de dados primário, a fim de melhorar a qualidade dos insights gerados pelo modelo de IA.
D) O processo de enriquecimento de dados é exclusivamente manual e não pode ser otimizado ou automatizado por meio de técnicas de IA.
E) O enriquecimento de dados é uma prática que envolve a exclusão de grande parte dos dados existentes para que os algoritmos de IA trabalhem com um volume de dados mais gerenciável.""

",C,"

Explicação dos itens:

A) Item incorreto. O enriquecimento de dados não diminui a necessidade de dados de qualidade; pelo contrário, o propósito é melhorar um conjunto existente com informações adicionais relevantes para aprimorar a precisão dos modelos de IA.

B) Item incorreto. O enriquecimento de dados é muito relevante para a IA, pois contribui para que os modelos tenham um desempenho melhor, especialmente em situações onde os dados nativos são limitados ou de qualidade questionável.

C) Item correto. O enriquecimento de dados realmente envolve a adição de dados relevantes ao conjunto primário, o que pode incluir informações de diferentes fontes, auxiliando assim na melhoria da qualidade dos insights e na performance dos modelos de IA.

D) Item incorreto. O enriquecimento de dados pode ser tanto manual quanto automatizado, inclusive com o uso de IA para identificar e integrar novas fontes de informação de forma mais eficiente.

E) Item incorreto. A prática de enriquecimento de dados não é sobre excluir dados, mas sim sobre complementá-los com informações adicionais para enriquecer a análise que os algoritmos de IA realizarão.",C,206239
tópico 1,"Banco de dados relacional: SQL Server, PostgreSQL, MySQL","Questão: No contexto de banco de dados relacional, considerando o uso de SQL Server, PostgreSQL e MySQL, qual das seguintes afirmações é INCORRETA a respeito da implementação de Stored Procedures nos três sistemas de gerenciamento de banco de dados (SGBDs)?

A) No SQL Server, as Stored Procedures são compiladas e armazenadas no banco de dados, o que pode aumentar a performance em sua execução.

B) PostgreSQL suporta a criação de Stored Procedures em várias linguagens, incluindo SQL, PL/pgSQL e outras linguagens como Perl e Python.

C) Stored Procedures no MySQL são escritas usando exclusivamente a linguagem SQL e não suportam outras linguagens de programação.

D) Em todos os três SGBDs, as Stored Procedures podem ser utilizadas para encapsular uma lógica de negócio complexa, tornando a manutenção do código mais fácil e centralizada.

E) Tanto o PostgreSQL quanto o SQL Server oferecem a capacidade de definir variáveis temporárias dentro de Stored Procedures que existem durante o tempo de execução da procedure.

",C,"

Explicação dos itens:

A) Correta. No SQL Server, as Stored Procedures são compiladas e armazenadas, o que as torna mais rápidas na execução, pois o plano de execução pode ser reutilizado, diminuindo o overhead de compilação em chamadas subsequentes.

B) Correta. O PostgreSQL é conhecido por sua extensibilidade e suporta várias linguagens para Stored Procedures, que vão além do SQL, como PL/pgSQL, que é a linguagem padrão de procedural, e suporte para linguagens como Perl e Python através de extensões.

C) Incorreta. Embora SQL seja a principal linguagem para escrever Stored Procedures no MySQL, não é verdade que MySQL suporta exclusivamente SQL. A partir de MySQL 5.0, é possível escrever Stored Procedures em SQL, mas não há suporte nativo para outras linguagens de programação como acontece no PostgreSQL, que de fato possui essa capacidade.

D) Correta. Em todos os SGBDs mencionados, é uma prática comum utilizar Stored Procedures para encapsular lógicas de negócios complexas. Isso permite que operações reutilizáveis e possivelmente complexas sejam mantidas em um único local, facilitando a manutenção e garantindo consistência nos processos de negócio.

E) Correta. Tanto PostgreSQL quanto SQL Server permitem a definição de variáveis temporárias dentro de Stored Procedures. Essas variáveis temporárias existem durante o tempo de execução de uma Stored Procedure e são descartadas assim que a execução termina.",A,8362169
tópico 0,Conceitos de processamento massivo e paralelo,"Questão:
A eficiência no processamento de grandes volumes de dados é um requisito crucial para muitas aplicações computacionais, e o processamento massivo e paralelo constitui uma abordagem fundamental para atingir tal objetivo. Com relação aos conceitos de processamento massivo e paralelo, analise as afirmativas abaixo e assinale a opção correta:

I. Em um sistema de processamento paralelo, múltiplas CPUs trabalham em conjunto para executar diferentes partes de um mesmo programa simultaneamente, dividindo tarefas e comunicando-se regularmente para sincronizar o progresso.

II. A técnica de MapReduce, utilizada frequentemente em frameworks como Apache Hadoop, baseia-se na divisão sequencial de tarefas e na consolidação individual de resultados, funcionando melhor em single-core CPUs para processamento intensivo de dados.

III. Processamento massivo em empresas como Google e Facebook envolve não apenas a capacidade de hardware para operar com grandes quantidades de dados, mas também o uso de algoritmos complexos de aprendizado de máquina e inteligência artificial para criar insights a partir dos dados processados.

IV. O conceito de NoSQL surge como uma resposta ao desafio do processamento massivo de dados, oferecendo esquemas flexíveis, suporte a distribuição horizontal e a capacidade de manusear grandes volumes de dados desestruturados ou semi-estruturados, o que é menos eficiente em bancos de dados relacionais tradicionais.

É correto o que se afirma apenas nas afirmativas:

A) I e II.
B) II e III.
C) I e IV.
D) I, III e IV.
E) Todas as afirmativas estão corretas.

",C," 

Explicação dos itens:

I. Correta. A descrição está alinhada com o princípio fundamental do processamento paralelo, onde várias CPUs ou cores trabalham em paralelo para melhorar o desempenho do processamento de dados, o que inclui dividir tarefas complexas e sincronização entre processos.

II. Incorreta. A técnica de MapReduce é projetada para processamento paralelo e distribuído de grandes conjuntos de dados. Ela funciona justamente utilizando vários nós de processamento que podem não ser single-core CPUs. O conceito de MapReduce envolve duas funções principais: 'Map', que processa e transforma os dados de entrada em pares chave/valor intermediários, e 'Reduce', que consolida os resultados. Portanto, a afirmativa II é falsa.

III. Correta. Empresas que lidam com grandes volumes de dados realmente utilizam algoritmos avançados de aprendizado de máquina e inteligência artificial para extrair conhecimento dos dados que processam, o que é parte crucial do processamento massivo.

IV. Correta. O modelo NoSQL é uma resposta aos desafios do processamento massivo de dados e tem como características o suporte a escalabilidade horizontal e a habilidade de armazenar e gerenciar grandes quantidades de dados não estruturados ou semi-estruturados, algo que os bancos de dados relacionais tradicionais têm dificuldades em fazer eficientemente.",E,8309578
tópico 2,Tratamento de dados ausentes,"Questão:
A análise de dados é fundamental para a tomada de decisões baseadas em evidências. No entanto, muitas vezes os conjuntos de dados apresentam o problema de dados ausentes, o que pode levar a distorções nos resultados e conclusões das análises. Supondo que você está lidando com um conjunto de dados que apresenta valores ausentes de maneira não completamente aleatória (Missing Not At Random - MNAR), qual das seguintes técnicas NÃO é apropriada para tratar esses dados ausentes?

A) Imputação pela média ou mediana.
B) Imputação por regressão múltipla.
C) Uso do método Maximum Likelihood Estimation (MLE).
D) Utilização de um modelo de equações estruturais para modelar tanto os mecanismos de missingness quanto as relações entre as variáveis.
E) Exclusão completa de casos (listwise deletion).

",E,"

A) Imputação pela média ou mediana é uma técnica simples de imputação que pode ser usada como uma tentativa inicial de tratamento, mesmo em situações MNAR, apesar de não ser a mais indicada, pois ignora a possibilidade de haver um padrão nos dados ausentes.

B) Imputação por regressão múltipla leva em conta as relações lineares entre as variáveis e pode ser uma opção no tratamento de dados MNAR, mas deve ser usada com cautela, considerando os potenciais vieses.

C) O Método de Máxima Verossimilhança (Maximum Likelihood Estimation - MLE) tenta encontrar os parâmetros que tornam a probabilidade de observar os dados coletados o mais alta possível e pode ser uma forma eficiente de lidar com dados MNAR quando aplicado corretamente.

D) O uso de modelo de equações estruturais permite incorporar o processo de faltantes como parte do modelo, ajudando a lidar com os dados MNAR ao modelar adequadamente a estrutura de dependência tanto do mecanismo de ausência quanto das variáveis observadas.

E) A exclusão completa de casos (listwise deletion) descarta qualquer caso com pelo menos um valor ausente, o que pode levar a uma grande perda de informações e a distorções significativas em análises MNAR. Este método normalmente só é considerado apropriado para situações em que os dados são MCAR (Missing Completely At Random) e, portanto, pode não ser adequado para MNAR, o que a torna a opção incorreta para a pergunta dada.",E,3716160
tópico 3,"Visualização de dados ggplot, matplotlib","Questão:
A visualização de dados é uma ferramenta essencial na análise de grandes conjuntos de dados, permitindo aos analistas identificar tendências, padrões e outliers de forma rápida e intuitiva. Duas das bibliotecas mais populares para visualização de dados em linguagem de programação são o ggplot2 em R e o Matplotlib em Python.

Considerando as bibliotecas mencionadas, avalie as seguintes afirmativas e assinale a opção correta.

I. O ggplot2 é baseado na Gramática de Gráficos, que permite a construção de gráficos de acordo com uma sintaxe consistente, onde se define um conjunto de elementos e suas propriedades estéticas.

II. O Matplotlib é uma biblioteca do Python que permite a criação de gráficos em duas dimensões de alta qualidade, e suas funções podem ser estendidas através da biblioteca Seaborn, que oferece uma interface de alto nível para gráficos estatísticos.

III. O ggplot2 e o Matplotlib permitem facilmente a criação de gráficos tridimensionais complexos, como scatterplots 3D, sem a necessidade de qualquer plugin ou biblioteca adicional.

IV. Tanto o ggplot2 quanto o Matplotlib possuem sistemas de temas que permitem a personalização visual dos gráficos, no entanto, o ggplot2 não permite a personalização total do gráfico sem uma compreensão da sua gramática subjacente.

Alternativas:

A) Apenas as afirmativas I e II estão corretas.

B) Apenas as afirmativas I, II e III estão corretas.

C) Apenas as afirmativas I e IV estão corretas.

D) Apenas as afirmativas II e IV estão corretas.

E) Todas as afirmativas estão corretas.

",A,"

Explicação dos itens:

I. Correto. A ggplot2 segue a filosofia de Gramática de Gráficos, onde cada componente do gráfico é mapeado seguindo uma sintaxe lógica e consistente, o que a torna bastante poderosa e flexível.

II. Correto. Matplotlib é a biblioteca base de visualização no Python e permite a execução de diversas funções gráficas. Seaborn é de fato uma biblioteca baseada no Matplotlib que fornece uma abstração de alto nível para gráficos estatísticos.

III. Incorreto. Enquanto tanto ggplot2 quanto Matplotlib suportam gráficos tridimensionais, eles não são tão facilmente gerados quanto os gráficos bidimensionais e frequentemente requerem módulos adicionais, como o `plotly` para ggplot2 e o `mplot3d` toolkit para Matplotlib.

IV. Incorreto. Esta afirmação é falsa porque embora o ggplot2 tenha um sistema de temas que permite uma boa personalização dos gráficos, é possível sim alterar quase todos os aspectos de um gráfico no ggplot2 sem uma compreensão profunda de toda sua gramática, ainda que isso possa requerer algumas pesquisas adicionais ou experiência para personalizações complexas. No Matplotlib, também é possível personalizar quase todos os elementos de um gráfico, muitas vezes com mais facilidade em comparação ao processamento de temas dentro da gramática do ggplot2, principalmente para usuários novos ou ocasionais.",B,1014614
tópico 2,Algoritmos fuzzy matching e stemming,"Questão:
Considere que um cientista de dados está trabalhando em um sistema de busca e recuperação de informações que necessita ser otimizado para entender a intenção do usuário mesmo em casos de grafias imprecisas ou consulta por palavras em diferentes formas gramaticais. O cientista pretende implementar técnicas de algoritmos de 'fuzzy matching' e 'stemming'. Qual das seguintes alternativas melhor descreve a função específica de cada uma dessas técnicas e como elas podem contribuir para o sistema?

A) 'Fuzzy matching' é uma técnica que identifica palavras-chave exatas no documento, enquanto 'stemming' é usada para expandir a busca para incluir todas as palavras com grafias erradas ou variações gramaticais.

B) 'Fuzzy matching' envolve encontrar correspondências aproximadas, o que permite que o sistema lide com erros de grafia ou variações fonéticas, e 'stemming' é um processo que reduz as palavras às suas raízes ou formas básicas para padronizar variações morfológicas.

C) 'Fuzzy matching' e 'stemming' são sinônimos e ambos referem-se ao processo de buscar correspondência perfeita entre palavras-chave e conteúdo textual.

D) 'Stemming' trata-se de uma técnica para encontrar correspondências precisas em grandes volumes de texto, enquanto 'fuzzy matching' reduz as palavras ao seu radical, facilitando a consistência nas respostas do sistema.

E) 'Fuzzy matching' é um método de indexação de documentos que ignora as stop words, e 'stemming' é uma forma de criptografia de palavras que melhora a segurança do sistema de busca.

",B,"

Explicação dos itens:
A) Incorreto, pois 'fuzzy matching' não busca palavras-chave exatas, e 'stemming' não é usado para variações de grafias erradas, mas sim para reduzir as palavras a suas raízes ou formas básicas.
B) Correto, 'fuzzy matching' permite ao sistema identificar palavras que não são idênticas, mas semelhantes o suficiente para serem consideradas uma correspondência, e 'stemming' reduz as palavras a um formato padrão para que diferentes formas da mesma palavra sejam reconhecidas como relacionadas.
C) Incorreto, pois 'fuzzy matching' e 'stemming' têm funções distintas no processamento de linguagem natural e não são sinônimos.
D) Incorreto, 'stemming' não é uma técnica para encontrar correspondências precisas, mas sim para reduzir palavras à sua forma base. 'Fuzzy matching' não reduz palavras ao seu radical, mas busca aproximações.
E) Incorreto, 'fuzzy matching' não está relacionado à indexação de documentos e nem ignora stop words, e 'stemming' não tem relação com criptografia, mas sim com o processamento de palavras para a busca e correspondência.",B,6718436
tópico 5,Técnicas de classificação: Naive Bayes; Árvores de decisão (algoritmos ID3 e C4.5); Florestas aleatórias (random forest); Máquinas de vetores de suporte (SVM – support vector machines); K vizinhos mais próximos (KNN – K-nearest neighbours),"Questão: Sobre as técnicas de classificação em Aprendizado de Máquina, é INCORRETO afirmar que:

A) O classificador Naive Bayes assume independência condicional entre os preditores, o que muitas vezes simplifica os cálculos, apesar de esta suposição nem sempre ser verdadeira nos dados reais.

B) As árvores de decisão utilizando os algoritmos ID3 e C4.5 geram árvores binárias, onde cada nó interno representa um teste de um atributo e cada folha representa uma classe.

C) Florestas aleatórias (random forest) são um conjunto de árvores de decisão que são treinadas com subconjuntos dos dados e características, visando aumentar a diversidade entre as árvores e, por consequência, melhorar a generalização do modelo.

D) Máquinas de vetores de suporte (SVM) procuram encontrar o hiperplano que maximiza a margem entre as classes, sendo particularmente eficazes em espaços de alta dimensão.

E) O método dos K vizinhos mais próximos (KNN) é sensível ao desbalanceamento de classes e à presença de atributos irrelevantes ou redundantes, podendo requerer uma etapa de seleção ou ponderação de atributos para melhorar seu desempenho.

",B,"

Explicação dos itens:

A) Correto. O classificador Naive Bayes realmente faz essa suposição de independência. Esse é um dos motivos pelos quais é chamado de ""Naive"" ou ingênuo.

B) Incorreto. Os algoritmos ID3 e C4.5 geram árvores que não são necessariamente binárias. Os nós podem ter mais de duas saídas. Isto é, cada decisão não é limitada a uma separação binária, podendo dividir os dados com base em múltiplos valores de um único atributo.

C) Correto. As florestas aleatórias são uma técnica de ensemble que cria um conjunto de árvores de decisão durante o treinamento. Isso é feito para melhorar a robustez e precisão dos resultados de previsão.

D) Correto. As SVMs são utilizadas para classificar dados encontrando um hiperplano ótimo que separa os dados em classes, maximizando a margem entre diferentes classes de dados.

E) Correto. De fato, o KNN é um algoritmo que pode ser bastante afetado pela presença de atributos irrelevantes ou redundantes, assim como pela distribuição desigual das classes no conjunto de treinamento, o que pode prejudicar sua precisão e exigir pré-processamento dos dados.",B,3367554
tópico 3,"Classes de objetos e suas propriedades (vetores, listas, data.frames)","Questão:

A linguagem de programação R é amplamente utilizada em estatística e análise de dados, sendo frequentemente escolhida por sua capacidade de manipular e analisar uma ampla variedade de estruturas de dados. Com relação às classes de objetos em R, como vetores, listas e data.frames, avalie as afirmativas a seguir e escolha a opção correta:

I. Vetores em R podem armazenar elementos de diferentes tipos, como números, strings e valores lógicos, em uma única estrutura.

II. Listas são objetos heterogêneos que podem conter outros objetos de R, como vetores, listas e até mesmo data.frames, diferentemente de vetores que são homogêneos.

III. Data.frames são estruturas de dados bidimensionais onde cada coluna pode conter tipos de dados diferentes, mas cada linha deve conter dados do mesmo tipo para todas as suas colunas.

IV. As matrizes em R são uma extensão dos vetores e podem armazenar dados bidimensionais de diferentes tipos em cada entrada.

Assinale a alternativa correta sobre as propriedades das classes de objetos em R:

A) Somente as afirmativas II e III estão corretas.

B) Somente as afirmativas I e II estão corretas.

C) Todas as afirmativas estão corretas.

D) Somente as afirmativas II e IV estão corretas.

E) Somente a afirmativa II está correta.

",A,"

Explicação dos itens:

I. Esta afirmativa é incorreta porque vetores em R são homogêneos e só podem conter elementos de um mesmo tipo. Se elementos de diferentes tipos são combinados em um vetor, haverá uma coerção para que todos os elementos sejam do mesmo tipo.

II. Esta afirmativa é correta porque listas em R podem conter diferentes tipos de elementos e estruturas, incluindo vetores, outras listas e data.frames.

III. Esta afirmativa é correta pois um data.frame em R é uma coleção de vetores de mesmo comprimento, onde cada vetor (coluna) pode conter um tipo diferente de dado, mas todas as colunas seguem a mesma lógica de organização por linha.

IV. Esta afirmativa é incorreta. As matrizes em R não podem armazenar dados de diferentes tipos; elas seguem a mesma regra dos vetores, sendo estruturas homogêneas. Se qualquer elemento de uma matriz possuir um tipo diferente, todos serão convertidos (coerção) para serem compatíveis entre si.",C,725214
tópico 2,Contexto de IA: Enriquecimento,"Questão: A aplicação do conceito de enriquecimento no contexto da Inteligência Artificial (IA) é uma operação crucial no processamento e análise de grandes conjuntos de dados, contribuindo para a melhoria da acurácia dos modelos de aprendizado de máquina. Qual das seguintes alternativas descreve melhor o enriquecimento de dados no contexto da IA?

A) A redução da dimensão dos dados através de métodos como Principal Component Analysis (PCA), visando diminuir a complexidade computacional dos modelos.
B) O processo de limpar e organizar os dados removendo informações duplicadas ou inconsistentes, sem adicionar novas informações externas.
C) A transformação de dados brutos em um formato estruturado mais adequado para a modelagem, mas sem incluir dados externos ao conjunto original.
D) A adição de novas variáveis ou características provenientes de fontes externas, que podem ser relevantes para o modelo, aumentando assim seu potencial preditivo.
E) A codificação de variáveis categóricas em valores numéricos utilizando técnicas como One-Hot Encoding, mantendo-se estritamente dentro do conjunto de dados original.

",D,"

Explicação dos itens:

A) Incorreta. PCA é uma técnica de redução de dimensionalidade e não se caracteriza como enriquecimento, pois busca simplificar os dados sem adicionar novos contextos ou variáveis.
B) Incorreta. A limpeza de dados é importante, mas não representa enriquecimento, pois não expande o conjunto de dados com informações adicionais, apenas organiza o existente.
C) Incorreta. Embora a estruturação de dados seja um passo importante antes da modelagem, o enriquecimento implica a introdução de novos dados ou características, não apenas a transformação do formato dos dados já disponíveis.
D) Correta. O enriquecimento de dados no contexto de IA geralmente envolve a adição de novas informações ou características de fontes externas ao conjunto de dados original, proporcionando assim um contexto mais rico para a construção de modelos preditivos.
E) Incorreta. A codificação de variáveis categóricas, como One-Hot Encoding, é uma técnica de pré-processamento que adapta os dados para técnicas de aprendizado de máquina, mas não é um enriquecimento, já que não adiciona dados externos.",D,2689611
tópico 0,Ingestão de dados em lote (batch),"Questão:
A ingestão de dados em lote, ou batch data ingestion, é uma técnica comumente utilizada em ambientes de Big Data para o processamento de grandes volumes de informações que não exigem análise em tempo real. Considerando as estratégias de implementação de sistemas de batch data ingestion em um Data Lake corporativo, analise os itens a seguir e identifique qual opção apresenta somente característica(s) verdadeira(s) associada(s) a este tipo de ingestão:

A) Os dados são processados em micro-batches, permitindo análises quase em tempo real e garantindo uma menor latência na disponibilização dos dados.
B) A ingestão em lote geralmente ocorre em intervalos de tempo programados ou desencadeada por eventos específicos, e não de forma contínua.
C) Este tipo de ingestão não permite a manipulação de diferentes formatos de dados, limitando-se a dados estruturados como tabelas em bancos de dados relacionais.
D) A ingestão em lote é ideal para cenários onde é necessário processamento em tempo real, como monitoramento de transações financeiras online.
E) A escalabilidade e a manutenção são aspectos complicados na ingestão de dados em lote devido ao alto nível de intervenção manual requerido.

",B,"

Explicações dos itens:

A) Incorreto. O processamento em micro-batches é associado com sistemas de processamento de stream, que permitem análises quase em tempo real, enquanto a ingestão em lote lida com grandes volumes de dados de uma vez, geralmente com latência mais alta.

B) Correto. A ingestão em lote acontece em janelas predefinidas de tempo ou como resposta a eventos específicos, diferentemente do processamento de fluxo contínuo que caracteriza a ingestão de dados em tempo real. Este intervalo de processamento pode ser horário, diário ou outro intervalo definido conforme a necessidade do negócio.

C) Incorreto. Sistemas de ingestão de dados em lote podem manipular diversos formatos de dados, incluindo dados estruturados, semi-estruturados e não estruturados. A flexibilidade para lidar com diferentes formatos é uma das vantagens deste tipo de sistema.

D) Incorreto. A ingestão em lote não é o ideal para processamento em tempo real, que é necessário em casos como monitoramento de transações onde a ação imediata é crítica. O processamento em lote possui latência inerente devido ao modo como os dados são coletados e processados.

E) Incorreto. Embora a ingestão em lote possa ter desafios de escalabilidade e manutenção, esses não são aspectos necessariamente complicados. Muitas soluções de Big Data oferecem ferramentas para automatizar e facilitar a escalabilidade e manutenção do sistema de ingestão. Além disso, a afirmação de que requer alto nível de intervenção manual é muito genérica e não necessariamente verdadeira com o uso de ferramentas e plataformas modernas de gerenciamento de dados.",B,4359037
tópico 4,"Diagramas causais: gráficos acíclicos dirigidos; variáveis confundidoras, colisoras e de mediação","Questão:
Em estudos estatísticos e de causalidade, especialmente na área de epidemiologia, os Diagramas Acíclicos Dirigidos (DAGs) são utilizados para representar as relações causais entre diversas variáveis. É de suma importância distinguir os diferentes papéis que as variáveis podem desempenhar em um DAG. Considere as seguintes definições:

I. Variáveis Confundidoras: são variáveis que afetam tanto a variável de exposição quanto o desfecho, podendo criar uma associação espúria ou ocultar uma associação verdadeira entre eles.

II. Variáveis Colisoras: Quando duas variáveis independentes influenciam uma terceira variável, que é o colisor, o controle estatístico deste pode induzir uma associação artificial entre as variáveis que de outra forma seriam independentes.

III. Variáveis de Mediação: Uma variável mediadora é aquela que transmite o efeito de uma variável de exposição para a variável de desfecho.

Dado um DAG em que há uma trajetória causal da variável A para B, e de B para C, e também uma trajetória direta de A para C. Se um pesquisador está interessado em estabelecer a relação causal entre A e C, controlando B, qual seria o papel de B nessa análise?

A) B é um confundidor e deve ser controlado para estabelecer a relação causal entre A e C.
B) B é um colisor e seu controle pode induzir uma associação artificial entre A e C.
C) B é uma variável de mediação e seu controle pode bloquear parte do efeito causal de A sobre C.
D) B é uma variável independente e o seu controle não altera a relação causal entre A e C.
E) B é uma variável colidora e deve ser controlada para revelar a relação causal verdadeira entre A e C.

",C,"

A alternativa correta é a letra C. B atua como uma variável de mediação entre A e C, onde o efeito de A em C passa por B. Controlar B significa que o pesquisador está bloqueando o caminho mediador, o que poderia impedir a observação do efeito total de A em C, incluindo o efeito indireto através de B. O conceito de mediação é essencial em análises causais para compreender os mecanismos pelos quais uma exposição afeta o desfecho. A alternativa A está incorreta porque um confundidor afeta tanto a exposição quanto o desfecho, mas não faz parte do caminho causal entre eles. A alternativa B está incorreta porque o colisor é uma variável que é afetada por duas outras variáveis, e controlá-la poderia criar uma associação espúria; que não é o caso apresentado na pergunta. A alternativa D está incorreta porque B não é independente neste contexto, já que está no caminho entre A e C. A alternativa E também está incorreta pois confunde as definições de colisor e mediador.",D,3085055
tópico 4,"Diagramas causais: gráficos acíclicos dirigidos; variáveis confundidoras, colisoras e de mediação","Questão: Em um estudo epidemiológico sobre a relação entre a exposição ao fumo passivo (F) e o desenvolvimento de doenças cardíacas (D), os pesquisadores identificam uma variável potencialmente relacionada a ambas, o nível de estresse (E). Um diagrama causal é esboçado para representar as possíveis relações entre essas variáveis, levantando a hipótese de que o estresse possa ser uma variável confundidora, colisora ou de mediação. Com base nos princípios dos diagramas causais e dos conceitos associados às variáveis em estudos epidemiológicos, qual das seguintes estruturas gráficas acíclicas dirigidas (DAGs) representa corretamente o nível de estresse como uma variável colisora no contexto da relação entre fumo passivo e doenças cardíacas?

A) F → D ← E
B) F ← E → D
C) F → E → D
D) E → F → D
E) F → D; E → D

",B,"

Explicação dos itens:

A) Representa o nível de estresse como uma variável confundidora, pois está causando tanto a exposição ao fumo passivo quanto as doenças cardíacas, sendo uma fonte comum de variação para ambas.

B) Essa opção é correta pois mostra o estresse como uma variável colisora ou efeito colisor, onde a exposição ao fumo passivo e o desenvolvimento de doenças cardíacas convergem para o estresse, mas não há um caminho direto entre F e D.

C) Demonstra o estresse como uma variável de mediação, indicando que a exposição ao fumo passivo afeta o nível de estresse, que por sua vez influencia o desenvolvimento de doenças cardíacas, representando uma cadeia causal.

D) Sugere que o estresse causa exposição ao fumo passivo, que por sua vez causa doenças cardíacas, não incorporando a relação de colisão entre as variáveis

E) Representa uma confusão parcial, onde o estresse é um confundidor apenas para a relação entre doenças cardíacas e ele próprio, mas não entre fumo passivo e doenças cardíacas.",D,8808143
tópico 0,Armazenamento de big data,"Questão:
Em um cenário de big data, onde o volume, a variedade e a velocidade dos dados são significativamente elevados, a escolha do sistema de armazenamento é fundamental para garantir a eficiência e eficácia no processamento e análise desses dados. Considerando os sistemas de armazenamento mais comuns nesse contexto, analise as seguintes afirmativas sobre suas características e aplicações:

I. HDFS (Hadoop Distributed File System) é uma solução de armazenamento que divide os dados em blocos distribuídos em vários nós, oferecendo alta disponibilidade e escalabilidade horizontal, sendo particularmente útil em ambientes que processam grandes quantidades de dados não estruturados.

II. NoSQL é um tipo de banco de dados que pode tratar grandes volumes de dados com esquemas dinâmicos, sendo adequado para dados semi-estruturados e não estruturados, mas com a desvantagem de não suportar transações ACID (Atomicidade, Consistência, Isolamento e Durabilidade).

III. Sistemas de armazenamento baseados em armazenamento em nuvem, como Amazon S3 e Google Cloud Storage, oferecem serviços de armazenamento de objetos com durabilidade e disponibilidade elevadas, mas podem apresentar latências mais altas para acessos frequentes e pequenas leituras/gravações comparativamente aos sistemas locais.

Está(ão) correta(s) a(s) afirmativa(s):

A) I apenas.
B) II apenas.
C) I e III apenas.
D) I, II e III.
E) II e III apenas.

",C,"

Explicação:

I. Correta. O HDFS é um sistema de arquivos distribuído que permite o armazenamento de grandes volumes de dados em clusters de computadores. Ele foi desenhado para ser robusto a falhas, mantendo o sistema altamente disponível e escalável horizontalmente, o que o torna ideal para processar grandes quantidades de dados não estruturados.

II. Incorreta. Embora os bancos de dados NoSQL sejam projetados para gerenciar grandes volumes de dados e tenham esquemas flexíveis para se adaptar a dados semi-estruturados e não estruturados, a afirmativa de que eles não suportam transações ACID como uma desvantagem generalizada é incorreta. Alguns bancos de dados NoSQL, como MongoDB e Apache Cassandra, têm se esforçado para oferecer suporte a transações ACID ou aspectos dessas transações.

III. Correta. Os serviços de armazenamento em nuvem como Amazon S3 e Google Cloud Storage fornecem armazenamento de objetos altamente durável e disponível. No entanto, devido à natureza da armazenagem em nuvem, acessos frequentes e operações de pequenas leituras/gravações podem experimentar latências mais altas do que operações em sistemas de armazenamento local, que não estão sujeitas à latência de rede.",A,3978768
tópico 0,Ingestão de dados em streaming,"Questão:
A ingestão de dados em streaming é um processo fundamental no cenário de big data, onde o fluxo contínuo de dados é processado e analisado em tempo real. Uma empresa de telecomunicações deseja implementar uma solução de ingestão de dados para monitorar e analisar chamadas telefônicas em tempo real. Dentre as seguintes alternativas, qual representa a melhor escolha de tecnologia para lidar com grandes volumes de dados em streaming, oferecendo alta disponibilidade, escalabilidade e baixa latência?

A) Base de dados relacional tradicional, otimizada com índices e clusters.
B) Hadoop Distributed File System (HDFS) com processamento batch periódico.
C) Apache Kafka combinado com Spark Streaming para processamento em tempo real.
D) Sistema de arquivos local (como ext4 ou NTFS) com scripts cron para processamento periódico.
E) Base de dados NoSQL orientada a colunas, como o Apache Cassandra, sem um sistema de processamento de stream.

",C,"

Explicação dos itens:

A) Base de dados relacional tradicional não é adequada para processamento de dados em streaming devido à sua estrutura rígida e escalabilidade limitada.

B) Hadoop Distributed File System (HDFS) é otimizado para processamento batch e não para processamento em tempo real, o que torna essa escolha inadequada para a necessidade de processamento de streaming em tempo real.

C) Apache Kafka é um sistema de mensageria distribuído projetado para lidar com altos volumes de dados em streaming, enquanto o Spark Streaming fornece capacidades de processamento em tempo real. A combinação dos dois oferece uma solução robusta para os requisitos da empresa.

D) Sistema de arquivos local com scripts cron é uma abordagem simplista e não escalável, inadequada para processamento em tempo real e grandes volumes de dados.

E) Bases de dados NoSQL orientadas a colunas são efetivas para armazenamentos de grandes volumes de dados com alta performance de leitura e gravação, mas, por si só, sem um sistema de processamento de stream, não seria suficiente para atender aos requisitos de processamento em tempo real.",C,9217129
tópico 1,Álgebra relacional e SQL (padrão ANSI),"Questão: Considere duas tabelas de um banco de dados relacional, Cliente (CodCliente, NomeCliente, Email) e Pedido (NumPedido, DataPedido, CodCliente), onde Cliente.CodCliente é chave primária para a tabela Cliente e Pedido.CodCliente é chave estrangeira que referencia Cliente.CodCliente. Pretende-se realizar uma consulta SQL para listar todos os clientes que fizeram mais de três pedidos no ano de 2022. Qual das seguintes alternativas representa corretamente essa consulta em padrão ANSI SQL?

A) SELECT NomeCliente FROM Cliente WHERE CodCliente IN (SELECT CodCliente FROM Pedido GROUP BY CodCliente HAVING COUNT(NumPedido) > 3 AND YEAR(DataPedido) = 2022);

B) SELECT NomeCliente FROM Cliente, Pedido WHERE Cliente.CodCliente = Pedido.CodCliente AND COUNT(Pedido.NumPedido) > 3 AND YEAR(Pedido.DataPedido) = 2022 GROUP BY Cliente.NomeCliente;

C) SELECT NomeCliente FROM Cliente LEFT JOIN Pedido ON Cliente.CodCliente = Pedido.CodCliente WHERE COUNT(Pedido.NumPedido) > 3 AND YEAR(Pedido.DataPedido) = 2022 GROUP BY Cliente.NomeCliente;

D) SELECT NomeCliente FROM Cliente JOIN (SELECT CodCliente FROM Pedido WHERE YEAR(DataPedido) = 2022 GROUP BY CodCliente HAVING COUNT(NumPedido) > 3) AS Pedidos2022 ON Cliente.CodCliente = Pedidos2022.CodCliente;

E) SELECT NomeCliente FROM Cliente INNER JOIN Pedido ON Cliente.CodCliente = Pedido.CodCliente GROUP BY Cliente.NomeCliente HAVING COUNT(Pedido.NumPedido) > 3 AND YEAR(Pedido.DataPedido) = 2022;

",D," 

Explicação dos itens:

A) Incorreto. O sub-select nesta consulta não aplica o filtro do ano diretamente ao agrupamento, sendo aplicado somente depois de realizar a contagem, o que pode incluir pedidos de outros anos.

B) Incorreto. A consulta está utilizando uma junção implícita entre as tabelas Cliente e Pedido (produto cartesiano) e também coloca a função de agregação COUNT no WHERE, o que não é permitido. As funções de agregação devem ser utilizadas no HAVING após um GROUP BY.

C) Incorreto. Similar ao item B, ela tenta utilizar a função de agregação COUNT no WHERE, que deveria estar no HAVING. Além disso, a junção LEFT JOIN não é necessária, já que precisamos apenas de clientes com pedidos.

D) Correto. Esta consulta utiliza uma sub-query que primeiro filtra os pedidos do ano de 2022 e agrupa os pedidos por cliente, contando o número de pedidos. Somente então são selecionados os clientes que possuem mais de três pedidos neste ano. A junção é realizada entre a tabela Cliente e o resultado da sub-consulta, garantindo assim que apenas os clientes com mais de três pedidos em 2022 sejam listados.

E) Incorreto. Ainda que essa consulta realize a junção correta das tabelas e o agrupamento, a condição do ano em YEAR(Pedido.DataPedido) = 2022 está localizada incorretamente no HAVING quando deveria estar no WHERE, pois se refere a uma condição de linha e não de agrupamento.",E,3748637
tópico 1,"Banco de dados relacional: SQL Server, PostgreSQL, MySQL","Questão: Em um cenário de alta demanda por performance em uma aplicação de banco de dados relacional que envolve o SQL Server, PostgreSQL e MySQL, um DBA precisa escolher uma estratégia para implementar índices eficientes. Considerando as características de índices de cada sistema gerenciador de banco de dados (SGBD), avalie as seguintes afirmações:

I. O índice ""Clustered"" no SQL Server define a ordenação física dos dados na tabela, e uma tabela pode ter apenas um índice desse tipo.
II. O PostgreSQL utiliza o método de índice B-tree por padrão, mas suporta uma variedade de tipos de índices, incluindo hash, GiST, SP-GiST, GIN e BRIN.
III. O MySQL possui o índice FULLTEXT, que é ideal para realizar buscas rápidas em colunas que armazenam grandes quantidades de texto, mas não é suportado em tabelas com o motor de armazenamento InnoDB.

Assinale a alternativa que indica todas as afirmações verdadeiras.

A) Apenas I.
B) Apenas II.
C) I e II.
D) I e III.
E) II e III.

",C," 
A alternativa correta é a letra C.

I. Verdadeiro. No SQL Server, um índice ""Clustered"" altera a ordem física de armazenamento dos dados em uma tabela e uma tabela pode possuir somente um desses índices, pois ele define a ordenação dos registros.

II. Verdadeiro. O PostgreSQL oferece diversas opções de índices. O método B-tree é o padrão, e os outros tipos (hash, GiST, SP-GiST, GIN e BRIN) são usados para casos específicos onde podem oferecer melhor performance.

III. Falso. No MySQL, é verdade que o índice FULLTEXT é usado para indexar colunas de texto para pesquisas rápidas em texto. No entanto, a afirmação está desatualizada. Embora nas versões iniciais o índice FULLTEXT não fosse suportado no motor InnoDB, desde o MySQL 5.6 o motor InnoDB também suporta índices FULLTEXT.",B,9066212
tópico 2,Matching,"Questão:
De acordo com a teoria dos grafos, o problema de Matching, também conhecido como emparelhamento ou casamento, pode ser aplicado em diversas situações práticas. Seja G = (V, E) um grafo bipartido, com conjuntos de vértices V1 e V2, onde |V1| = |V2| = n, e E o conjunto de arestas que conectam vértices de V1 a vértices de V2. Considerando um Matching M perfeito neste grafo, qual das seguintes afirmações é verdadeira?

A) A cardinalidade de M é igual a n/2.
B) Cada vértice de V1 é conectado por M a exatamente dois vértices de V2.
C) É garantido que exista um ciclo Hamiltoniano em G.
D) M contém todas as arestas do grafo G.
E) Nenhum vértice de V1 compartilha a mesma aresta em M com outro vértice de V1.

",E,"

Explicação dos itens:

A) A cardinalidade de M é igual a n/2. - Incorreta, pois em um Matching perfeito, todos os vértices são emparelhados, o que significa que a cardinalidade de M seria n, e não n/2.

B) Cada vértice de V1 é conectado por M a exatamente dois vértices de V2. - Incorreta, cada vértice em V1 é pareado com exatamente um vértice de V2 em um Matching perfeito.

C) É garantido que exista um ciclo Hamiltoniano em G. - Incorreta, a existência de um Matching perfeito não garante um ciclo Hamiltoniano, que é um ciclo que passa por todos os vértices uma única vez sem repetição.

D) M contém todas as arestas do grafo G. - Incorreta, M é um conjunto de arestas que não compartilha vértices, e nem todas as arestas de G necessariamente serão parte de M, particularmente se houver mais arestas do que vértices.

E) Nenhum vértice de V1 compartilha a mesma aresta em M com outro vértice de V1. - Correta, em um Matching perfeito, cada vértice é conectado a um único parceiro, o que significa que não há compartilhamento de arestas entre os vértices de V1 - e o mesmo vale para os vértices de V2.",,9939381
tópico 1,Banco de dados NoSQL,"Questão: Qual das seguintes opções melhor descreve o modelo de consistência de dados ""Eventual Consistency"" comumente encontrado em sistemas de banco de dados NoSQL?

A) Eventual Consistency é uma estratégia que garante que todas as transações são processadas apenas uma vez, de forma sequencial, o que pode resultar em desempenho diminuído em sistemas distribuídos.

B) No modelo Eventual Consistency, as atualizações são sincronizadas em todos os nós imediatamente, garantindo que leituras subsequentes reflitam as últimas alterações com consistência forte.

C) Eventual Consistency permite que cópias de dados possam divergir temporariamente, assegurando que, eventualmente, todas as réplicas dos dados se tornarão consistentes se nenhum novo update for feito.

D) Esse modelo assegura que os dados estarão consistentes apenas durante transações, mas, após sua conclusão, não há garantia de que réplicas de dados serão consistentes em diferentes nós.

E) Eventual Consistency refere-se a um modelo no qual a consistência dos dados é verificada em tempo real por um sistema centralizado de gerenciamento, que atualiza todas as réplicas de forma síncrona.

",C,"

Explicação dos itens:

A) Este item está incorreto porque Eventual Consistency não se preocupa com o processamento sequencial de transações, nem implica necessariamente um desempenho diminuído.

B) Está errado porque Eventual Consistency não implica em sincronização imediata das atualizações em todos os nós, nem consistência forte nas leituras subsequentes. Na verdade, é o oposto disso.

C) Esta é a resposta correta. Eventual Consistency é um modelo de consistência em bancos de dados distribuídos NoSQL onde se aceita que, por algum tempo, diferentes cópias de dados possam não estar iguais (ou seja, podem divergir temporariamente), mas eventualmente, após um período sem atualizações, elas se tornarão consistentes.

D) Este item é falso, pois Eventual Consistency não se aplica apenas durante as transações. É um modelo que lida com a consistência de réplicas de dados ao longo do tempo e não especificamente durante transações.

E) Esse conceito está errado porque Eventual Consistency não envolve um sistema centralizado de gerenciamento que atualiza todas as réplicas de forma síncrona; em vez disso, as atualizações podem ocorrer de maneira assíncrona, e a consistência é alcançada ao longo do tempo, não em tempo real.",B,4784808
tópico 0,Conceitos de processamento massivo e paralelo,"Questão: No contexto de sistemas de processamento de dados em larga escala, é essencial compreender os conceitos fundamentais que regem o processamento massivo e paralelo. Considerando um cenário de análise de grandes conjuntos de dados (Big Data), quais dos seguintes afirmativas estão corretas em relação ao MapReduce, um modelo de programação popular para processamento de dados em larga escala?

I. O MapReduce permite a distribuição automática do processamento de dados em um cluster de computadores, o que aumenta a redundância e a tolerância a falhas no processamento dos dados.
II. No modelo de programação MapReduce, a etapa ""Map"" é responsável por ordenar os pares chave-valor produzidos, garantindo que os dados estejam organizados para a etapa ""Reduce"".
III. A etapa ""Reduce"" do MapReduce consome os pares chave-valor fornecidos pela etapa ""Map"", realizando operações de agregação ou resumo, como soma, média ou contagem.
IV. MapReduce é tipicamente implementado sobre sistemas de arquivos distribuídos, como o HDFS (Hadoop Distributed File System), que suportam armazenamento e recuperação eficiente de grandes quantidades de dados.

A) Apenas I e IV são corretas.
B) Apenas II e III são corretas.
C) Apenas I, III e IV são corretas.
D) Apenas I e II são corretas.
E) Todas as afirmativas são corretas.

",C," 

Explicação dos itens:

I. Correto. O MapReduce foi projetado para processar grandes volumes de dados de forma distribuída, aumentando a redundância e tolerância a falhas através de replicação de tarefas e dados em um cluster.

II. Incorreto. A etapa ""Map"" é responsável por processar os dados de entrada e gerar pares chave-valor. A organização e ordenação destes pares ocorre entre as etapas ""Map"" e ""Reduce"" em uma fase intermediária denominada ""Shuffle"".

III. Correto. A função ""Reduce"" do MapReduce, de fato, processa cada grupo de valores associados a uma chave específica, permitindo a realização de operações de agregação e síntese dos dados.

IV. Correto. O MapReduce geralmente opera em conjunto com sistemas de arquivos distribuídos, como o HDFS, que são otimizados para armazenamento e acesso a grandes volumes de dados distribuídos por um cluster de máquinas.

Portanto, as afirmativas I, III e IV estão corretas, enquanto a afirmativa II está incorreta, por isso a alternativa C) é a correta.",C,4924220
tópico 1,"Banco de dados e formatos de arquivo orientado a colunas: Parquet, MonetDB, duckDB","Questão:
A análise e processamento de grandes volumes de dados exigem soluções eficientes de armazenamento e consulta. Os bancos de dados e formatos de arquivos orientados a colunas, como Parquet, MonetDB e DuckDB, apresentam características distintas que os tornam apropriados para diferentes contextos de uso em análises de dados. Com base no cenário descrito, analise as afirmações a seguir:

I. O Parquet é um formato de arquivo otimizado para operações de leitura e escrita em HDFS (Hadoop Distributed File System), oferecendo alta compressão e eficiência em consultas analíticas.

II. MonetDB é um banco de dados relacional pioneiro ao empregar uma arquitetura orientada a colunas, sendo desenhado para operações OLTP (Online Transaction Processing) com foco em rápido processamento de transações.

III. DuckDB é um banco de dados orientado a colunas, desenhado para ser leve e mais adequado para cenários OLAP (Online Analytical Processing), com ênfase em análises e consultas ad-hoc de dados.

IV. Parquet e DuckDB são específicos para ambientes distribuídos, enquanto MonetDB é mais utilizado em ambientes single-node devido à sua arquitetura interna.

Assinale a opção que indica as afirmações corretas.

A) I e III apenas
B) I, II e IV apenas
C) II e III apenas
D) I, II e III apenas
E) Todas as afirmações estão corretas

",A,"

Explicação dos itens:

I. Correta. O Parquet é um formato de arquivo columnar, amplamente utilizado no ecossistema Hadoop, concebido para oferecer uma compactação eficiente e otimizado para trabalho em sistemas de arquivos distribuídos como o HDFS, beneficiando cargas de trabalho de leitura intensiva como as típicas em consultas analíticas.

II. Incorreta. MonetDB é de fato um banco de dados orientado a colunas, mas ele é focado em análises OLAP (Online Analytical Processing) e não OLTP. Ele é projetado para otimizar consultas complexas de análises e não para processamento rápido de transações.

III. Correta. DuckDB é um banco de dados analítico orientado a colunas. É projetado para ser um sistema de gestão de base de dados embutido, leve e voltado para cenários OLAP, onde a análise de dados e as consultas ad-hoc são a prioridade.

IV. Incorreta. O Parquet é um formato de arquivo e não um sistema de banco de dados, e por si só, não é específico para ambientes distribuídos, embora seja comum seu uso em tais ambientes. DuckDB é projetado para ser um mecanismo de banco de dados embutido e pode funcionar tanto em ambientes simples como distribuídos. MonetDB pode ser utilizado tanto em ambientes single-node como em distribuídos, mas é conhecido por sua eficiência em ambientes single-node.",A,4172218
tópico 3,Linguagem de programação Scala,"Questão: 
Considere as seguintes declarações de código em Scala:

```scala
abstract class Instrumento {
  def tocar: String
}

class Piano extends Instrumento {
  def tocar = ""Piano está tocando""
}

class Violino extends Instrumento {
  def tocar = ""Violino está tocando""
}

object Orquestra {
  def iniciarConcerto(instrumentos: List[Instrumento]) = {
    instrumentos.map(_.tocar).mkString("", "")
  }
}

val instrumentos = List(new Piano, new Violino)
println(Orquestra.iniciarConcerto(instrumentos))
```

Quando o método `iniciarConcerto` da classe `Orquestra` é invocado, o que é impresso na saída padrão?

A) ""Piano está tocando, Violino está tocando""
B) ""Instrumento está tocando, Instrumento está tocando""
C) A compilação do código falha devido a um erro de tipo nas classes `Piano` e `Violino`.
D) NullPointerException é lançado durante a execução.
E) Nada é impresso porque o método `iniciarConcerto` não possui um comando de impressão.

",A," 
Explicação dos itens:
A) Este item é a resposta correta. O método `iniciarConcerto` invoca a função `tocar` em cada elemento da lista de `Instrumento`, e a sequência de representações de strings resultantes (""Piano está tocando"", ""Violino está tocando"") é unida em uma única string com vírgulas entre elas.

B) Este item está incorreto porque cada classe que estende `Instrumento` fornece sua própria implementação de `tocar`, resultando em strings específicas para cada tipo de instrumento.

C) Este item está incorreto. As classes `Piano` e `Violino` estendem a classe abstrata `Instrumento` corretamente ao fornecerem suas próprias implementações da função abstrata `tocar`, permitindo que a compilação seja bem-sucedida.

D) Este item está incorreto porque não há operações no código fornecido que resultariam em um `NullPointerException`.

E) Este item está incorreto pois, embora o método `iniciarConcerto` por si só não imprima nada, a última linha de código no exemplo invoca `println` e passa o resultado de `iniciarConcerto` como argumento, causando a impressão da string composta na saída padrão.",A,2777498
tópico 1,"Banco de dados relacional: SQL Server, PostgreSQL, MySQL","Questão:
Em um ambiente de desenvolvimento de banco de dados relacional, três sistemas gerenciadores de banco de dados (SGBDs) SQL Server, PostgreSQL e MySQL são frequentemente utilizados. Considerando aspectos de transações e isolamento em SGBDs, analise as afirmativas a seguir:

I) SQL Server oferece cinco níveis de isolamento de transação, que incluem Read Uncommitted, Read Committed, Repeatable Read, Snapshot e Serializable.

II) PostgreSQL utiliza por padrão o nível de isolamento Read Committed, mas permite a configuração do nível Serializable para garantir a consistência das transações por meio do controle de versão de linha.

III) MySQL, em sua engine InnoDB, suporta todos os níveis de isolamento de transação definidos pelo padrão SQL, mas o nível de isolamento por padrão é Repeatable Read, diferentemente do SQL Server e PostgreSQL.

Está(ão) correta(s) a(s) afirmativa(s):
a) Apenas I
b) Apenas II
c) I e II
d) II e III
e) I, II e III

",E,"

Explicação dos itens:

a) ""Apenas I"" é incorreto porque também as afirmativas II e III estão corretas.

b) ""Apenas II"" é incorreto porque as afirmativas I e III também estão corretas.

c) ""I e II"" é incorreto porque a afirmativa III também está correta, indicando que MySQL suporta todos os níveis de isolamento definidos pelo padrão SQL e seu nível default é Repeatable Read.

d) II e III"" é incorreto porque inclui a afirmativa correta sobre o PostgreSQL e MySQL, mas deixa de fora informações importantes sobre os níveis de isolamento oferecidos pelo SQL Server.

e) ""I, II e III"" é a alternativa correta porque todas as afirmativas estão corretas:
- I: SQL Server realmente oferece cinco níveis de isolamento, contribuindo para a flexibilidade no controle das transações.
- II: PostgreSQL por padrão usa o nível de isolamento Read Committed e pode ser configurado para usar Serializable, o que faz uso de controle de versão de linha para garantir a consistência.
- III: MySQL realmente suporta todos os níveis de isolamento do padrão SQL em sua engine InnoDB, tendo Repeatable Read como o nível de isolamento padrão.",A,4649436
tópico 1,"Banco de dados e formatos de arquivo orientado a colunas: Parquet, MonetDB, duckDB","Questão:
Considerando os sistemas de banco de dados e os formatos de arquivo orientados a coluna, é fundamental entender as vantagens de performance e as otimizações que esses formatos podem oferecer em análises de grandes volumes de dados. Com base em seu conhecimento sobre formatos de arquivo como Parquet e sistemas de gerenciamento de banco de dados (SGBD) como MonetDB e DuckDB, analise as seguintes afirmações.

I. O formato Parquet é otimizado para uso com processamento de dados em paralelo através de frameworks como o Apache Hadoop e o Apache Spark, permitindo que seja altamente eficiente para operações de leitura e escrita em grande escala.

II. MonetDB é um SGBD orientado a colunas pioneiro, projetado para alta performance em consultas analíticas, oferecendo compressão de dados e execução de consultas em memória, facilitando o processamento analítico online (OLAP).

III. DuckDB é um SGBD relacional que segue um modelo de armazenamento orientado a linhas, não sendo adequado para cenários onde a leitura seletiva de colunas é frequente e o desempenho em consultas analíticas é crítico.

Assinale a opção que contém todas as afirmações corretas:

a) Apenas I e II.
b) Apenas II e III.
c) Apenas I e III.
d) Todas estão corretas.
e) Nenhuma está correta.

",A,"

Explicação dos itens:

I. Correta. O formato Parquet é de fato um formato de arquivo colunar otimizado para operações com grandes conjuntos de dados e é amplamente utilizado com ferramentas de processamento de dados distribuídos como Hadoop e Spark devido à sua eficiência na leitura e escrita paralelas.

II. Correta. MonetDB é um SGBD otimizado para consultas analíticas e é orientado a colunas. Ele utiliza técnicas como compressão de dados e execução de consultas em memória para melhorar o desempenho de consultas OLAP, o que confirma a afirmação.

III. Incorreta. DuckDB é, de fato, um SGBD analítico orientado a colunas, e é desenhado para ser utilizado em workloads OLAP, oferecendo um bom desempenho para leitura seletiva de colunas. Portanto, não é correto afirmar que ele segue um modelo orientado a linhas e seria inadequado para consultas analíticas.

Portanto, as alternativas I e II estão corretas e a alternativa III está incorreta, fazendo da opção ""Apenas I e II"" a resposta certa.",A,7979921
tópico 3,Linguagem de programação Scala,"Questão:
A linguagem de programação Scala integra características de linguagens funcionais com orientação a objetos. Sua interoperabilidade com Java se dá por meio da JVM (Java Virtual Machine), o que fomenta seu uso para projetos que visam conciliar o paradigma funcional com ecossistemas estabelecidos em Java. Considerando os conceitos e funcionalidades da linguagem Scala, avalie as seguintes afirmativas:

I. A inferência de tipos em Scala permite que o programador omita o tipo de uma variável em sua declaração, deixando para o compilador a tarefa de identificar o tipo adequado, com base no contexto de uso da variável.

II. Em Scala, toda função é um objeto e pode ser atribuída a uma variável, passada como argumento ou retornada como valor de outra função, devido ao seu suporte a funções de primeira classe.

III. Scala permite a criação de classes case, que são especialmente úteis na construção de padrões de correspondência (pattern matching), além de fornecer automaticamente métodos de comparação e cópia.

IV. A imutabilidade em Scala é forçada em todos os níveis da linguagem, obrigando que todas as variáveis sejam declaradas com o modificador val, que é o equivalente ao final do Java.

É correto o que se afirma em:

A) I, II e III, apenas.
B) I e II, apenas.
C) I, II e IV, apenas.
D) II, III e IV, apenas.
E) I, II, III e IV.

",A,"

Explicação dos itens:

I. Correto. Scala proporciona um sofisticado recurso de inferência de tipos, permitindo ao programador muitas vezes omitir explicitamente o tipo da variável.
      
II. Correto. Funções em Scala são tratadas como objetos de primeira classe, o que significa que elas podem ser utilizadas como qualquer outro objeto no sistema de tipos, incluindo serem atribuídas a variáveis, ou usadas como parâmetros e retornos de outras funções.
      
III. Correto. Scala inclui o conceito de classes case que são projetadas para serem imutáveis e são comparadas por valor ao invés de por referência. Essas classes facilitam a utilização em padrões de correspondência, entre outras funcionalidades.

IV. Incorreto. Em Scala, o uso de val define uma variável como imutável, mas o uso de var permite definir variáveis mutáveis. Logo, não é verdadeiro que Scala force imutabilidade em todos os níveis, dado que é uma escolha do programador usar val ou var.",A,8941429
tópico 3,Linguagem de programação Scala,"Questão: Em Scala, os padrões de case class são altamente utilizados para realizar operações de correspondência de padrões (pattern matching), que simplificam a manipulação de dados estruturados. Considerando o seguinte código Scala:

```scala
abstract class Notification

case class Email(sender: String, title: String, body: String) extends Notification
case class SMS(caller: String, message: String) extends Notification
case class VoiceRecording(contactName: String, link: String) extends Notification

def showNotification(notification: Notification): String = {
  notification match {
    case Email(email, title, _) => s""You got an email from $email with title: $title""
    case SMS(number, message) => s""You got an SMS from $number! Message: $message""
    case VoiceRecording(name, link) => s""You received a Voice Recording from $name! Click here to hear it: $link""
  }
}

val someSms = SMS(""12345"", ""Are you there?"")
val someVoiceRecording = VoiceRecording(""Tom"", ""voicerecording.com/id/123"")

println(showNotification(someSms))
println(showNotification(someVoiceRecording))
```

Supondo que o código seja executado como mostrado, qual será a saída exibida no console?

A) 
```
You got an SMS from 12345! Message: Are you there?
You received a Voice Recording from Tom! Click here to hear it: voicerecording.com/id/123
```

B)
```
You got an email from 12345 with title: Are you there?
You received a Voice Recording from voicerecording.com/id/123! Click here to hear it: Tom
```

C)
```
Error: No match found for showNotification function.
```

D)
```
You got an SMS from Are you there? Message: 12345
You got an email from Tom with title: voicerecording.com/id/123
```

E)
```
You received a Voice Recording from 12345! Click here to hear it: Are you there?
You got an SMS from Tom! Message: voicerecording.com/id/123
```

",A," 
A explicação para cada item é a seguinte:

A) Este item é a resposta correta. O código realiza o pattern matching para cada tipo de notificação e extrai as informações corretas para imprimir a mensagem formada no console.

B) Este item é incorreto. Ele mistura os retornos das funções de pattern matching, atribuindo o corpo de uma mensagem SMS como título de um email e invertendo os parâmetros para o VoiceRecording.

C) Este item está incorreto. Haverá uma correspondência para cada tipo de notificação devido aos três cases definidos na função `showNotification`. Portanto, não ocorrerá erro de correspondência.

D) Este item está incorreto. Ele inverte as informações extraídas dos objetos SMS e VoiceRecording, colocando a mensagem do SMS como número de telefone e atribuindo o link do VoiceRecording como título do Email.

E) Este item também está incorreto. Ele inverte os tipos de notificação, atribuindo as propriedades de um objeto SMS a um VoiceRecording e vice-versa.",A,2929157
tópico 1,"Banco de dados e formatos de arquivo orientado a colunas: Parquet, MonetDB, duckDB","Questão: Em ambientes de Big Data, a eficiência na leitura e escrita de grandes conjuntos de dados é crucial para o desempenho de operações analíticas. Considerando o cenário de bancos de dados e formatos de arquivos orientados a colunas, avalie as seguintes afirmações sobre Parquet, MonetDB e duckDB:

I. Parquet é um formato de armazenamento orientado a colunas otimizado para o ecossistema Hadoop, oferecendo alta compressão e eficiência em operações de leitura, especialmente em sistemas de arquivos distribuídos.

II. MonetDB é um sistema de gerenciamento de banco de dados relacional que utiliza um modelo de armazenamento em linha, o que o torna inadequado para análises que requerem varreduras eficientes de grandes volumes de dados.

III. duckDB é um sistema de gerenciamento de banco de dados analítico orientado a colunas, projetado para processamento ad-hoc e análise de dados, podendo ser integrado com linguagens de programação como Python e R para manipulação de dados.

Assinale a alternativa que apresenta a(s) afirmação(ões) correta(s):

A) Apenas I e III.
B) Apenas I e II.
C) Apenas II e III.
D) Apenas I.
E) I, II e III.

",A,"

A explicação dos itens:

I. Correta: Parquet é de fato um formato de arquivo orientado a colunas, amplamente utilizado no ecossistema Hadoop, sendo muito eficiente para operações de leitura e oferecendo boas taxas de compressão. Suporta esquemas aninhados e é projetado para funcionar bem com armazenamento distribuído.

II. Incorreta: MonetDB é um banco de dados relacional, mas diferentemente do que afirma a questão, ele é baseado em um modelo de armazenamento orientado a colunas. Isso o torna apropriado para operações analíticas e capaz de realizar varreduras rápidas em grandes conjuntos de dados.

III. Correta: duckDB é um sistema de gerenciamento de banco de dados orientado a colunas, otimizado para análises de dados e consultas ad-hoc. Ele fornece integração com linguagens de programação como Python e R, facilitando a análise de dados dentro destes ambientes de programação.",A,1017879
tópico 1,Álgebra relacional e SQL (padrão ANSI),"Questão:
Analise as seguintes operações de álgebra relacional e consultas SQL correspondentes:

I. Seleção (σ)
Álgebra Relacional: σ_(condição)(R)
SQL: SELECT * FROM R WHERE condição;

II. Projeção (π)
Álgebra Relacional: π_(atributo1, atributo2,...,atributon)(R)
SQL: SELECT atributo1, atributo2,...,atributon FROM R;

III. Renomeação (ρ)
Álgebra Relacional: ρ_(S ← R)
SQL: Não há equivalente direto em SQL para a renomeação de toda a relação, mas pode-se usar alias para atributos ou subconsultas.

IV. União (∪)
Álgebra Relacional: R ∪ S
SQL: SELECT * FROM R UNION SELECT * FROM S;

V. Produto Cartesiano (×)
Álgebra Relacional: R × S
SQL: SELECT * FROM R, S;

Com base nas operações e consultas citadas acima, assinale a opção que contém uma equivalência INCORRETA entre a álgebra relacional e a consulta SQL.

A) I - Seleção
B) II - Projeção
C) III - Renomeação
D) IV - União
E) V - Produto Cartesiano

",C,"

Explicação dos Itens:

A) I - Seleção: A operação de seleção na álgebra relacional é equivalentemente representada pela cláusula WHERE no SQL. A alternativa está correta.

B) II - Projeção: A projeção na álgebra relacional, que seleciona determinados atributos das tuplas, é diretamente equivalente à especificação de colunas na cláusula SELECT no SQL. A alternativa está correta.

C) III - Renomeação: A alternativa C afirma uma equivalência incorreta. Enquanto a renomeação na álgebra relacional altera o nome da relação (ou tabela) ou de seus atributos, não existe um comando direto em SQL para renomear uma tabela inteira na consulta; apenas é possível dar apelidos (aliases) para atributos ou tabelas em determinadas consultas. Portanto, a alternativa C é a correta pois representa a equivalência incorreta.

D) IV - União: A união de duas relações na álgebra relacional é representada em SQL pela cláusula UNION aplicada a duas consultas SELECT. A alternativa está correta.

E) V - Produto Cartesiano: O produto cartesiano entre duas relações na álgebra relacional é realizado em SQL selecionando-se todas as colunas das duas tabelas sem especificar uma condição de junção, o que é representado pela listagem das tabelas separadas por vírgulas no FROM. A alternativa está correta.",C,7467467
tópico 1,"Banco de dados relacional: SQL Server, PostgreSQL, MySQL","Questão:

Em um ambiente de banco de dados relacional, profissionais de TI devem escolher o sistema de gestão de banco de dados (SGBD) mais adequado para as necessidades da organização. Considerando os SGBDs SQL Server, PostgreSQL e MySQL, analise as seguintes afirmativas:

I - O SQL Server, um SGBD proprietário da Microsoft, oferece uma integração avançada com outros produtos Microsoft, como o .NET Framework e o Microsoft Azure.

II - PostgreSQL é conhecido por sua conformidade com os padrões SQL e por oferecer suporte a um conjunto mais amplo de recursos do SQL padrão em comparação aos seus concorrentes.

III - MySQL é um SGBD de código aberto amplamente utilizado para aplicações web, mas possui algumas limitações no que diz respeito à implementação completa de triggers e stored procedures em comparação com o SQL Server e PostgreSQL.

É correto o que se afirma em:

A) I, II e III.  
B) I e II, apenas.  
C) II e III, apenas.  
D) I e III, apenas.  
E) II, apenas.

",B,"

Explicação dos itens:

A) Esta alternativa é incorreta porque afirma que todas as afirmativas estão corretas. Entretanto, a afirmativa III possui uma informação desatualizada, já que as versões recentes do MySQL têm melhorado consideravelmente no que diz respeito a triggers e stored procedures.

B) Esta é a alternativa correta porque as afirmativas I e II estão corretas. O SQL Server possui integração avançada com outros produtos Microsoft, e o PostgreSQL é reconhecido pela sua conformidade com os padrões SQL e extensão de funcionalidades que muitas vezes ultrapassam seus concorrentes.

C) Esta alternativa é incorreta porque inclui a afirmativa III, que contém informações desatualizadas sobre o MySQL.

D) Esta alternativa é incorreta porque, apesar da afirmativa I estar correta, a inclusão da afirmativa III a torna incorreta pelo mesmo motivo que a alternativa C.

E) Esta alternativa é incorreta porque exclui a afirmativa I, que está correta.",B,117920
tópico 0,"Arquitetura de cloud computing para ciência de dados (AWS, Azure, GCP)","Questão:
A Arquitetura de Cloud Computing oferece diversas soluções que atendem às necessidades específicas da Ciência de Dados, com serviços que permitem o armazenamento, processamento e análise de grandes volumes de dados. Supondo que uma equipe de cientistas de dados pretenda construir um pipeline de dados escalável, altamente disponível e com capacidade de realizar análises complexas em tempo real, qual combinação de serviços poderia ser recomendada se estiverem utilizando a AWS (Amazon Web Services)?

A) Amazon S3 para armazenamento de dados, Amazon EC2 para processamento de dados e Amazon QuickSight para análise de dados.

B) Amazon Redshift para armazenamento de dados, AWS Lambda para processamento de dados e Amazon QuickSight para análise de dados.

C) Amazon Kinesis para ingestão de dados em tempo real, AWS Glue para processamento de dados e Amazon Athena para análise de dados.

D) Amazon RDS para armazenamento de banco de dados relacional, Amazon EMR para processamento de dados e Amazon QuickSight para análise de dados.

E) AWS Data Pipeline para ingestão de dados, Amazon Redshift para processamento de dados e AWS Data Pipeline para análise de dados.

",C,"

Explicação dos itens:
A) Amazon S3 é um serviço de armazenamento de objetos, Amazon EC2 permite a execução de servidores virtuais e Amazon QuickSight é uma ferramenta de análise de negócios, mas essa combinação não atende especificamente à necessidade de análise em tempo real.

B) Amazon Redshift é um serviço de data warehousing, e AWS Lambda é um serviço de computação sem servidor, mas não são projetados especificamente para pipelines de análise em tempo real, embora possam ser usados em partes de um.

C) Amazon Kinesis é especializado na ingestão de dados em tempo real, AWS Glue é um serviço de ETL (extrair, transformar, carregar) administrado, e Amazon Athena é um serviço de consulta interativo que facilita a análise de dados no Amazon S3, usando SQL. Esta combinação de serviços encaixa-se perfeitamente para a análise em tempo real e pipelines escaláveis.

D) Amazon RDS é um serviço de banco de dados relacional, e Amazon EMR é um serviço de processamento de big data baseado em clusters, porém, a combinação não prioriza o cenário de análise em tempo real como o Kinesis ou Glue.

E) AWS Data Pipeline é um serviço de orquestração de fluxo de dados e não é uma ferramenta de análise. Além disso, AWS Data Pipeline está listado tanto para ingestão quanto análise, o que é um mal-entendido do papel do serviço na arquitetura de dados.",A,3331781
tópico 2,Contexto de IA: Tratamento de outliers e agregações,"Questão:
A Inteligência Artificial (IA) é um vasto campo de estudo e aplicação que demanda uma abordagem cuidadosa aos dados que alimentam os modelos algorítmicos. Durante a etapa de pré-processamento, o tratamento de outliers e a aplicação de técnicas de agregação são passos cruciais para assegurar a qualidade e a relevância dos dados. Considere um conjunto de dados com uma variável quantitativa X, onde se observam valores significativamente distantes da média. Qual das seguintes alternativas melhor representa uma abordagem apropriada para o tratamento dos outliers, combinada com uma técnica de agregação para análise subsequente?

A) Substituir todos os outliers por valores ausentes e aplicar a mediana para agregar os dados de X.

B) Excluir todos os registros contendo outliers sem análise adicional e utilizar a média aritmética para agregar os dados de X.

C) Aplicar a transformação logarítmica nos outliers para reduzir a variação, seguido pela aplicação da média geométrica para agregar os dados de X.

D) Manter os outliers no conjunto de dados e utilizar a soma como método de agregação para os dados de X.

E) Utilizar um método de imputação baseado no modelo, como k-NN ou Árvore de Decisão, para substituir os outliers, e empregar a moda como técnica de agregação.

",C),"  
A alternativa C é correta, pois a transformação logarítmica é uma técnica comumente utilizada para diminuir a influência de outliers em variáveis numéricas, restabelecendo uma distribuição mais próxima à normalidade. Se a variável X for positiva e os outliers forem números muito grandes, a transformação logarítmica ajuda a diminuir a discrepância entre esses e os valores mais frequentes. Além disso, a média geométrica é uma forma de agregação que é menos influenciada por valores extremos quando comparada à média aritmética, o que a torna mais apropriada em contextos onde a transformação logarítmica é aplicada.

Item A está incorreto, pois simplesmente substituir os outliers por valores ausentes pode distorcer a análise, especialmente se a quantidade de outliers for significativa. A mediana poderia ser uma agregação adequada para dados com outliers, mas descartar informações pode não ser o melhor caminho.

Item B está incorreto, pois excluir todos os registros com outliers é uma medida drástica e pode resultar em perda importante de informações. A média aritmética também é suscetível à influência de outliers, o que a torna uma má escolha para agregação em presença de valores extremos.

Item D está incorreto, porque, ao manter os outliers sem tratamento adequado, a análise pode ser fortemente enviesada por eles. A soma como método de agregação pouco diz sobre a distribuição dos dados e pode ser enganosa em presença de outliers.

Item E está incorreto, pois embora a imputação via k-NN ou Árvore de Decisão possa ser uma estratégia válida para tratar outliers, a utilização da moda como técnica de agregação não é apropriada para dados quantitativos contínuos, onde pode ser que não existam valores repetidos ou a modalidade não tenha sentido prático.",B,5761972
tópico 3,"R ou Python: Classes de objetos e suas propriedades (vetores, listas, data.frames)","Questão:

Considere os seguintes trechos de código em R, cada um criando uma estrutura de dado diferente.

I. `a <- c(1, 2, 3, 4, 5)`

II. `b <- list(1, ""a"", TRUE, 3.14)`

III. `df <- data.frame(Nome=c(""Ana"", ""Bruno"", ""Carlos""), Idade=c(23, 25, 28))`

Sobre as estruturas criadas acima, analise as afirmativas a seguir e assinale a opção correta.

a) Somente as estruturas I e III podem ser acessadas por índices numéricos.

b) Todas as estruturas permitem armazenar elementos de tipos diferentes.

c) A estrutura III não permite a manipulação de suas colunas individualmente.

d) A estrutura II permite armazenar mais de um tipo de dado, enquanto I e III são homogêneas em tipo.

e) A estrutura III, além de ser acessada por índices numéricos, permite a manipulação de suas colunas por meio de seus nomes.

",E,"

Explicação dos itens:

a) Incorreto. Todas as estruturas podem ser acessadas por índices numéricos.

b) Incorreto. A estrutura I, que é um vetor, não permite armazenar elementos de tipos diferentes. R irá realizar uma coerção de tipo e converterá todos os elementos para o tipo mais flexível dentre os presentes.

c) Incorreto. É possível manipular as colunas de um data.frame (estrutura III) individualmente, seja por meio de índices numéricos, seja por meio dos nomes das colunas.

d) Incorreto. A afirmativa está parcialmente correta ao dizer que a estrutura II permite armazenar diferentes tipos de dados. No entanto, também é incorreta ao sugerir que o data.frame (estrutura III) é homogêneo em tipo. Um data.frame pode conter colunas de diferentes tipos, porém cada coluna em si deve ser homogênea em tipo.

e) Correto. A estrutura III, que corresponde a um data.frame em R, pode ser acessada tanto por índices numéricos como pelos nomes das colunas, além de permitir a manipulação individual das suas colunas, seja adicionando, removendo ou modificando dados.",E,4022278
tópico 3,Programação funcional,"Questão: Considere o paradigma de programação funcional e seu impacto em sistemas computacionais. Uma característica central dessa abordagem é a imutabilidade dos dados, fundamental para evitar efeitos colaterais em funções. Além disso, conceitos como funções de ordem superior e expressões lambda são amplamente utilizados. Dado esse contexto e pensando na linguagem Haskell, a qual é fortemente baseada em programação funcional, qual das seguintes afirmações melhor exemplifica um princípio fundamental da programação funcional?

A) Em Haskell, todas as variáveis podem ser modificadas após a sua criação para acomodar mudanças de estado do programa.

B) Haskell suporta efeitos colaterais em todas as suas funções para promover maior flexibilidade no controle do fluxo da aplicação.

C) Funções de ordem superior em Haskell são utilizadas para modificar o estado global da aplicação, garantindo melhor performance na execução concorrente.

D) Haskell emprega expressões lambda para criar funções anônimas que não estão ligadas a um identificador.

E) Em Haskell, os loops de repetição são a principal estrutura de controle usada para implementar algoritmos recursivos e iterativos.

",D,"

A) Incorreto. Uma das diferenças fundamentais da programação funcional e imperativa é que na funcional, as variáveis são imutáveis após sua criação. Elas são mais propriamente chamadas de 'valores' devido a essa característica.

B) Incorreto. Haskell restringe efeitos colaterais utilizando o sistema de tipos com tipos especiais como 'IO'. Funções que realizam efeitos colaterais são claramente distinguidas das funções puras.

C) Incorreto. Funções de ordem superior em programação funcional são tipicamente usadas para abstrair padrões de computação, como mapeamento, dobramento, e filtragem, e não para modificar o estado global do programa, o qual é evitado em programação funcional.

D) Correto. As expressões lambda permitem criar funções anônimas, que são uma parte central da programação funcional. Elas são úteis para operações que requerem pequenas funções que não necessitam ser nomeadas, tornando o código mais conciso e promovendo uma forma declarativa de programação.

E) Incorreto. Embora os loops possam ser expressos em Haskell, a programação funcional favorece a utilização de recursão e abstração de controle com funções de ordem superior para esses propósitos. O uso direto de loops é mais típico da programação imperativa.",,9742679
tópico 4,Medidas de tendência central e dispersão e correlação,"Questão:
A análise de dados é uma parte fundamental do processo de tomada de decisão em diversos campos do conhecimento. Em um estudo estatístico, um pesquisador coletou uma amostra de dados relativos a duas variáveis X e Y. Com base nos cálculos preliminares, identificou-se que as medidas de tendência central e dispersão apresentaram os seguintes valores: a média de X é 48, a média de Y é 75, o desvio padrão de X é 12 e o desvio padrão de Y é 20. Além disso, a correlação entre X e Y foi computada e encontrou-se um coeficiente de correlação de Pearson (r) de 0,85.

Com base nessas informações, assinale a alternativa que apresenta uma interpretação correta dos dados:

A) As variáveis X e Y possuem uma correlação perfeita, visto que o coeficiente de correlação é maior que 0,8.
B) A variável X apresenta maior variabilidade que a variável Y, dada a relação entre seus desvios padrão.
C) A correlação positiva forte entre X e Y indica que, para aumentos nos valores de X, são esperados aumentos nos valores de Y.
D) A média de X ser menor que a de Y necessariamente indica que todos os valores de X são menores que os de Y.
E) O coeficiente de correlação de Pearson igual a 0,85 indica que 85% da variação em Y é explicada pela variação em X.

",C," 
A) Esta alternativa está incorreta porque, embora o coeficiente seja alto, uma correlação perfeita seria indicada por um coeficiente igual a 1 ou -1.

B) Esta alternativa está incorreta porque o desvio padrão é uma medida de dispersão que indica a variabilidade dos dados. O fato de X ter um desvio padrão de 12 e Y de 20 sugere que Y possui maior dispersão ou variabilidade comparada a X, não o contrário.

C) Esta alternativa está correta porque um coeficiente de correlação de Pearson de 0,85 indica uma correlação positiva forte. Isso significa que, de maneira geral, um aumento em X tende a estar associado a um aumento em Y.

D) Esta alternativa está incorreta porque a média é uma medida de tendência central e não determina a relação de cada ponto de dados individualmente. A média menor para X indica somente que o valor central da distribuição de X é menor do que o de Y, mas não fornece informações sobre a relação entre os valores individuais de X e Y.

E) Esta alternativa está incorreta porque o coeficiente de correlação de Pearson indica o grau e a direção da relação linear entre duas variáveis, mas não pode ser diretamente interpretado como a porcentagem de variância explicada de uma variável pela outra. Para isso, seria necessário calcular o coeficiente de determinação (r^2).",C,5774048
tópico 6,"Diagramas causais: gráficos acíclicos dirigidos; variáveis confundidoras, colisoras e de mediação","Questão:
Considere que você está realizando um estudo epidemiológico para investigar os fatores associados à incidência de uma doença cardiovascular específica. Você decide utilizar diagramas causais, mais especificamente gráficos acíclicos dirigidos (DAGs), para identificar as relações entre diversas variáveis. No contexto desse estudo, as variáveis são classificadas de acordo com o tipo de papel que desempenham na relação causal. Assinale a opção que CORRETAMENTE define as variáveis confundidoras, colisoras e de mediação.

A) Uma variável confundidora é uma variável que está causalmente entre uma exposição e um resultado final, representando um passo no mecanismo pelo qual a exposição influencia o resultado.
B) Uma variável colisora é uma variável que é influenciada por duas outras variáveis e, ao controlá-la, cria-se uma associação espúria entre essas variáveis que de outra forma não estariam associadas.
C) Uma variável de mediação é uma variável que está no caminho causal entre a exposição e o resultado, mas ao ajustar por ela, a relação verdadeira entre a exposição e o resultado é revelada.
D) Uma variável confundidora é uma variável que está causalmente independentemente associada tanto à exposição quanto ao resultado, potencialmente distorcendo a relação verdadeira entre ambos.
E) Uma variável colisora é aquela que está diretamente causando tanto a exposição quanto o resultado, sendo, portanto, uma forte candidata a ser controlada na análise.

",D," 
Uma explicação dos itens:
A) Item A está incorreto porque descreve uma variável de mediação, não uma variável confundidora.
B) Item B está incorreto porque define corretamente o que é uma variável colisora, mas a afirmação é que, ao controlar uma colisora, cria-se uma associação espúria, o que é verdade, mas a pergunta solicita definições para confundidora, colisora e de mediação, e este item apenas aborda a colisora.
C) Item C está incorreto porque inverte a definição de uma variável de mediação com a de uma variável confundidora. Ao ajustar por uma variável de mediação, estaríamos de fato removendo o efeito que ela medeia, e não revelando a relação verdadeira como sugerido.
D) Item D está correto. Ele define uma variável confundidora de forma adequada, como uma variável que está associada tanto à exposição quanto ao resultado, deixando implícito que essa associação é confundidora ao estar no caminho causal e não ser consequência de uma cadeia causal que comece na exposição.
E) Item E está incorreto porque confunde a definição de variável colisora com uma variável comum causadora tanto da exposição quanto do resultado, o que na verdade seria um confundidor comum, não uma colisora.",C,7084663
tópico 2,Contexto de IA: Tratamento de dados ausentes,"Questão:
Em um projeto de análise de dados com aplicação de Inteligência Artificial, uma equipe de cientistas de dados se depara com o problema de dados ausentes em um conjunto significativo de observações de sua base de dados. Considerando as abordagens para o tratamento desses dados faltantes, qual é a estratégia que NÃO é recomendada para lidar com essa situação?

A) Imputação de média/média condicional, onde os dados ausentes são substituídos com a média da variável observada ou de um subconjunto condicional da variável.

B) Eliminação de casos, removendo todas as observações onde há pelo menos um valor ausente, o que pode ser útil quando os dados ausentes são aleatoriamente distribuídos.

C) Imputação de dados por meio de algoritmos preditivos, como k-vizinhos mais próximos (k-NN), que utilizam padrões dos dados para estimar valores ausentes.

D) Uso de técnicas com base em modelos de regressão múltipla ou modelos baseados em árvore para a imputação de dados ausentes, tirando vantagem da correlação entre variáveis.

E) Substituição unívoca, na qual todos os dados ausentes em uma variável são substituídos por um único valor fixo, ignorando a estrutura subjacente e a distribuição dos dados.

",E,"

Explicação dos itens:

A) A imputação de média é uma técnica padrão para tratar dados ausentes quando são numericamente razoáveis e não distorce significativamente a distribuição da variável.

B) A eliminação de casos (listwise deletion) é uma opção recomendada se os dados ausentes são poucos e acredita-se que não introduzam viés na análise, embora possa reduzir o tamanho da amostra.

C) A imputação por meio de algoritmos preditivos é uma abordagem sofisticada e comum utilizada para preencher os dados ausentes, aproveitando as relações entre os dados.

D) A imputação baseada em modelos de regressão ou árvores é uma estratégia muito utilizada e pode fornecer estimativas plausíveis para os valores ausentes, sobretudo quando há uma clara relação teórica ou empírica entre as variáveis.

E) A substituição unívoca consiste em preencher todos os dados ausentes com um único valor, como zero ou a média, sem considerar a variabilidade dos dados. Isso pode levar a conclusões errôneas e distorcer análises estatísticas, sendo por isso a opção não recomendada.",B,4950797
tópico 1,Banco de dados NoSQL,"Questão:
Considere uma aplicação de redes sociais de grande escala que implementa um serviço de mensagens instantâneas entre seus usuários. Com o crescimento exponencial do número de usuários e mensagens trocadas diariamente, a equipe de tecnologia da informação está considerando migrar o sistema de gerenciamento de banco de dados para uma solução NoSQL mais adequada à natureza distribuída e à demanda de escalabilidade em tempo real do serviço. Qual das seguintes opções representa o tipo de banco de dados NoSQL mais apropriado para acomodar essa necessidade?

A) Bancos de Dados Relacionais
B) Bancos de Dados de Grafos
C) Bancos de Dados de Documentos
D) Bancos de Dados Colunares
E) Key-Value Stores

",ALTERNATIVACORRETA:C,"

Explicação dos itens:

A) Bancos de Dados Relacionais - São inadequados para cenários descritos, pois podem apresentar limitações de escalabilidade horizontal e flexibilidade em termos de esquema, que são fundamentais em uma aplicação de redes sociais com crescimento exponencial.

B) Bancos de Dados de Grafos - Embora possam ser úteis para modelar relações complexas entre usuários, como redes de amizade, não são a melhor escolha para armazenar mensagens instantâneas, que não se beneficiam tanto da modelagem gráfica.

C) Bancos de Dados de Documentos - São ideais para armazenamento de mensagens instantâneas por oferecerem flexibilidade de esquemas, acomodação de grandes volumes de dados semi-estruturados e facilidade para escalabilidade horizontal. Exemplos populares incluem MongoDB e Couchbase, que são amplamente utilizados em aplicações de redes sociais.

D) Bancos de Dados Colunares - São mais adequados para análises de grandes volumes de dados, onde operações de leitura e agregação são frequentes, como em sistemas de Business Intelligence. Eles não são a melhor escolha para o cenário de mensagens instantâneas, cujo foco está no armazenamento e recuperação rápida de dados.

E) Key-Value Stores - Proporcionam rápida leitura e escrita de pares de chave-valor, mas são muito simples e não suportam a complexidade estrutural que um sistema de mensagens instantâneas geralmente precisa, como o armazenamento de metadados associados a cada mensagem.",B,5859717
tópico 1,Banco de dados NoSQL,"Questão:
Considere o contexto dos bancos de dados NoSQL, que são projetados para superar as limitações dos bancos de dados relacionais em determinados tipos de aplicações modernas, possibilitando a manipulação de grandes volumes de dados, realizações de consultas complexas em tempo hábil e garantindo alta disponibilidade. Dentre as categorias de bancos NoSQL existentes, qual das opções a seguir descreve corretamente um banco de dados orientado a documentos?

A) Utiliza uma estrutura tabular de linhas e colunas, suportando relacionamentos entre as tabelas por meio de chaves estrangeiras.

B) Armazena os dados em pares chave-valor, sendo uma opção eficiente para aplicações que necessitam de armazenamento e recuperação rápida de dados.

C) Organiza os dados como documentos, geralmente no formato JSON ou BSON, permitindo estruturas aninhadas e a indexação eficiente de campos dentro desses documentos.

D) Baseia-se em grafos para representar e armazenar dados, com nós, arestas e propriedades para representar e armazenar informações sobre as relações entre os elementos.

E) Divide os dados em uma distribuição horizontal, conhecida como sharding, para distribuir a carga por múltiplos hosts, melhorando a performance e escalabilidade.

",C,"

Alternativa A descreve um banco de dados relacional, que utiliza tabelas para armazenar os dados. Alternativa B refere-se aos bancos de dados chave-valor, que são otimizados para cenários onde a rapidez na recuperação de dados é crucial. Alternativa C é a correta, pois descreve um banco de dados orientado a documentos, que armazena os dados em estruturas de documentos e é comum o uso de JSON ou BSON. Alternativa D está descrevendo bancos de dados orientados a grafos, que são usados para armazenar dados interconectados. Por fim, a alternativa E menciona o conceito de sharding, uma técnica comum em diversos tipos de bancos de dados NoSQL para distribuir os dados entre diferentes servidores, mas não é uma descrição específica de bancos de dados orientados a documentos.",C,1380466
